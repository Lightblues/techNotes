
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="O'ver curious">
      
      
        <meta name="author" content="Easonshi">
      
      
        <link rel="canonical" href="https://lightblues.github.io/techNotes/stat/ASL-note2/">
      
      
        <link rel="prev" href="../ASL-note1/">
      
      
        <link rel="next" href="../ASL-note3/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.30">
    
    
      
        <title>ASL2 - techNotes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#basis-expansions-and-regularization" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="techNotes" class="md-header__button md-logo" aria-label="techNotes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            techNotes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ASL2
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/lightblues/techNotes" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    lightblues/techNotes
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../code/CS/git-note/" class="md-tabs__link">
          
  
  Code

        </a>
      </li>
    
  

    
  

      
        
  
  
    
  
  
    
    
      
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../" class="md-tabs__link">
          
  
  Stat

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../Linux/Linux/" class="md-tabs__link">
          
  
  Linux

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../os/EFI/" class="md-tabs__link">
          
  
  OS

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../NLP/Lexicon-%E8%AF%8D%E6%B1%87%E5%A2%9E%E5%BC%BA/" class="md-tabs__link">
          
  
  NLP

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="https://blog.easonsi.site" class="md-tabs__link">
        
  
    
  
  üîóBlog

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="techNotes" class="md-nav__button md-logo" aria-label="techNotes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    techNotes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/lightblues/techNotes" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    lightblues/techNotes
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Code
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Code
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    CS
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            CS
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/CS/git-note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Git
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Python
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Python
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/Python/Python-note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ËØæÁ®ãÁ¨îËÆ∞
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    JavaScripe
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            JavaScripe
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/js-mindmap/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mindmap
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/js-note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    js note
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/jQuery-note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jQuery
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/node-js-note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Node.js
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Stat
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Stat
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" checked>
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="">
            
  
  <span class="md-ellipsis">
    È´òÁ≠âÁªüËÆ°Â≠¶‰π†
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            È´òÁ≠âÁªüËÆ°Â≠¶‰π†
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    index
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-mindmap/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ASL mindmap
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-note1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ASL1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    ASL2
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    ASL2
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basis-expansions-and-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Basis Expansions and Regularization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Basis Expansions and Regularization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intro" class="md-nav__link">
    <span class="md-ellipsis">
      Intro
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#piecewise-polynomials-and-splines" class="md-nav__link">
    <span class="md-ellipsis">
      Piecewise Polynomials and Splines
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smoothing-splines" class="md-nav__link">
    <span class="md-ellipsis">
      Smoothing Splines
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nonparametric-logistic-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Nonparametric Logistic Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multidimensional-splines" class="md-nav__link">
    <span class="md-ellipsis">
      Multidimensional Splines
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kernel-smoothing-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Kernel Smoothing Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Kernel Smoothing Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-dim-kernel-smoother" class="md-nav__link">
    <span class="md-ellipsis">
      1-Dim Kernel Smoother
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-regression-in-rprp" class="md-nav__link">
    <span class="md-ellipsis">
      Local Regression in R^pR^p
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#structured-local-regression-models-in-rprp" class="md-nav__link">
    <span class="md-ellipsis">
      Structured Local Regression Models in R^pR^p
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-likelihood-other-models" class="md-nav__link">
    <span class="md-ellipsis">
      Local likelihood &amp; Other Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kernel-density-estimation-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Kernel Density Estimation &amp; ClassiÔ¨Åcation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#radial-basis-function-kernels" class="md-nav__link">
    <span class="md-ellipsis">
      Radial Basis Function &amp; Kernels ÂæÑÂêëÂü∫ÂáΩÊï∞
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixture-models-for-density-estimation-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Mixture Models for Density Estimation &amp; ClassiÔ¨Åcation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-assessment-selection" class="md-nav__link">
    <span class="md-ellipsis">
      Model Assessment &amp; Selection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model Assessment & Selection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intro-of-model-assessment-selection" class="md-nav__link">
    <span class="md-ellipsis">
      Intro of Model Assessment &amp; Selection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-bias-variance-decomposition" class="md-nav__link">
    <span class="md-ellipsis">
      The Bias-Variance Decomposition
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimism-of-training-error-rate" class="md-nav__link">
    <span class="md-ellipsis">
      Optimism of Training Error Rate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#estimates-of-err_inerr_in" class="md-nav__link">
    <span class="md-ellipsis">
      Estimates of Err_{in}Err_{in}
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-validation" class="md-nav__link">
    <span class="md-ellipsis">
      Cross Validation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bootstrap-method" class="md-nav__link">
    <span class="md-ellipsis">
      Bootstrap Method
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-note3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ASL3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-note4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ASL4
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    ÁÆóÊ≥ïÂØºËÆ∫
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            ÁÆóÊ≥ïÂØºËÆ∫
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-MinCut-2SAT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    basic
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-prob-Markov-Chebyshev-Hoeffding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ê¶ÇÁéáÂü∫Á°Ä
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-computation-%E8%AE%A1%E7%AE%97%E7%90%86%E8%AE%BA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ËÆ°ÁÆóÁêÜËÆ∫
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-graph-Adjacency-Laplacian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ÂõæÁõ∏ÂÖ≥, Laplacian
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-differential-privacy-%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Â∑ÆÂàÜÈöêÁßÅ
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../optimization-%E5%87%B8%E4%BC%98%E5%8C%96/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Âá∏‰ºòÂåñ
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Linux
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Linux
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/Linux/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    General
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/tutor-%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83%E7%9B%B8%E5%85%B3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ÂÆûÈ™åÁéØÂ¢ÉÁõ∏ÂÖ≥
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/Linux-maintain/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Á≥ªÁªüÁª¥Êä§
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/Linux-configure-make-install-%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ÁºñËØëÂÆâË£ÖËΩØ‰ª∂
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    OS
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            OS
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/EFI/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    EFI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
        
          
          <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    OSs
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_2">
            <span class="md-nav__icon md-icon"></span>
            OSs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/macOS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    macOS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/macOS-intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    macOS-intro
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/macOS-softwares/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    macOS ËΩØ‰ª∂ÂàóË°®
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Hackintosh/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hackintosh
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Arch/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Arch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/CentOS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CentOS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Ubuntu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ubuntu
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Windows/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Windows
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    softwares
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            softwares
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/vscode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VSCode
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/vscode-markdown/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VSCode Markdown
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    NLP
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            NLP
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/Lexicon-%E8%AF%8D%E6%B1%87%E5%A2%9E%E5%BC%BA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lexicon ËØçÊ±áÂ¢ûÂº∫
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/Prompt-%E6%8F%90%E7%A4%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt ÊèêÁ§∫ËØ≠Ë®ÄÊ®°Âûã
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/ABSA-Sentiment-Analysis-%E5%9F%BA%E4%BA%8E%E6%96%B9%E9%9D%A2%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ABSA
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://blog.easonsi.site" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üîóBlog
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basis-expansions-and-regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Basis Expansions and Regularization
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Basis Expansions and Regularization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intro" class="md-nav__link">
    <span class="md-ellipsis">
      Intro
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#piecewise-polynomials-and-splines" class="md-nav__link">
    <span class="md-ellipsis">
      Piecewise Polynomials and Splines
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smoothing-splines" class="md-nav__link">
    <span class="md-ellipsis">
      Smoothing Splines
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nonparametric-logistic-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Nonparametric Logistic Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multidimensional-splines" class="md-nav__link">
    <span class="md-ellipsis">
      Multidimensional Splines
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kernel-smoothing-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Kernel Smoothing Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Kernel Smoothing Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-dim-kernel-smoother" class="md-nav__link">
    <span class="md-ellipsis">
      1-Dim Kernel Smoother
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-regression-in-rprp" class="md-nav__link">
    <span class="md-ellipsis">
      Local Regression in R^pR^p
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#structured-local-regression-models-in-rprp" class="md-nav__link">
    <span class="md-ellipsis">
      Structured Local Regression Models in R^pR^p
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-likelihood-other-models" class="md-nav__link">
    <span class="md-ellipsis">
      Local likelihood &amp; Other Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kernel-density-estimation-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Kernel Density Estimation &amp; ClassiÔ¨Åcation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#radial-basis-function-kernels" class="md-nav__link">
    <span class="md-ellipsis">
      Radial Basis Function &amp; Kernels ÂæÑÂêëÂü∫ÂáΩÊï∞
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixture-models-for-density-estimation-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Mixture Models for Density Estimation &amp; ClassiÔ¨Åcation
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-assessment-selection" class="md-nav__link">
    <span class="md-ellipsis">
      Model Assessment &amp; Selection
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model Assessment & Selection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intro-of-model-assessment-selection" class="md-nav__link">
    <span class="md-ellipsis">
      Intro of Model Assessment &amp; Selection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-bias-variance-decomposition" class="md-nav__link">
    <span class="md-ellipsis">
      The Bias-Variance Decomposition
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimism-of-training-error-rate" class="md-nav__link">
    <span class="md-ellipsis">
      Optimism of Training Error Rate
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#estimates-of-err_inerr_in" class="md-nav__link">
    <span class="md-ellipsis">
      Estimates of Err_{in}Err_{in}
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-validation" class="md-nav__link">
    <span class="md-ellipsis">
      Cross Validation
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bootstrap-method" class="md-nav__link">
    <span class="md-ellipsis">
      Bootstrap Method
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>ASL2</h1>

<h2 id="basis-expansions-and-regularization">Basis Expansions and Regularization<a class="headerlink" href="#basis-expansions-and-regularization" title="Permanent link">&para;</a></h2>
<p>‰πãÂâçËÆ®ËÆ∫ÁöÑÈÉΩÊòØ(Âπø‰πâ‰∏äÁöÑ)Á∫øÊÄßÊ®°Âûã</p>
<p>(Generalized) Linear Models</p>
<ul>
<li>Linear regression</li>
<li>Linear discriminant analysis</li>
<li>Logistic regression</li>
<li>Separating hyperplanes</li>
</ul>
<p>Extremely unlikely that <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> is linear</p>
<ul>
<li>Linear model is usually a convenient and sometimes necessary approximation</li>
<li>Easy to interpret, first order Taylor expansion, avoid overfitting</li>
</ul>
<p>A Taylor series expansion of a function <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> about a point a
<span class="arithmatex"><span class="MathJax_Preview">f(x)=f(a)+f^{\prime}(a)(x-a)+\frac{f^{\prime \prime}(a)}{2 !}(x-a)^{2}+\frac{f^{(3)}(a)}{3 !}(x-a)^{3}+\ldots+\frac{f^{(n)}(a)}{n !}(x-a)^{n}+\ldots</span><script type="math/tex">f(x)=f(a)+f^{\prime}(a)(x-a)+\frac{f^{\prime \prime}(a)}{2 !}(x-a)^{2}+\frac{f^{(3)}(a)}{3 !}(x-a)^{3}+\ldots+\frac{f^{(n)}(a)}{n !}(x-a)^{n}+\ldots</script></span></p>
<h3 id="intro">Intro<a class="headerlink" href="#intro" title="Permanent link">&para;</a></h3>
<h4 id="moving-beyond-linearity">Moving Beyond Linearity<a class="headerlink" href="#moving-beyond-linearity" title="Permanent link">&para;</a></h4>
<p>Augment inputs <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> with transformations, then use linear models in the new space</p>
<ul>
<li>The <span class="arithmatex"><span class="MathJax_Preview">m_{t h}</span><script type="math/tex">m_{t h}</script></span> transformation, <span class="arithmatex"><span class="MathJax_Preview">h_{m}(X): \mathbb{R}^{p} \rightarrow \mathbb{R}</span><script type="math/tex">h_{m}(X): \mathbb{R}^{p} \rightarrow \mathbb{R}</script></span></li>
</ul>
<p>The new approximated function: <span class="arithmatex"><span class="MathJax_Preview">f(X)=\sum_{m=1}^{M} \beta_{m} h_{m}(X), m=1, \ldots, M</span><script type="math/tex">f(X)=\sum_{m=1}^{M} \beta_{m} h_{m}(X), m=1, \ldots, M</script></span></p>
<p>Once basis functions <span class="arithmatex"><span class="MathJax_Preview">h_{m}(X)</span><script type="math/tex">h_{m}(X)</script></span> have been determined, model estimation proceeds as in ordinary linear regression.</p>
<p>Some simple and widely used <span class="arithmatex"><span class="MathJax_Preview">h_{m}(X)</span><script type="math/tex">h_{m}(X)</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{m}(X)=X_{m}</span><script type="math/tex">h_{m}(X)=X_{m}</script></span>, the original input variables</li>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{m}(X)=X_{j}^{2}</span><script type="math/tex">h_{m}(X)=X_{j}^{2}</script></span> or <span class="arithmatex"><span class="MathJax_Preview">X_{j} X_{k}, O\left(p^{d}\right)</span><script type="math/tex">X_{j} X_{k}, O\left(p^{d}\right)</script></span> terms for degree-d polynomial)</li>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{m}(X)=\log \left(X_{j}\right), \sqrt{X_{j}},\|X\|, \ldots</span><script type="math/tex">h_{m}(X)=\log \left(X_{j}\right), \sqrt{X_{j}},\|X\|, \ldots</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{m}(X)=I\left(L_{m} \leqslant X_{k} \leqslant U_{m}\right)</span><script type="math/tex">h_{m}(X)=I\left(L_{m} \leqslant X_{k} \leqslant U_{m}\right)</script></span>, indicator for region of <span class="arithmatex"><span class="MathJax_Preview">X_{k}</span><script type="math/tex">X_{k}</script></span> ÂàÜÊÆµ</li>
</ul>
<h4 id="controlling-for-model-complexity">Controlling for Model Complexity<a class="headerlink" href="#controlling-for-model-complexity" title="Permanent link">&para;</a></h4>
<p>Êàë‰ª¨ÈúÄË¶Å‰∏ÄÁßçÊñπÂºèÊù•ÊéßÂà∂Êàë‰ª¨Ê®°ÂûãÔºà‰ªéÂ≠óÂÖ∏‰∏≠ÈÄâÊã©Âü∫ÂáΩÊï∞ÔºâÁöÑÂ§çÊùÇÂ∫¶, ‰∏âÁßçÊñπÂºè</p>
<p>Restriction methods: limit the class of functions before-hand</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">f(x)=\sum_{j=1}^{p} f_{j}\left(X_{j}\right)=\sum_{j=1}^{p} \sum_{m=1}^{M_{j}} \beta_{j m} h_{j m}\left(X_{j}\right)</span><script type="math/tex">f(x)=\sum_{j=1}^{p} f_{j}\left(X_{j}\right)=\sum_{j=1}^{p} \sum_{m=1}^{M_{j}} \beta_{j m} h_{j m}\left(X_{j}\right)</script></span></li>
<li>limit the number of basis functions <span class="arithmatex"><span class="MathJax_Preview">M_{j}</span><script type="math/tex">M_{j}</script></span> used for each component function <span class="arithmatex"><span class="MathJax_Preview">f_{j}</span><script type="math/tex">f_{j}</script></span>.</li>
</ul>
<p>Selection methods: include basis that improve model fit significantly</p>
<ul>
<li>Variable selection</li>
<li>Stagewise greedy approaches, CART, MARS and boosting</li>
</ul>
<p>Regularization methods: use the entire dictionary but restrict the coefficients</p>
<ul>
<li>Ridge</li>
<li>LASSO</li>
</ul>
<h3 id="piecewise-polynomials-and-splines">Piecewise Polynomials and Splines<a class="headerlink" href="#piecewise-polynomials-and-splines" title="Permanent link">&para;</a></h3>
<h4 id="piecewise-polynomials-assume-1-dim">Piecewise Polynomials - Assume 1-dim<a class="headerlink" href="#piecewise-polynomials-assume-1-dim" title="Permanent link">&para;</a></h4>
<p>A <strong>piecewise polynomial</strong> function ÂàÜÊÆµÂ§öÈ°πÂºè <span class="arithmatex"><span class="MathJax_Preview">f(X)</span><script type="math/tex">f(X)</script></span> is obtained by dividing the domain of <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> into contiguous intervals, and representing <span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> by a separate polynomial in each interval.</p>
<p>Piecewise constant
<span class="arithmatex"><span class="MathJax_Preview">h_{1}(X)=I\left(X&lt;\xi_{1}\right), h_{2}(X)=I\left(\xi_{1} \leqslant X&lt;\xi_{2}\right), h_{3}(X)=I\left(\xi_{2} \leqslant X\right)</span><script type="math/tex">h_{1}(X)=I\left(X<\xi_{1}\right), h_{2}(X)=I\left(\xi_{1} \leqslant X<\xi_{2}\right), h_{3}(X)=I\left(\xi_{2} \leqslant X\right)</script></span></p>
<p>Piecewise linear
<span class="arithmatex"><span class="MathJax_Preview">h_{4}(X)=I\left(X&lt;\xi_{1}\right) X, h_{5}(X)=I\left(\xi_{1} \leqslant X&lt;\xi_{2}\right) X, h_{6}(X)=I\left(\xi_{2} \leqslant X\right) X</span><script type="math/tex">h_{4}(X)=I\left(X<\xi_{1}\right) X, h_{5}(X)=I\left(\xi_{1} \leqslant X<\xi_{2}\right) X, h_{6}(X)=I\left(\xi_{2} \leqslant X\right) X</script></span></p>
<p>Continuous piecewise linear
<span class="arithmatex"><span class="MathJax_Preview">h_{1}(X)=1, h_{2}(X)=X, h_{3}(X)=\left(X-\xi_{1}\right)_{+} h_{4}(X)=\left(X-\xi_{2}\right)_{+}</span><script type="math/tex">h_{1}(X)=1, h_{2}(X)=X, h_{3}(X)=\left(X-\xi_{1}\right)_{+} h_{4}(X)=\left(X-\xi_{2}\right)_{+}</script></span>,</p>
<p><img alt="" src="../media/ASL-note2/2021-12-09-17-08-06.png" /></p>
<p>ÈááÁî®‰∏çÂêåÂü∫ÂáΩÊï∞ÂíåÁ∫¶ÊùüÊù°‰ª∂ÁöÑÂõûÂΩíÁªìÊûú.</p>
<p><img alt="" src="../media/ASL-note2/2021-12-09-17-08-54.png" /></p>
<h4 id="piecewise-cubic-polynomial">Piecewise Cubic Polynomial<a class="headerlink" href="#piecewise-cubic-polynomial" title="Permanent link">&para;</a></h4>
<p><strong>Cubic spline</strong> ‰∏âÊ¨°Ê†∑Êù°: piecewise cubic polynomial with continuous first and second derivatives at the <strong>knots</strong>.ËøôÈáåÊòØ‰∏âÊ¨°Ê†∑Êù°ÁöÑÂÆö‰πâ, Ê≥®ÊÑèÂà∞ËøôÈáåÊòØË¶ÅÊ±Ç‰∫ÜÂà∞‰∫åÈò∂ÂæÆÂàÜÁöÑËøûÁª≠ÊÄß; ËÄå‰∏ãÈù¢ÁöÑÁªôÂá∫‰∫Ü‰∏ÄÁªÑÂü∫ÂáΩÊï∞, ËøôÈáåÊòØÈúÄË¶ÅËØÅÊòéÁöÑ. ÂèÇËßÅ <a href="https://github.com/szcf-weiya/ESL-CN/issues/29">here</a>. Ê≥®ÊÑè: 1. ËøôÈáåÁöÑÂÖ≠‰∏™ÂáΩÊï∞ÊòØÁ∫øÊÄßÊó†ÂÖ≥ÁöÑ; 2. Êª°Ë∂≥‰∏âÊ¨°Ê†∑Êù°ÁöÑÁ∫¶Êùü, Âç≥knotÂ§Ñ0-2Èò∂ÂæÆÂàÜËøûÁª≠.</p>
<p>Basis for cubic splines</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;h_{1}(X)=1, h_{2}(X)=X, h_{3}(X)=X^{2}, h_{4}(X)=X^{3} \\
&amp;h_{5}(X)=\left(X-\xi_{1}\right)_{+}^{3}, h_{6}(X)=\left(X-\xi_{2}\right)_{+}^{3}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&h_{1}(X)=1, h_{2}(X)=X, h_{3}(X)=X^{2}, h_{4}(X)=X^{3} \\
&h_{5}(X)=\left(X-\xi_{1}\right)_{+}^{3}, h_{6}(X)=\left(X-\xi_{2}\right)_{+}^{3}
\end{aligned}
</script>
</div>
<p>Total number of parameters: <span class="arithmatex"><span class="MathJax_Preview">3 \times 4-2 \times 3=6</span><script type="math/tex">3 \times 4-2 \times 3=6</script></span> ËÆ°ÁÆóÂèÇÊï∞ÁöÑ‰∏™Êï∞ÔºöËøôÈáåÊúâ‰∏â‰∏™Âå∫ÂüüÔºåÊØè‰∏™Âå∫ÂüüÊãüÂêà‰∏Ä‰∏™‰∏âÊ¨°ÂáΩÊï∞ÔºåÂõ†Ê≠§ÊòØ <span class="arithmatex"><span class="MathJax_Preview">3*4=12</span><script type="math/tex">3*4=12</script></span> ‰∏™ÂèÇÊï∞ÔºõÁ∫¶ÊùüÊù°‰ª∂Êúâ‰∏§‰∏™ knotsÔºåÊØè‰∏™ËäÇÁÇπ‰∏äÊúâ‰∏â‰∏™Á∫¶ÊùüÔºà0-2Èò∂ËøûÁª≠Ôºâ„ÄÇ[Âè¶Â§ñÂèØÁü•, Â¶ÇÊûúÂÜçÂä†‰∏âÈò∂ÂæÆÂàÜËøûÁª≠, Ê≠§Êó∂ÁöÑËá™Áî±Â∫¶Âèò‰∏∫ 4, ÂíåÂÖ®Â±ÄÁöÑ‰∏âÊ¨°ÂáΩÊï∞‰∏ÄÊ†∑, ‰πüÂç≥„ÄåEnforcing one more order of continuity would lead to a global cubic polynomial.„Äç]</p>
<ul>
<li>Cubic splines are the lowest-order spline for which the knot-discontinuity is not visible to the human eye. ÊçÆËØ¥‰∏âÊ¨°Ê†∑Êù°ÊòØ‰∫∫ÁúºÁúã‰∏çÂá∫ÁªìÁÇπ‰∏çËøûÁª≠ÁöÑÊúÄ‰ΩéÈò∂Ê†∑Êù°Ôºé</li>
</ul>
<h4 id="order-m-spline">Order-M Spline<a class="headerlink" href="#order-m-spline" title="Permanent link">&para;</a></h4>
<p>ÂÆö‰πâÊõ¥‰∏ÄËà¨ÁöÑÊ†∑Êù°orderÔºöorder ‰∏∫ M ÁöÑÊ†∑Êù°ÊòØ order ‰∏∫ M ÁöÑÂàÜÊÆµÂ§ö È°πÂºèÔºåËÄå‰∏îÊúâËøûÁª≠ÁöÑ M ‚àí 2 Ê¨°ÂæÆÂàÜÔºé[Ê≥®ÊÑèËøôÈáåÁöÑ order = degree+1, degree ÊâçÊòØÊàë‰ª¨ÁÜüÊÇâÁöÑÈò∂Êï∞, ‰æãÂ¶ÇËøôÈáåÁöÑ‰∏âÊ¨°Ê†∑Êù° order=4]</p>
<ul>
<li>‰∏âÊ¨°Ê†∑Êù°ÁöÑ M = 4</li>
<li>ÂàÜÊÆµÂ∏∏Êï∞ÂáΩÊï∞ÊòØ order ‰∏∫ 1 ÁöÑÊ†∑Êù°</li>
<li>ËøûÁª≠ÁöÑÂàÜÊÆµÁ∫øÊÄßÂáΩÊï∞ÊòØ order ‰∏∫ 2 ÁöÑÊ†∑Êù°</li>
</ul>
<p><strong>Order</strong>-M spline with knots <span class="arithmatex"><span class="MathJax_Preview">\xi_{j}, j=1, \ldots, K</span><script type="math/tex">\xi_{j}, j=1, \ldots, K</script></span> :</p>
<ul>
<li>Piecewise-polynomial of order M</li>
<li>Continuous derivatives up to order M-2</li>
</ul>
<p>ÂèØ‰ª•ÁªôÂá∫ M order Ê†∑Êù°ÁöÑÂü∫ÂáΩÊï∞:</p>
<p><strong>Truncated power</strong> Êà™Êñ≠ÂπÇ basis</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{j}(X)=X^{j-1}, j=1, \ldots, M</span><script type="math/tex">h_{j}(X)=X^{j-1}, j=1, \ldots, M</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{M+l}=\left(X-\xi_{l}\right)_{+}^{M-1}, l=1, \ldots, K</span><script type="math/tex">h_{M+l}=\left(X-\xi_{l}\right)_{+}^{M-1}, l=1, \ldots, K</script></span></li>
</ul>
<p>In practice, the most widely used orders are <span class="arithmatex"><span class="MathJax_Preview">M=1,2</span><script type="math/tex">M=1,2</script></span> and 4 .</p>
<h4 id="construct-spline-basis">Construct Spline Basis<a class="headerlink" href="#construct-spline-basis" title="Permanent link">&para;</a></h4>
<p>Ê≥®ÊÑèÂà∞ <strong>order ‰∏∫ <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> ÔºåÂê´Êúâ <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> ‰∏™ÁªìÁÇπ</strong>ÁöÑÊ†∑Êù°ÂÖ∂Ëá™Áî±Â∫¶‰∏∫</p>
<div class="arithmatex">
<div class="MathJax_Preview">
M(K+1)-(M-1) K=K+M
</div>
<script type="math/tex; mode=display">
M(K+1)-(M-1) K=K+M
</script>
</div>
<p>Â∑¶ËæπÁ¨¨‰∏ÄÈ°πË°®Á§∫ <span class="arithmatex"><span class="MathJax_Preview">K+1</span><script type="math/tex">K+1</script></span> ‰∏™Âå∫Âüü‰∏≠ÊØè‰∏™Âå∫ÂüüÈúÄË¶Å <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> ‰∏™ÂèÇÊï∞ÔºåËÄåÁ¨¨‰∫åÈ°πË°®Êòé <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> ‰∏™ÁªìÁÇπ‰∏≠ÈúÄË¶Å <span class="arithmatex"><span class="MathJax_Preview">M-1</span><script type="math/tex">M-1</script></span> ‰∏™ÈôêÂà∂. ÊØîÂ¶ÇÔºåÂØπ‰∫é‰∏âÊ¨°Ê†∑Êù°Ôºå <span class="arithmatex"><span class="MathJax_Preview">M=4</span><script type="math/tex">M=4</script></span> ÔºåÂàôËá™Áî±Â∫¶‰∏∫ <span class="arithmatex"><span class="MathJax_Preview">K+4</span><script type="math/tex">K+4</script></span>.</p>
<p>‰∏ãÈù¢‰ªãÁªçÂà©Áî® R <code>splines</code> ‰∏≠ÁöÑÂáΩÊï∞Êù•ËøõË°å Spline ÁöÑ order, knot ÁöÑÊï∞ÈáèÂíå‰ΩçÁΩÆÁöÑÈÄâÊã©. Choose order of the spline, the number of knots and their placement. [Ê≥®ÊÑè, Âú®‰∏ãÈù¢ÁöÑ <code>bs(x, df=7)</code> ‰∏≠ÔºåÂéüÊú¨Âõõ‰∏™ÁªìÁÇπÁöÑ‰∏âÊ¨°Ê†∑Êù°Ëá™Áî±Â∫¶‰∏∫ 8 Ôºå‰ΩÜÊòØ <code>bs()</code> ÂáΩÊï∞Êú¨Ë∫´ÈªòËÆ§Âú®Âü∫‰∏≠ÂéªÊéâÂ∏∏Êï∞È°π. ‰πüÂç≥, ËøôÈáåÊª°Ë∂≥ <code>df = degree + len(knots)</code>; df=freedom-1; degree=M-1]</p>
<ul>
<li><code>bs(x, df=7)</code><ul>
<li>Default order <span class="arithmatex"><span class="MathJax_Preview">M=4</span><script type="math/tex">M=4</script></span> cubic spline (‰πüÂç≥ degree=3)</li>
<li><span class="arithmatex"><span class="MathJax_Preview">7-3=4</span><script type="math/tex">7-3=4</script></span> interior knots at the <span class="arithmatex"><span class="MathJax_Preview">20_{t h}, 40_{t h}, 60_{t h}, 80_{t h}</span><script type="math/tex">20_{t h}, 40_{t h}, 60_{t h}, 80_{t h}</script></span> percentiles of <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span></li>
<li>Return a <span class="arithmatex"><span class="MathJax_Preview">N \times 8</span><script type="math/tex">N \times 8</script></span> matrix of ‚Äî‚Äî Ê≥®ÊÑè, ËøîÂõûÁöÑÁü©ÈòµËøòÊòØÊ†πÊçÆËá™Áî±Â∫¶‰Ωú‰∏∫ÂÆΩÁöÑ</li>
</ul>
</li>
<li><code>bs (x, degree=1, knots=c(0.2,0.4,0.6))</code><ul>
<li>Piecewise constant</li>
<li>return <span class="arithmatex"><span class="MathJax_Preview">\mathrm{N} \times 4</span><script type="math/tex">\mathrm{N} \times 4</script></span> matrix</li>
</ul>
</li>
</ul>
<h4 id="problems-with-polynomials-and-splines">Problems with polynomials and splines<a class="headerlink" href="#problems-with-polynomials-and-splines" title="Permanent link">&para;</a></h4>
<ul>
<li>Poor fit near boundaries</li>
<li>Extrapolation beyond boundaries Â§ñÊé®</li>
</ul>
<p><img alt="" src="../media/ASL-note2/2021-12-13-11-54-59.png" /></p>
<h4 id="natural-cubic-splines">Natural Cubic Splines<a class="headerlink" href="#natural-cubic-splines" title="Permanent link">&para;</a></h4>
<p>Natural cubic spline Â¢ûÂä†‰∫ÜÁ∫¶Êùü: Ë¶ÅÊ±ÇÂú®ËæπÁïå‰πãÂ§ñÁöÑÂáΩÊï∞Ë¶ÅÊòØÁ∫øÊÄßÁöÑ, Âõ†Ê≠§ÂáèÂ∞ë‰∫Ü2*2 ‰∏™ÂèÇÊï∞</p>
<ul>
<li>Assume function is linear beyond the boundary knots</li>
<li>Frees up <span class="arithmatex"><span class="MathJax_Preview">2 \times 2</span><script type="math/tex">2 \times 2</script></span> degrees of freedom</li>
</ul>
<p>Natural cubic spline with <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span> knots - <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span> basis functions</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">N_{1}(X)=1, N_{2}(X)=X, N_{k+2}(X)=d_{k}(X)-d_{K-1}(X)</span><script type="math/tex">N_{1}(X)=1, N_{2}(X)=X, N_{k+2}(X)=d_{k}(X)-d_{K-1}(X)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">d_{k}(X)=\frac{\left(X-\xi_{k}\right)_{+}^{3}-\left(X-\xi_{K}\right)_{+}^{3}}{\xi_{K}-\xi_{k}}</span><script type="math/tex">d_{k}(X)=\frac{\left(X-\xi_{k}\right)_{+}^{3}-\left(X-\xi_{K}\right)_{+}^{3}}{\xi_{K}-\xi_{k}}</script></span></li>
<li>Easy to prove: zero second and third derivatives when <span class="arithmatex"><span class="MathJax_Preview">X \geqslant \xi_{K}</span><script type="math/tex">X \geqslant \xi_{K}</script></span> or <span class="arithmatex"><span class="MathJax_Preview">X \leqslant \xi_{1}</span><script type="math/tex">X \leqslant \xi_{1}</script></span></li>
</ul>
<p>ËøôÈáå‰ΩøÁî®Ëøá‰πãÂâçÁöÑ‰∏âÊ¨°Ê†∑Êù°ÁöÑÂπÇÂü∫ÂáΩÊï∞Êé®Âá∫Êù•ÁöÑ, ÂèÇËßÅ <a href="https://github.com/szcf-weiya/ESL-CN/issues/31">here</a>.</p>
<h5 id="_1">Ëá™Áî±Â∫¶ÁöÑËÆ°ÁÆóËØ¥Êòé<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h5>
<p>‰∏ãÈù¢ËØ¥Êòé<strong>ÂØπ‰∫éËá™ÁÑ∂‰∏âÊ¨°Ê†∑Êù°ËÄåË®Ä, Âü∫ÂáΩÊï∞‰∏™Êï∞Âç≥‰∏∫ÁªìÁÇπ‰∏™Êï∞</strong>. ËÆæÊúâ <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> ‰∏™ÁªìÁÇπ, ÂàôÊúâ <span class="arithmatex"><span class="MathJax_Preview">K-2</span><script type="math/tex">K-2</script></span> ‰∏™ÂÜÖÁªìÁÇπ, <span class="arithmatex"><span class="MathJax_Preview">(K-2+1)</span><script type="math/tex">(K-2+1)</script></span>
‰∏™Âå∫Âüü, ÊØè‰∏™Âå∫ÂüüÂèÇÊï∞‰∏∫ 4 ‰∏™, ÊØè‰∏™ÂÜÖÁªìÁÇπÂáèÊéâ 3 ‰∏™ÂèÇÊï∞, ÊØè‰∏™ËæπÁïåÁÇπÂáèÊéâ‰∏Ä‰∏™ÂèÇÊï∞, ÂàôËøòÂâ©‰∏ã</p>
<div class="arithmatex">
<div class="MathJax_Preview">
(K-1) \cdot 4-3(K-2)-2 \times 1=K
</div>
<script type="math/tex; mode=display">
(K-1) \cdot 4-3(K-2)-2 \times 1=K
</script>
</div>
<p>‰πüÂ∞±ÊòØ‰∏™Âü∫ÂáΩÊï∞.</p>
<p>ÂÖ∑‰ΩìÂú∞, ÂÅáËÆæÊüêÂÜÖÁªìÁÇπ <span class="arithmatex"><span class="MathJax_Preview">\xi</span><script type="math/tex">\xi</script></span> ÁöÑÂ∑¶Âè≥Âå∫ÂüüÂáΩÊï∞ÂàÜÂà´‰∏∫</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f_{i}(x)=a_{i} x^{3}+b_{i} x^{2}+c_{i} x+d_{i}, \quad i=1,2,
</div>
<script type="math/tex; mode=display">
f_{i}(x)=a_{i} x^{3}+b_{i} x^{2}+c_{i} x+d_{i}, \quad i=1,2,
</script>
</div>
<p>ÂàôËá™Áî±ÂèÇÊï∞Êúâ 8 ‰∏™ <span class="arithmatex"><span class="MathJax_Preview">\left(a_{i}, b_{i}, c_{i}, d_{i}\right), i=1,2</span><script type="math/tex">\left(a_{i}, b_{i}, c_{i}, d_{i}\right), i=1,2</script></span>, ÁªìÁÇπ <span class="arithmatex"><span class="MathJax_Preview">\xi</span><script type="math/tex">\xi</script></span> Â§ÑÈúÄË¶ÅÊª°Ë∂≥ ‚ÄúÂáΩÊï∞ÂÄºÁõ∏Á≠â‚Äù„ÄÅ‚ÄúÈò∂ÂØºÁõ∏Á≠â‚Äù„ÄÅ‚ÄúÈò∂ÂØºÁõ∏Á≠â‚Äù, Âç≥</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
f_{1}(\xi)=f_{2}(\xi)\\
f_{1}^{\prime}(\xi)=f_{2}^{\prime}(\xi)\\
f_{1}^{\prime \prime}(\xi)=f_{2}^{\prime \prime}(\xi)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
f_{1}(\xi)=f_{2}(\xi)\\
f_{1}^{\prime}(\xi)=f_{2}^{\prime}(\xi)\\
f_{1}^{\prime \prime}(\xi)=f_{2}^{\prime \prime}(\xi)
\end{aligned}
</script>
</div>
<p>Áõ∏ÂΩì‰∫éÂáèÂ∞ë‰∫Ü 3 ‰∏™Ëá™Áî±ÂèÇÊï∞ÔºàÊç¢Âè•ËØùËØ¥, Êúâ‰∏â‰∏™ÂèÇÊï∞ÂèØ‰ª•Ë¢´ÂÖ∂‰ªñ 5 ‰∏™ÂèÇÊï∞Ë°®Á§∫Âá∫Êù•Ôºâ„ÄÇ</p>
<p>ÂØπ‰∫éÊüêËæπÁïåÁÇπ <span class="arithmatex"><span class="MathJax_Preview">\xi_{0}</span><script type="math/tex">\xi_{0}</script></span>, ÂÖ∂ÊâÄÂú®Âå∫ÂüüÁöÑÂáΩÊï∞‰∏∫ <span class="arithmatex"><span class="MathJax_Preview">f(x)=a x^{3}+b x^{2}+c x+d</span><script type="math/tex">f(x)=a x^{3}+b x^{2}+c x+d</script></span>, ËæπÁïåÁÇπÈúÄË¶ÅÊª°Ë∂≥ ‚Äú‰∫åÈò∂ÂØº‰∏∫Èõ∂‚ÄùÁöÑÁ∫¶Êùü, Âç≥</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f^{\prime \prime}\left(\xi_{0}\right)=0,
</div>
<script type="math/tex; mode=display">
f^{\prime \prime}\left(\xi_{0}\right)=0,
</script>
</div>
<p>ËøôÂáèÂ∞ë‰∫Ü 1 ‰∏™Ëá™Áî±ÂèÇÊï∞„ÄÇ</p>
<h4 id="example-south-african-heart-disease-continued">Example: South African Heart Disease (Continued)<a class="headerlink" href="#example-south-african-heart-disease-continued" title="Permanent link">&para;</a></h4>
<p><img alt="" src="../media/ASL-note2/2021-12-13-12-17-40.png" /></p>
<h3 id="smoothing-splines">Smoothing Splines<a class="headerlink" href="#smoothing-splines" title="Permanent link">&para;</a></h3>
<p>ËøôÈáåÊàë‰ª¨ËÆ®ËÆ∫‰ΩøÁî®ÊúÄÂ§ßÁªìÁÇπÈõÜÂêàÁöÑÊ†∑Êù°Âü∫ÊñπÊ≥ïÊù•ÂΩªÂ∫ïÈÅøÂÖçÁªìÁÇπÈÄâÊã©ÁöÑÈóÆÈ¢òÔºéÊãüÂêàÁöÑÂ§çÊùÇÂ∫¶Áî±Ê≠£ÂàôÂåñÊù•ÊéßÂà∂Ôºé</p>
<p>With <strong>regression splines</strong>, choosing knots is a tricky business. ‰∏äÈù¢ËÆ≤ÁöÑ„ÄåÂõûÂΩíÊ†∑Êù°„Äç, ËøôÈáåÊòØÂÖâÊªëÊ†∑Êù°.</p>
<p>Smoothing Splines</p>
<ul>
<li>avoid knot selection problem by using a maximal set of knots</li>
<li>control for overfitting by <strong>shrinking the coefficients</strong> of the estimated function</li>
</ul>
<p>Smoothing splines can be motivated directly by 1 . natural cubic splines with knots at <span class="arithmatex"><span class="MathJax_Preview">\left(x_{1}, x_{2}, \ldots, x_{n}\right)</span><script type="math/tex">\left(x_{1}, x_{2}, \ldots, x_{n}\right)</script></span> and regularization on parameters or 2. a function minimization perspective. We&rsquo;ll talk about 2 in accordance with the text.</p>
<p>Among all functions <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> with <strong>two continuous derivatives</strong>, find one that minimizes the penalized residual sum of squares</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{RSS}(f, \lambda)=\sum_{i=1}^{N}\left(y_{i}-f\left(x_{i}\right)\right)^{2}+\lambda \int f^{\prime \prime}(t)^{2} d(t)
</div>
<script type="math/tex; mode=display">
\operatorname{RSS}(f, \lambda)=\sum_{i=1}^{N}\left(y_{i}-f\left(x_{i}\right)\right)^{2}+\lambda \int f^{\prime \prime}(t)^{2} d(t)
</script>
</div>
<ul>
<li>the first term quantifies the goodness-of-fit to the data</li>
<li>the second term measures the roughness (curvature in <span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> )</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> : smoothing parameter</li>
</ul>
<p>Special case</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\lambda=0: f</span><script type="math/tex">\lambda=0: f</script></span> can be any function that interpolates the data.</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\lambda=\infty: f</span><script type="math/tex">\lambda=\infty: f</script></span> is simple least squares line</li>
</ul>
<p>With <span class="arithmatex"><span class="MathJax_Preview">\lambda \in(0, \infty)</span><script type="math/tex">\lambda \in(0, \infty)</script></span>, there exists a unique minimizer - a natural cubic spline with knots at the unique values of the <span class="arithmatex"><span class="MathJax_Preview">x_{i}, i=1, \ldots, N</span><script type="math/tex">x_{i}, i=1, \ldots, N</script></span>. ËøôÈáåÁöÑ f ÊòØ‰ªªÊÑèÁöÑ, Âõ†Ê≠§ÊòØÊó†ÈôêÁª¥Á©∫Èó¥; ÁÑ∂ËÄåÂèØ‰ª•ËØÅÊòé, Âú®‰∏äÈù¢ÁöÑ‰∫åÈò∂ÂæÆÂàÜÊÉ©ÁΩöÈ°π‰∏ã, Â≠òÂú®‰∏Ä‰∏™Âú®ÊúâÈôêÁª¥ÁöÑÂîØ‰∏ÄÊúÄÂ∞èÁÇπ, ÊòØ‰∏Ä‰∏™ÁªìÁÇπÂú®‰∏çÈáçÂ§çÁöÑ <span class="arithmatex"><span class="MathJax_Preview">x_i, i=1,2,...,N</span><script type="math/tex">x_i, i=1,2,...,N</script></span> Â§ÑÁöÑËá™ÁÑ∂‰∏âÊ¨°Ê†∑Êù°. Áõ∏ÂÖ≥ËØÅÊòéËßÅ <a href="https://esl.hohoweiya.xyz/05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines/index.html">here</a></p>
<p>THEOREM :
Let <span class="arithmatex"><span class="MathJax_Preview">g</span><script type="math/tex">g</script></span> be any differentiable function on <span class="arithmatex"><span class="MathJax_Preview">[a, b]</span><script type="math/tex">[a, b]</script></span> for which <span class="arithmatex"><span class="MathJax_Preview">g\left(x_{i}\right)=z_{i}</span><script type="math/tex">g\left(x_{i}\right)=z_{i}</script></span> for <span class="arithmatex"><span class="MathJax_Preview">i=1, \ldots, n</span><script type="math/tex">i=1, \ldots, n</script></span>. Suppose <span class="arithmatex"><span class="MathJax_Preview">n \geqslant 2</span><script type="math/tex">n \geqslant 2</script></span>, and that <span class="arithmatex"><span class="MathJax_Preview">\tilde{g}</span><script type="math/tex">\tilde{g}</script></span> is the natural cubic spline interpolant to the values <span class="arithmatex"><span class="MathJax_Preview">z_{1}, \ldots, z_{n}</span><script type="math/tex">z_{1}, \ldots, z_{n}</script></span> at points <span class="arithmatex"><span class="MathJax_Preview">x_{1}, \ldots, x_{n}</span><script type="math/tex">x_{1}, \ldots, x_{n}</script></span> with <span class="arithmatex"><span class="MathJax_Preview">a&lt;x_{1}&lt;&lt;x_{n}&lt;b</span><script type="math/tex">a<x_{1}<<x_{n}<b</script></span>.
Then <span class="arithmatex"><span class="MathJax_Preview">\int\left(g^{\prime \prime}\right)^{2} \geqslant \int\left(\tilde{g}^{\prime \prime}\right)^{2}</span><script type="math/tex">\int\left(g^{\prime \prime}\right)^{2} \geqslant \int\left(\tilde{g}^{\prime \prime}\right)^{2}</script></span> with equality only if <span class="arithmatex"><span class="MathJax_Preview">\tilde{g}=g</span><script type="math/tex">\tilde{g}=g</script></span></p>
<p>Âõ†Ê≠§, ÂèØ‰ª•Âü∫‰∫éËøô‰∏ÄÁªìËÆ∫ËøõË°åËøõ‰∏ÄÊ≠•ÁöÑÊé®ÂØº</p>
<p>Since now <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> is a natural cubic spline, <span class="arithmatex"><span class="MathJax_Preview">f(x)=\sum_{j=1}^{N} N_{j}(x) \theta_{j}</span><script type="math/tex">f(x)=\sum_{j=1}^{N} N_{j}(x) \theta_{j}</script></span>
The penalized RSS reduces to:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
R S S(\theta, \lambda)=(y-N \theta)^{\top}(y-N \theta)+\lambda \theta^{\top} \Omega_{N} \theta
</div>
<script type="math/tex; mode=display">
R S S(\theta, \lambda)=(y-N \theta)^{\top}(y-N \theta)+\lambda \theta^{\top} \Omega_{N} \theta
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> is <span class="arithmatex"><span class="MathJax_Preview">n \times n</span><script type="math/tex">n \times n</script></span>, with <span class="arithmatex"><span class="MathJax_Preview">N_{i j}=N_{j}\left(X_{i}\right)</span><script type="math/tex">N_{i j}=N_{j}\left(X_{i}\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\Omega</span><script type="math/tex">\Omega</script></span> is <span class="arithmatex"><span class="MathJax_Preview">n \times n</span><script type="math/tex">n \times n</script></span>, with <span class="arithmatex"><span class="MathJax_Preview">\Omega_{N_{i j}}=\int N_{i}^{\prime \prime}(t) N_{j}^{\prime \prime}(t) d t</span><script type="math/tex">\Omega_{N_{i j}}=\int N_{i}^{\prime \prime}(t) N_{j}^{\prime \prime}(t) d t</script></span></li>
</ul>
<p>Âõ†Ê≠§ [ÊòØÂπø‰πâÂ≤≠ÂõûÂΩí]</p>
<ul>
<li>The solution: <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}=\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top} y</span><script type="math/tex">\hat{\theta}=\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top} y</script></span></li>
<li>The fitted: <span class="arithmatex"><span class="MathJax_Preview">\hat{y}=N \hat{\theta}=N\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top} y=S_{\lambda} y</span><script type="math/tex">\hat{y}=N \hat{\theta}=N\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top} y=S_{\lambda} y</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">S_{\lambda}:</span><script type="math/tex">S_{\lambda}:</script></span> smoother matrix</li>
</ul>
<h4 id="pre-specify-the-amount-of-smoothing">Pre-specify the Amount of Smoothing<a class="headerlink" href="#pre-specify-the-amount-of-smoothing" title="Permanent link">&para;</a></h4>
<p>Êàë‰ª¨ËøòÊ≤°ÊúâÊåáÂá∫ÂÖâÊªëÊ†∑Êù°ÁöÑ Œª ÊòØÊÄé‰πàÈÄâÂèñÁöÑÔºéÊú¨Á´†ÁöÑÂêéÈù¢Êàë‰ª¨ÊèèËø∞‰ΩøÁî®Ëá™Âä®ÂåñÊñπ Ê≥ïÔºåÊØîÂ¶Ç‰∫§ÂèâÈ™åËØÅÔºéÂú®ËøôÈÉ®ÂàÜ‰∏≠Êàë‰ª¨ËÆ®ËÆ∫È¢ÑÂÖàÁ°ÆÂÆöÂÖâÊªëÊÄªÈáèÁöÑÁõ¥ËßÇÊñπÂºèÔºé</p>
<p>Assume <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> is fixed, smoothing spline operates through linear operator</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}=N\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top} y=S_{\lambda} y
</div>
<script type="math/tex; mode=display">
\hat{f}=N\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top} y=S_{\lambda} y
</script>
</div>
<ul>
<li>fits are linear in <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> ‰πüÂç≥, Á∫øÊÄßÂÖâÊªë (<strong>linear smoother</strong>) ÂíåÁ∫øÊÄßÂõûÂΩí‰∏ÄÊ†∑</li>
<li><strong>smoother matrix</strong> <span class="arithmatex"><span class="MathJax_Preview">S_{\lambda}</span><script type="math/tex">S_{\lambda}</script></span> depends on <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span></li>
</ul>
<p>Compare hat matrix <span class="arithmatex"><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span> and smoother matrix <span class="arithmatex"><span class="MathJax_Preview">S_{\lambda}</span><script type="math/tex">S_{\lambda}</script></span></p>
<ul>
<li>symmetric, positive semidefinite?</li>
<li>idempotent?</li>
<li>rank?</li>
</ul>
<h4 id="smoother-matrix">Smoother Matrix<a class="headerlink" href="#smoother-matrix" title="Permanent link">&para;</a></h4>
<p>‰∏äÈù¢ÁöÑÂπ≥ÊªëÁü©Èòµ <span class="arithmatex"><span class="MathJax_Preview">S_{\lambda} = N\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top}</span><script type="math/tex">S_{\lambda} = N\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top}</script></span> ÂèØ‰ª•ÂÜôÊàê‰ª•‰∏ãÂΩ¢Âºè, ÂèÇËßÅ <a href="https://github.com/szcf-weiya/ESL-CN/issues/35">here</a> ; ËøôÊ†∑ÂèØ‰ª•ÂàÜÁ¶ªÂá∫Ë∂ÖÂèÇÊï∞ <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>, ‰ªéËÄåËøõË°åÂ•áÂºÇÂÄºÂàÜËß£</p>
<ol>
<li>Re-write <span class="arithmatex"><span class="MathJax_Preview">S_{\lambda}</span><script type="math/tex">S_{\lambda}</script></span> in the <strong>Reinsch form</strong></li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
S_{\lambda}=(I+\lambda K)^{-1}
</div>
<script type="math/tex; mode=display">
S_{\lambda}=(I+\lambda K)^{-1}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">K=N^{-\top} \Omega_{N} N^{-1}</span><script type="math/tex">K=N^{-\top} \Omega_{N} N^{-1}</script></span>, does not depend on <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> : <strong>penalty matrix</strong> since <span class="arithmatex"><span class="MathJax_Preview">P R S S=(y-f)^{\top}(y-f)+\lambda f^{\top} K f</span><script type="math/tex">P R S S=(y-f)^{\top}(y-f)+\lambda f^{\top} K f</script></span></li>
</ul>
<ol start="2">
<li>Eigen decomposition of <span class="arithmatex"><span class="MathJax_Preview">S_{\lambda}</span><script type="math/tex">S_{\lambda}</script></span></li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
S_{\lambda}=\sum_{k=1}^{N} \rho_{k}(\lambda) \mu_{k} \mu_{k}^{\top}
</div>
<script type="math/tex; mode=display">
S_{\lambda}=\sum_{k=1}^{N} \rho_{k}(\lambda) \mu_{k} \mu_{k}^{\top}
</script>
</div>
<ul>
<li>Eigenvalue <span class="arithmatex"><span class="MathJax_Preview">\rho_{k}(\lambda)=\frac{1}{1+\lambda d_{k}}</span><script type="math/tex">\rho_{k}(\lambda)=\frac{1}{1+\lambda d_{k}}</script></span></li>
<li>Eigenvector <span class="arithmatex"><span class="MathJax_Preview">\mu_{k}</span><script type="math/tex">\mu_{k}</script></span></li>
</ul>
<p>Note: <span class="arithmatex"><span class="MathJax_Preview">\mu_{k}</span><script type="math/tex">\mu_{k}</script></span> are also the eigenvector of <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>, and <span class="arithmatex"><span class="MathJax_Preview">d_{k}</span><script type="math/tex">d_{k}</script></span> are the eigenvalue of <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span></p>
<p><strong>Effective degree of freedom</strong> <span class="arithmatex"><span class="MathJax_Preview">d f_{\lambda}=\operatorname{tr}\left(S_{\lambda}\right)</span><script type="math/tex">d f_{\lambda}=\operatorname{tr}\left(S_{\lambda}\right)</script></span></p>
<p><img alt="" src="../media/ASL-note2/2021-12-13-13-33-50.png" /></p>
<h4 id="highlights-of-eigen-representation">Highlights of Eigen Representation<a class="headerlink" href="#highlights-of-eigen-representation" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
S_{\lambda}=\sum_{k=1}^{N} \rho_{k}(\lambda) \mu_{k} \mu_{k}^{\top}
</div>
<script type="math/tex; mode=display">
S_{\lambda}=\sum_{k=1}^{N} \rho_{k}(\lambda) \mu_{k} \mu_{k}^{\top}
</script>
</div>
<ul>
<li>The eigenvectors are not affected by <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> Â•áÂºÇÂêëÈáè‰∏éË∂ÖÂèÇÊó†ÂÖ≥</li>
<li><span class="arithmatex"><span class="MathJax_Preview">S_{\lambda} y=\sum_{k=1}^{N} \mu_{k} \rho_{k}(\lambda)\left\langle\mu_{k}^{\top} y\right\rangle</span><script type="math/tex">S_{\lambda} y=\sum_{k=1}^{N} \mu_{k} \rho_{k}(\lambda)\left\langle\mu_{k}^{\top} y\right\rangle</script></span>, compare to regression spline with <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> basis? ÂèØ‰ª•ÁúãÂà∞, ËøôÈáåÊòØÈÄöÂØπ <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> ÂÖ≥‰∫éÂü∫ <span class="arithmatex"><span class="MathJax_Preview">\mu_k</span><script type="math/tex">\mu_k</script></span> ËøõË°å‰∫ÜÂàÜËß£, ÈÄöËøá <span class="arithmatex"><span class="MathJax_Preview">\rho_{k}(\lambda)</span><script type="math/tex">\rho_{k}(\lambda)</script></span> ÊéßÂà∂‰∫ÜÊî∂Áº©ÁöÑÂ§ßÂ∞è; ËøôÂ∞±Âíå‰∏ä‰∏ÄËäÇ‰∏≠ÁöÑ„ÄåÂõûÂΩíÊ†∑Êù°„Äç‰∏ç‰∏ÄÊ†∑, Âú®ÂõûÂΩíÊ†∑Êù°‰∏≠, ÁªÑÂàÜË¶Å‰πà‰∏çÂèòË¶Å‰πàÊî∂Áº©‰∏∫ 0. ‚Äî‚Äî ÔºéÂü∫‰∫éËøô‰∏™ÂéüÂõ†ÂÖâÊªëÊ†∑Êù° Ë¢´Áß∞‰Ωú<strong>Êî∂Áº© (shrinking)ÂÖâÊªëÂô®</strong>ÔºåËÄåÂõûÂΩíÊ†∑Êù°Ë¢´Áß∞‰Ωú<strong>ÊäïÂΩ± (projection) ÂÖâÊªëÂô®</strong>.</li>
<li><span class="arithmatex"><span class="MathJax_Preview">d_{1}=d_{2}=0</span><script type="math/tex">d_{1}=d_{2}=0</script></span>, thus <span class="arithmatex"><span class="MathJax_Preview">\rho_{1}(\lambda)=\rho_{2}(\lambda)=1</span><script type="math/tex">\rho_{1}(\lambda)=\rho_{2}(\lambda)=1</script></span>, linear functions are never shrunk</li>
<li>The eigenvectors have increasing complexity, and the higher the complexity, the more they are shrunk</li>
<li>Reparametrize RSS with new basis <span class="arithmatex"><span class="MathJax_Preview">\mu_{k}, R S S=\|y-U \theta\|+\lambda \theta^{\top} D \theta</span><script type="math/tex">\mu_{k}, R S S=\|y-U \theta\|+\lambda \theta^{\top} D \theta</script></span>, thoughts? ÂÖ∂‰∏≠ <span class="arithmatex"><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> ÂàóÂêëÈáè‰∏∫ <span class="arithmatex"><span class="MathJax_Preview">u_k</span><script type="math/tex">u_k</script></span> Ôºå‰∏î <span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span> ‰∏∫ÂÖÉÁ¥†‰∏∫ <span class="arithmatex"><span class="MathJax_Preview">d_k</span><script type="math/tex">d_k</script></span> ÁöÑÂØπËßíÁü©ÈòµÔºé</li>
<li><span class="arithmatex"><span class="MathJax_Preview">d f_{\lambda}=\operatorname{tr}\left(S_{\lambda}\right)=\sum_{k=1}^{N} \rho_{k}(\lambda)</span><script type="math/tex">d f_{\lambda}=\operatorname{tr}\left(S_{\lambda}\right)=\sum_{k=1}^{N} \rho_{k}(\lambda)</script></span>, compare to regression splines? Âú®ÊäïÂΩ±ÂÖâÊªëÂô®‰∏≠, ÊâÄÊúâÁöÑÁâπÂæÅÂÄº‰∏∫ 1.</li>
</ul>
<h4 id="choosing-lambdalambda">Choosing <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span><a class="headerlink" href="#choosing-lambdalambda" title="Permanent link">&para;</a></h4>
<p>see <a href="https://esl.hohoweiya.xyz/notes/spline/sim-5-9/index.html">here</a></p>
<p>Crucial and tricky&hellip;</p>
<ol>
<li>Fixing the Degrees of Freedom ÂèØ‰ª•Áõ¥Êé•ËÆ§‰∏∫ÊåáÂÆö
   - <span class="arithmatex"><span class="MathJax_Preview">d f_{\lambda}=\operatorname{tr}\left(S_{\lambda}\right)</span><script type="math/tex">d f_{\lambda}=\operatorname{tr}\left(S_{\lambda}\right)</script></span> is monotone in <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> for smoothing splines
   - R ‰∏≠ÂèØ‰ª•ÈááÁî® <code>smooth.spline(x, y, df=6)</code> Á°ÆÂÆöÂÖâÊªëÁ®ãÂ∫¶</li>
<li>Cross validation<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{EPE}\left(\hat{f}_{\lambda}\right)=\sigma^{2}+\operatorname{Var}\left(\hat{f}_{\lambda}\right)+\operatorname{Bias}(\hat{f}(\lambda))^{2}</span><script type="math/tex">\operatorname{EPE}\left(\hat{f}_{\lambda}\right)=\sigma^{2}+\operatorname{Var}\left(\hat{f}_{\lambda}\right)+\operatorname{Bias}(\hat{f}(\lambda))^{2}</script></span></li>
<li>K-fold cross-validation</li>
<li>seek a good compromise between bias and variance</li>
</ul>
</li>
</ol>
<p><img alt="" src="../media/ASL-note2/2021-12-13-14-16-48.png" /></p>
<h3 id="nonparametric-logistic-regression">Nonparametric Logistic Regression<a class="headerlink" href="#nonparametric-logistic-regression" title="Permanent link">&para;</a></h3>
<p>see <a href="https://esl.hohoweiya.xyz/05-Basis-Expansions-and-Regularization/5.6-Nonparametric-Logistic-Regression/index.html">here</a></p>
<p>Logistic regression with single quantitative input <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\log \frac{\operatorname{Pr}(Y=1 \mid X=x)}{\operatorname{Pr}(Y=0 \mid X=x)}=f(x)
</div>
<script type="math/tex; mode=display">
\log \frac{\operatorname{Pr}(Y=1 \mid X=x)}{\operatorname{Pr}(Y=0 \mid X=x)}=f(x)
</script>
</div>
<ul>
<li>Fitting <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> in a smooth fashion leads to a smooth estimate of the conditional probability <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(Y=1 \mid x)</span><script type="math/tex">\operatorname{Pr}(Y=1 \mid x)</script></span></li>
</ul>
<p>ËøôÊòØ‰πãÂâçÁöÑ‰ººÁÑ∂ÂáΩÊï∞. The likelihood function, where <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(Y=1 \mid X=x)=p\left(x_{i}\right)</span><script type="math/tex">\operatorname{Pr}(Y=1 \mid X=x)=p\left(x_{i}\right)</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
l=\sum_{i=1}^{N}\left\{y_{i} \log p\left(x_{i}\right)+\left(1-y_{i}\right) \log \left(1-p\left(x_{i}\right)\right)\right\}
</div>
<script type="math/tex; mode=display">
l=\sum_{i=1}^{N}\left\{y_{i} \log p\left(x_{i}\right)+\left(1-y_{i}\right) \log \left(1-p\left(x_{i}\right)\right)\right\}
</script>
</div>
<p>since <span class="arithmatex"><span class="MathJax_Preview">p\left(x_{i}\right)=\frac{e^{f\left(x_{i}\right)}}{\left.1+e^{f\left(x_{i}\right)}\right)}</span><script type="math/tex">p\left(x_{i}\right)=\frac{e^{f\left(x_{i}\right)}}{\left.1+e^{f\left(x_{i}\right)}\right)}</script></span>,</p>
<div class="arithmatex">
<div class="MathJax_Preview">
l=\sum_{i=1}^{N}\left\{y_{i} f\left(x_{i}\right)-\log \left(1+e^{f\left(x_{i}\right)}\right)\right\}
</div>
<script type="math/tex; mode=display">
l=\sum_{i=1}^{N}\left\{y_{i} f\left(x_{i}\right)-\log \left(1+e^{f\left(x_{i}\right)}\right)\right\}
</script>
</div>
<h4 id="penalized-log-likelihood">Penalized log-likelihood<a class="headerlink" href="#penalized-log-likelihood" title="Permanent link">&para;</a></h4>
<p>Âà©Áî®ÂÖâÊªëÂåñÁöÑÊÄùÊÉ≥, Âä†‰∏äÊÉ©ÁΩöÈ°π</p>
<div class="arithmatex">
<div class="MathJax_Preview">
l=\sum_{i=1}^{N}\left\{y_{i} f\left(x_{i}\right)-\log \left(1+e^{f\left(x_{i}\right)}\right)-\frac{1}{2} \int\left\{f^{\prime \prime}(t)\right\}^{2} d t\right\}
</div>
<script type="math/tex; mode=display">
l=\sum_{i=1}^{N}\left\{y_{i} f\left(x_{i}\right)-\log \left(1+e^{f\left(x_{i}\right)}\right)-\frac{1}{2} \int\left\{f^{\prime \prime}(t)\right\}^{2} d t\right\}
</script>
</div>
<p>Optimal <span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> is a <strong>finite-dimensional natural spline with knots</strong> at the unique values of <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>.</p>
<p>Represent <span class="arithmatex"><span class="MathJax_Preview">f(x)=\sum_{j=1}^{N} N_{j}(x) \theta_{j}</span><script type="math/tex">f(x)=\sum_{j=1}^{N} N_{j}(x) \theta_{j}</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\frac{\partial l(\theta)}{\partial \theta}=N^{\top}(y-p)-\lambda \Omega \theta</span><script type="math/tex">\frac{\partial l(\theta)}{\partial \theta}=N^{\top}(y-p)-\lambda \Omega \theta</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\frac{\partial^{2} /(\theta)}{\partial \theta \partial \theta^{\top}}=-N^{\top} W N-\lambda \Omega</span><script type="math/tex">\frac{\partial^{2} /(\theta)}{\partial \theta \partial \theta^{\top}}=-N^{\top} W N-\lambda \Omega</script></span></li>
</ul>
<p>Newton-Raphson update</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;\theta^{\text {new }}=\left(N^{\top} W N+\lambda \Omega\right)^{-1} N^{\top} W\left(N \theta^{\text {old }}+W^{-1}(y-p)\right)= \left(N^{\top} W N+\lambda \Omega\right)^{-1} N^{\top} W * z \\
&amp;f^{\text {new }}=N\left(N^{\top} W N+\lambda \Omega\right)^{-1} N^{\top} W\left(f^{\text {old }}+W^{-1}(y-p)\right)=S_{\lambda, W} z
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&\theta^{\text {new }}=\left(N^{\top} W N+\lambda \Omega\right)^{-1} N^{\top} W\left(N \theta^{\text {old }}+W^{-1}(y-p)\right)= \left(N^{\top} W N+\lambda \Omega\right)^{-1} N^{\top} W * z \\
&f^{\text {new }}=N\left(N^{\top} W N+\lambda \Omega\right)^{-1} N^{\top} W\left(f^{\text {old }}+W^{-1}(y-p)\right)=S_{\lambda, W} z
\end{aligned}
</script>
</div>
<p>ËøôÈáåÁöÑÊõ¥Êñ∞ÂÖ¨ÂºèÁöÑÂΩ¢ÂºèÂæàÊúâÊåáÂØºÊÑè‰πâÔºéÂÆÉËØïÂõæÁî®‰ªª‰ΩïÈùûÂèÇÔºàÂä†ÊùÉÔºâÂõûÂΩíÁÆóÂ≠ê‰ª£Êõø <span class="arithmatex"><span class="MathJax_Preview">S_{\lambda, W}</span><script type="math/tex">S_{\lambda, W}</script></span>ÔºåÂπ∂‰∏îÂæóÂà∞‰∏ÄËà¨ÁöÑÈùûÂèÇÈÄªËæëÊñØËíÇÂõûÂΩíÊ®°ÂûãÁöÑÊóèÔºéÂ∞ΩÁÆ°ËøôÈáå <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> ÊòØ‰∏ÄÁª¥ÁöÑÔºå‰ΩÜËøô‰∏™ËøáÁ®ãÂèØ‰ª•ÂæàËá™ÁÑ∂Âú∞Êé®ÂπøÂà∞È´òÁª¥ÁöÑ <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>ÔºéËøô‰∫õÊãìÂ±ïÊòØ Âπø‰πâÂèØÂä†Ê®°Âûã (generalized additive models) ÁöÑÊ†∏ÂøÉÔºåÊàë‰ª¨Â∞ÜÂú®Á¨¨ 9 Á´†‰∏≠ËÆ®ËÆ∫Ôºé</p>
<h3 id="multidimensional-splines">Multidimensional Splines<a class="headerlink" href="#multidimensional-splines" title="Permanent link">&para;</a></h3>
<h4 id="from-1-dim-to-2-dim">From 1-dim to 2-dim<a class="headerlink" href="#from-1-dim-to-2-dim" title="Permanent link">&para;</a></h4>
<p>Suppose <span class="arithmatex"><span class="MathJax_Preview">X \in \mathbb{R}^{2}</span><script type="math/tex">X \in \mathbb{R}^{2}</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{1 j}\left(X_{1}\right), j=1, \ldots, M_{1}</span><script type="math/tex">h_{1 j}\left(X_{1}\right), j=1, \ldots, M_{1}</script></span> are the basis for coordinate <span class="arithmatex"><span class="MathJax_Preview">X_{1}</span><script type="math/tex">X_{1}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{2 k}\left(X_{2}\right), k=1, \ldots, M_{2}</span><script type="math/tex">h_{2 k}\left(X_{2}\right), k=1, \ldots, M_{2}</script></span> are the basis for coordinate <span class="arithmatex"><span class="MathJax_Preview">X_{2}</span><script type="math/tex">X_{2}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">g_{j k}(X)=h_{1 j}\left(X_{1}\right) h_{2 k}\left(X_{2}\right)</span><script type="math/tex">g_{j k}(X)=h_{1 j}\left(X_{1}\right) h_{2 k}\left(X_{2}\right)</script></span> are the <span class="arithmatex"><span class="MathJax_Preview">M_{1} \times M_{2}</span><script type="math/tex">M_{1} \times M_{2}</script></span>-dim <strong>tensor product basis</strong> Âº†ÈáèÁßØÂü∫Â∫ï</li>
</ul>
<p>The new basis representation of the model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
g(X)=\sum_{j=1}^{M_{1}} \sum_{k=1}^{M_{2}} \theta_{j k} g_{j k}(X)
</div>
<script type="math/tex; mode=display">
g(X)=\sum_{j=1}^{M_{1}} \sum_{k=1}^{M_{2}} \theta_{j k} g_{j k}(X)
</script>
</div>
<p>Note, the dimension of the basis grows exponentially fast - curse of dimensionality.</p>
<p><img alt="" src="../media/ASL-note2/2021-12-13-14-38-35.png" /></p>
<p>ËÄåÂØπ‰∫é‰∏ÄÁª¥ÂÖâÊªëÊ†∑Êù°(Ê≠£ÂàôÂåñ) ‰πüÂèØ‰ª•Êé®Âπø. The 1-dim penalty</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\lambda \int f^{\prime \prime}(t) d t
</div>
<script type="math/tex; mode=display">
\lambda \int f^{\prime \prime}(t) d t
</script>
</div>
<p>Natural generalization to 2-dim</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\iint_{\mathbb{R}^{2}}\left\{\left(\frac{\partial^{2} f(x)}{\partial x_{1}^{2}}\right)^{2}+2\left(\frac{\partial^{2} f(x)}{\partial x_{1} \partial x_{2}}\right)^{2}+\left(\frac{\partial f(x)}{\partial x_{2}^{2}}\right)^{2}\right\} d x_{1} d x_{2}
</div>
<script type="math/tex; mode=display">
\iint_{\mathbb{R}^{2}}\left\{\left(\frac{\partial^{2} f(x)}{\partial x_{1}^{2}}\right)^{2}+2\left(\frac{\partial^{2} f(x)}{\partial x_{1} \partial x_{2}}\right)^{2}+\left(\frac{\partial f(x)}{\partial x_{2}^{2}}\right)^{2}\right\} d x_{1} d x_{2}
</script>
</div>
<p>The solution is a smooth two-dimensional surface - <strong>thin plate spline</strong></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\lambda \rightarrow 0, f \rightarrow</span><script type="math/tex">\lambda \rightarrow 0, f \rightarrow</script></span> interpolating function</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\lambda \rightarrow \infty, f \rightarrow</span><script type="math/tex">\lambda \rightarrow \infty, f \rightarrow</script></span> least square plane</li>
<li>For <span class="arithmatex"><span class="MathJax_Preview">\lambda \in(0, \infty)</span><script type="math/tex">\lambda \in(0, \infty)</script></span>, Ëß£ÂèØ‰ª•Ë°®Á§∫ÊàêÂü∫ÂáΩÊï∞ÁöÑÁ∫øÊÄßÂ±ïÂºÄÔºåÂÖ∂‰∏≠Á≥ªÊï∞ÂèØ‰ª•ÈÄöËøáÂπø ‰πâÁöÑÂ≤≠ÂõûÂΩíÂæóÂà∞Ôºé<ul>
<li>solution has the form <span class="arithmatex"><span class="MathJax_Preview">f(x)=\beta_{0}+\beta^{\top} x+\sum_{j=1}^{N} \alpha_{j} h_{j}(x)</span><script type="math/tex">f(x)=\beta_{0}+\beta^{\top} x+\sum_{j=1}^{N} \alpha_{j} h_{j}(x)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{j}(x)=\left\|x-x_{j}\right\|^{2} \log \left\|x-x_{j}\right\|</span><script type="math/tex">h_{j}(x)=\left\|x-x_{j}\right\|^{2} \log \left\|x-x_{j}\right\|</script></span>, <strong>radial basis function</strong> ÂæÑÂêëÂü∫ÂáΩÊï∞</li>
</ul>
</li>
</ul>
<h4 id="d-dim">d-dim<a class="headerlink" href="#d-dim" title="Permanent link">&para;</a></h4>
<p>More generally, when <span class="arithmatex"><span class="MathJax_Preview">X \in \mathbb{R}^{d}</span><script type="math/tex">X \in \mathbb{R}^{d}</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\min \sum_{i=1}^{N}\left\{y_{i}-f\left(x_{i}\right)\right\}^{2}+\lambda J|f|
</div>
<script type="math/tex; mode=display">
\min \sum_{i=1}^{N}\left\{y_{i}-f\left(x_{i}\right)\right\}^{2}+\lambda J|f|
</script>
</div>
<p>Restricted class of multidimensional splines</p>
<div class="arithmatex">
<div class="MathJax_Preview">
J|f|=J\left(f_{1}+f_{2}+\ldots+f_{d}\right)=\sum_{j=1}^{d} \int f_{j}^{\prime \prime}\left(t_{j}\right) d t_{j}
</div>
<script type="math/tex; mode=display">
J|f|=J\left(f_{1}+f_{2}+\ldots+f_{d}\right)=\sum_{j=1}^{d} \int f_{j}^{\prime \prime}\left(t_{j}\right) d t_{j}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> is additive</li>
<li>additive penalty on each of the component functions</li>
</ul>
<h2 id="kernel-smoothing-methods">Kernel Smoothing Methods<a class="headerlink" href="#kernel-smoothing-methods" title="Permanent link">&para;</a></h2>
<p>ËøôÁ´†‰∏≠Êàë‰ª¨ÊèèËø∞‰∏ÄÁ±ª<strong>ÂõûÂΩíÊäÄÂ∑ß</strong>, ËøôÁ±ªÊäÄÂ∑ßËÉΩÂ§üÈÄöËøáÊüêÁßçÊñπÂºèÂÆûÁé∞Âú®ÂÆö‰πâÂüü <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{p}</span><script type="math/tex">\mathbb{R}^{p}</script></span> ‰∏≠‰º∞ËÆ°ÂõûÂΩíÂáΩÊï∞ <span class="arithmatex"><span class="MathJax_Preview">f(X)</span><script type="math/tex">f(X)</script></span> ÁöÑÁÅµÊ¥ªÊÄß, ËøôÁßçÊñπÂºèÊòØÂú®ÊØè‰∏™Êü•ËØ¢ÁÇπ <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span> Â§ÑÂàÜÂà´ÊãüÂêà‰∏çÂêå‰ΩÜÁÆÄÂçïÁöÑÊ®°Âûã. ‰ªÖ‰ªÖ‰ΩøÁî®Á¶ªÁõÆÊ†áÁÇπ ÂæàËøëÁöÑËßÇÊµãÁÇπÊù•ÊãüÂêàËøô‰∏™ÁÆÄÂçïÁöÑÊ®°Âûã, ËøôÁßçÊñπÂºèÂæóÂà∞ÁöÑ‰º∞ËÆ°ÂáΩÊï∞ <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(X)</span><script type="math/tex">\hat{f}(X)</script></span> Âú® <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{p}</span><script type="math/tex">\mathbb{R}^{p}</script></span> ÊòØÂÖâÊªëÁöÑ. Ëøô‰∏™Â±Ä ÈÉ®ÂåñÂèØ‰ª•ÈÄöËøá‰∏Ä‰∏™Âä†ÊùÉÁöÑÂáΩÊï∞ÊàñËÄÖ Ê†∏ (kernel) ÂáΩÊï∞ <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, x_{i}\right)</span><script type="math/tex">K_{\lambda}\left(x_{0}, x_{i}\right)</script></span> Êù•ÂÆûÁé∞, Ê†∏ÂáΩÊï∞ÊòØÂü∫‰∫é <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> Âà∞ <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span> ÁöÑ Ë∑ùÁ¶ªËµã‰∫à‰∏Ä‰∏™ÊùÉÈáç. Ê†∏ <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}</span><script type="math/tex">K_{\lambda}</script></span> ‰∏ÄËà¨Âú∞ÈÄöËøáÂèÇÊï∞ <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> Êù•ÁºñÂè∑, ÂèÇÊï∞ <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> ËßÑÂÆö‰∫ÜÈÇªÂüüÁöÑÂÆΩÂ∫¶. ÂéüÂàô‰∏ä, Ëøô‰∫õ Âü∫‰∫éËÆ∞ÂøÜÊÄß (memory-based) ÁöÑÊñπÊ≥ïÈúÄË¶ÅÂæàÂ∞ëÊàñËÄÖ‰∏çÈúÄË¶ÅËÆ≠ÁªÉ; ÊâÄÊúâÁöÑÂ∑•‰ΩúÂú® ËµãÂÄº (evaluation) Èò∂ ÊÆµ‰æøÂÆåÊàê‰∫Ü. Ê†πÊçÆËÆ≠ÁªÉÈõÜÂîØ‰∏ÄÈúÄË¶ÅÁ°ÆÂÆöÁöÑÂèÇÊï∞ÊòØ <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>. ÁÑ∂ËÄå, ËØ•Ê®°ÂûãÊòØÊï¥‰∏™ËÆ≠ÁªÉÊï∞ÊçÆÈõÜ.</p>
<p>Êàë‰ª¨‰πüËÆ®ËÆ∫Êõ¥Âä†‰∏ÄËà¨Á±ªÂà´ÁöÑÂü∫‰∫éÊ†∏ÁöÑÊäÄÂ∑ß, ÂÆÉ‰ª¨‰∏éÂÖ∂‰ªñÁ´†ËäÇ‰∏≠ÁªìÊûÑÂåñÁöÑÊñπÊ≥ïËÅîÁ≥ªÂú®‰∏ÄËµ∑‰∫Ü, ËøôÂú®<strong>ÂØÜÂ∫¶‰º∞ËÆ°</strong>Âíå<strong>ÂàÜÁ±ª</strong>‰∏≠ÂæàÊúâÁî®.</p>
<p>Êú¨Á´†‰∏≠ÁöÑÊäÄÂ∑ß‰∏çÂ∫îËØ•‰∏éÊúÄËøë‰ΩøÁî®ÊúÄÂ§öÁöÑ‚ÄúÊ†∏ÊñπÊ≥ï‚ÄùÊ∑∑Ê∑Ü. ËøôÁ´†‰∏≠, Ê†∏Â§ßÂ§ö‰Ωú‰∏∫Â±ÄÈÉ®ÂåñÁöÑÂ∑•ÂÖ∑. Êàë‰ª¨Âú® <span class="arithmatex"><span class="MathJax_Preview">5.8</span><script type="math/tex">5.8</script></span> ËäÇ, <span class="arithmatex"><span class="MathJax_Preview">14.5 .4</span><script type="math/tex">14.5 .4</script></span> ËäÇ, <span class="arithmatex"><span class="MathJax_Preview">18.5</span><script type="math/tex">18.5</script></span> ËäÇÂíåÁ¨¨ 12 Á´†ËÆ®ËÆ∫Ê†∏ÊñπÊ≥ï; Âú®Ëøô‰∫õÈÉ®ÂàÜ, Ê†∏Âú®‰∏Ä‰∏™È´òÁª¥ÁöÑÔºàÈöêÂºèÁöÑÔºâÁâπÂæÅ Á©∫Èó¥‰∏≠ËÆ°ÁÆóÂÜÖÁßØ, ËÄå‰∏îË¢´Áî®‰∫éÊ≠£ËßÑÂåñÈùûÁ∫øÊÄßÂª∫Ê®°‰∏≠„ÄÇÊàë‰ª¨Â∞ÜÂú®Êú¨Á´†ÁöÑ <span class="arithmatex"><span class="MathJax_Preview">6.7</span><script type="math/tex">6.7</script></span> ËäÇÁöÑÊúÄÂêéÂ∞ÜËøô‰∫õÊñπÊ≥ïËÅîÁ≥ª Ëµ∑Êù•.</p>
<h3 id="1-dim-kernel-smoother">1-Dim Kernel Smoother<a class="headerlink" href="#1-dim-kernel-smoother" title="Permanent link">&para;</a></h3>
<p>Áî®KNNÊù•ÊòæÁ§∫ÊúÄÁõ¥ËßÇ. Â∑¶ÂõæÊòØÁõ¥Êé•Áî®KNNÁöÑÁªìÊûú, ÂèØËßÅÊãüÂêàÁöÑÂáΩÊï∞ÊòØÈùûËøûÁª≠ÁöÑ; Âè≥ÂõæÂ±ïÁ§∫‰∫ÜÈááÁî® Epanechnikov quadratic kernel ÁöÑÂπ≥ÊªëÁªìÊûú.</p>
<p><img alt="" src="../media/ASL-note2/2021-12-13-17-07-32.png" /></p>
<p>Kernel weighted avarage:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}\left(x_{0}\right)=\frac{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) y_{i}}{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)}
</div>
<script type="math/tex; mode=display">
\hat{f}\left(x_{0}\right)=\frac{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) y_{i}}{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)}
</script>
</div>
<p>with the Epanechnikov quadratic kernel</p>
<div class="arithmatex">
<div class="MathJax_Preview">
K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left|x-x_{0}\right|}{\lambda}\right)
</div>
<script type="math/tex; mode=display">
K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left|x-x_{0}\right|}{\lambda}\right)
</script>
</div>
<p>with</p>
<div class="arithmatex">
<div class="MathJax_Preview">
D(t)= \begin{cases}\frac{3}{4}\left(1-t^{2}\right), &amp; \text { if }|t| \leqslant 1 \\ 0, &amp; \text { otherwise }\end{cases}
</div>
<script type="math/tex; mode=display">
D(t)= \begin{cases}\frac{3}{4}\left(1-t^{2}\right), & \text { if }|t| \leqslant 1 \\ 0, & \text { otherwise }\end{cases}
</script>
</div>
<h4 id="kernel-weighted-varying-metric-window-size">Kernel Weighted - Varying Metric Window Size<a class="headerlink" href="#kernel-weighted-varying-metric-window-size" title="Permanent link">&para;</a></h4>
<p>The previous Epanechnikov kernel weighted average</p>
<ul>
<li>constant metric window size <span class="arithmatex"><span class="MathJax_Preview">\lambda=0.2</span><script type="math/tex">\lambda=0.2</script></span></li>
<li>does not change while <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span> moves</li>
<li>KNN adapts to the local density of <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span></li>
</ul>
<p>Kernel with adaptive neighborhood</p>
<div class="arithmatex">
<div class="MathJax_Preview">
K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left|x-x_{0}\right|}{h_{\lambda}\left(x_{0}\right)}\right)
</div>
<script type="math/tex; mode=display">
K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left|x-x_{0}\right|}{h_{\lambda}\left(x_{0}\right)}\right)
</script>
</div>
<ul>
<li>For KNN, <span class="arithmatex"><span class="MathJax_Preview">h_{k}\left(x_{0}\right)=\left|x_{0}-x_{[k]}\right|</span><script type="math/tex">h_{k}\left(x_{0}\right)=\left|x_{0}-x_{[k]}\right|</script></span></li>
<li>where <span class="arithmatex"><span class="MathJax_Preview">x_{[k]}</span><script type="math/tex">x_{[k]}</script></span> is the <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> closest <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> to <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span></li>
<li>Boundary issues with metric neighbourhood and KNN</li>
</ul>
<h4 id="popular-kernels-for-smoothing">Popular Kernels for Smoothing<a class="headerlink" href="#popular-kernels-for-smoothing" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}\left(x_{0}\right)=\frac{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) y_{i}}{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)} \quad K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left|x-x_{0}\right|}{\lambda}\right)
</div>
<script type="math/tex; mode=display">
\hat{f}\left(x_{0}\right)=\frac{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) y_{i}}{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)} \quad K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left|x-x_{0}\right|}{\lambda}\right)
</script>
</div>
<ul>
<li>Epanechnikov - Compact, non-differentiable at the boundary</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
D(t)= \begin{cases}\frac{3}{4}\left(1-t^{2}\right), &amp; \text { if }|t| \leqslant 1 \\ 0, &amp; \text { otherwise }\end{cases}
</div>
<script type="math/tex; mode=display">
D(t)= \begin{cases}\frac{3}{4}\left(1-t^{2}\right), & \text { if }|t| \leqslant 1 \\ 0, & \text { otherwise }\end{cases}
</script>
</div>
<ul>
<li>Tri-cube kernel - Compact, differentiable at the boudary</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
D(t)= \begin{cases}\left(1-|t|^{3}\right)^{3}, &amp; \text { if }|t| \leqslant 1 \\ 0, &amp; \text { otherwise }\end{cases}
</div>
<script type="math/tex; mode=display">
D(t)= \begin{cases}\left(1-|t|^{3}\right)^{3}, & \text { if }|t| \leqslant 1 \\ 0, & \text { otherwise }\end{cases}
</script>
</div>
<ul>
<li>Gaussian kernel - non-compact, continuously differentiable</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
D(t)=\phi(t)
</div>
<script type="math/tex; mode=display">
D(t)=\phi(t)
</script>
</div>
<p><img alt="" src="../media/ASL-note2/2021-12-13-17-16-57.png" /></p>
<h4 id="local-linear-regression">Local Linear Regression Â±ÄÈÉ®Á∫øÊÄßÂõûÂΩí<a class="headerlink" href="#local-linear-regression" title="Permanent link">&para;</a></h4>
<p>Boundary Problem: ‰∏äÈù¢ÊèêÂà∞ÁöÑ locally weighted average Â±ÄÈÉ®Âä†ÊùÉÂπ≥ÂùáÁöÑÊñπÊ≥ï, Âú®ËæπÁïå‰∏ä‰ºöÊúâÈóÆÈ¢ò</p>
<ul>
<li>Boundary problems occur due to asymmetry of the kernel</li>
<li>Can happen in the interior region if X is not equally spaced ‰∏ç‰ªÖ‰ºöÂèëÁîüÂú®ËæπÁïå‰∏ä‰∏ä, ËøòÊòØÁî±‰∫éÊ†∏ÁöÑ‰∏çÂØπÁß∞ÊÄß, ÂÖ∂Âú®Âå∫Èó¥ÂÜÖÈÉ®‰πüÂèØËÉΩÂèëÁîü</li>
</ul>
<p>Ëß£ÂÜ≥ÊñπÊ≥ï: Â±ÄÈÉ®Âä†ÊùÉÁ∫øÊÄßÂõûÂΩí ‚Äî‚Äî Â±ÄÈÉ®Âä†ÊùÉÁ∫øÊÄßÂõûÂΩí‰ºöÁ∫†Ê≠£‰∏∫‰∏ÄÈò∂ËØØÂ∑Æ.</p>
<p><img alt="" src="../media/ASL-note2/2021-12-13-17-19-48.png" /></p>
<p>Locally weighted regression solves a weighted least squares problem at each target point <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\min_{\alpha\left(x_{0}\right), \beta\left(x_{0}\right)} \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left[y_{i}-\alpha\left(x_{0}\right)-\beta\left(x_{0}\right) x_{i}\right]^{2}
</div>
<script type="math/tex; mode=display">
\min_{\alpha\left(x_{0}\right), \beta\left(x_{0}\right)} \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left[y_{i}-\alpha\left(x_{0}\right)-\beta\left(x_{0}\right) x_{i}\right]^{2}
</script>
</div>
<ul>
<li>Design matrix <span class="arithmatex"><span class="MathJax_Preview">B_{N \times 2}</span><script type="math/tex">B_{N \times 2}</script></span>, with <span class="arithmatex"><span class="MathJax_Preview">i_{t h}</span><script type="math/tex">i_{t h}</script></span> row <span class="arithmatex"><span class="MathJax_Preview">b(x)^{\top}=\left(1, x_{i}\right)</span><script type="math/tex">b(x)^{\top}=\left(1, x_{i}\right)</script></span></li>
<li>Weight matrix <span class="arithmatex"><span class="MathJax_Preview">W_{N \times N}\left(x_{0}\right)</span><script type="math/tex">W_{N \times N}\left(x_{0}\right)</script></span>, with <span class="arithmatex"><span class="MathJax_Preview">i_{t h}</span><script type="math/tex">i_{t h}</script></span> diagonal element <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, x_{i}\right)</span><script type="math/tex">K_{\lambda}\left(x_{0}, x_{i}\right)</script></span></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}\left(x_{0}\right)=b\left(x_{0}\right)^{\top}\left(B^{\top} W\left(x_{0}\right) B\right)^{-1} B^{\top} W\left(x_{0}\right) y=\sum_{i=1}^{N} l_{i}\left(x_{0}\right) y_{i}
</div>
<script type="math/tex; mode=display">
\hat{f}\left(x_{0}\right)=b\left(x_{0}\right)^{\top}\left(B^{\top} W\left(x_{0}\right) B\right)^{-1} B^{\top} W\left(x_{0}\right) y=\sum_{i=1}^{N} l_{i}\left(x_{0}\right) y_{i}
</script>
</div>
<ul>
<li>Estimate is linear in <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">l_{i}\left(x_{0}\right)</span><script type="math/tex">l_{i}\left(x_{0}\right)</script></span> combines the weighting kernel <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, \cdot\right)</span><script type="math/tex">K_{\lambda}\left(x_{0}, \cdot\right)</script></span> and least square operation, also referred to as <strong>equivalent kernel</strong> Á≠â‰ª∑Ê†∏</li>
</ul>
<p><img alt="" src="../media/ASL-note2/2021-12-13-17-27-34.png" /></p>
<p><strong>Automatic Kernel Carpentry</strong> Â±ÄÈÉ®Á∫øÊÄßÂõûÂΩíËá™Âä®Âú∞‰øÆÊîπÊ†∏Â∞ÜÂÅèÂ∑ÆÁü´Ê≠£Âà∞ÊÅ∞Â•Ω‰∏∫‰∏ÄÈò∂ÔºåËøôÊòØË¢´Áß∞‰∏∫ Ëá™Âä®Ê†∏‰ΩúÂìÅ (automatic kernel carpentry) ÁöÑÁé∞Ë±°</p>
<p>Local linear regression automatically modifies the kernel to correct bias exactly to first order</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
E\left(\hat{f}\left(x_{0}\right)\right)=\sum_{i=1}^{N} l_{i}\left(x_{0}\right) f\left(x_{i}\right)= \\
f\left(x_{0}\right) \sum_{i=1}^{N} l_{i}\left(x_{0}\right)+f^{\prime}\left(x_{0}\right) \sum_{i=1}^{N}\left(x_{i}-x_{0}\right) l_{i}\left(x_{0}\right)+\frac{f^{\prime \prime}\left(x_{0}\right.}{2} \sum_{i=1}^{N}\left(x_{i}-x_{0}\right)^{2} l_{i}\left(x_{0}\right)+R
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
E\left(\hat{f}\left(x_{0}\right)\right)=\sum_{i=1}^{N} l_{i}\left(x_{0}\right) f\left(x_{i}\right)= \\
f\left(x_{0}\right) \sum_{i=1}^{N} l_{i}\left(x_{0}\right)+f^{\prime}\left(x_{0}\right) \sum_{i=1}^{N}\left(x_{i}-x_{0}\right) l_{i}\left(x_{0}\right)+\frac{f^{\prime \prime}\left(x_{0}\right.}{2} \sum_{i=1}^{N}\left(x_{i}-x_{0}\right)^{2} l_{i}\left(x_{0}\right)+R
\end{gathered}
</script>
</div>
<p>It can be shown that for local linear regression (ËØÅÊòéËßÅ <a href="https://github.com/szcf-weiya/ESL-CN/issues/148">here</a>; ‰ΩÜÊõ¥‰∏∫ÁÆÄÂçïÁöÑ‰∫ãÊòØËßÇÂØü <span class="arithmatex"><span class="MathJax_Preview">L^TB = b(x_0)</span><script type="math/tex">L^TB = b(x_0)</script></span>, ‰πüÂç≥ <span class="arithmatex"><span class="MathJax_Preview">L^T(1,x)=(1,x_0)</span><script type="math/tex">L^T(1,x)=(1,x_0)</script></span>)</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\sum_{i=1}^{N} l_{i}\left(x_{0}\right)=1</span><script type="math/tex">\sum_{i=1}^{N} l_{i}\left(x_{0}\right)=1</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\sum_{i=1}^{N}\left(x_{i}-x_{0}\right) l_{i}\left(x_{0}\right)=0</span><script type="math/tex">\sum_{i=1}^{N}\left(x_{i}-x_{0}\right) l_{i}\left(x_{0}\right)=0</script></span></li>
</ul>
<p>Therefore, the bias only depends on quadratic and higher older terms</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">E \hat{f}\left(x_{0}\right)-f\left(x_{0}\right)=\frac{f^{\prime \prime}(x)}{2} \sum_{i=1}^{N}\left(x_{i}-x_{0}\right)^{2} l_{i}\left(x_{0}\right)+R</span><script type="math/tex">E \hat{f}\left(x_{0}\right)-f\left(x_{0}\right)=\frac{f^{\prime \prime}(x)}{2} \sum_{i=1}^{N}\left(x_{i}-x_{0}\right)^{2} l_{i}\left(x_{0}\right)+R</script></span></li>
<li>Typically small under suitable smoothness assumption</li>
</ul>
<h4 id="local-polynomial-regression">Local Polynomial Regression<a class="headerlink" href="#local-polynomial-regression" title="Permanent link">&para;</a></h4>
<p>Local linear fits bias: trimming the hills and filling the valleys</p>
<p>Local polynomial regression of degree-d</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\min _{\alpha\left(x_{0}\right), \beta_{j}\left(x_{0}\right), j=1, \ldots, d} \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left[y_{i}-\alpha\left(x_{0}\right)-\sum_{j=1}^{d} \beta_{j}\left(x_{0}\right) x_{i}^{j}\right]^{2}
</div>
<script type="math/tex; mode=display">
\min _{\alpha\left(x_{0}\right), \beta_{j}\left(x_{0}\right), j=1, \ldots, d} \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left[y_{i}-\alpha\left(x_{0}\right)-\sum_{j=1}^{d} \beta_{j}\left(x_{0}\right) x_{i}^{j}\right]^{2}
</script>
</div>
<ul>
<li>Able to correct the bias in the regions of curvature</li>
<li>But with increased variance (bias-variance tradeoff)</li>
<li>ÊñπÂ∑Æ <span class="arithmatex"><span class="MathJax_Preview">\left\|l\left(x_{0}\right)\right\|^2</span><script type="math/tex">\left\|l\left(x_{0}\right)\right\|^2</script></span> increase with d</li>
</ul>
<p>Whether or not to choose local quadratic regression</p>
<ul>
<li>If interested in <strong>extrapolation</strong> (boundary) <span class="arithmatex"><span class="MathJax_Preview">\rightarrow</span><script type="math/tex">\rightarrow</script></span> local linear</li>
</ul>
<p><img alt="" src="../media/ASL-note2/2021-12-13-18-22-14.png" /></p>
<h3 id="local-regression-in-rprp">Local Regression in <span class="arithmatex"><span class="MathJax_Preview">R^p</span><script type="math/tex">R^p</script></span><a class="headerlink" href="#local-regression-in-rprp" title="Permanent link">&para;</a></h3>
<p>Ê†∏ÂÖâÊªëÂíåÂ±ÄÈÉ®ÂõûÂΩíÂèØ‰ª•ÈùûÂ∏∏Ëá™ÁÑ∂Âú∞Êé®ÂπøÂà∞‰∫åÁª¥ÊàñÊõ¥È´òÁª¥Á©∫Èó¥‰∏≠Ôºé</p>
<p>Convert distance based kernel to radius based kernel</p>
<div class="arithmatex">
<div class="MathJax_Preview">
K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left\|x-x_{0}\right\|}{\lambda}\right)
</div>
<script type="math/tex; mode=display">
K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left\|x-x_{0}\right\|}{\lambda}\right)
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\|\cdot\|</span><script type="math/tex">\|\cdot\|</script></span> is the Euclidean norm</li>
<li>Since the Euclidean norm depends on the units in each coordinate, need to <strong>standardize</strong> each <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span></li>
</ul>
<h4 id="problems-with-local-regression-in-mathbbrpmathbbrp">Problems with local regression in <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{p}</span><script type="math/tex">\mathbb{R}^{p}</script></span><a class="headerlink" href="#problems-with-local-regression-in-mathbbrpmathbbrp" title="Permanent link">&para;</a></h4>
<p>Boundary problem</p>
<ul>
<li>More and more points can be found on the boundary as dimension increases</li>
<li>Áõ¥Êé•‰øÆÊîπÊ†∏Êù•ÈÄÇÂ∫î‰∫åÁª¥ËæπÁïå‰ºöÂèòÂæóÂæàÂ§çÊùÇÔºåÁâπÂà´ÊòØÂØπ‰∫é‰∏çËßÑÂàôÁöÑËæπÁïå</li>
<li>Local polynomial regression still helps automatically deal with boundary issues for high dimensions (but not desired)</li>
</ul>
<p>Curse of Dimensionality</p>
<ul>
<li>Impossible to simultaneously maintain localness (low bias) and a sizable sample in the neighbourhood (low variance) without total sample size increasing exponentially in <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span></li>
</ul>
<p>Non-visualizable</p>
<ul>
<li>Goal of getting a smooth fitting function is to visualize the data which is difficult in high dimensions</li>
<li>‰∏çÂ§üÁõ¥ËßÇ, it is quite diÔ¨Écult to interpret the results except at a gross level. From a data analysis perspective, conditional plots are far more useful. ËøôÈáåÁöÑ conditional plots ÊåáÁöÑÂ∞±ÊòØ‰∏ãÈù¢ÁöÑ‰æãÂ≠ê.</li>
</ul>
<p>Ëá≠Ê∞ßÊµìÂ∫¶ÔΩûÂ§™Èò≥ËæêÂ∞Ñ„ÄÅÊ∏©Â∫¶„ÄÅÈ£éÈÄü. ÂÖ±Êúâ‰∏â‰∏™ÂèòÈáè, ÊéßÂà∂ Wind Âíå Temp ÁöÑÊÉÖÂÜµ‰∏ãËøõË°åÂçïÂèòÈáèÁöÑÂõûÂΩí.</p>
<p><img alt="" src="../media/ASL-note2/2021-12-13-18-35-11.png" /></p>
<h3 id="structured-local-regression-models-in-rprp">Structured Local Regression Models in <span class="arithmatex"><span class="MathJax_Preview">R^p</span><script type="math/tex">R^p</script></span><a class="headerlink" href="#structured-local-regression-models-in-rprp" title="Permanent link">&para;</a></h3>
<p>ÂΩìÁª¥Â∫¶‰∏éÊ†∑Êú¨Â§ßÂ∞èÁöÑÊØîÁéá‰∏çÊòØÂæàÂ•ΩÔºåÂàôÂ±ÄÈÉ®ÂõûÂΩíÂØπÊàë‰ª¨Ê≤°ÊúâÂ§™Â§ßÂ∏ÆÂä©ÔºåÈô§ÈùûÊàë‰ª¨ÊÉ≥Ë¶ÅÂØπÊ®°ÂûãÂÅöÂá∫‰∏Ä‰∫õÁªìÊûÑÂåñÁöÑÂÅáËÆæÔºéËøôÊú¨‰π¶ÁöÑÂæàÂ§öÈÉ®ÂàÜÊòØÂÖ≥‰∫éÁªìÊûÑÂåñÂõûÂΩíÂíåÂàÜÁ±ªÊ®°ÂûãÁöÑÔºéËøôÈáåÊàë‰ª¨ÂÖ≥Ê≥®‰∏Ä‰∫õ‰∏éÊ†∏ÊñπÊ≥ïÁõ¥Êé•Áõ∏ÂÖ≥ÁöÑÊñπÊ≥ïÔºé</p>
<p><strong>Sperical kernel</strong> (ÁêÉÈù¢Ê†∏, Â∞±ÊòØÊ≤°Êúâ‰∏≠Èó¥ÁöÑÂèÇÊï∞Áü©Èòµ A) gives equal weight to each coordinate, while a structured kernel emphasizes weight on certain coordinates using matrix <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
K_{\lambda, A}\left(x_{0}, x\right)=D\left(\frac{\left(x-x_{0}\right)^{\top} A\left(x-x_{0}\right)}{\lambda}\right)
</div>
<script type="math/tex; mode=display">
K_{\lambda, A}\left(x_{0}, x\right)=D\left(\frac{\left(x-x_{0}\right)^{\top} A\left(x-x_{0}\right)}{\lambda}\right)
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> is positive semidefinite ÂçäÊ≠£ÂÆöÁü©Èòµ</li>
<li>Entire coordinates or directions can be downgraded or omitted by imposing appropriate restrictions on <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>.</li>
<li>A - diagonal?</li>
<li>A - low rank?</li>
<li>General forms of <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> not recommended</li>
</ul>
<p>Áõ∏ËæÉ‰∫é‰∏çÂä†ÈôêÂÆöÁöÑ‰ªªÊÑèÂèÇÊï∞Áü©Èòµ, ÂèØ‰ª•ÂØπ A ËøõË°å‰∏ÄÂÆöÁöÑÈôêÂà∂.</p>
<h4 id="structured-regression-functions">Structured Regression Functions ÁªìÊûÑÂõûÂΩíÂáΩÊï∞<a class="headerlink" href="#structured-regression-functions" title="Permanent link">&para;</a></h4>
<p>Suppose now we are trying to fit a regression function <span class="arithmatex"><span class="MathJax_Preview">E(Y \mid X)=f\left(X_{1}, X_{2}, \ldots, X_{p}\right)</span><script type="math/tex">E(Y \mid X)=f\left(X_{1}, X_{2}, \ldots, X_{p}\right)</script></span> in <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{p}</span><script type="math/tex">\mathbb{R}^{p}</script></span>, in which every level of interaction is potentially present</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f\left(X_{1}, X_{2}, \ldots, X_{p}\right)=\alpha+\sum_{j} g_{j}\left(X_{j}\right)+\sum_{k&lt;l} g_{k l}\left(X_{k}, X_{l}\right)+\ldots
</div>
<script type="math/tex; mode=display">
f\left(X_{1}, X_{2}, \ldots, X_{p}\right)=\alpha+\sum_{j} g_{j}\left(X_{j}\right)+\sum_{k<l} g_{k l}\left(X_{k}, X_{l}\right)+\ldots
</script>
</div>
<p>Eliminate some higher-order terms</p>
<ul>
<li>Additive model (only main effects) ÂøΩÁï•‰∫§ÂèâÈ°π</li>
<li>Second order model (interactions up to degree 2)</li>
</ul>
<p>Estimate low order models - <strong>Iterative backfitting</strong> algorithms Á¨¨ 9 Á´†‰∏≠ÔºåÊàë‰ª¨ÊèèËø∞‰∫ÜÂØπ‰∫éÊãüÂêàËøôÊ†∑‰ΩéÈò∂‰∫§ÂèâÊ®°ÂûãÁöÑ Ëø≠‰ª£ÂêëÂêéÊãüÂêà (iterative backfiting) ÁÆóÊ≥ï</p>
<ul>
<li>Assume all but the <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> term is known</li>
<li>Fit local regression of <span class="arithmatex"><span class="MathJax_Preview">Y-\sum_{j \neq k} g_{j}\left(X_{j}\right)</span><script type="math/tex">Y-\sum_{j \neq k} g_{j}\left(X_{j}\right)</script></span> on <span class="arithmatex"><span class="MathJax_Preview">X_{k}</span><script type="math/tex">X_{k}</script></span></li>
<li>Repeat for each variable until convergence</li>
</ul>
<h4 id="varying-coefficient-models">Varying Coefficient Models ÂèØÂèòÂèÇÊï∞Ê®°Âûã<a class="headerlink" href="#varying-coefficient-models" title="Permanent link">&para;</a></h4>
<p>Suppose <span class="arithmatex"><span class="MathJax_Preview">X \in \mathbb{R}^{p}</span><script type="math/tex">X \in \mathbb{R}^{p}</script></span>, divide <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> into <span class="arithmatex"><span class="MathJax_Preview">\left(X_{1}, X_{2}, \ldots, X_{q}\right)</span><script type="math/tex">\left(X_{1}, X_{2}, \ldots, X_{q}\right)</script></span> with <span class="arithmatex"><span class="MathJax_Preview">q&lt;p</span><script type="math/tex">q<p</script></span> and the remainder collected in vector <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
f(X)=\alpha(Z)+\beta_{1}(Z) X_{1}+\ldots+\beta_{q}(Z) X_{q}
</div>
<script type="math/tex; mode=display">
f(X)=\alpha(Z)+\beta_{1}(Z) X_{1}+\ldots+\beta_{q}(Z) X_{q}
</script>
</div>
<ul>
<li>Given <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>, model is linear in <span class="arithmatex"><span class="MathJax_Preview">X_{1}, \ldots, X_{q}</span><script type="math/tex">X_{1}, \ldots, X_{q}</script></span></li>
<li>Each coefficient <span class="arithmatex"><span class="MathJax_Preview">\beta_{1}(Z), \ldots, \beta_{q}(Z)</span><script type="math/tex">\beta_{1}(Z), \ldots, \beta_{q}(Z)</script></span> vary with <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span></li>
</ul>
<p>Estimate varying coefficient models - <strong>locally weighted least squares</strong> ÈÄöËøáÂ±ÄÈÉ®Âä†ÊùÉÊúÄÂ∞è‰∫å‰πòÊãüÂêàËøô‰∏™Ê®°Âûã</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\min _{\alpha\left(z_{0}\right), \beta\left(z_{0}\right)} \sum_{i=1}^{N} K_{\lambda}\left(z_{0}, z_{i}\right)\left(y_{i}-\alpha\left(z_{0}\right)-x_{1 i} \beta_{1}\left(z_{0}\right)-\ldots-x_{q i} \beta_{q}\left(z_{0}\right)\right)^{2}
</div>
<script type="math/tex; mode=display">
\min _{\alpha\left(z_{0}\right), \beta\left(z_{0}\right)} \sum_{i=1}^{N} K_{\lambda}\left(z_{0}, z_{i}\right)\left(y_{i}-\alpha\left(z_{0}\right)-x_{1 i} \beta_{1}\left(z_{0}\right)-\ldots-x_{q i} \beta_{q}\left(z_{0}\right)\right)^{2}
</script>
</div>
<p>‰æãÂ≠ê: ‰∏ªÂä®ËÑâÂçäÂæÑÔΩûÂπ¥ÈæÑ; Áõ∏ÂÖ≥ÁöÑÂèòÈáèÊòØÊÄßÂà´ÂíåÂä®ËÑâË∑ùÁ¶ªÁöÑËøúËøë. Âú®<strong>ÂèØÂèòÁ≥ªÊï∞Ê®°Âûã</strong>‰∏≠, ËØ•‰æãÂ≠êÁöÑÂØπÂ∫îÂÖ≥Á≥ª‰∏∫ X - (age, gender, depth) , Y - aorta diameter, Z - (gender, depth)</p>
<p><img alt="" src="../media/ASL-note2/2021-12-13-18-56-53.png" /></p>
<p>Ê†πÊçÆ‰∏äÈù¢ÁöÑÂõæ, ÂèØ‰ª•ÁúãÂà∞‰∏ªÂä®ËÑâÂçäÂæÑÁ°ÆÂÆûÈöèÁùÄÂπ¥ÈæÑÁöÑÂ¢ûÈïøËÄåÂèòÂéö; ËÄå‰ªé‰∏çÂêå depth ÁöÑËÆæÁΩÆ‰∏ä, ÂèØ‰ª•ÁúãÂà∞ the relationship fades with distance down the aorta(Á≥ªÊï∞ÂèòÂ∞è). ‰∏ãÈù¢ÁöÑÂõæÁõ¥Êé•ÁîªÂá∫‰∫ÜÁ≥ªÊï∞ÁöÑÂèòÂåñ</p>
<p><img alt="" src="../media/ASL-note2/2021-12-13-19-02-47.png" /></p>
<h3 id="local-likelihood-other-models">Local likelihood &amp; Other Models<a class="headerlink" href="#local-likelihood-other-models" title="Permanent link">&para;</a></h3>
<h4 id="local-likelihood">Local Likelihood<a class="headerlink" href="#local-likelihood" title="Permanent link">&para;</a></h4>
<p>Likelihood - (generalized) linear model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
l(\beta)=\sum_{i=1}^{N} l \left(y_{i}, x_{i}^{\top} \beta\right)
</div>
<script type="math/tex; mode=display">
l(\beta)=\sum_{i=1}^{N} l \left(y_{i}, x_{i}^{\top} \beta\right)
</script>
</div>
<p>Local likelihood - relax the global linear assumption to local linear</p>
<div class="arithmatex">
<div class="MathJax_Preview">
l\left(\beta\left(x_{0}\right)\right)=\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) l \left(y_{i}, x_{i}^{\top} \beta\left(x_{0}\right)\right)
</div>
<script type="math/tex; mode=display">
l\left(\beta\left(x_{0}\right)\right)=\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) l \left(y_{i}, x_{i}^{\top} \beta\left(x_{0}\right)\right)
</script>
</div>
<h4 id="example-multiclass-logistic-regression-model">Example: Multiclass logistic regression model<a class="headerlink" href="#example-multiclass-logistic-regression-model" title="Permanent link">&para;</a></h4>
<p>Multiclass logistic regression model, <span class="arithmatex"><span class="MathJax_Preview">\mathcal{G}=(1,2, \ldots, J)</span><script type="math/tex">\mathcal{G}=(1,2, \ldots, J)</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{Pr}(G=j \mid X=x)=\frac{e^{\beta_{j 0}+\beta_{j}^{\top} x}}{1+\sum_{k=1}^{J-1} e^{\beta_{k 0}+\beta_{k}^{\top} x}}
</div>
<script type="math/tex; mode=display">
\operatorname{Pr}(G=j \mid X=x)=\frac{e^{\beta_{j 0}+\beta_{j}^{\top} x}}{1+\sum_{k=1}^{J-1} e^{\beta_{k 0}+\beta_{k}^{\top} x}}
</script>
</div>
<p>The <strong>local version</strong> of multiclass logistic regression ‰øÆÊîπ‰∏∫Â±ÄÈÉ®ÂΩ¢ÂºèÁöÑ‰ººÁÑ∂ÂáΩÊï∞</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
l \left(\beta\left(x_{0}\right)\right)=&amp; \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left\{\beta_{g_{i}, 0}\left(x_{0}\right)+\beta_{g_{i}}\left(x_{0}\right)^{\top}\left(x_{i}-x_{0}\right)\right.\\
&amp;\left.-\log \left[1+\sum_{k=1}^{J-1} \exp \left(\beta_{k, 0}\left(x_{0}\right)+\beta_{k}\left(x_{0}\right)^{\top}\left(x_{i}-x_{0}\right)\right)\right]\right\}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
l \left(\beta\left(x_{0}\right)\right)=& \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left\{\beta_{g_{i}, 0}\left(x_{0}\right)+\beta_{g_{i}}\left(x_{0}\right)^{\top}\left(x_{i}-x_{0}\right)\right.\\
&\left.-\log \left[1+\sum_{k=1}^{J-1} \exp \left(\beta_{k, 0}\left(x_{0}\right)+\beta_{k}\left(x_{0}\right)^{\top}\left(x_{i}-x_{0}\right)\right)\right]\right\}
\end{aligned}
</script>
</div>
<p>The fitted posterior probabilities</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{Pr}\left(G=j \mid X=x_{0}\right)=\frac{e^{\hat{\beta}_{j 0}\left(x_{0}\right)}}{1+\sum_{k=1}^{j-1} e^{\hat{\beta}_{k 0}\left(x_{0}\right)}}
</div>
<script type="math/tex; mode=display">
\operatorname{Pr}\left(G=j \mid X=x_{0}\right)=\frac{e^{\hat{\beta}_{j 0}\left(x_{0}\right)}}{1+\sum_{k=1}^{j-1} e^{\hat{\beta}_{k 0}\left(x_{0}\right)}}
</script>
</div>
<p><img alt="" src="../media/ASL-note2/2021-12-13-19-26-30.png" /></p>
<h3 id="kernel-density-estimation-classification">Kernel Density Estimation &amp; ClassiÔ¨Åcation<a class="headerlink" href="#kernel-density-estimation-classification" title="Permanent link">&para;</a></h3>
<h4 id="kernel-density-estimation">Kernel Density Estimation<a class="headerlink" href="#kernel-density-estimation" title="Permanent link">&para;</a></h4>
<p><strong>Empirical local estimate</strong> - bumpy
$$
\hat{f}<em 0="0">{X}\left(x</em>
$$}\right)=\frac{# x_{i} \in \mathcal{N}\left(x_{0}\right)}{N{\lambda}</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathcal{N}\left(x_{0}\right)</span><script type="math/tex">\mathcal{N}\left(x_{0}\right)</script></span> - small metric neighbourhood around <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span> of width <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span></li>
</ul>
<p><strong>Parzen estimate</strong> - smooth
$$
\hat{f}<em 0="0">{X}\left(x</em>\right)
$$}\right)=\frac{1}{N{\lambda}} \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i</p>
<ul>
<li>Counts observations with weights that decrease with distance</li>
<li>Choice of <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}</span><script type="math/tex">K_{\lambda}</script></span> : Gaussian Kernel <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, x\right)=\phi\left(\frac{\left|x-x_{0}\right|}{\lambda}\right)</span><script type="math/tex">K_{\lambda}\left(x_{0}, x\right)=\phi\left(\frac{\left|x-x_{0}\right|}{\lambda}\right)</script></span></li>
</ul>
<h4 id="gaussian-kernel-density-estimate">Gaussian kernel density estimate<a class="headerlink" href="#gaussian-kernel-density-estimate" title="Permanent link">&para;</a></h4>
<p>ÂèÇËßÅ <a href="https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/">https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/</a></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}_{X}(x)=\frac{1}{N} \sum_{i=1}^{N} \phi_{\lambda}\left(x-x_{i}\right)=\left(\hat{F} * \phi_{\lambda}\right)(x)
</div>
<script type="math/tex; mode=display">
\hat{f}_{X}(x)=\frac{1}{N} \sum_{i=1}^{N} \phi_{\lambda}\left(x-x_{i}\right)=\left(\hat{F} * \phi_{\lambda}\right)(x)
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\phi_{\lambda}</span><script type="math/tex">\phi_{\lambda}</script></span> denotes Gaussian density with mean 0 and standard deviation <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span></li>
<li>Convolution of the sample empirical distribution <span class="arithmatex"><span class="MathJax_Preview">\hat{F}</span><script type="math/tex">\hat{F}</script></span> (jumpy) with <span class="arithmatex"><span class="MathJax_Preview">\phi_{\lambda}(</span><script type="math/tex">\phi_{\lambda}(</script></span> smooth <span class="arithmatex"><span class="MathJax_Preview">)</span><script type="math/tex">)</script></span></li>
</ul>
<p>Gaussian kernel density estimate in <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{p}</span><script type="math/tex">\mathbb{R}^{p}</script></span> Âú®È´òÈò∂Á©∫Èó¥‰∏≠ÁöÑËá™ÁÑ∂Êé®Âπø
$$
\hat{f}<em 0="0">{X}\left(x</em>
$$}\right)=\frac{1}{N\left(2 \lambda^{2} \pi\right)^{\frac{p}{2}}} \sum_{i=1}^{N} e^{-\frac{1}{2}\left(\left|x_{i}-x_{0}\right| / \lambda\right)^{2}</p>
<p><img alt="" src="../media/ASL-note2/2021-12-13-19-36-47.png" /></p>
<h4 id="convolution">ÂÖ≥‰∫éÂç∑ÁßØ Convolution<a class="headerlink" href="#convolution" title="Permanent link">&para;</a></h4>
<p>ÊåâÁÖßÂç∑ÁßØ [https://en.wikipedia.org/wiki/Convolution]ÁöÑÂÆö‰πâÔºå</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\left(\hat{F} \star \phi_{\lambda}\right)(x)=\sum_{i=1}^{n} \hat{F}\left(x_{i}\right) \phi_{\lambda}\left(x-x_{i}\right)=\frac{1}{N} \sum_{i=1}^{n} \phi_{\lambda}\left(x-x_{i}\right)
</div>
<script type="math/tex; mode=display">
\left(\hat{F} \star \phi_{\lambda}\right)(x)=\sum_{i=1}^{n} \hat{F}\left(x_{i}\right) \phi_{\lambda}\left(x-x_{i}\right)=\frac{1}{N} \sum_{i=1}^{n} \phi_{\lambda}\left(x-x_{i}\right)
</script>
</div>
<p>Âè¶Â§ñÔºå‰∏§‰∏™Áã¨Á´ãÈöèÊú∫ÂèòÈáèÂíåÁöÑÂàÜÂ∏É‰πüÂèØ‰ª•Ë°®Á§∫‰∏∫Âç∑ÁßØÂΩ¢Âºè. ÂÖ∑‰ΩìÂú∞ÔºåÂÅáËÆæ <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> Âíå <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> Áã¨Á´ãÔºåÂàÜÂ∏ÉÂáΩ Êï∞ÂàÜÂà´‰∏∫ <span class="arithmatex"><span class="MathJax_Preview">F(x)=P(X \leq x), G(y)=P(Y \leq y)</span><script type="math/tex">F(x)=P(X \leq x), G(y)=P(Y \leq y)</script></span> ÔºåÂàô <span class="arithmatex"><span class="MathJax_Preview">X+Y</span><script type="math/tex">X+Y</script></span> ÁöÑÂàÜÂ∏ÉÂáΩÊï∞‰∏∫</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P(X+Y \leq z)=\int F(z-y) d G(y)=F * G(z) .
</div>
<script type="math/tex; mode=display">
P(X+Y \leq z)=\int F(z-y) d G(y)=F * G(z) .
</script>
</div>
<h4 id="kernel-density-classification">Kernel Density Classification<a class="headerlink" href="#kernel-density-classification" title="Permanent link">&para;</a></h4>
<p>For a J class problem, suppose we can fit nonparametric density estimates <span class="arithmatex"><span class="MathJax_Preview">\hat{f}_{j}(X)</span><script type="math/tex">\hat{f}_{j}(X)</script></span> separately for each class, and we also have estimates of the class priors <span class="arithmatex"><span class="MathJax_Preview">\hat{\pi}_{j}</span><script type="math/tex">\hat{\pi}_{j}</script></span></p>
<p>Then by Bayes Theorem, we can derive the class posterior probabilities</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\operatorname{Pr}}\left(G=j \mid X=x_{0}\right)=\frac{\hat{\pi}_{j} \hat{f}_{j}\left(x_{0}\right)}{\sum_{k=1}^{j} \hat{n}_{k} \hat{t}_{k}\left(x_{0}\right)}
</div>
<script type="math/tex; mode=display">
\hat{\operatorname{Pr}}\left(G=j \mid X=x_{0}\right)=\frac{\hat{\pi}_{j} \hat{f}_{j}\left(x_{0}\right)}{\sum_{k=1}^{j} \hat{n}_{k} \hat{t}_{k}\left(x_{0}\right)}
</script>
</div>
<p>If classification is the ultimate goal</p>
<ul>
<li>Learning the separate class densities well may be unnecessary or misleading</li>
<li>Need only estimate the posterior well near the decision boundary</li>
</ul>
<p><img alt="" src="../media/ASL-note2/2021-12-13-20-31-24.png" /></p>
<p>‰∏ªË¶ÅÁöÑÂ∑ÆÂºÇÂá∫Áé∞Âú®Âõæ 6.14 ‰∏≠Âè≥ÂõæÁöÑÈ´ò SBP Âå∫ÂüüÔºéËøô‰∏™Âå∫Âüü‰∏≠ÂØπ‰∫é‰∏§‰∏™Á±ªÂà´ÁöÑÊï∞ÊçÆÈÉΩÊòØÁ®ÄÁñèÁöÑÔºåÂπ∂‰∏îÂõ†‰∏∫È´òÊñØÊ†∏ÂØÜÂ∫¶‰º∞ËÆ°ÈááÁî®Â∫¶ÈáèÊ†∏ÔºåÊâÄ‰ª•ÂØÜÂ∫¶‰º∞ËÆ°Âú®ÂÖ∂‰ªñÂå∫Âüü‰∏≠ÊïàÊûúÂæàÂ∑ÆÔºàÈ´òÊñπÂ∑ÆÔºâÔºéÂ±ÄÈÉ®ÈÄªËæëÊñØËíÇÂõûÂΩíÊñπÊ≥ï Ôºà6.20ÔºâÈááÁî® k-NN Â∏¶ÂÆΩÁöÑ‰∏âÊ¨°Á´ãÊñπÊ†∏ÔºõËøôÊúâÊïàÂú∞ÊãìÂÆΩ‰∫ÜËøô‰∏™Âå∫Âüü‰∏≠ÁöÑÊ†∏ÔºåÂπ∂‰∏îÂà© Áî®Â±ÄÈÉ®Á∫øÊÄßÂÅáËÆæÊù•ÂØπ‰º∞ËÆ°ËøõË°åÂÖâÊªëÔºàÂú®ÈÄªËæëÊñØËíÇÂ∞∫Â∫¶‰∏äÔºâÔºé</p>
<h4 id="naive-bayes-classifier">Naive Bayes Classifier<a class="headerlink" href="#naive-bayes-classifier" title="Permanent link">&para;</a></h4>
<p>Assumes that given class <span class="arithmatex"><span class="MathJax_Preview">G=j</span><script type="math/tex">G=j</script></span>, the features <span class="arithmatex"><span class="MathJax_Preview">X_{k}</span><script type="math/tex">X_{k}</script></span> are independent</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f_{j}(X)=\prod_{k=1}^{p} f_{j k}\left(X_{k}\right)
</div>
<script type="math/tex; mode=display">
f_{j}(X)=\prod_{k=1}^{p} f_{j k}\left(X_{k}\right)
</script>
</div>
<ul>
<li>Useful when <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> is large, when density estimation is unattractive</li>
<li>Each <span class="arithmatex"><span class="MathJax_Preview">f_{j k}</span><script type="math/tex">f_{j k}</script></span> can be estimated using 1-dim kernel density estimates Áî®‰∏ÄÁª¥Ê†∏ÂØÜÂ∫¶‰º∞ËÆ°ÂçïÁã¨ÁöÑÁ±ªÂà´Êù°‰ª∂ÁöÑËæπÁºòÂØÜÂ∫¶</li>
<li>If <span class="arithmatex"><span class="MathJax_Preview">X_{j}</span><script type="math/tex">X_{j}</script></span> is discrete, can be estimated by appropriate histogram</li>
<li>Sometimes outperform far more sophisticated models</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
\log \frac{\operatorname{Pr}(G=I \mid X)}{\operatorname{Pr}(G=J \mid X)} &amp;=\log \frac{\pi_{l} f_{l}(X)}{\pi_{J} f_{J}(X)}=\log \frac{\pi_{l} \prod_{k=1}^{p} f_{l k}\left(X_{k}\right)}{\pi_{J} \prod_{k=1}^{p} f_{J k}\left(X_{k}\right)} \\
&amp;=\log \frac{\pi_{l}}{\pi_{J}}+\sum_{k=1}^{p} \log \frac{f_{l k}\left(X_{k}\right)}{f_{J k}\left(X_{k}\right)}=\alpha_{l}+\sum_{k=1}^{p} g_{l k}\left(X_{k}\right)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\log \frac{\operatorname{Pr}(G=I \mid X)}{\operatorname{Pr}(G=J \mid X)} &=\log \frac{\pi_{l} f_{l}(X)}{\pi_{J} f_{J}(X)}=\log \frac{\pi_{l} \prod_{k=1}^{p} f_{l k}\left(X_{k}\right)}{\pi_{J} \prod_{k=1}^{p} f_{J k}\left(X_{k}\right)} \\
&=\log \frac{\pi_{l}}{\pi_{J}}+\sum_{k=1}^{p} \log \frac{f_{l k}\left(X_{k}\right)}{f_{J k}\left(X_{k}\right)}=\alpha_{l}+\sum_{k=1}^{p} g_{l k}\left(X_{k}\right)
\end{aligned}
</script>
</div>
<ul>
<li>ËøôÊúâÂπø‰πâÂä†ÊÄßÊ®°ÂûãÁöÑÂΩ¢ÂºèÔºåÊõ¥Â§öÁªÜËäÇÂ∞ÜÂú® Á¨¨ 9 Á´†ÊèèËø∞. ÂèÇËßÅ <a href="https://github.com/szcf-weiya/ESL-CN/issues/188">here</a></li>
<li>Êú¥Á¥†Ë¥ùÂè∂ÊñØÂíåÂπø‰πâÂä†ÊÄßÊ®°ÂûãÈó¥ÁöÑÂÖ≥Á≥ªÂèØ‰ª•Á±ªÊØîÊàêÁ∫øÊÄßÂà§Âà´ÂàÜÊûêÂíåÈÄªËæëÊñØËíÇÂõûÂΩí</li>
</ul>
<h3 id="radial-basis-function-kernels">Radial Basis Function &amp; Kernels ÂæÑÂêëÂü∫ÂáΩÊï∞<a class="headerlink" href="#radial-basis-function-kernels" title="Permanent link">&para;</a></h3>
<p>see <a href="https://esl.hohoweiya.xyz/06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels/index.html">here</a></p>
<div class="arithmatex">
<div class="MathJax_Preview">
f(x)=\sum_{j=1}^{M} K_{\lambda_{j}}\left(\xi_{j}, x\right) \beta_{j}=\sum_{j=1}^{M} D\left(\frac{\left\|x-\xi_{j}\right\|}{\lambda_{j}}\right) \beta_{j}
</div>
<script type="math/tex; mode=display">
f(x)=\sum_{j=1}^{M} K_{\lambda_{j}}\left(\xi_{j}, x\right) \beta_{j}=\sum_{j=1}^{M} D\left(\frac{\left\|x-\xi_{j}\right\|}{\lambda_{j}}\right) \beta_{j}
</script>
</div>
<ul>
<li>Treat the kernel function <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}(\xi, x)</span><script type="math/tex">K_{\lambda}(\xi, x)</script></span> as basis functions</li>
<li>Each basis element is indexed by a location parameter <span class="arithmatex"><span class="MathJax_Preview">\xi_{j}</span><script type="math/tex">\xi_{j}</script></span> and a scale parameter <span class="arithmatex"><span class="MathJax_Preview">\lambda_{j}</span><script type="math/tex">\lambda_{j}</script></span></li>
<li>Popular choice for <span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span> - standard Gaussian</li>
</ul>
<p>Approaches for learning parameters <span class="arithmatex"><span class="MathJax_Preview">\left\{\lambda_{j}, \xi_{j}, \beta_{j}\right\}</span><script type="math/tex">\left\{\lambda_{j}, \xi_{j}, \beta_{j}\right\}</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\min_{\left\{\lambda_{j}, \xi_{j}, \beta_{j}\right\}_{1}^{M}} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{M} \beta_{j} \exp \left\{-\frac{\left(x_{i}-\xi_{j}\right)^{\top}\left(x_{i}-\xi_{j}\right)}{\lambda_{j}^{2}}\right\}\right)^{2}</span><script type="math/tex">\min_{\left\{\lambda_{j}, \xi_{j}, \beta_{j}\right\}_{1}^{M}} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{M} \beta_{j} \exp \left\{-\frac{\left(x_{i}-\xi_{j}\right)^{\top}\left(x_{i}-\xi_{j}\right)}{\lambda_{j}^{2}}\right\}\right)^{2}</script></span> [È´òÊñØÊ†∏, ÂõûÂΩí. Áß∞‰∏∫ RBF ÁΩëÁªúÔºåËøôÊòØ S ÂûãÁ•ûÁªèÁΩëÁªúÁöÑÊõø‰ª£ÈÄâÊã©, ÂèÇËßÅ 11Á´†.]</li>
<li>Estimate <span class="arithmatex"><span class="MathJax_Preview">\left\{\lambda_{j}, \xi_{j}\right\}</span><script type="math/tex">\left\{\lambda_{j}, \xi_{j}\right\}</script></span> (often unsupervised) separately from <span class="arithmatex"><span class="MathJax_Preview">\beta_{j}</span><script type="math/tex">\beta_{j}</script></span>. ÁªôÂÆöÂâçËÄÖÔºåÂêéËÄÖÁöÑ‰º∞ËÆ°ÊòØÁÆÄÂçïÁöÑÊúÄÂ∞è‰∫å‰πòÈóÆÈ¢ò.</li>
</ul>
<h3 id="mixture-models-for-density-estimation-classification">Mixture Models for Density Estimation &amp; ClassiÔ¨Åcation<a class="headerlink" href="#mixture-models-for-density-estimation-classification" title="Permanent link">&para;</a></h3>
<h4 id="gaussian-mixture">Gaussian Mixture<a class="headerlink" href="#gaussian-mixture" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
f(x)=\sum_{m=1}^{M} \alpha_{m} \phi\left(x ; \mu_{m}, \Sigma_{m}\right)
</div>
<script type="math/tex; mode=display">
f(x)=\sum_{m=1}^{M} \alpha_{m} \phi\left(x ; \mu_{m}, \Sigma_{m}\right)
</script>
</div>
<ul>
<li>Mixing proportions <span class="arithmatex"><span class="MathJax_Preview">\alpha_{m}</span><script type="math/tex">\alpha_{m}</script></span> with <span class="arithmatex"><span class="MathJax_Preview">\sum_{m} \alpha_{m}=1</span><script type="math/tex">\sum_{m} \alpha_{m}=1</script></span></li>
<li>Mean <span class="arithmatex"><span class="MathJax_Preview">\mu_{m}</span><script type="math/tex">\mu_{m}</script></span>, covariance matrix <span class="arithmatex"><span class="MathJax_Preview">\Sigma_{m}</span><script type="math/tex">\Sigma_{m}</script></span></li>
<li>Usually fit by maximum likelihood using EM algorithm</li>
</ul>
<p>Special cases</p>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">\Sigma_{m}=\sigma_{m} I, f(x)</span><script type="math/tex">\Sigma_{m}=\sigma_{m} I, f(x)</script></span> has the form of a radial basis expansion ÂæÑÂêëÂü∫ Â±ïÂºÄÁöÑÂΩ¢Âºè</li>
<li>If in addition <span class="arithmatex"><span class="MathJax_Preview">\sigma_{m}=\sigma&gt;0</span><script type="math/tex">\sigma_{m}=\sigma>0</script></span> is fixed and <span class="arithmatex"><span class="MathJax_Preview">M \uparrow N</span><script type="math/tex">M \uparrow N</script></span>, the MLE approaches the kernel density estimate where <span class="arithmatex"><span class="MathJax_Preview">\hat{\alpha}_{m}=\frac{1}{N}</span><script type="math/tex">\hat{\alpha}_{m}=\frac{1}{N}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\hat{\mu}_{m}=x_{m}</span><script type="math/tex">\hat{\mu}_{m}=x_{m}</script></span></li>
</ul>
<p>Ê∑∑ÂêàÊ®°ÂûãÂêåÊ†∑ÁªôÂá∫‰∫ÜËßÇÊµã i Â±û‰∫éÁªÑÂàÜ m ÁöÑÊ¶ÇÁéáÁöÑ‰º∞ËÆ°. Estimate of the probability that the <span class="arithmatex"><span class="MathJax_Preview">i_{t h}</span><script type="math/tex">i_{t h}</script></span> observation belongs to component <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>
$$
\hat{r}<em m="m">{i m}=\frac{\hat{\alpha}</em>} \phi\left(x_{i} ; \hat{\mu<em m="m">{m}, \hat{\Sigma}</em>}\right)}{\sum_{k=1}^{M} \hat{\alpha<em i="i">{k} \phi\left(x</em>} ; \hat{\mu<em k="k">{k}, \hat{\Sigma}</em>
$$}\right)</p>
<p><img alt="" src="../media/ASL-note2/2021-12-13-22-28-28.png" /></p>
<h2 id="model-assessment-selection">Model Assessment &amp; Selection<a class="headerlink" href="#model-assessment-selection" title="Permanent link">&para;</a></h2>
<h3 id="intro-of-model-assessment-selection">Intro of Model Assessment &amp; Selection<a class="headerlink" href="#intro-of-model-assessment-selection" title="Permanent link">&para;</a></h3>
<h4 id="regression-a-review">Regression - A Review<a class="headerlink" href="#regression-a-review" title="Permanent link">&para;</a></h4>
<ul>
<li>Estimate target variable <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> from a vector of inputs <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>.</li>
<li>A prediction model <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(X)</span><script type="math/tex">\hat{f}(X)</script></span> estimated from training data <span class="arithmatex"><span class="MathJax_Preview">T=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</span><script type="math/tex">T=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</script></span></li>
<li>The loss function <span class="arithmatex"><span class="MathJax_Preview">L(Y, \hat{f}(X))</span><script type="math/tex">L(Y, \hat{f}(X))</script></span> measures the error / loss between <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(X)</span><script type="math/tex">\hat{f}(X)</script></span></li>
<li>Common choices for loss function</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
L(Y, \hat{f}(X))= \begin{cases}(Y-\hat{f}(X))^{2}, &amp; \text { squared error } \\ |Y-\hat{f}(X)|, &amp; \text { absolute error }\end{cases}
</div>
<script type="math/tex; mode=display">
L(Y, \hat{f}(X))= \begin{cases}(Y-\hat{f}(X))^{2}, & \text { squared error } \\ |Y-\hat{f}(X)|, & \text { absolute error }\end{cases}
</script>
</div>
<p>Test Error</p>
<p><strong>Test error</strong>, also called <strong>generalization error</strong></p>
<div class="arithmatex">
<div class="MathJax_Preview">
E r r_{T}=E[L(Y, \hat{f}(X)) \mid T]
</div>
<script type="math/tex; mode=display">
E r r_{T}=E[L(Y, \hat{f}(X)) \mid T]
</script>
</div>
<ul>
<li>Prediction error over an independent test sample</li>
<li><span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> and <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> are drawn randomly from <span class="arithmatex"><span class="MathJax_Preview">p(X, Y)</span><script type="math/tex">p(X, Y)</script></span></li>
<li>The training set <span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> is fixed</li>
</ul>
<p>Expected Prediction Error</p>
<p><strong>Expected prediction error</strong>, also called <strong>expected test error</strong></p>
<div class="arithmatex">
<div class="MathJax_Preview">
E r r=E[L(Y, \hat{f}(X))]=E\left[E r r_{T}\right]
</div>
<script type="math/tex; mode=display">
E r r=E[L(Y, \hat{f}(X))]=E\left[E r r_{T}\right]
</script>
</div>
<ul>
<li>Average over everything that is random</li>
<li>Including the randomness in the training set that produced <span class="arithmatex"><span class="MathJax_Preview">\hat{f}</span><script type="math/tex">\hat{f}</script></span></li>
</ul>
<p>We would be interested in estimating <span class="arithmatex"><span class="MathJax_Preview">Err_T</span><script type="math/tex">Err_T</script></span>, but in most cases it is easier to estimate <span class="arithmatex"><span class="MathJax_Preview">Err</span><script type="math/tex">Err</script></span></p>
<p>Training Error</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\overline{e r r}=\frac{1}{n} \sum_{i=1}^{n} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right)
</div>
<script type="math/tex; mode=display">
\overline{e r r}=\frac{1}{n} \sum_{i=1}^{n} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right)
</script>
</div>
<p>As model complexity increases</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\overline{e r r} \rightarrow 0</span><script type="math/tex">\overline{e r r} \rightarrow 0</script></span></li>
<li>but <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}_{T}</span><script type="math/tex">\operatorname{Err}_{T}</script></span> increases, tendency to overfit</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span> is not a good estimate of <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}_{T}</span><script type="math/tex">\operatorname{Err}_{T}</script></span> or <span class="arithmatex"><span class="MathJax_Preview">E r r</span><script type="math/tex">E r r</script></span></li>
</ul>
<p><img alt="" src="../media/ASL-note2/2021-12-17-16-53-26.png" /></p>
<ul>
<li>Light Blue Curve: training error <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span></li>
<li>Solid blue curve: expected training error <span class="arithmatex"><span class="MathJax_Preview">E[\overrightarrow{e r r}]</span><script type="math/tex">E[\overrightarrow{e r r}]</script></span></li>
<li>Light red curve: conditional test error Err,</li>
<li>Solid red curve: expected test error Err</li>
</ul>
<h4 id="classification-a-review">Classification - A Review<a class="headerlink" href="#classification-a-review" title="Permanent link">&para;</a></h4>
<ul>
<li>Estimate categorical response <span class="arithmatex"><span class="MathJax_Preview">G \in\{1, \ldots, K\}</span><script type="math/tex">G \in\{1, \ldots, K\}</script></span> from a vector of inputs <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span></li>
<li>Typically model <span class="arithmatex"><span class="MathJax_Preview">p_{k}(X)=\operatorname{Pr}(G=k \mid X)</span><script type="math/tex">p_{k}(X)=\operatorname{Pr}(G=k \mid X)</script></span> and define <span class="arithmatex"><span class="MathJax_Preview">\hat{G}(X)=\operatorname{argmax}_{k} p_{k}(X)</span><script type="math/tex">\hat{G}(X)=\operatorname{argmax}_{k} p_{k}(X)</script></span></li>
</ul>
<p>Common choices of loss function</p>
<ul>
<li>0-1 loss</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
L(G, \hat{G}(X))=\operatorname{lnd}(G \neq \hat{G}(X))
</div>
<script type="math/tex; mode=display">
L(G, \hat{G}(X))=\operatorname{lnd}(G \neq \hat{G}(X))
</script>
</div>
<ul>
<li>log-likelihood (deviance) Ë¥üÂØπÊï∞‰ººÁÑ∂</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
L(G, \hat{p}(X))=-2 \sum_{k=1}^{K} \operatorname{lnd}(G=k) \log \hat{p}_{k}(X)=-2 \log \hat{p}_{G}(X)
</div>
<script type="math/tex; mode=display">
L(G, \hat{p}(X))=-2 \sum_{k=1}^{K} \operatorname{lnd}(G=k) \log \hat{p}_{k}(X)=-2 \log \hat{p}_{G}(X)
</script>
</div>
<p>ËøôÈáåÁöÑ <span class="arithmatex"><span class="MathJax_Preview">‚àí2 √ó \operatorname{log-likelihood}</span><script type="math/tex">‚àí2 √ó \operatorname{log-likelihood}</script></span> ÂÄºÊúâÊó∂‰πüË¢´Áß∞‰∏∫ <strong>ÂÅèÂ∑Æ (deviance)</strong>Ôºé</p>
<h4 id="ulimate-goal">Ulimate Goal<a class="headerlink" href="#ulimate-goal" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\alpha}=\operatorname{argmin}_{\alpha} E\left[L\left(Y, \hat{f}_{\alpha}(X)\right)\right]
</div>
<script type="math/tex; mode=display">
\hat{\alpha}=\operatorname{argmin}_{\alpha} E\left[L\left(Y, \hat{f}_{\alpha}(X)\right)\right]
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{f}_{\alpha}(x)</span><script type="math/tex">\hat{f}_{\alpha}(x)</script></span> typically has a tuning parameter <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> controlling its complexity</li>
<li>Estimate <span class="arithmatex"><span class="MathJax_Preview">E\left[L\left(Y, \hat{f}_{\alpha}(X)\right)\right]</span><script type="math/tex">E\left[L\left(Y, \hat{f}_{\alpha}(X)\right)\right]</script></span> for different values of <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span></li>
<li>Find the value of <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> that produces the minimum expected prediction error</li>
</ul>
<p>Two Separate Goals</p>
<ul>
<li>Model selection: Estimate the performance of diÔ¨Äerent models in order to choose the best one</li>
<li>Model assessment: Having chosen a Ô¨Ånal model, estimate its prediction error (generalization error) on new data.</li>
</ul>
<p>For a <strong>Data-rich</strong> Situation</p>
<ul>
<li>Randomly divide the dataset into 3 parts: Train Validation Test</li>
<li>Common split ratio <span class="arithmatex"><span class="MathJax_Preview">50 \%, 25 \%, 25 \%</span><script type="math/tex">50 \%, 25 \%, 25 \%</script></span></li>
<li>Model selection<ul>
<li>Use training set to fit each model</li>
<li>Use validation set to estimate <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}_{T}</span><script type="math/tex">\operatorname{Err}_{T}</script></span> for each model</li>
<li>Choose model with lowest <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}_{T}</span><script type="math/tex">\operatorname{Err}_{T}</script></span> estimate</li>
</ul>
</li>
<li>Model assessment<ul>
<li>Use the test set - unseen until this stage - to estimate <span class="arithmatex"><span class="MathJax_Preview">E r r_{T}</span><script type="math/tex">E r r_{T}</script></span></li>
</ul>
</li>
</ul>
<p>For a <strong>Data-insufficient</strong> Situation</p>
<p>Approximate the validation step either</p>
<ul>
<li>analytically with approahces such as ÂàÜÊûêÁöÑÊâãÊÆµ (<strong>AICÔºåBICÔºåMDLÔºåSRM</strong>)
  1. Akaike Information Criterion
  2. Bayesian Information Criterion
  3. Minimum Description Length
  4. Structural Risk Minimization</li>
<li>with efficient sample re-use ÊúâÊïàÁöÑÊ†∑Êú¨ÈáçÂà©Áî®Ôºà‰∫§ÂèâÈ™åËØÅÂíåËá™Âä©Ê≥ïÔºâ
  1. cross-validation
  2. the bootstrap</li>
</ul>
<p>Each method also provides estimates of the <span class="arithmatex"><span class="MathJax_Preview">Err</span><script type="math/tex">Err</script></span> or <span class="arithmatex"><span class="MathJax_Preview">Err_T</span><script type="math/tex">Err_T</script></span> of the final chosen model</p>
<h3 id="the-bias-variance-decomposition">The Bias-Variance Decomposition<a class="headerlink" href="#the-bias-variance-decomposition" title="Permanent link">&para;</a></h3>
<p>Assume an additive model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Y=f(X)+\epsilon
</div>
<script type="math/tex; mode=display">
Y=f(X)+\epsilon
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">E(\epsilon)=0</span><script type="math/tex">E(\epsilon)=0</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Var}(\epsilon)=\sigma_{\epsilon}^{2}</span><script type="math/tex">\operatorname{Var}(\epsilon)=\sigma_{\epsilon}^{2}</script></span></p>
<p>The <strong>expected prediction error</strong> of <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(X)</span><script type="math/tex">\hat{f}(X)</script></span> at <span class="arithmatex"><span class="MathJax_Preview">X=x_{0}</span><script type="math/tex">X=x_{0}</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{Err}\left(x_{0}\right)=E\left(\left(Y-\hat{f}\left(x_{0}\right)\right)^{2} \mid X=x_{0}\right)
</div>
<script type="math/tex; mode=display">
\operatorname{Err}\left(x_{0}\right)=E\left(\left(Y-\hat{f}\left(x_{0}\right)\right)^{2} \mid X=x_{0}\right)
</script>
</div>
<p>can be expressed as</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{Err}\left(x_{0}\right) = \sigma_{\epsilon}^{2} + MSE =\operatorname{IrreducibleError}+\operatorname{Bias}^{2}+\text { Variance }
</div>
<script type="math/tex; mode=display">
\operatorname{Err}\left(x_{0}\right) = \sigma_{\epsilon}^{2} + MSE =\operatorname{IrreducibleError}+\operatorname{Bias}^{2}+\text { Variance }
</script>
</div>
<ul>
<li>Irreducible error: <span class="arithmatex"><span class="MathJax_Preview">\sigma_{\epsilon}^{2}</span><script type="math/tex">\sigma_{\epsilon}^{2}</script></span></li>
<li>Bias: <span class="arithmatex"><span class="MathJax_Preview">E\left[f\left(x_{0}\right)-E\left[\hat{f}\left(x_{0}\right)\right]\right]</span><script type="math/tex">E\left[f\left(x_{0}\right)-E\left[\hat{f}\left(x_{0}\right)\right]\right]</script></span></li>
<li>Variance: <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Var}\left[\hat{f}\left(x_{0}\right)\right]=E\left[\hat{f}\left(x_{0}\right)-E \hat{f}\left(x_{0}\right)\right]^{2}</span><script type="math/tex">\operatorname{Var}\left[\hat{f}\left(x_{0}\right)\right]=E\left[\hat{f}\left(x_{0}\right)-E \hat{f}\left(x_{0}\right)\right]^{2}</script></span></li>
</ul>
<p>Á¨¨‰∏ÄÈ°πÊòØÁõÆÊ†áÂú®ÁúüÂÆûÂùáÂÄº <span class="arithmatex"><span class="MathJax_Preview">f\left(x_{0}\right)</span><script type="math/tex">f\left(x_{0}\right)</script></span> Â§ÑÁöÑÊñπÂ∑ÆÔºåÊó†ËÆ∫Êàë‰ª¨ÂØπ <span class="arithmatex"><span class="MathJax_Preview">f\left(x_{0}\right)</span><script type="math/tex">f\left(x_{0}\right)</script></span> ÁöÑ‰º∞ËÆ°ÊúâÂ§öÂ•ΩÔºåËøôÊòØ‰∏çÂèØÈÅøÂÖçÁöÑÔºåÈô§Èùû <span class="arithmatex"><span class="MathJax_Preview">\sigma_{\varepsilon}^{2}=0</span><script type="math/tex">\sigma_{\varepsilon}^{2}=0</script></span>. (Âõ†Ê≠§, Êàë‰ª¨Âè™Ë¶ÅÂÖ≥Ê≥® <strong>MSE</strong> Âç≥ÂèØ.) Á¨¨‰∫åÈ°πÊòØÂÅèÂ∑ÆÁöÑÂπ≥ÊñπÔºåÊòØÊàë‰ª¨‰º∞ËÆ°ÁöÑÂùáÂÄº‰∏éÁúüÂÆûÁöÑÂùáÂÄºÈó¥ÁöÑÂÅèÂ∑ÆÈáèÔºõÊúÄÂêé‰∏ÄÈ°πÊòØÊñπÂ∑ÆÔºåÊòØ‰º∞ËÆ°ÁöÑ <span class="arithmatex"><span class="MathJax_Preview">\hat{f}\left(x_{0}\right)</span><script type="math/tex">\hat{f}\left(x_{0}\right)</script></span> Âú®ÂÖ∂ÂùáÂÄºÂ§ÑÁöÑÂπ≥ÊñπÂÅèÂ∑ÆÁöÑÊúü ÊúõÂÄº. ‰∏ÄËà¨Âú∞ÔºåÊàë‰ª¨Âª∫Á´ãÁöÑÊ®°Âûã <span class="arithmatex"><span class="MathJax_Preview">\hat{f}</span><script type="math/tex">\hat{f}</script></span> Ë∂äÂ§çÊùÇÔºå (Âπ≥Êñπ) ÂÅèÂ∑ÆË∂ä‰Ωé‰ΩÜÊòØÊñπÂ∑ÆË∂äÂ§ß.</p>
<h4 id="k-nearest-neighbor-regression-fit">K-Nearest-Neighbor Regression Fit<a class="headerlink" href="#k-nearest-neighbor-regression-fit" title="Permanent link">&para;</a></h4>
<p>Assuming <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> are fixed,</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{Err}\left(x_{0}\right)=\sigma_{\epsilon}^{2}+\left[f\left(x_{0}\right)-\frac{1}{k} \sum_{l=1}^{k} f\left(x_{(l)}\right)\right]^{2}+\frac{\sigma_{\epsilon}^{2}}{k}
</div>
<script type="math/tex; mode=display">
\operatorname{Err}\left(x_{0}\right)=\sigma_{\epsilon}^{2}+\left[f\left(x_{0}\right)-\frac{1}{k} \sum_{l=1}^{k} f\left(x_{(l)}\right)\right]^{2}+\frac{\sigma_{\epsilon}^{2}}{k}
</script>
</div>
<ul>
<li>Complexity of model is inversely related with <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span></li>
<li>As <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> increases, the variance decreases</li>
<li>As <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> increases, the squared bias increases</li>
</ul>
<p>ËøôÈáå‰∏∫‰∫ÜÁÆÄÂçïÊàë‰ª¨ÂÅáËÆæËÆ≠ÁªÉËæìÂÖ• <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> ‰∏∫Âõ∫ÂÆöÁöÑÔºåÂàôÈöèÊú∫ÊÄßÊù•Ëá™ <span class="arithmatex"><span class="MathJax_Preview">y_i</span><script type="math/tex">y_i</script></span> ÔºéÈÇªÂ±ÖÁöÑ‰∏™Êï∞ <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> ‰∏éÊ®°ÂûãÂ§çÊùÇÂ∫¶Ë¥üÁõ∏ÂÖ≥ÔºéÂØπ‰∫éËæÉÂ∞èÁöÑ k Ôºå‰º∞ËÆ°Âá∫ÁöÑ <span class="arithmatex"><span class="MathJax_Preview">\hat{f}_k (x)</span><script type="math/tex">\hat{f}_k (x)</script></span> ÂèØ‰ª•ÂØπÊΩúÂú®ÁöÑÂáΩÊï∞ <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> ÂèØËÉΩÊúâÊõ¥Â•ΩÁöÑÈÄÇÂ∫îÊÄßÔºéÂΩì k Â¢ûÂ§ßÔºåÂÅèÂ∑Æ ‚Äî‚Äî <span class="arithmatex"><span class="MathJax_Preview">f(x_0)</span><script type="math/tex">f(x_0)</script></span> ‰∏é k-ÊúÄËøëÈÇª‰∏≠ÁöÑ <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> Âπ≥ÂùáÂÄºÁöÑÂ∑ÆÁöÑÂπ≥Êñπ‚Äî‚Äî ‰∏ÄËà¨‰ºöÂ¢ûÂ§ßÔºåËÄåÊñπÂ∑Æ‰ºöÈôç‰ΩéÔºé</p>
<h4 id="linear-model-least-square-fit">Linear Model - least square fit<a class="headerlink" href="#linear-model-least-square-fit" title="Permanent link">&para;</a></h4>
<p>Now assume a linear model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}_{p}(x)=x^{\top} \hat{\beta}
</div>
<script type="math/tex; mode=display">
\hat{f}_{p}(x)=x^{\top} \hat{\beta}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}</span><script type="math/tex">\hat{\beta}</script></span> is <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>-dimensional and fit by least squares, then</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{Err}\left(x_{0}\right)=\sigma_{\epsilon}^{2}+\left[f\left(x_{0}\right)-E\left(\hat{f}_{p}\left(x_{0}\right)\right)\right]^{2}+\left\|h\left(x_{0}\right)\right\|^{2} \sigma_{\epsilon}^{2}
</div>
<script type="math/tex; mode=display">
\operatorname{Err}\left(x_{0}\right)=\sigma_{\epsilon}^{2}+\left[f\left(x_{0}\right)-E\left(\hat{f}_{p}\left(x_{0}\right)\right)\right]^{2}+\left\|h\left(x_{0}\right)\right\|^{2} \sigma_{\epsilon}^{2}
</script>
</div>
<p>with <span class="arithmatex"><span class="MathJax_Preview">h\left(x_{0}\right)=X\left(X^{\top} X\right)^{-1} x_{0}</span><script type="math/tex">h\left(x_{0}\right)=X\left(X^{\top} X\right)^{-1} x_{0}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\hat{f}_{p}\left(x_{0}\right)=x_{0}^{\top}\left(X^{\top} X\right)^{-1} X^{\top} y</span><script type="math/tex">\hat{f}_{p}\left(x_{0}\right)=x_{0}^{\top}\left(X^{\top} X\right)^{-1} X^{\top} y</script></span></p>
<p>Ê≥®ÊÑèÂà∞ËøôÈáåÁöÑÊñπÂ∑ÆÈöèÁùÄ <span class="arithmatex"><span class="MathJax_Preview">x_0</span><script type="math/tex">x_0</script></span> ÂèòÂåñÔºå ‰ΩÜÊòØÂÆÉÁöÑÂπ≥Âùá‰∏∫ <span class="arithmatex"><span class="MathJax_Preview">(p/N)\sigma_\epsilon^2</span><script type="math/tex">(p/N)\sigma_\epsilon^2</script></span>.ËøôÊòØÂõ†‰∏∫: Ëã•ËÆ∞ <span class="arithmatex"><span class="MathJax_Preview">\mathbf{X}=\left[x_{1}^{\prime}, x_{2}^{\prime}, \ldots, x_{N}^{\prime}\right]^{\prime}</span><script type="math/tex">\mathbf{X}=\left[x_{1}^{\prime}, x_{2}^{\prime}, \ldots, x_{N}^{\prime}\right]^{\prime}</script></span>, Âàô</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
\sum_{i=1}^{N}\left\|\mathbf{h}\left(x_{i}\right)\right\|^{2} &amp;=\sum_{i=1}^{N} \mathbf{h}^{T}\left(x_{i}\right) \mathbf{h}\left(x_{i}\right) \\
&amp;=\sum_{i=1}^{N} x_{i}^{T}\left(\mathbf{X}^{\mathbf{T}} \mathbf{X}\right)^{-1} x_{i} \\
&amp;=\operatorname{trace}\left[\mathbf{X}\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T}\right] \\
&amp;=\operatorname{trace}\left[\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \mathbf{X}\right] \\
&amp;=p
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\sum_{i=1}^{N}\left\|\mathbf{h}\left(x_{i}\right)\right\|^{2} &=\sum_{i=1}^{N} \mathbf{h}^{T}\left(x_{i}\right) \mathbf{h}\left(x_{i}\right) \\
&=\sum_{i=1}^{N} x_{i}^{T}\left(\mathbf{X}^{\mathbf{T}} \mathbf{X}\right)^{-1} x_{i} \\
&=\operatorname{trace}\left[\mathbf{X}\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T}\right] \\
&=\operatorname{trace}\left[\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \mathbf{X}\right] \\
&=p
\end{aligned}
</script>
</div>
<p>Âõ†Ê≠§Êúâ</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{1}{N} \sum_{i=1}^{N} \operatorname{Err}\left(x_{i}\right)=\sigma_{\varepsilon}^{2}+\frac{1}{N} \sum_{i=1}^{N}\left[f\left(x_{i}\right)-\mathrm{E} \hat{f}\left(x_{i}\right)\right]^{2}+\frac{p}{N} \sigma_{\varepsilon}^{2}
</div>
<script type="math/tex; mode=display">
\frac{1}{N} \sum_{i=1}^{N} \operatorname{Err}\left(x_{i}\right)=\sigma_{\varepsilon}^{2}+\frac{1}{N} \sum_{i=1}^{N}\left[f\left(x_{i}\right)-\mathrm{E} \hat{f}\left(x_{i}\right)\right]^{2}+\frac{p}{N} \sigma_{\varepsilon}^{2}
</script>
</div>
<p>ËøôÁß∞‰Ωú <strong>Ê†∑Êú¨ÂÜÖ (in-sample) ËØØÂ∑Æ</strong>. ËøôÈáåÊ®°ÂûãÂ§çÊùÇÂ∫¶Áõ¥Êé•‰∏éÂèÇÊï∞‰∏™Êï∞ <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> ÊúâÂÖ≥.</p>
<h4 id="linear-model-ridge-fit">Linear Model - Ridge Fit<a class="headerlink" href="#linear-model-ridge-fit" title="Permanent link">&para;</a></h4>
<p>Still assume a linear model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}_{p, \alpha}(x)=x^{\top} \hat{\beta}_{\alpha}
</div>
<script type="math/tex; mode=display">
\hat{f}_{p, \alpha}(x)=x^{\top} \hat{\beta}_{\alpha}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}_{\alpha}</span><script type="math/tex">\hat{\beta}_{\alpha}</script></span> is <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>-dimensional and fit via ridge regression, then</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{Err}\left(x_{0}\right)=\sigma_{\epsilon}^{2}+\left[f\left(x_{0}\right)-E\left(\hat{f}_{p, \alpha}\left(x_{0}\right)\right)\right]^{2}+\left|h_{\alpha}\left(x_{0}\right)\right|^{2} \sigma_{\epsilon}^{2}
</div>
<script type="math/tex; mode=display">
\operatorname{Err}\left(x_{0}\right)=\sigma_{\epsilon}^{2}+\left[f\left(x_{0}\right)-E\left(\hat{f}_{p, \alpha}\left(x_{0}\right)\right)\right]^{2}+\left|h_{\alpha}\left(x_{0}\right)\right|^{2} \sigma_{\epsilon}^{2}
</script>
</div>
<p>with <span class="arithmatex"><span class="MathJax_Preview">h_{\alpha}\left(x_{0}\right)=X\left(X^{\top} X+\alpha I\right)^{-1} x_{0}</span><script type="math/tex">h_{\alpha}\left(x_{0}\right)=X\left(X^{\top} X+\alpha I\right)^{-1} x_{0}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\hat{f}_{p, \alpha}\left(x_{0}\right)=x_{0}^{\top}\left(X^{\top} X+\alpha I\right)^{-1} X^{\top} y</span><script type="math/tex">\hat{f}_{p, \alpha}\left(x_{0}\right)=x_{0}^{\top}\left(X^{\top} X+\alpha I\right)^{-1} X^{\top} y</script></span>
Therefore, this regression fit model has a different bias and variance to the least square fit</p>
<h4 id="linear-model-fine-decomposition-of-bias">Linear Model - Fine Decomposition of Bias<a class="headerlink" href="#linear-model-fine-decomposition-of-bias" title="Permanent link">&para;</a></h4>
<p>ÂØπ‰∫éÁ∫øÊÄßÊ®°ÂûãÊóèÊØîÂ¶ÇÂ≤≠ÂõûÂΩíÔºåÊàë‰ª¨ÂèØ‰ª•Êõ¥Á≤æÁªÜÂú∞ÂàÜËß£ÂÅèÂ∑ÆÔºé</p>
<p>Let <span class="arithmatex"><span class="MathJax_Preview">\beta_{*}</span><script type="math/tex">\beta_{*}</script></span> denote the parameters of the best-fitting linear approx to <span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> :</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\beta_{*}=\operatorname{argmin}_{\beta} E\left[\left(f(X)-X^{\top} \beta\right)^{2}\right]
</div>
<script type="math/tex; mode=display">
\beta_{*}=\operatorname{argmin}_{\beta} E\left[\left(f(X)-X^{\top} \beta\right)^{2}\right]
</script>
</div>
<p>Now re-write the averaged squared bias ÂÅèÂ∑ÆÂπ≥ÊñπÁöÑÂπ≥Âùá</p>
<div class="arithmatex">
<div class="MathJax_Preview">
E_{x_{0}}\left[\left(f\left(x_{0}\right)-E\left[\hat{f}_{\alpha}\left(x_{0}\right)\right]\right)^{2}\right]
</div>
<script type="math/tex; mode=display">
E_{x_{0}}\left[\left(f\left(x_{0}\right)-E\left[\hat{f}_{\alpha}\left(x_{0}\right)\right]\right)^{2}\right]
</script>
</div>
<p>as</p>
<div class="arithmatex">
<div class="MathJax_Preview">
E_{x_{0}}\left[\left(f\left(x_{0}\right)-x_{0}^{\top} \beta_{*}\right)^{2}\right]+E_{x_{0}}\left[\left(x_{0}^{\top} \beta_{*}-E\left[x_{0}^{\top} \hat{\beta}_{\alpha}\right]\right)^{2}\right]
</div>
<script type="math/tex; mode=display">
E_{x_{0}}\left[\left(f\left(x_{0}\right)-x_{0}^{\top} \beta_{*}\right)^{2}\right]+E_{x_{0}}\left[\left(x_{0}^{\top} \beta_{*}-E\left[x_{0}^{\top} \hat{\beta}_{\alpha}\right]\right)^{2}\right]
</script>
</div>
<p>where</p>
<ul>
<li>First term: Ave[Model Bias] <span class="arithmatex"><span class="MathJax_Preview">^{2}</span><script type="math/tex">^{2}</script></span> Ê®°ÂûãÂÅèÂ∑Æ</li>
<li>Second term: Ave[Estimation Bias] <span class="arithmatex"><span class="MathJax_Preview">^{2}</span><script type="math/tex">^{2}</script></span> ‰º∞ËÆ°ÂÅèÂ∑Æ</li>
</ul>
<p>Estimation bias is zero for least square estimate, and positive for ridge regression estimate. ÂØπ‰∫éÈÄöËøáÊôÆÈÄöÊúÄÂ∞è‰∫å‰πòÊãüÂêàÁöÑÁ∫øÊÄßÊ®°ÂûãÔºå‰º∞ËÆ°ÈáèÁöÑÂÅèÂ∑Æ‰∏∫ 0ÔºéÂØπ‰∫éÁ∫¶ÊùüÁöÑÊãüÂêàÔºå ÊØîÂ¶ÇÂ≤≠ÂõûÂΩíÔºåÂÆÉÊòØÊ≠£ÁöÑÔºåËÄå‰∏îÊàë‰ª¨Áî®ÂáèÂ∞èÊñπÂ∑ÆÁöÑÂ•ΩÂ§ÑËøõË°å‰∫§ÊòìÔºéÊ®°ÂûãÂÅèÂ∑ÆÂè™ÂèØËÉΩÈÄöËøáÂ∞ÜÁ∫øÊÄßÊ®°ÂûãÁ±ªÊâ©Â§ß‰∏∫Êõ¥ÂπøÁöÑÊ®°ÂûãÁ±ªÊâçËÉΩÈôç‰ΩéÔºåÊàñËÄÖÈÄöËøáÂú®Ê®°Âûã‰∏≠Âä†ÂÖ•ÂèòÈáèÁöÑ‰∫§ÂèâÈ°π‰ª•ÂèäÂèòÊç¢È°πÔºàÈÄöËøáÂèòÈáèÁöÑÂèòÊç¢ÂæóÂà∞ÁöÑÔºâÊù•Èôç‰ΩéÔºé</p>
<p><img alt="" src="../media/ASL-note2/2021-12-17-20-14-51.png" /></p>
<p>‰∏äÂõæÁªô‰∫Ü‰∏Ä‰∏™ÂΩ¢Ë±°ÁöÑ‰æãÂ≠ê, Â±ïÁé∞‰∫ÜÈÄöËøáËøõË°å shrink ÊàñËÄÖ regularization, ËôΩÁÑ∂Ê®°ÂûãÁöÑÂÅèÂ∑ÆÂèòÂ§ß‰∫Ü, Â¶ÇÊûúÊñπÂ∑ÆÂáèÂ∞èÊõ¥Â§ö‰πüÊòØÂÄºÂæóÁöÑ.</p>
<h4 id="example-1">Example 1<a class="headerlink" href="#example-1" title="Permanent link">&para;</a></h4>
<p>Set-up</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">n=80</span><script type="math/tex">n=80</script></span> observations and <span class="arithmatex"><span class="MathJax_Preview">p=20</span><script type="math/tex">p=20</script></span> predictors</li>
<li><span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> is uniformly distributed in <span class="arithmatex"><span class="MathJax_Preview">[0,1]^{20}</span><script type="math/tex">[0,1]^{20}</script></span> Âú®ËøôÊ†∑‰∏Ä‰∏™Ë∂ÖÁ´ãÊñπ‰Ωì‰∏≠ÂùáÂåÄÂàÜÂ∏É</li>
<li>Use squared error loss to measure Err for the regression task</li>
<li>Use 0-1 loss to measure Err for the classification task</li>
</ul>
<p>‰∏ãÂõæ‰∏≠Â∑¶ËæπÁöÑËÆæÁΩÆ: Êï∞ÊçÆÂàÜÂ∏ÉÂ¶Ç‰∏ã, ÂàÜÁ±ªÂíåÂõûÂΩí‰ªªÂä°ÂùáÈááÁî® KNN</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Y= \begin{cases}0 &amp; \text { if } X_{1} \leqslant 0.5 \\ 1 &amp; \text { if } X_{1}&gt;0.5\end{cases}
</div>
<script type="math/tex; mode=display">
Y= \begin{cases}0 & \text { if } X_{1} \leqslant 0.5 \\ 1 & \text { if } X_{1}>0.5\end{cases}
</script>
</div>
<p>Âè≥ËæπÁöÑËÆæÁΩÆ: ÂàÜÂ∏ÉÂ¶Ç‰∏ã, ÂØπ‰∫éÂàÜÁ±ªÂíåÂõûÂΩí‰ªªÂä°ÂùáÈááÁî® best subset linear regression of size <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
Y= \begin{cases}1 &amp; \text { if } \sum_{j=1}^{10} X_{j} \leqslant 5 \\ 0 &amp; \text { otherwise }\end{cases}
</div>
<script type="math/tex; mode=display">
Y= \begin{cases}1 & \text { if } \sum_{j=1}^{10} X_{j} \leqslant 5 \\ 0 & \text { otherwise }\end{cases}
</script>
</div>
<p><img alt="" src="../media/ASL-note2/2021-12-17-20-25-22.png" /></p>
<p>Âõæ‰∏≠: È¢ÑÊµãËØØÂ∑ÆÔºàÁ∫¢Ëâ≤Ôºâ„ÄÅÂπ≥ÊñπËØØÂ∑ÆÔºàÁªøËâ≤ÔºâÂíåÊñπÂ∑ÆÔºàËìùËâ≤Ôºâ.</p>
<p>Âú®ÂõûÂΩí‰ªªÂä°ÁöÑ(‰∏äÈù¢‰∏§Âº†Âõæ), È¢ÑÊµãËØØÂ∑ÆÊòØÂπ≥ÊñπËØØÂ∑ÆÂíåÊñπÂ∑Æ‰πãÂíå; ‰ΩÜÂú®ÂàÜÁ±ª‰ªªÂä°(‰∏ãÈù¢‰∏§Âº†Âõæ)Âàô‰∏çÊòØ, ËøôÊòØÂõ†‰∏∫ÂàÜÁ±ªÁöÑÈ¢ÑÊµãËØØÂ∑ÆÁî®ÁöÑÊçüÂ§±ÂáΩÊï∞‰∏ç‰∏ÄÊ†∑. (ÊØîÂ¶ÇÁªôÂÆö‰∏Ä‰∏™ÁÇπ, Á±ªÂà´1ÁöÑÁúüÂÆûÊ¶ÇÁéá‰∏∫0.9, Ê®°ÂûãÈ¢ÑÊµã‰∏∫0.6, Ê≠§Êó∂ÁöÑÂÅèÂ∑ÆÂπ≥Êñπ <span class="arithmatex"><span class="MathJax_Preview">(0.6-0.9)^2</span><script type="math/tex">(0.6-0.9)^2</script></span> ËæÉÂ§ß, ‰ΩÜÊòØÈ¢ÑÊµãËØØÂ∑Æ‰∏∫Èõ∂). ÊÄªËÄåË®Ä‰πã, „ÄåÂÅèÂ∑ÆÊñπÂ∑ÆÈó¥ÁöÑÊùÉË°°Âú® 0-1 ÊçüÂ§±ÁöÑË°®Áé∞‰∏éÂú®Âπ≥ÊñπËØØÂ∑ÆÊçüÂ§±ÁöÑË°®Áé∞‰∏ç‰∏ÄÊ†∑„Äç. ÂèçËøáÊù•ÊÑèÂë≥ÁùÄË∞ÉÊï¥ÂèÇÊï∞ÁöÑÊúÄ‰ºòÈÄâÊã©ÂèØËÉΩÂú®‰∏§ÁßçËÆæÂÆö‰∏ãÊú¨Ë¥®‰∏ä‰∏çÂêåÔºéÊ≠£Â¶ÇÂêéÈù¢Á´†ËäÇ‰∏≠ÊèèËø∞ÁöÑÈÇ£Ê†∑Ôºå<strong>Êàë‰ª¨Â∫îËØ•Â∞ÜË∞ÉÊï¥ÂèÇÊï∞ÁöÑÈÄâÊã©Âª∫Á´ã‰∫éÂØπÈ¢ÑÊµãËØØÂ∑ÆÁöÑ‰º∞ËÆ°‰πã‰∏ä</strong>Ôºé</p>
<h3 id="optimism-of-training-error-rate">Optimism of Training Error Rate<a class="headerlink" href="#optimism-of-training-error-rate" title="Permanent link">&para;</a></h3>
<h4 id="optimism-of-overlinetext-err-overlinetext-err">Optimism of <span class="arithmatex"><span class="MathJax_Preview">\overline{\text { err }}</span><script type="math/tex">\overline{\text { err }}</script></span><a class="headerlink" href="#optimism-of-overlinetext-err-overlinetext-err" title="Permanent link">&para;</a></h4>
<p>Âå∫ÂàÜ <strong>ËÆ≠ÁªÉËØØÂ∑Æ</strong> <span class="arithmatex"><span class="MathJax_Preview">\overline{err}</span><script type="math/tex">\overline{err}</script></span> Âíå <strong>Ê≥õÂåñ/È¢ÑÊµãËØØÂ∑Æ</strong> generalization/prediction error <span class="arithmatex"><span class="MathJax_Preview">Err_{T}</span><script type="math/tex">Err_{T}</script></span></p>
<p>Suppose <span class="arithmatex"><span class="MathJax_Preview">\left(x_{0}, y_{0}\right)</span><script type="math/tex">\left(x_{0}, y_{0}\right)</script></span> is a new test data point, drawn from the joint distribution of the data. Ê≥®ÊÑè‰∏ãÈù¢ÊúüÊúõËØØÂ∑Æ‰∏≠, training set <span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> ÊòØÂõ∫ÂÆöÁöÑ, <span class="arithmatex"><span class="MathJax_Preview">X_{0}, Y_{0}</span><script type="math/tex">X_{0}, Y_{0}</script></span> new test data point.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
\overline{e r r}=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right) \\
E r r_{T}=E_{X_{0}, Y_{0}}\left(L\left(Y_{0}, \hat{f}\left(X_{0}\right)\right) \mid T\right)
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
\overline{e r r}=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right) \\
E r r_{T}=E_{X_{0}, Y_{0}}\left(L\left(Y_{0}, \hat{f}\left(X_{0}\right)\right) \mid T\right)
\end{gathered}
</script>
</div>
<p>Training error <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}&lt;&lt;\operatorname{Err}_{T}</span><script type="math/tex">\overline{e r r}<<\operatorname{Err}_{T}</script></span> as it uses <span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> for both fitting and assessment.</p>
<p>ËøôÈáåÁöÑ <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span> ÊòØËÆ≠ÁªÉËØØÂ∑Æ, Ê≥õÂåñËØØÂ∑Æ <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}_{T}</span><script type="math/tex">\operatorname{Err}_{T}</script></span> ÂèØ‰ª•ÁúãÊàêÊòØ <strong>Ê†∑Êú¨Â§ñ (extra-sample) ËØØÂ∑Æ</strong> (Âõ†‰∏∫ÊµãËØïËæìÂÖ•ÂêëÈáè‰∏çÈúÄË¶Å‰∏éËÆ≠ÁªÉËæìÂÖ•ÂêëÈáè‰∏ÄËá¥). ‰∏ãÈù¢ÂÆö‰πâÊ†∑Êú¨ÂÜÖËØØÂ∑Æ, ‰ªéËÄåÁêÜËß£ËÆ≠ÁªÉËØØÂ∑Æ <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span> ÊòØ‰πêËßÇ‰º∞ËÆ°.</p>
<p>Define <strong>in-sample error</strong> as ÂÆö‰πâÊ†∑Êú¨ÂÜÖËØØÂ∑Æ [ËÄÉËôëÂà∞‰∫ÜÂô™Â£∞È°π]</p>
<div class="arithmatex">
<div class="MathJax_Preview">
E r r_{i n}=\frac{1}{n} \sum_{i=1}^{n} E_{Y^{\prime}}\left[L\left(y_{i}^{\prime}, \hat{f}\left(x_{i}\right)\right) \mid T\right]
</div>
<script type="math/tex; mode=display">
E r r_{i n}=\frac{1}{n} \sum_{i=1}^{n} E_{Y^{\prime}}\left[L\left(y_{i}^{\prime}, \hat{f}\left(x_{i}\right)\right) \mid T\right]
</script>
</div>
<p>where the expectation is over new responses <span class="arithmatex"><span class="MathJax_Preview">y_{i}^{\prime}</span><script type="math/tex">y_{i}^{\prime}</script></span> at each training point <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> ‰πüÂç≥, Âú®ÊØè‰∏Ä‰∏™Ê†∑Êú¨ÁÇπ <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> Â§Ñ, ÈÉΩËßÇÊµã‰∏ÄÁ≥ªÂàóÊñ∞ÁöÑÂìçÂ∫îÂèòÈáè, Ê±ÇÊúüÊúõ.</p>
<p>Define the <strong>optimism</strong> as</p>
<div class="arithmatex">
<div class="MathJax_Preview">
o p=E r r_{i n}-\overline{e r r}
</div>
<script type="math/tex; mode=display">
o p=E r r_{i n}-\overline{e r r}
</script>
</div>
<p>The <strong>average optimism</strong></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\omega=E_{y}[o p]
</div>
<script type="math/tex; mode=display">
\omega=E_{y}[o p]
</script>
</div>
<p>where</p>
<ul>
<li>the training input vectors are held fixed</li>
<li>the expectation is over the <strong>training output values</strong> [Âõ†‰∏∫ÊúâÈöèÊú∫Âô™Â£∞ÁöÑÂ≠òÂú®]</li>
</ul>
<p>For squared error, 0-1, and other loss functions, one can show quite generally that</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\omega=\frac{2}{n} \sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)
</div>
<script type="math/tex; mode=display">
\omega=\frac{2}{n} \sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)
</script>
</div>
<blockquote>
<p>Ë°•ÂÖÖËØ¥Êòé: Here the predictors in the training set are fixed, and the expectation is over the training set outcome values; hence we have used the notation <span class="arithmatex"><span class="MathJax_Preview">\mathrm{E}_{\mathbf{y}}</span><script type="math/tex">\mathrm{E}_{\mathbf{y}}</script></span> instead of <span class="arithmatex"><span class="MathJax_Preview">\mathrm{E}_{\mathcal{T}}</span><script type="math/tex">\mathrm{E}_{\mathcal{T}}</script></span>. We can usually estimate only the expected error <span class="arithmatex"><span class="MathJax_Preview">\omega</span><script type="math/tex">\omega</script></span> rather than <span class="arithmatex"><span class="MathJax_Preview">op</span><script type="math/tex">op</script></span>, in the same way that we can estimate the expected error <span class="arithmatex"><span class="MathJax_Preview">Err</span><script type="math/tex">Err</script></span> rather than the conditional error <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}_{\mathcal{T}}</span><script type="math/tex">\operatorname{Err}_{\mathcal{T}}</script></span>.</p>
</blockquote>
<p>ÂØπ‰∫éËøô‰∫õÊ¶ÇÂøµÁöÑÁêÜËß£ÂíåÂÖ¨ÂºèËØÅÊòéÂèÇËßÅ <a href="https://github.com/szcf-weiya/ESL-CN/issues/27">here</a> ÁöÑ‰π†È¢ò. ÊÄªËÄåË®Ä‰πã, ËÆ≠ÁªÉËØØÂ∑Æ‰ªÖÂíåÊ†∑Êú¨Áõ∏ÂÖ≥, ‰πüÂç≥‰∏Ä‰∏™ <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> ÂÖ∂ÂìçÂ∫îÂÄºÊòØÂõ∫ÂÆöÁöÑ; ÁÑ∂ËÄåÂØπ‰∫éÂêå‰∏Ä‰∏™Ê†∑Êú¨ÁÇπ, Áî±‰∫éÂô™Â£∞ÁöÑÂ≠òÂú®ÂÖ∂ÁúüÂÆûÁöÑÂìçÂ∫îÂÄºÊª°Ë∂≥‰∏Ä‰∏™ÂàÜÂ∏É, ËøôÈáåÁöÑÊ†∑Êú¨ÂÜÖËØØÂ∑Æ <span class="arithmatex"><span class="MathJax_Preview">Err_{in}</span><script type="math/tex">Err_{in}</script></span> Â∞±ÊòØËÄÉËôë‰∫ÜËøô‰∏ÄÁâπÁÇπ. ‰∏ÄËà¨ËÄåË®Ä, Ê†∑Êú¨ÂÜÖËØØÂ∑Æ‰ºöÊØîËÆ≠ÁªÉËØØÂ∑ÆÊõ¥Â§ß. ËÄå average optimism ÂàôÊòØÂú®Ê≠§Âü∫Á°Ä‰∏ä, ËÄÉËôë‰∫ÜËÆ≠ÁªÉÊ†∑Êú¨ÁöÑÂàÜÂ∏É, ÂØπ‰∫é y ÁöÑÂàÜÂ∏ÉËøõË°å‰∫ÜÊ±ÇÊúüÊúõ.</p>
<ul>
<li>The harder we fit the model, the stronger <span class="arithmatex"><span class="MathJax_Preview">y_{i}</span><script type="math/tex">y_{i}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\hat{y}_{i}</span><script type="math/tex">\hat{y}_{i}</script></span> are associated, and the larger <span class="arithmatex"><span class="MathJax_Preview">\omega</span><script type="math/tex">\omega</script></span></li>
<li>Large <span class="arithmatex"><span class="MathJax_Preview">\omega \rightarrow</span><script type="math/tex">\omega \rightarrow</script></span> greater optimism of <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span></li>
</ul>
<p>ÈÄöËøáÂØπ‰∫é op ÁöÑÂàÜÊûê, ÂèØ‰ª•ÂØπ‰∫éÊñπÂ∑Æ-ÂÅèÂ∑ÆÁöÑÂàÜËß£ÊúâÊõ¥Â•ΩÁöÑÁêÜËß£. Ê®°ÂûãÁöÑÂ§çÊùÇÂ∫¶Ë∂äÈ´ò, Ê†∑Êú¨ÁÇπ <span class="arithmatex"><span class="MathJax_Preview">y_{i}</span><script type="math/tex">y_{i}</script></span> ÂíåÊãüÂêàÂÄº <span class="arithmatex"><span class="MathJax_Preview">\hat{y}_{i}</span><script type="math/tex">\hat{y}_{i}</script></span> ÁöÑÁõ∏ÂÖ≥ÊÄßË∂äÈ´ò, Âàô average optimism  <span class="arithmatex"><span class="MathJax_Preview">\omega</span><script type="math/tex">\omega</script></span> ‰πüË∂äÈ´ò.</p>
<p>In summary, we have the <strong>important relation</strong></p>
<div class="arithmatex">
<div class="MathJax_Preview">
E_{y}\left[E r r_{i n}\right]=E_{y}[\overline{e r r}]+\frac{2}{n} \sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)
</div>
<script type="math/tex; mode=display">
E_{y}\left[E r r_{i n}\right]=E_{y}[\overline{e r r}]+\frac{2}{n} \sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)
</script>
</div>
<h4 id="how-to-estimate-prediction-error">How to Estimate Prediction Error<a class="headerlink" href="#how-to-estimate-prediction-error" title="Permanent link">&para;</a></h4>
<p>Option 1 [‰º∞ËÆ°Ê†∑Êú¨ÂÜÖËØØÂ∑Æ]</p>
<ul>
<li>Estimate the optimism and add it to <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">C_{p}, A I C, B I C</span><script type="math/tex">C_{p}, A I C, B I C</script></span> work in this way for a special class of estimates that are linear in their parameters</li>
<li>Can use in-sample error for model selection but not a good estimate of Err</li>
</ul>
<p>Option 2 [Áõ¥Êé•‰º∞ËÆ°Ê†∑Êú¨Â§ñËØØÂ∑Æ]</p>
<ul>
<li>Use cross-validation or bootstrap as direct estimates of the extra-sample <span class="arithmatex"><span class="MathJax_Preview">Err</span><script type="math/tex">Err</script></span> Êú¨Á´†ÂêéÈù¢ÊèèËø∞ÁöÑ‰∫§ÂèâÈ™åËØÅ‰ª•ÂèäËá™Âä©Ê≥ïÊòØÂØπ Ê†∑Êú¨Â§ñ (extra-sample) ËØØÂ∑Æ <span class="arithmatex"><span class="MathJax_Preview">Err</span><script type="math/tex">Err</script></span> Áõ¥Êé•‰º∞ËÆ°ÁöÑÊñπÊ≥ïÔºéËøô‰∫õ‰∏ÄËà¨Â∑•ÂÖ∑ÂèØ‰ª•Áî®‰∫é‰ªªÊÑèÊçüÂ§±ÂáΩÊï∞‰ª•ÂèäÈùûÁ∫øÊÄßËá™ÈÄÇÂ∫îÊãüÂêàÊäÄÂ∑ßÔºé</li>
</ul>
<p>Ê†∑Êú¨ÂÜÖËØØÂ∑ÆÈÄöÂ∏∏‰∏çÊòØÁõ¥Êé•ÊÑüÂÖ¥Ë∂£ÁöÑÔºåÂõ†‰∏∫ÁâπÂæÅÁöÑÊú™Êù•ÂÄº‰∏çÂèØËÉΩ‰∏éÂÆÉ‰ª¨ËÆ≠ÁªÉÈõÜÂÄº‰∏Ä Ëá¥Ôºé‰ΩÜÊòØ‰∏∫‰∫ÜÊ®°Âûã‰πãÈó¥ÁöÑÊØîËæÉÔºåÊ†∑Êú¨ÂÜÖËØØÂ∑ÆÊòØÂæàÊñπ‰æøÁöÑÔºåÂπ∂‰∏îÁªèÂ∏∏ËÉΩÂ§üÊúâÊïàÂú∞ËøõË°åÊ®°ÂûãÈÄâÊã©ÔºéÂéüÂõ†Âú®‰∫éËØØÂ∑ÆÁöÑÁõ∏ÂØπÔºàËÄå‰∏çÊòØÁªùÂØπÔºâÂ§ßÂ∞èÊòØÊàë‰ª¨ÊâÄÂÖ≥ÂøÉÁöÑÔºé</p>
<h3 id="estimates-of-err_inerr_in">Estimates of <span class="arithmatex"><span class="MathJax_Preview">Err_{in}</span><script type="math/tex">Err_{in}</script></span><a class="headerlink" href="#estimates-of-err_inerr_in" title="Permanent link">&para;</a></h3>
<h4 id="err_inerr_in-estimate-c_pc_p-statistic"><span class="arithmatex"><span class="MathJax_Preview">Err_{in}</span><script type="math/tex">Err_{in}</script></span> Estimate: <span class="arithmatex"><span class="MathJax_Preview">C_{p}</span><script type="math/tex">C_{p}</script></span> Statistic<a class="headerlink" href="#err_inerr_in-estimate-c_pc_p-statistic" title="Permanent link">&para;</a></h4>
<p>If <span class="arithmatex"><span class="MathJax_Preview">\hat{y}_{i}</span><script type="math/tex">\hat{y}_{i}</script></span> is obtained by a linear fit with <span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span> inputs, then</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)=d \sigma_{\epsilon}^{2}
</div>
<script type="math/tex; mode=display">
\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)=d \sigma_{\epsilon}^{2}
</script>
</div>
<p>for the additive error model <span class="arithmatex"><span class="MathJax_Preview">Y=f(X)+\epsilon</span><script type="math/tex">Y=f(X)+\epsilon</script></span></p>
<p>So (holds approximately when <span class="arithmatex"><span class="MathJax_Preview">N \rightarrow \infty</span><script type="math/tex">N \rightarrow \infty</script></span> )</p>
<div class="arithmatex">
<div class="MathJax_Preview">
E_{y}\left[E r r_{i n}\right]=E_{y}[\overline{e r r}]+2 \frac{d}{n} \sigma_{\epsilon}^{2}
</div>
<script type="math/tex; mode=display">
E_{y}\left[E r r_{i n}\right]=E_{y}[\overline{e r r}]+2 \frac{d}{n} \sigma_{\epsilon}^{2}
</script>
</div>
<p>Adapting this expression leads to the <span class="arithmatex"><span class="MathJax_Preview">C_{p}</span><script type="math/tex">C_{p}</script></span> statistic</p>
<div class="arithmatex">
<div class="MathJax_Preview">
C_{p}=\overline{e r r}+2 \frac{d}{n} \hat{\sigma}_{\epsilon}^{2}
</div>
<script type="math/tex; mode=display">
C_{p}=\overline{e r r}+2 \frac{d}{n} \hat{\sigma}_{\epsilon}^{2}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\hat{\sigma}_{\epsilon}^{2}</span><script type="math/tex">\hat{\sigma}_{\epsilon}^{2}</script></span> is an estimate of the <strong>noise variance</strong></p>
<h4 id="akaike-information-criterion">Akaike Information Criterion<a class="headerlink" href="#akaike-information-criterion" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
A I C=-\frac{2}{n} \log l i k+2 \frac{d}{n}
</div>
<script type="math/tex; mode=display">
A I C=-\frac{2}{n} \log l i k+2 \frac{d}{n}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\log l i k=\sum_{i=1}^{n} \log P_{\hat{\theta}}\left(y_{i}\right)</span><script type="math/tex">\log l i k=\sum_{i=1}^{n} \log P_{\hat{\theta}}\left(y_{i}\right)</script></span>, and <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}</span><script type="math/tex">\hat{\theta}</script></span> is the MLE of <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">-\frac{2}{n} \log l i k</span><script type="math/tex">-\frac{2}{n} \log l i k</script></span> rewards the fit between model and the data</li>
<li><span class="arithmatex"><span class="MathJax_Preview">2 \frac{d}{n}</span><script type="math/tex">2 \frac{d}{n}</script></span> is the penalty for including extra predictors in the model</li>
</ul>
<p>AIC can be seen as an estimate of <span class="arithmatex"><span class="MathJax_Preview">E r r_{i n}</span><script type="math/tex">E r r_{i n}</script></span> in this case with a <strong>log-likelihood loss</strong></p>
<p>Âè¶Â§ñ, Âú® ÈöèÊú∫È°πÊ≠£ÊÄÅÊÄßÂÅáËÆæ‰∏ã, ÂèØÁü•Âü∫‰∫é RSS ÁöÑ‰º∞ËÆ°Âíå AIC Á≠â‰ª∑, ÂèÇËßÅ <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion#Comparison_with_least_squares">wiki</a>. Âç≥ÊúâÂΩ¢Âºè <span class="arithmatex"><span class="MathJax_Preview">AIC = 2k + n \log(RSS)</span><script type="math/tex">AIC = 2k + n \log(RSS)</script></span></p>
<p><img alt="" src="../media/ASL-note2/2021-12-22-10-55-39.png" /></p>
<p>Ê≥®ÊÑè,</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)=d \sigma_{\epsilon}^{2}
</div>
<script type="math/tex; mode=display">
\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)=d \sigma_{\epsilon}^{2}
</script>
</div>
<p>ÂØπ‰∫éÂê´ÊúâÂä†ÊÄßËØØÂ∑ÆÁöÑÁ∫øÊÄßÊ®°ÂûãÂú®Âπ≥ÊñπËØØÂ∑ÆÊçüÂ§±‰∏ãÊòØÁ≤æÁ°ÆÊàêÁ´ãÁöÑÔºåÂú®ÂØπÊï∞‰ººÁÑ∂ÊçüÂ§±‰∏ãÊòØËøë‰ººÊàêÁ´ãÁöÑÔºéÁâπÂà´Âú∞ÔºåËøô‰∏™Ê≥ïÂàô‰∏ÄËà¨Âú∞ÂØπ‰∫é 0-1 ÊçüÂ§±ÊòØ‰∏çÊàêÁ´ãÁöÑ.</p>
<h4 id="effective-of-paramaters">EÔ¨Äective # of Paramaters<a class="headerlink" href="#effective-of-paramaters" title="Permanent link">&para;</a></h4>
<p>For regularized fitting need to generalize the concept of number of parameters, why?</p>
<p>Consider regularized linear fitting - ridge, smoothing splines</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{y}=S y
</div>
<script type="math/tex; mode=display">
\hat{y}=S y
</script>
</div>
<p>where</p>
<ol>
<li><span class="arithmatex"><span class="MathJax_Preview">y=\left(y_{1}, y_{2}, \ldots, y_{n}\right)</span><script type="math/tex">y=\left(y_{1}, y_{2}, \ldots, y_{n}\right)</script></span> is the vector of training outputs</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{y}=\left(\hat{y}_{1}, \ldots, \hat{y}_{n}\right)</span><script type="math/tex">\hat{y}=\left(\hat{y}_{1}, \ldots, \hat{y}_{n}\right)</script></span> is the vector of predictions</li>
<li><span class="arithmatex"><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> is an <span class="arithmatex"><span class="MathJax_Preview">n \times n</span><script type="math/tex">n \times n</script></span> matrix, which depends on <span class="arithmatex"><span class="MathJax_Preview">x_{1}, \ldots, x_{n}</span><script type="math/tex">x_{1}, \ldots, x_{n}</script></span> but not <span class="arithmatex"><span class="MathJax_Preview">y_{1}, \ldots, y_{n}</span><script type="math/tex">y_{1}, \ldots, y_{n}</script></span></li>
</ol>
<p>Define the <strong>effective number of parameters</strong> as ÊúâÊïàÂèÇÊï∞‰∏™Êï∞, ‰πüË¢´Áß∞‰Ωú ÊúâÊïàËá™Áî±Â∫¶ (effective degrees-of-freedom)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
d f(S)=\operatorname{trace}(S)
</div>
<script type="math/tex; mode=display">
d f(S)=\operatorname{trace}(S)
</script>
</div>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> is an <strong>orthogonal-projection</strong> matrix onto a basis set spanned by <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> features, then <span class="arithmatex"><span class="MathJax_Preview">\operatorname{trace}(S)=M</span><script type="math/tex">\operatorname{trace}(S)=M</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{tr}(S)</span><script type="math/tex">\operatorname{tr}(S)</script></span> - the correct quantity to replace <span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span> as the number of parameters in the <span class="arithmatex"><span class="MathJax_Preview">C_{p}</span><script type="math/tex">C_{p}</script></span> statistic. Âú® <span class="arithmatex"><span class="MathJax_Preview">C_{p}</span><script type="math/tex">C_{p}</script></span> ÁªüËÆ°Èáè‰∏≠ÁöÑ d Â∫îËØ•ÊòØËøôÈáåÊâÄÂÆö‰πâÁöÑ„ÄåÊúâÊïàËá™Áî±Â∫¶„Äç</li>
<li>If <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> arises from an additive error model <span class="arithmatex"><span class="MathJax_Preview">Y=f(X)+\epsilon</span><script type="math/tex">Y=f(X)+\epsilon</script></span> with <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Var}(\epsilon)=\sigma_{\epsilon}^{2}</span><script type="math/tex">\operatorname{Var}(\epsilon)=\sigma_{\epsilon}^{2}</script></span>, then [‰∏äÈù¢ÊúâÊïàËá™Áî±Â∫¶ÁöÑÂÆö‰πâÂü∫‰∫éÁ∫øÊÄßÂèòÊç¢ÁöÑÁü©Èòµ, ËÄåÂú®Âä†ÊÄßÈöèÊú∫È°πÂÅáËÆæ‰∏ã, ÂèØ‰ª•ÂæóÂà∞Êõ¥‰∏ÄËà¨ÊÄßÁöÑÂÆö‰πâ] ËØÅÊòé see <a href="https://github.com/szcf-weiya/ESL-CN/issues/195">here</a></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)=\operatorname{trace}(S) \sigma_{\epsilon}^{2}
</div>
<script type="math/tex; mode=display">
\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)=\operatorname{trace}(S) \sigma_{\epsilon}^{2}
</script>
</div>
<p>So the more general definitin of dof is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
d y(\hat{y})=\frac{\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)}{\sigma_{\epsilon}^{2}}
</div>
<script type="math/tex; mode=display">
d y(\hat{y})=\frac{\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)}{\sigma_{\epsilon}^{2}}
</script>
</div>
<h4 id="bayesian-approach-bic">Bayesian Approach &amp; BIC<a class="headerlink" href="#bayesian-approach-bic" title="Permanent link">&para;</a></h4>
<p>Generic Form of BIC</p>
<p><strong>Bayesian Information Criterion</strong> (BIC)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
B I C=-2 \log l i k+(\log N) \cdot d
</div>
<script type="math/tex; mode=display">
B I C=-2 \log l i k+(\log N) \cdot d
</script>
</div>
<ul>
<li>BIC statistic <span class="arithmatex"><span class="MathJax_Preview">\left(\times \frac{1}{2}\right)</span><script type="math/tex">\left(\times \frac{1}{2}\right)</script></span> is also known as the Schwarz criterion (SBC)</li>
</ul>
<p>Under Gaussian model with known variance <span class="arithmatex"><span class="MathJax_Preview">\sigma_{\epsilon}^{2}</span><script type="math/tex">\sigma_{\epsilon}^{2}</script></span>, for squared error loss</p>
<div class="arithmatex">
<div class="MathJax_Preview">
-2 \log \operatorname{lik} \propto \sum_{i} \frac{\left(y_{i}-\hat{f}\left(x_{i}\right)\right)^{2}}{\sigma_{\epsilon}^{2}}=\frac{N \cdot \overline{e r r}}{\sigma_{\epsilon}^{2}}
</div>
<script type="math/tex; mode=display">
-2 \log \operatorname{lik} \propto \sum_{i} \frac{\left(y_{i}-\hat{f}\left(x_{i}\right)\right)^{2}}{\sigma_{\epsilon}^{2}}=\frac{N \cdot \overline{e r r}}{\sigma_{\epsilon}^{2}}
</script>
</div>
<p>As a result, BIC can be re-written as</p>
<div class="arithmatex">
<div class="MathJax_Preview">
B I C=\frac{n}{\sigma_{\epsilon}^{2}}\left(\overline{e r r}+\log (N) \frac{d}{N} \sigma_{\epsilon}^{2}\right)
</div>
<script type="math/tex; mode=display">
B I C=\frac{n}{\sigma_{\epsilon}^{2}}\left(\overline{e r r}+\log (N) \frac{d}{N} \sigma_{\epsilon}^{2}\right)
</script>
</div>
<ul>
<li>BIC is proportional to AIC, with factor 2 replaced by <span class="arithmatex"><span class="MathJax_Preview">\log N</span><script type="math/tex">\log N</script></span> [Âõ†Ê≠§ÂèØÁü•ÂΩì <span class="arithmatex"><span class="MathJax_Preview">N&gt;e^2=7.4</span><script type="math/tex">N>e^2=7.4</script></span> Êó∂, BIC ÂØπ‰∫éÊ®°ÂûãÂèÇÊï∞ÁöÑÊÉ©ÁΩöÊõ¥Â§ß]</li>
</ul>
<h5 id="derivation-of-bic">Derivation of BIC<a class="headerlink" href="#derivation-of-bic" title="Permanent link">&para;</a></h5>
<p>Starting point:</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\left\{M_{1}, \ldots, M_{m}\right\}</span><script type="math/tex">\left\{M_{1}, \ldots, M_{m}\right\}</script></span>, a set of candidate models and their corresponding parameters <span class="arithmatex"><span class="MathJax_Preview">\theta_{1}, \ldots, \theta_{m}</span><script type="math/tex">\theta_{1}, \ldots, \theta_{m}</script></span></li>
</ul>
<p>Goal:</p>
<ul>
<li>Choose the best model <span class="arithmatex"><span class="MathJax_Preview">M_{i}</span><script type="math/tex">M_{i}</script></span></li>
</ul>
<p>How</p>
<ul>
<li>Training data <span class="arithmatex"><span class="MathJax_Preview">Z=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</span><script type="math/tex">Z=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</script></span></li>
<li>Priors <span class="arithmatex"><span class="MathJax_Preview">p\left(\theta_{m} \mid M_{m}\right)</span><script type="math/tex">p\left(\theta_{m} \mid M_{m}\right)</script></span></li>
<li>Derive the Posterior of model <span class="arithmatex"><span class="MathJax_Preview">M_{m}</span><script type="math/tex">M_{m}</script></span></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
P\left(M_{m} \mid Z\right) &amp; \propto P\left(M_{m}\right) p\left(Z \mid M_{m}\right) \\
&amp; \propto P\left(M_{m}\right) \int p\left(Z \mid \theta_{m}, M_{m}\right) p\left(\theta_{m} \mid M_{m}\right) d \theta_{m}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
P\left(M_{m} \mid Z\right) & \propto P\left(M_{m}\right) p\left(Z \mid M_{m}\right) \\
& \propto P\left(M_{m}\right) \int p\left(Z \mid \theta_{m}, M_{m}\right) p\left(\theta_{m} \mid M_{m}\right) d \theta_{m}
\end{aligned}
</script>
</div>
<p>In comparision of two models <span class="arithmatex"><span class="MathJax_Preview">M_{m}</span><script type="math/tex">M_{m}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">M_{1}</span><script type="math/tex">M_{1}</script></span> ‰∏∫‰∫ÜÊØîËæÉ‰∏§‰∏™Ê®°Âûã, ÊûÑÈÄ† ÂêéÈ™å odds (ÈÄâÊã©Ê¶ÇÁéáËæÉÂ§ßÁöÑÊ®°Âûã)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{\operatorname{Pr}\left(M_{m} \mid Z\right)}{\operatorname{Pr}\left(M_{l} \mid Z\right)}=\frac{\operatorname{Pr}\left(M_{m}\right)}{\operatorname{Pr}\left(M_{l}\right)} \times \frac{\operatorname{Pr}\left(Z \mid M_{m}\right)}{\operatorname{Pr}\left(Z \mid M_{l}\right)}
</div>
<script type="math/tex; mode=display">
\frac{\operatorname{Pr}\left(M_{m} \mid Z\right)}{\operatorname{Pr}\left(M_{l} \mid Z\right)}=\frac{\operatorname{Pr}\left(M_{m}\right)}{\operatorname{Pr}\left(M_{l}\right)} \times \frac{\operatorname{Pr}\left(Z \mid M_{m}\right)}{\operatorname{Pr}\left(Z \mid M_{l}\right)}
</script>
</div>
<ul>
<li>Posterior odds <span class="arithmatex"><span class="MathJax_Preview">=</span><script type="math/tex">=</script></span> prior odds <span class="arithmatex"><span class="MathJax_Preview">* \operatorname{BF}(Z)</span><script type="math/tex">* \operatorname{BF}(Z)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">B F(Z)=\frac{\operatorname{Pr}\left(Z \mid M_{m}\right)}{\operatorname{Pr}\left(Z \mid M_{l}\right)}</span><script type="math/tex">B F(Z)=\frac{\operatorname{Pr}\left(Z \mid M_{m}\right)}{\operatorname{Pr}\left(Z \mid M_{l}\right)}</script></span> is called <strong>Bayes factor</strong> Ë¥ùÂè∂ÊñØÂõ†Â≠ê</li>
<li>Indicates the contribution of the data toward the posterior odds Êï∞ÊçÆÂØπ‰∫éÂêéÈ™å odds ÁöÑË¥°ÁåÆ</li>
</ul>
<p>The posterior of model <span class="arithmatex"><span class="MathJax_Preview">M_{m}</span><script type="math/tex">M_{m}</script></span> is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(M_{m} \mid Z\right) \propto P\left(M_{m}\right) \int p\left(Z \mid \theta_{m}, M_{m}\right) p\left(\theta_{m} \mid M_{m}\right) d \theta_{m}
</div>
<script type="math/tex; mode=display">
P\left(M_{m} \mid Z\right) \propto P\left(M_{m}\right) \int p\left(Z \mid \theta_{m}, M_{m}\right) p\left(\theta_{m} \mid M_{m}\right) d \theta_{m}
</script>
</div>
<ul>
<li>Usually assume uniform prior <span class="arithmatex"><span class="MathJax_Preview">P\left(M_{m}\right)=1 / M</span><script type="math/tex">P\left(M_{m}\right)=1 / M</script></span></li>
<li>Approximate <span class="arithmatex"><span class="MathJax_Preview">\int p\left(Z \mid \theta_{m}, M_{m}\right) p\left(\theta_{m} \mid M_{m}\right) d \theta_{m}</span><script type="math/tex">\int p\left(Z \mid \theta_{m}, M_{m}\right) p\left(\theta_{m} \mid M_{m}\right) d \theta_{m}</script></span> by <strong>simplification and Laplace approximation</strong></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\log P\left(Z \mid M_{m}\right)=\log P\left(Z \mid \hat{\theta}_{m}, M_{m}\right)-\frac{d_{m}}{2} \log N+O(1)
</div>
<script type="math/tex; mode=display">
\log P\left(Z \mid M_{m}\right)=\log P\left(Z \mid \hat{\theta}_{m}, M_{m}\right)-\frac{d_{m}}{2} \log N+O(1)
</script>
</div>
<ul>
<li>where <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}_{m}</span><script type="math/tex">\hat{\theta}_{m}</script></span> is MLE and <span class="arithmatex"><span class="MathJax_Preview">d_{m}</span><script type="math/tex">d_{m}</script></span> is # of free parameters in <span class="arithmatex"><span class="MathJax_Preview">M_{m}</span><script type="math/tex">M_{m}</script></span></li>
</ul>
<p>If we define the loss function as <span class="arithmatex"><span class="MathJax_Preview">-2 \log \operatorname{Pr}\left(Z \mid \hat{\theta}_{m}, M_{m}\right)</span><script type="math/tex">-2 \log \operatorname{Pr}\left(Z \mid \hat{\theta}_{m}, M_{m}\right)</script></span>, then it&rsquo;s equivalent to BIC</p>
<h5 id="aic-vs-bic">AIC vs BIC<a class="headerlink" href="#aic-vs-bic" title="Permanent link">&para;</a></h5>
<p>If <span class="arithmatex"><span class="MathJax_Preview">M_{\text {true }} \in\left\{M_{1}, \ldots, M_{M}\right\}</span><script type="math/tex">M_{\text {true }} \in\left\{M_{1}, \ldots, M_{M}\right\}</script></span> then as <span class="arithmatex"><span class="MathJax_Preview">N \rightarrow \infty</span><script type="math/tex">N \rightarrow \infty</script></span></p>
<ul>
<li>BIC will select <span class="arithmatex"><span class="MathJax_Preview">M_{\text {true }}</span><script type="math/tex">M_{\text {true }}</script></span> with probability 1</li>
<li>AIC will not, tends to choose models which are too complex</li>
</ul>
<p>However, when <span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> is small</p>
<ul>
<li>BIC often choose models which are too simple</li>
</ul>
<p>In addition, for BIC</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\frac{e^{-\frac{1}{2} B I C_{m}}}{\sum_{l=1}^{M} e^{-\frac{1}{2} B I C_{l}}}</span><script type="math/tex">\frac{e^{-\frac{1}{2} B I C_{m}}}{\sum_{l=1}^{M} e^{-\frac{1}{2} B I C_{l}}}</script></span> [ÂØπ <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> ‰∏™ÂÖÉÁ¥†ÁöÑÊ®°ÂûãÈõÜÂêàËøõË°åËÆ°ÁÆó BIC ÂáÜÂàô, ÂèØ‰ª•ÂæóÂà∞ÊØè‰∏™Ê®°ÂûãÁöÑÂêéÈ™åÊ¶ÇÁéáÂç≥‰∏∫Ê≠§]</li>
<li>Estimate not only the best model, but also their relative merits</li>
</ul>
<h3 id="cross-validation">Cross Validation<a class="headerlink" href="#cross-validation" title="Permanent link">&para;</a></h3>
<h4 id="k-fold-cross-validation">K-Fold Cross Validation<a class="headerlink" href="#k-fold-cross-validation" title="Permanent link">&para;</a></h4>
<p>General Approach</p>
<ul>
<li>Split the data into <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span> roughly equal-size parts</li>
<li>For the <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> part, calculate the prediction error of the model fit using the other K-1 parts</li>
<li>Do this for <span class="arithmatex"><span class="MathJax_Preview">k=1,2, \ldots, K</span><script type="math/tex">k=1,2, \ldots, K</script></span> and combine the <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span> estimates of the <strong>prediction error</strong></li>
</ul>
<p>When and Why</p>
<ul>
<li>It is applied when labelled training data is relatively sparse ÈÄÇÂêàËÆ≠ÁªÉÊï∞ÊçÆËæÉÂ∞ëÁöÑÊÉÖÂÜµ</li>
<li>This method directly estimates <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}=E[L(Y, \hat{f}(X))]</span><script type="math/tex">\operatorname{Err}=E[L(Y, \hat{f}(X))]</script></span> Áõ¥Êé•‰º∞ËÆ°‰∫ÜÊ†∑Êú¨Â§ñËØØÂ∑Æ</li>
</ul>
<p>ÂÖ∑‰ΩìËÄåË®Ä</p>
<ul>
<li>The mapping <span class="arithmatex"><span class="MathJax_Preview">\kappa: 1, \ldots, n \rightarrow 1, \ldots, K</span><script type="math/tex">\kappa: 1, \ldots, n \rightarrow 1, \ldots, K</script></span> indicates observation <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> belongs to partition <span class="arithmatex"><span class="MathJax_Preview">\kappa(i)</span><script type="math/tex">\kappa(i)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{f}^{-k}(x)</span><script type="math/tex">\hat{f}^{-k}(x)</script></span> is the function fitted with the <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> part of the data removed</li>
<li>Cross-validation estimate the prediction error</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
C V(\hat{f})=\frac{1}{n} \sum_{i=1}^{n} L\left(y_{i}, \hat{f}^{-\kappa(i)}\left(x_{i}\right)\right.
</div>
<script type="math/tex; mode=display">
C V(\hat{f})=\frac{1}{n} \sum_{i=1}^{n} L\left(y_{i}, \hat{f}^{-\kappa(i)}\left(x_{i}\right)\right.
</script>
</div>
<ul>
<li>Typical choices for <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span> are 5 or 10</li>
<li>The case <span class="arithmatex"><span class="MathJax_Preview">K=n</span><script type="math/tex">K=n</script></span> is shown as <strong>leave-one-out cross-validation</strong> Áïô‰∏ÄÊ≥ï (<strong>LOOCV</strong>)</li>
</ul>
<p>ËøôÊ†∑ÁöÑËØù, Êàë‰πàÂà©Áî®‰∫ÜËÆ≠ÁªÉÊï∞ÊçÆÂØπ‰∫éÊ≥õÂåñËØØÂ∑ÆÊúâ‰∫Ü‰∏Ä‰∏™‰º∞ËÆ°. ÈÇ£‰πàÂ¶Ç‰ΩïÂà©Áî® CV Êù•ËøõË°åÊ®°ÂûãÈÄâÊã©? Êàë‰ª¨Áõ¥Êé•ÈÄâÊã©Âú® CV ÁöÑ‰º∞ËÆ°‰∏≠Ê≥õÂåñËØØÂ∑ÆÊúÄÂ∞èÁöÑÈÇ£‰∏™Ê®°Âûã.</p>
<p>Candidate models <span class="arithmatex"><span class="MathJax_Preview">f(x, \alpha)</span><script type="math/tex">f(x, \alpha)</script></span> indexed by a parameter <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> <span class="arithmatex"><span class="MathJax_Preview">\hat{f}^{-k}(x, \alpha)</span><script type="math/tex">\hat{f}^{-k}(x, \alpha)</script></span> is the <span class="arithmatex"><span class="MathJax_Preview">\alpha_{t h}</span><script type="math/tex">\alpha_{t h}</script></span> model fit with <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> part of the data removed. The define</p>
<div class="arithmatex">
<div class="MathJax_Preview">
C V(\hat{f}, \alpha)=\frac{1}{n} \sum_{i=1}^{n} L\left(y_{i}, \hat{f}^{-\kappa(i)}\left(x_{i}, \alpha\right)\right.
</div>
<script type="math/tex; mode=display">
C V(\hat{f}, \alpha)=\frac{1}{n} \sum_{i=1}^{n} L\left(y_{i}, \hat{f}^{-\kappa(i)}\left(x_{i}, \alpha\right)\right.
</script>
</div>
<p>Choose the model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\alpha}=\operatorname{argmin}_{\alpha} C V(\hat{f}, \alpha)
</div>
<script type="math/tex; mode=display">
\hat{\alpha}=\operatorname{argmin}_{\alpha} C V(\hat{f}, \alpha)
</script>
</div>
<h5 id="what-does-cross-validation-estimate">What Does Cross Validation Estimate<a class="headerlink" href="#what-does-cross-validation-estimate" title="Permanent link">&para;</a></h5>
<p>Intuition says</p>
<ul>
<li>When <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}=5</span><script type="math/tex">\mathrm{K}=5</script></span> or <span class="arithmatex"><span class="MathJax_Preview">10, C V(\hat{f}) \approx \operatorname{Err}</span><script type="math/tex">10, C V(\hat{f}) \approx \operatorname{Err}</script></span> as training sets for each fold are fairly different</li>
<li>When <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}=\mathrm{n}, C V(\hat{f}) \approx \operatorname{Err}_{T}</span><script type="math/tex">\mathrm{K}=\mathrm{n}, C V(\hat{f}) \approx \operatorname{Err}_{T}</script></span> as training sets for each fold are almost identical</li>
</ul>
<p>However, according to theory and simulation experiments</p>
<ul>
<li>Cross validation really only effectively estimates Err [ÂèÇËßÅ 7.12 ËäÇ]</li>
</ul>
<h5 id="what-values-of-mathrmkmathrmk">What Values of <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span><a class="headerlink" href="#what-values-of-mathrmkmathrmk" title="Permanent link">&para;</a></h5>
<p>When <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}=\mathrm{n}</span><script type="math/tex">\mathrm{K}=\mathrm{n}</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">C V(\hat{f})</span><script type="math/tex">C V(\hat{f})</script></span> is approximately an unbiased estimate of <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}</span><script type="math/tex">\operatorname{Err}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">C V(\hat{f})</span><script type="math/tex">C V(\hat{f})</script></span> has high variance as the <span class="arithmatex"><span class="MathJax_Preview">\mathrm{n}</span><script type="math/tex">\mathrm{n}</script></span> training sets are similar</li>
<li>Computational burden high (except for a few exceptions)</li>
</ul>
<p>When <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}=5</span><script type="math/tex">\mathrm{K}=5</script></span> (lowish)</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">C V(\hat{f})</span><script type="math/tex">C V(\hat{f})</script></span> has low variance</li>
<li><span class="arithmatex"><span class="MathJax_Preview">C V(\hat{f})</span><script type="math/tex">C V(\hat{f})</script></span> is potentially an upward biased estimate of <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}</span><script type="math/tex">\operatorname{Err}</script></span> ÂÅèÂ∑ÆËæÉÂ§ß</li>
</ul>
<p><img alt="" src="../media/ASL-note2/2021-12-22-12-03-08.png" /></p>
<h4 id="cross-validation-right-wrong">Cross Validation - Right \&amp; Wrong<a class="headerlink" href="#cross-validation-right-wrong" title="Permanent link">&para;</a></h4>
<p>Task: classification problem with a large number of predictors</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">N=50</span><script type="math/tex">N=50</script></span> samples in two equal-sized classes</li>
<li><span class="arithmatex"><span class="MathJax_Preview">p=5000</span><script type="math/tex">p=5000</script></span> quantitative predictors (standard Gaussian) independent of class labels</li>
<li>True test error rate of any classifier <span class="arithmatex"><span class="MathJax_Preview">50 \%</span><script type="math/tex">50 \%</script></span></li>
</ul>
<p>Strategy 1</p>
<ol>
<li>Screen the predictors: Find a subset of good predictors that are correlated with the class labels</li>
<li>Build a classifier: based on the subset of good predictors</li>
<li>Perform cross-validation: estimate the unknown tuning parameters and Err of the final model</li>
</ol>
<p>Strategy 2</p>
<ol>
<li>Divide the samples into <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span> groups randomly</li>
<li>For each fold <span class="arithmatex"><span class="MathJax_Preview">k=1, \ldots, K</span><script type="math/tex">k=1, \ldots, K</script></span>
   - Find a subset of good predictors using all the samples minus the <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> fold
   - Build a classifier using all the samples minus the <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> fold
   - Use the classifier to predict the labels for the samples in the <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> fold</li>
<li>The error estimates from step 2 are then accumulated over all <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> folds to produce the cross-validation estimate of prediction error</li>
</ol>
<p><img alt="" src="../media/ASL-note2/2021-12-22-14-07-53.png" /></p>
<p>‰∏ÄËà¨Âú∞ÔºåÂú®Â§öÊ≠•Âª∫Ê®°ËøáÁ®ã‰∏≠Ôºå‰∫§ÂèâÈ™åËØÅÂøÖÈ°ªÂ∫îÁî®Âà∞Êï¥‰∏™Ê®°ÂûãÊ≠•È™§ÁöÑÂ∫èÂàó‰∏≠ÔºéÁâπÂà´Âú∞Ôºå<strong>‚Äú‰∏¢ÂºÉ‚ÄùÊ†∑Êú¨ÂøÖÈ°ªÂú®‰ªª‰ΩïÈÄâÊã©ÊàñËÄÖËøáÊª§‰πãÂâç</strong>ÔºéÊúâ‰∏Ä‰∏™Êù°‰ª∂ÔºöÂàùÂßãÈùûÁõëÁù£Á≠õÈÄâÊ≠•È™§ÂèØ‰ª•Âú®‰∏¢ÂºÉÊ†∑Êú¨‰πãÂâçÂÆåÊàêÔºé‰∏æ‰∏™‰æãÂ≠êÔºåÂºÄÂßã‰∫§ÂèâÈ™åËØÅÂâçÔºåÊàë‰ª¨ÂèØ‰ª•ÈÄâÊã© 1000 ‰∏™Âú® 50 ‰∏™Ê†∑Êú¨‰∏äÊúâÁùÄÊúÄÂ§ßÊñπÂ∑ÆÁöÑÈ¢ÑÊµãÂèòÈáèÔºéÂõ†‰∏∫Ëøô‰∏™ËøáÊª§‰∏çÊ∂âÂèäÂà∞Á±ªÂà´ÔºåÊâÄ‰ª•ÂÆÉ‰∏ç‰ºöÁªôÈ¢ÑÊµãÂèòÈáè‰∏çÂÖ¨Âπ≥ÁöÑÂ•ΩÂ§ÑÔºé</p>
<h3 id="bootstrap-method">Bootstrap Method<a class="headerlink" href="#bootstrap-method" title="Permanent link">&para;</a></h3>
<p>The Bootstrap
Given a training set <span class="arithmatex"><span class="MathJax_Preview">X=\left(z_{1}, z_{2}, \ldots, z_{n}\right)</span><script type="math/tex">X=\left(z_{1}, z_{2}, \ldots, z_{n}\right)</script></span> with each <span class="arithmatex"><span class="MathJax_Preview">z_{i}=\left(x_{i}, y_{i}\right)</span><script type="math/tex">z_{i}=\left(x_{i}, y_{i}\right)</script></span></p>
<p>The <strong>bootstrap</strong> idea is
For <span class="arithmatex"><span class="MathJax_Preview">b=1,2, \ldots, B</span><script type="math/tex">b=1,2, \ldots, B</script></span></p>
<ol>
<li>Randomly draw <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> samples <strong>with replacement</strong> from <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> to get <span class="arithmatex"><span class="MathJax_Preview">Z^{*b}</span><script type="math/tex">Z^{*b}</script></span> that is</li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
Z^{* b}=\left(z_{b_{1}}, z_{b_{2}}, \ldots, z_{b_{n}}\right) \text { with } b_{i} \in 1,2, \ldots, n
</div>
<script type="math/tex; mode=display">
Z^{* b}=\left(z_{b_{1}}, z_{b_{2}}, \ldots, z_{b_{n}}\right) \text { with } b_{i} \in 1,2, \ldots, n
</script>
</div>
<ol start="2">
<li>Refit the model using <span class="arithmatex"><span class="MathJax_Preview">Z^{*b}</span><script type="math/tex">Z^{*b}</script></span> to get <span class="arithmatex"><span class="MathJax_Preview">S\left(Z^{* b}\right)</span><script type="math/tex">S\left(Z^{* b}\right)</script></span></li>
</ol>
<p>Examine the behavior of the <span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span> fits</p>
<div class="arithmatex">
<div class="MathJax_Preview">
S\left(Z^{*1}\right), S\left(Z^{* 2}\right), \ldots, S\left(Z^{* B}\right)
</div>
<script type="math/tex; mode=display">
S\left(Z^{*1}\right), S\left(Z^{* 2}\right), \ldots, S\left(Z^{* B}\right)
</script>
</div>
<p><img alt="" src="../media/ASL-note2/2021-12-22-15-16-34.png" /></p>
<p><strong>Can Estimate Any Aspect of the Distribution</strong> of <span class="arithmatex"><span class="MathJax_Preview">S(Z)</span><script type="math/tex">S(Z)</script></span></p>
<p>For example its variance</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\operatorname{Var}}[S(Z)]=\frac{1}{B-1} \sum_{b=1}^{B}\left(S\left(Z^{* b}\right)-\bar{S}^{*}\right)^{2}
</div>
<script type="math/tex; mode=display">
\hat{\operatorname{Var}}[S(Z)]=\frac{1}{B-1} \sum_{b=1}^{B}\left(S\left(Z^{* b}\right)-\bar{S}^{*}\right)^{2}
</script>
</div>
<p>where</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\bar{S}^{*}=\frac{1}{B} \sum_{b=1}^{B} S\left(Z^{* b}\right)
</div>
<script type="math/tex; mode=display">
\bar{S}^{*}=\frac{1}{B} \sum_{b=1}^{B} S\left(Z^{* b}\right)
</script>
</div>
<h4 id="attempt-1">Attempt 1<a class="headerlink" href="#attempt-1" title="Permanent link">&para;</a></h4>
<p>$$
\hat{E r r r}<em b="1">{b o o t}=\frac{1}{B} \frac{1}{n} \sum</em>\right)\right)
$$
where }^{B} \sum_{i=1}^{n} L\left(y_{i}, \hat{f}^{*b}\left(x_{i<span class="arithmatex"><span class="MathJax_Preview">\hat{f}^{*b}\left(x_{i}\right)</span><script type="math/tex">\hat{f}^{*b}\left(x_{i}\right)</script></span> is the predicted value at <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> using the model computed from <span class="arithmatex"><span class="MathJax_Preview">Z^{* b}</span><script type="math/tex">Z^{* b}</script></span></p>
<ul>
<li>Is this a good estimate or not? Why</li>
<li>Overlap between the training and testing sets</li>
<li>How could we do better?</li>
<li>Mimic cross validation</li>
</ul>
<p>ËøôÁßçÊñπÂºè‰∏≠, ‰∫ãÂÆû‰∏ä‰∫åÂ±ÇÁöÑÊçüÂ§±ÂáΩÊï∞ÁÆóÁöÑÊòØÊ†∑Êú¨ÂÜÖËØØÂ∑Æ, Âõ†Ê≠§‰ºö‰Ωé‰º∞È¢ÑÊµãËØØÂ∑Æ.</p>
<h4 id="attempt-2-leave-one-out-bootstrap">Attempt 2: leave-one-out bootstrap<a class="headerlink" href="#attempt-2-leave-one-out-bootstrap" title="Permanent link">&para;</a></h4>
<p>Âõ†Ê≠§, ËøôÈáåÁöÑÊÉ≥Ê≥ïÊòØ‰ΩøÂæóËÆ≠ÁªÉÂíåÊµãËØïÈõÜÂàÜÁ¶ª.</p>
<p>$$
\hat{E r r}^{(1)}=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{\left|C^{-i}\right|} \sum_{b \in C^{-i}} L\left(y_{i}, \hat{f}^{* b}\left(x_{i}\right)\right)
$$
where <span class="arithmatex"><span class="MathJax_Preview">C^{-i}</span><script type="math/tex">C^{-i}</script></span> is the set of bootstrap samples <span class="arithmatex"><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> not containing observation <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span></p>
<ul>
<li>Either make <span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span> large enough so <span class="arithmatex"><span class="MathJax_Preview">\left|C^{-i}\right|&gt;0</span><script type="math/tex">\left|C^{-i}\right|>0</script></span> for all <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> or</li>
<li>Omit observation <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> from testing if <span class="arithmatex"><span class="MathJax_Preview">\left|C^{-i}\right|=0</span><script type="math/tex">\left|C^{-i}\right|=0</script></span></li>
</ul>
<p>Pros</p>
<ol>
<li>avoids the overfitting problems of <span class="arithmatex"><span class="MathJax_Preview">\bar{Err}_{boot}</span><script type="math/tex">\bar{Err}_{boot}</script></span></li>
</ol>
<p>Cons</p>
<ol>
<li>Has the training-set-size bias of corss-validation ‰ºöÊúâÂú®ËÆ®ËÆ∫‰∫§ÂèâÈ™åËØÅ‰∏≠ÊèêÂà∞ÁöÑ ËÆ≠ÁªÉÈõÜÂ§ßÂ∞èÂÅèÂ∑Æ (training-set-size) ÈóÆÈ¢ò</li>
<li>Probability that observation <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> in bootstrap sample <span class="arithmatex"><span class="MathJax_Preview">Z^{*b}</span><script type="math/tex">Z^{*b}</script></span></li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(i \in Z^{* b}\right)=1-\left(1-\frac{1}{n}\right)^{n} \approx 1-e^{-1}=0.632
</div>
<script type="math/tex; mode=display">
P\left(i \in Z^{* b}\right)=1-\left(1-\frac{1}{n}\right)^{n} \approx 1-e^{-1}=0.632
</script>
</div>
<ol start="3">
<li>Therefore the average number of distinct observations in <span class="arithmatex"><span class="MathJax_Preview">Z^{* b}</span><script type="math/tex">Z^{* b}</script></span> is <span class="arithmatex"><span class="MathJax_Preview">0.632 n</span><script type="math/tex">0.632 n</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\overline{E r r}^{(1)}</span><script type="math/tex">\overline{E r r}^{(1)}</script></span> bias similar to two fold cross validation (usually upward) Âêë‰∏äÁöÑÂÅèÂ∑Æ</li>
</ol>
<h4 id="attempt-3-the-632-estimator-to-alleviate-bias">Attempt 3: The .632 estimator To Alleviate Bias<a class="headerlink" href="#attempt-3-the-632-estimator-to-alleviate-bias" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
\overline{E r r}^{(.632)}=.368 \overline{e r r}+.632 \overline{E r r}^{(1)}
</div>
<script type="math/tex; mode=display">
\overline{E r r}^{(.632)}=.368 \overline{e r r}+.632 \overline{E r r}^{(1)}
</script>
</div>
<ul>
<li>Compromise between the training error <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span> and the leave-one-out bootstrap estimate</li>
<li>Derivation not easy</li>
<li>The constant <span class="arithmatex"><span class="MathJax_Preview">.632</span><script type="math/tex">.632</script></span> relates to <span class="arithmatex"><span class="MathJax_Preview">P\left(i \in Z^{* b}\right)</span><script type="math/tex">P\left(i \in Z^{* b}\right)</script></span></li>
</ul>
<p>However, .632 estimator does not do well if predictor overfits</p>
<h4 id="estimate-the-degree-of-overfitting">Estimate the Degree of Overfitting<a class="headerlink" href="#estimate-the-degree-of-overfitting" title="Permanent link">&para;</a></h4>
<p><strong>No-information error rate</strong> Êó†‰ø°ÊÅØËØØÂ∑ÆÁéá</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\gamma}=\frac{1}{n^{2}} \sum_{i=1}^{n} \sum_{j=1}^{n} L\left(y_{i}, \hat{f}\left(x_{j}\right)\right)
</div>
<script type="math/tex; mode=display">
\hat{\gamma}=\frac{1}{n^{2}} \sum_{i=1}^{n} \sum_{j=1}^{n} L\left(y_{i}, \hat{f}\left(x_{j}\right)\right)
</script>
</div>
<ul>
<li>Estimate of the error rate of <span class="arithmatex"><span class="MathJax_Preview">\hat{f}</span><script type="math/tex">\hat{f}</script></span> if <strong>inputs and outputs were independent</strong> ËæìÂÖ•ÂèòÈáè‰∏éÁ±ªÂà´Ê†áÁ≠æÁã¨Á´ãÊó∂ÁöÑÈ¢ÑÊµãËßÑÂàôÁöÑËØØÂ∑ÆÁéá</li>
<li>Note the prediction rule <span class="arithmatex"><span class="MathJax_Preview">\hat{f}</span><script type="math/tex">\hat{f}</script></span>, is evaluated on all possible combinations of targets <span class="arithmatex"><span class="MathJax_Preview">y_{i}</span><script type="math/tex">y_{i}</script></span> and predictors <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span></li>
</ul>
<p><strong>Relative overfitting rate</strong> Áõ∏ÂØπËøáÊãüÂêàÁéá</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{R}=\frac{\overline{Err^{(1)}}-\overline{e r r}}{\hat{\gamma}-\overline{e r r}}
</div>
<script type="math/tex; mode=display">
\hat{R}=\frac{\overline{Err^{(1)}}-\overline{e r r}}{\hat{\gamma}-\overline{e r r}}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">0 \leqslant \hat{R} \leqslant 1</span><script type="math/tex">0 \leqslant \hat{R} \leqslant 1</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{R}=0 \rightarrow</span><script type="math/tex">\hat{R}=0 \rightarrow</script></span> no overfitting</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{R}=1 \rightarrow</span><script type="math/tex">\hat{R}=1 \rightarrow</script></span> overfitting or no-information value <span class="arithmatex"><span class="MathJax_Preview">\hat{\gamma}-\overline{e r r}</span><script type="math/tex">\hat{\gamma}-\overline{e r r}</script></span></li>
</ul>
<h4 id="attempt-4-the-632632-estimator">Attempt 4: The <span class="arithmatex"><span class="MathJax_Preview">.632+</span><script type="math/tex">.632+</script></span> estimator<a class="headerlink" href="#attempt-4-the-632632-estimator" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{E r r}^{(.632+)}=(1-\hat{\omega}) \overline{e r r}+\hat{\omega} \hat{E r r}^{(1)}
</div>
<script type="math/tex; mode=display">
\hat{E r r}^{(.632+)}=(1-\hat{\omega}) \overline{e r r}+\hat{\omega} \hat{E r r}^{(1)}
</script>
</div>
<p>with</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\omega}=\frac{.632}{1-.632 \hat{R}}
</div>
<script type="math/tex; mode=display">
\hat{\omega}=\frac{.632}{1-.632 \hat{R}}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">.632 \leqslant \hat{\omega} \leqslant 1</span><script type="math/tex">.632 \leqslant \hat{\omega} \leqslant 1</script></span> as <span class="arithmatex"><span class="MathJax_Preview">\hat{R}</span><script type="math/tex">\hat{R}</script></span> ranges from 0 to 1</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{E r r}^{(.632+)}</span><script type="math/tex">\hat{E r r}^{(.632+)}</script></span> ranges from <span class="arithmatex"><span class="MathJax_Preview">\hat{E r r}^{(.632)}</span><script type="math/tex">\hat{E r r}^{(.632)}</script></span> to <span class="arithmatex"><span class="MathJax_Preview">\hat{E r r}^{(1)}</span><script type="math/tex">\hat{E r r}^{(1)}</script></span> Â§ßËá¥Êù•ËÆ≤ÔºåÂÆÉÂæóÂà∞Ëàç‰∏ÄËá™Âä©Ê≥ïÂíåËÆ≠ÁªÉËØØÂ∑ÆÁéá‰πãÈó¥ÁöÑÊùÉË°°ÔºåÊùÉË°°‰æùËµñ‰∫éÊ®°ÂûãËøáÊãüÂêàÁöÑÁ®ãÂ∫¶</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{E r r}^{(.632+)}</span><script type="math/tex">\hat{E r r}^{(.632+)}</script></span> is a compromise between <span class="arithmatex"><span class="MathJax_Preview">\hat{E r r}^{(.632)}</span><script type="math/tex">\hat{E r r}^{(.632)}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span> that depends on the amount of overfitting</li>
<li>Again, the derivation is non-trivial</li>
</ul>
<p><img alt="" src="../media/ASL-note2/2021-12-22-16-20-10.png" /></p>
<p>10-Fold Cross Validation \&amp; .632+ Bootstrap</p>
<p>In the above cases</p>
<ul>
<li>AlC, cross validation or bootstrap yields a model fairly close to the best available</li>
<li>AIC (BIC) overestimate Err by <span class="arithmatex"><span class="MathJax_Preview">38 \%, 37 \%, 51 \%</span><script type="math/tex">38 \%, 37 \%, 51 \%</script></span>, and <span class="arithmatex"><span class="MathJax_Preview">30 \%</span><script type="math/tex">30 \%</script></span></li>
<li>Cross validation (bootstrap) overestimate Err by <span class="arithmatex"><span class="MathJax_Preview">1 \%, 4 \%, 0 \%</span><script type="math/tex">1 \%, 4 \%, 0 \%</script></span>, and <span class="arithmatex"><span class="MathJax_Preview">4 \%</span><script type="math/tex">4 \%</script></span></li>
</ul>
<p>However, for many adaptive, nonlinear techniques (like trees), estimation of the effective number of parameters is very difficult, making AIC impractical.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2021-2022 Easonshi
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/Lightblues" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://lightblues.github.io/" target="_blank" rel="noopener" title="lightblues.github.io" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M575.8 255.5c0 18-15 32.1-32 32.1h-32l.7 160.2c0 2.7-.2 5.4-.5 8.1v16.2c0 22.1-17.9 40-40 40h-16c-1.1 0-2.2 0-3.3-.1-1.4.1-2.8.1-4.2.1L416 512h-24c-22.1 0-40-17.9-40-40v-88c0-17.7-14.3-32-32-32h-64c-17.7 0-32 14.3-32 32v88c0 22.1-17.9 40-40 40h-55.9c-1.5 0-3-.1-4.5-.2-1.2.1-2.4.2-3.6.2h-16c-22.1 0-40-17.9-40-40V360c0-.9 0-1.9.1-2.8v-69.7h-32c-18 0-32-14-32-32.1 0-9 3-17 10-24L266.4 8c7-7 15-8 22-8s15 2 21 7l255.4 224.5c8 7 12 15 11 24z"/></svg>
    </a>
  
    
    
    
    
    <a href="mailto:oldcitystal@gmail.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 112c-8.8 0-16 7.2-16 16v22.1l172.5 141.6c20.7 17 50.4 17 71.1 0L464 150.1V128c0-8.8-7.2-16-16-16H64zM48 212.2V384c0 8.8 7.2 16 16 16h384c8.8 0 16-7.2 16-16V212.2L322 328.8c-38.4 31.5-93.7 31.5-132 0L48 212.2zM0 128c0-35.3 28.7-64 64-64h384c35.3 0 64 28.7 64 64v256c0 35.3-28.7 64-64 64H64c-35.3 0-64-28.7-64-64V128z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.top"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
        <script src="../../js/baidu-tongji.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>