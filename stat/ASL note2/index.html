
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="技术博客">
      
      
        <meta name="author" content="Easonshi">
      
      
        <link rel="canonical" href="https://lightblues.github.io/techNotes/stat/ASL%20note2/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.2.3, mkdocs-material-8.1.0">
    
    
      
        <title>ASL2 - techNotes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.82f3c0b9.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.9204c3b2.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#basis-expansions-and-regularization" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="techNotes" class="md-header__button md-logo" aria-label="techNotes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            techNotes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ASL2
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/lightblues/techNotes/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    lightblues/techNotes
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../.." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../../code/Python/Python%20note/" class="md-tabs__link">
        Code
      </a>
    </li>
  

  

      
        
  
  
    
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../" class="md-tabs__link md-tabs__link--active">
        Stat
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../Linux/" class="md-tabs__link">
        Linux
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../os/EFI/" class="md-tabs__link">
        OS
      </a>
    </li>
  

      
        
  
  


  <li class="md-tabs__item">
    <a href="/" class="md-tabs__link">
      Blog
    </a>
  </li>

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="techNotes" class="md-nav__button md-logo" aria-label="techNotes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    techNotes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/lightblues/techNotes/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    lightblues/techNotes
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Code
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Code" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Code
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_1" type="checkbox" id="__nav_2_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_1">
          Python
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Python" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          Python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/Python/Python%20note/" class="md-nav__link">
        课程笔记
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_2">
          JavaScripe
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="JavaScripe" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          JavaScripe
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/mindmap/" class="md-nav__link">
        Mindmap
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/js%20note/" class="md-nav__link">
        js note
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/jQuery/" class="md-nav__link">
        jQuery
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/Node.js/" class="md-nav__link">
        Node.js
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Stat
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Stat" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Stat
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_1" type="checkbox" id="__nav_3_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3_1">
          高等统计学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="高等统计学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_1">
          <span class="md-nav__icon md-icon"></span>
          高等统计学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ASL%20mindmap/" class="md-nav__link">
        ASL mindmap
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ASL%20note/" class="md-nav__link">
        ASL1
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          ASL2
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        ASL2
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basis-expansions-and-regularization" class="md-nav__link">
    Basis Expansions and Regularization
  </a>
  
    <nav class="md-nav" aria-label="Basis Expansions and Regularization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intro" class="md-nav__link">
    Intro
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#piecewise-polynomials-and-splines" class="md-nav__link">
    Piecewise Polynomials and Splines
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smoothing-splines" class="md-nav__link">
    Smoothing Splines
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nonparametric-logistic-regression" class="md-nav__link">
    Nonparametric Logistic Regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multidimensional-splines" class="md-nav__link">
    Multidimensional Splines
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kernel-smoothing-methods" class="md-nav__link">
    Kernel Smoothing Methods
  </a>
  
    <nav class="md-nav" aria-label="Kernel Smoothing Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-dim-kernel-smoother" class="md-nav__link">
    1-Dim Kernel Smoother
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-regression-in-rprp" class="md-nav__link">
    Local Regression in R^pR^p
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#structured-local-regression-models-in-rprp" class="md-nav__link">
    Structured Local Regression Models in R^pR^p
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-likelihood-other-models" class="md-nav__link">
    Local likelihood &amp; Other Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kernel-density-estimation-classification" class="md-nav__link">
    Kernel Density Estimation &amp; Classiﬁcation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#radial-basis-function-kernels" class="md-nav__link">
    Radial Basis Function &amp; Kernels 径向基函数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixture-models-for-density-estimation-classification" class="md-nav__link">
    Mixture Models for Density Estimation &amp; Classiﬁcation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-assessment-selection" class="md-nav__link">
    Model Assessment &amp; Selection
  </a>
  
    <nav class="md-nav" aria-label="Model Assessment &amp; Selection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intro-of-model-assessment-selection" class="md-nav__link">
    Intro of Model Assessment &amp; Selection
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-bias-variance-decomposition" class="md-nav__link">
    The Bias-Variance Decomposition
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimism-of-training-error-rate" class="md-nav__link">
    Optimism of Training Error Rate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#estimates-of-err_inerr_in" class="md-nav__link">
    Estimates of Err_{in}Err_{in}
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-validation" class="md-nav__link">
    Cross Validation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bootstrap-method" class="md-nav__link">
    Bootstrap Method
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ASL%20note3/" class="md-nav__link">
        ASL3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ASL%20note4/" class="md-nav__link">
        ASL4
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Linux
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Linux" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Linux
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/" class="md-nav__link">
        General
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83%E7%9B%B8%E5%85%B3/" class="md-nav__link">
        实验环境相关
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/maintain/" class="md-nav__link">
        系统维护
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          OS
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="OS" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          OS
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/EFI/" class="md-nav__link">
        EFI
      </a>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_2" type="checkbox" id="__nav_5_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_2">
          OSs
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="OSs" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          OSs
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/macOS/" class="md-nav__link">
        macOS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Hackintosh/" class="md-nav__link">
        hackintosh
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Arch%20Linux/" class="md-nav__link">
        Arch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/CentOS/" class="md-nav__link">
        CentOS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Ubuntu/" class="md-nav__link">
        Ubuntu
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Windows/" class="md-nav__link">
        Windows
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="/" class="md-nav__link">
        Blog
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#basis-expansions-and-regularization" class="md-nav__link">
    Basis Expansions and Regularization
  </a>
  
    <nav class="md-nav" aria-label="Basis Expansions and Regularization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intro" class="md-nav__link">
    Intro
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#piecewise-polynomials-and-splines" class="md-nav__link">
    Piecewise Polynomials and Splines
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#smoothing-splines" class="md-nav__link">
    Smoothing Splines
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#nonparametric-logistic-regression" class="md-nav__link">
    Nonparametric Logistic Regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multidimensional-splines" class="md-nav__link">
    Multidimensional Splines
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#kernel-smoothing-methods" class="md-nav__link">
    Kernel Smoothing Methods
  </a>
  
    <nav class="md-nav" aria-label="Kernel Smoothing Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#1-dim-kernel-smoother" class="md-nav__link">
    1-Dim Kernel Smoother
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-regression-in-rprp" class="md-nav__link">
    Local Regression in R^pR^p
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#structured-local-regression-models-in-rprp" class="md-nav__link">
    Structured Local Regression Models in R^pR^p
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-likelihood-other-models" class="md-nav__link">
    Local likelihood &amp; Other Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#kernel-density-estimation-classification" class="md-nav__link">
    Kernel Density Estimation &amp; Classiﬁcation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#radial-basis-function-kernels" class="md-nav__link">
    Radial Basis Function &amp; Kernels 径向基函数
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mixture-models-for-density-estimation-classification" class="md-nav__link">
    Mixture Models for Density Estimation &amp; Classiﬁcation
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#model-assessment-selection" class="md-nav__link">
    Model Assessment &amp; Selection
  </a>
  
    <nav class="md-nav" aria-label="Model Assessment &amp; Selection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#intro-of-model-assessment-selection" class="md-nav__link">
    Intro of Model Assessment &amp; Selection
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-bias-variance-decomposition" class="md-nav__link">
    The Bias-Variance Decomposition
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#optimism-of-training-error-rate" class="md-nav__link">
    Optimism of Training Error Rate
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#estimates-of-err_inerr_in" class="md-nav__link">
    Estimates of Err_{in}Err_{in}
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cross-validation" class="md-nav__link">
    Cross Validation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bootstrap-method" class="md-nav__link">
    Bootstrap Method
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
<a href="https://github.com/lightblues/techNotes/edit/main/docs/stat/ASL note2.md" title="Edit this page" class="md-content__button md-icon">
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
</a>


  <h1>ASL2</h1>

<h2 id="basis-expansions-and-regularization">Basis Expansions and Regularization<a class="headerlink" href="#basis-expansions-and-regularization" title="Permanent link">&para;</a></h2>
<p>之前讨论的都是(广义上的)线性模型</p>
<p>(Generalized) Linear Models</p>
<ul>
<li>Linear regression</li>
<li>Linear discriminant analysis</li>
<li>Logistic regression</li>
<li>Separating hyperplanes</li>
</ul>
<p>Extremely unlikely that <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> is linear</p>
<ul>
<li>Linear model is usually a convenient and sometimes necessary approximation</li>
<li>Easy to interpret, first order Taylor expansion, avoid overfitting</li>
</ul>
<p>A Taylor series expansion of a function <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> about a point a
<span class="arithmatex"><span class="MathJax_Preview">f(x)=f(a)+f^{\prime}(a)(x-a)+\frac{f^{\prime \prime}(a)}{2 !}(x-a)^{2}+\frac{f^{(3)}(a)}{3 !}(x-a)^{3}+\ldots+\frac{f^{(n)}(a)}{n !}(x-a)^{n}+\ldots</span><script type="math/tex">f(x)=f(a)+f^{\prime}(a)(x-a)+\frac{f^{\prime \prime}(a)}{2 !}(x-a)^{2}+\frac{f^{(3)}(a)}{3 !}(x-a)^{3}+\ldots+\frac{f^{(n)}(a)}{n !}(x-a)^{n}+\ldots</script></span></p>
<h3 id="intro">Intro<a class="headerlink" href="#intro" title="Permanent link">&para;</a></h3>
<h4 id="moving-beyond-linearity">Moving Beyond Linearity<a class="headerlink" href="#moving-beyond-linearity" title="Permanent link">&para;</a></h4>
<p>Augment inputs <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> with transformations, then use linear models in the new space</p>
<ul>
<li>The <span class="arithmatex"><span class="MathJax_Preview">m_{t h}</span><script type="math/tex">m_{t h}</script></span> transformation, <span class="arithmatex"><span class="MathJax_Preview">h_{m}(X): \mathbb{R}^{p} \rightarrow \mathbb{R}</span><script type="math/tex">h_{m}(X): \mathbb{R}^{p} \rightarrow \mathbb{R}</script></span></li>
</ul>
<p>The new approximated function: <span class="arithmatex"><span class="MathJax_Preview">f(X)=\sum_{m=1}^{M} \beta_{m} h_{m}(X), m=1, \ldots, M</span><script type="math/tex">f(X)=\sum_{m=1}^{M} \beta_{m} h_{m}(X), m=1, \ldots, M</script></span></p>
<p>Once basis functions <span class="arithmatex"><span class="MathJax_Preview">h_{m}(X)</span><script type="math/tex">h_{m}(X)</script></span> have been determined, model estimation proceeds as in ordinary linear regression.</p>
<p>Some simple and widely used <span class="arithmatex"><span class="MathJax_Preview">h_{m}(X)</span><script type="math/tex">h_{m}(X)</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{m}(X)=X_{m}</span><script type="math/tex">h_{m}(X)=X_{m}</script></span>, the original input variables</li>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{m}(X)=X_{j}^{2}</span><script type="math/tex">h_{m}(X)=X_{j}^{2}</script></span> or <span class="arithmatex"><span class="MathJax_Preview">X_{j} X_{k}, O\left(p^{d}\right)</span><script type="math/tex">X_{j} X_{k}, O\left(p^{d}\right)</script></span> terms for degree-d polynomial)</li>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{m}(X)=\log \left(X_{j}\right), \sqrt{X_{j}},\|X\|, \ldots</span><script type="math/tex">h_{m}(X)=\log \left(X_{j}\right), \sqrt{X_{j}},\|X\|, \ldots</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{m}(X)=I\left(L_{m} \leqslant X_{k} \leqslant U_{m}\right)</span><script type="math/tex">h_{m}(X)=I\left(L_{m} \leqslant X_{k} \leqslant U_{m}\right)</script></span>, indicator for region of <span class="arithmatex"><span class="MathJax_Preview">X_{k}</span><script type="math/tex">X_{k}</script></span> 分段</li>
</ul>
<h4 id="controlling-for-model-complexity">Controlling for Model Complexity<a class="headerlink" href="#controlling-for-model-complexity" title="Permanent link">&para;</a></h4>
<p>我们需要一种方式来控制我们模型（从字典中选择基函数）的复杂度, 三种方式</p>
<p>Restriction methods: limit the class of functions before-hand</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">f(x)=\sum_{j=1}^{p} f_{j}\left(X_{j}\right)=\sum_{j=1}^{p} \sum_{m=1}^{M_{j}} \beta_{j m} h_{j m}\left(X_{j}\right)</span><script type="math/tex">f(x)=\sum_{j=1}^{p} f_{j}\left(X_{j}\right)=\sum_{j=1}^{p} \sum_{m=1}^{M_{j}} \beta_{j m} h_{j m}\left(X_{j}\right)</script></span></li>
<li>limit the number of basis functions <span class="arithmatex"><span class="MathJax_Preview">M_{j}</span><script type="math/tex">M_{j}</script></span> used for each component function <span class="arithmatex"><span class="MathJax_Preview">f_{j}</span><script type="math/tex">f_{j}</script></span>.</li>
</ul>
<p>Selection methods: include basis that improve model fit significantly</p>
<ul>
<li>Variable selection</li>
<li>Stagewise greedy approaches, CART, MARS and boosting</li>
</ul>
<p>Regularization methods: use the entire dictionary but restrict the coefficients</p>
<ul>
<li>Ridge</li>
<li>LASSO</li>
</ul>
<h3 id="piecewise-polynomials-and-splines">Piecewise Polynomials and Splines<a class="headerlink" href="#piecewise-polynomials-and-splines" title="Permanent link">&para;</a></h3>
<h4 id="piecewise-polynomials-assume-1-dim">Piecewise Polynomials - Assume 1-dim<a class="headerlink" href="#piecewise-polynomials-assume-1-dim" title="Permanent link">&para;</a></h4>
<p>A <strong>piecewise polynomial</strong> function 分段多项式 <span class="arithmatex"><span class="MathJax_Preview">f(X)</span><script type="math/tex">f(X)</script></span> is obtained by dividing the domain of <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> into contiguous intervals, and representing <span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> by a separate polynomial in each interval.</p>
<p>Piecewise constant
<span class="arithmatex"><span class="MathJax_Preview">h_{1}(X)=I\left(X&lt;\xi_{1}\right), h_{2}(X)=I\left(\xi_{1} \leqslant X&lt;\xi_{2}\right), h_{3}(X)=I\left(\xi_{2} \leqslant X\right)</span><script type="math/tex">h_{1}(X)=I\left(X<\xi_{1}\right), h_{2}(X)=I\left(\xi_{1} \leqslant X<\xi_{2}\right), h_{3}(X)=I\left(\xi_{2} \leqslant X\right)</script></span></p>
<p>Piecewise linear
<span class="arithmatex"><span class="MathJax_Preview">h_{4}(X)=I\left(X&lt;\xi_{1}\right) X, h_{5}(X)=I\left(\xi_{1} \leqslant X&lt;\xi_{2}\right) X, h_{6}(X)=I\left(\xi_{2} \leqslant X\right) X</span><script type="math/tex">h_{4}(X)=I\left(X<\xi_{1}\right) X, h_{5}(X)=I\left(\xi_{1} \leqslant X<\xi_{2}\right) X, h_{6}(X)=I\left(\xi_{2} \leqslant X\right) X</script></span></p>
<p>Continuous piecewise linear
<span class="arithmatex"><span class="MathJax_Preview">h_{1}(X)=1, h_{2}(X)=X, h_{3}(X)=\left(X-\xi_{1}\right)_{+} h_{4}(X)=\left(X-\xi_{2}\right)_{+}</span><script type="math/tex">h_{1}(X)=1, h_{2}(X)=X, h_{3}(X)=\left(X-\xi_{1}\right)_{+} h_{4}(X)=\left(X-\xi_{2}\right)_{+}</script></span>,</p>
<p><img alt="" src="../media/ASL%20note2/2021-12-09-17-08-06.png" /></p>
<p>采用不同基函数和约束条件的回归结果.</p>
<p><img alt="" src="../media/ASL%20note2/2021-12-09-17-08-54.png" /></p>
<h4 id="piecewise-cubic-polynomial">Piecewise Cubic Polynomial<a class="headerlink" href="#piecewise-cubic-polynomial" title="Permanent link">&para;</a></h4>
<p><strong>Cubic spline</strong> 三次样条: piecewise cubic polynomial with continuous first and second derivatives at the <strong>knots</strong>.这里是三次样条的定义, 注意到这里是要求了到二阶微分的连续性; 而下面的给出了一组基函数, 这里是需要证明的. 参见 <a href="https://github.com/szcf-weiya/ESL-CN/issues/29">here</a>. 注意: 1. 这里的六个函数是线性无关的; 2. 满足三次样条的约束, 即knot处0-2阶微分连续.</p>
<p>Basis for cubic splines</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;h_{1}(X)=1, h_{2}(X)=X, h_{3}(X)=X^{2}, h_{4}(X)=X^{3} \\
&amp;h_{5}(X)=\left(X-\xi_{1}\right)_{+}^{3}, h_{6}(X)=\left(X-\xi_{2}\right)_{+}^{3}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&h_{1}(X)=1, h_{2}(X)=X, h_{3}(X)=X^{2}, h_{4}(X)=X^{3} \\
&h_{5}(X)=\left(X-\xi_{1}\right)_{+}^{3}, h_{6}(X)=\left(X-\xi_{2}\right)_{+}^{3}
\end{aligned}
</script>
</div>
<p>Total number of parameters: <span class="arithmatex"><span class="MathJax_Preview">3 \times 4-2 \times 3=6</span><script type="math/tex">3 \times 4-2 \times 3=6</script></span> 计算参数的个数：这里有三个区域，每个区域拟合一个三次函数，因此是 <span class="arithmatex"><span class="MathJax_Preview">3*4=12</span><script type="math/tex">3*4=12</script></span> 个参数；约束条件有两个 knots，每个节点上有三个约束（0-2阶连续）。[另外可知, 如果再加三阶微分连续, 此时的自由度变为 4, 和全局的三次函数一样, 也即「Enforcing one more order of continuity would lead to a global cubic polynomial.」]</p>
<ul>
<li>Cubic splines are the lowest-order spline for which the knot-discontinuity is not visible to the human eye. 据说三次样条是人眼看不出结点不连续的最低阶样条．</li>
</ul>
<h4 id="order-m-spline">Order-M Spline<a class="headerlink" href="#order-m-spline" title="Permanent link">&para;</a></h4>
<p>定义更一般的样条order：order 为 M 的样条是 order 为 M 的分段多 项式，而且有连续的 M − 2 次微分．[注意这里的 order = degree+1, degree 才是我们熟悉的阶数, 例如这里的三次样条 order=4]</p>
<ul>
<li>三次样条的 M = 4</li>
<li>分段常数函数是 order 为 1 的样条</li>
<li>连续的分段线性函数是 order 为 2 的样条</li>
</ul>
<p><strong>Order</strong>-M spline with knots <span class="arithmatex"><span class="MathJax_Preview">\xi_{j}, j=1, \ldots, K</span><script type="math/tex">\xi_{j}, j=1, \ldots, K</script></span> :</p>
<ul>
<li>Piecewise-polynomial of order M</li>
<li>Continuous derivatives up to order M-2</li>
</ul>
<p>可以给出 M order 样条的基函数:</p>
<p><strong>Truncated power</strong> 截断幂 basis</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{j}(X)=X^{j-1}, j=1, \ldots, M</span><script type="math/tex">h_{j}(X)=X^{j-1}, j=1, \ldots, M</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{M+l}=\left(X-\xi_{l}\right)_{+}^{M-1}, l=1, \ldots, K</span><script type="math/tex">h_{M+l}=\left(X-\xi_{l}\right)_{+}^{M-1}, l=1, \ldots, K</script></span></li>
</ul>
<p>In practice, the most widely used orders are <span class="arithmatex"><span class="MathJax_Preview">M=1,2</span><script type="math/tex">M=1,2</script></span> and 4 .</p>
<h4 id="construct-spline-basis">Construct Spline Basis<a class="headerlink" href="#construct-spline-basis" title="Permanent link">&para;</a></h4>
<p>注意到 <strong>order 为 <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> ，含有 <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> 个结点</strong>的样条其自由度为</p>
<div class="arithmatex">
<div class="MathJax_Preview">
M(K+1)-(M-1) K=K+M
</div>
<script type="math/tex; mode=display">
M(K+1)-(M-1) K=K+M
</script>
</div>
<p>左边第一项表示 <span class="arithmatex"><span class="MathJax_Preview">K+1</span><script type="math/tex">K+1</script></span> 个区域中每个区域需要 <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> 个参数，而第二项表明 <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> 个结点中需要 <span class="arithmatex"><span class="MathJax_Preview">M-1</span><script type="math/tex">M-1</script></span> 个限制. 比如，对于三次样条， <span class="arithmatex"><span class="MathJax_Preview">M=4</span><script type="math/tex">M=4</script></span> ，则自由度为 <span class="arithmatex"><span class="MathJax_Preview">K+4</span><script type="math/tex">K+4</script></span>.</p>
<p>下面介绍利用 R <code>splines</code> 中的函数来进行 Spline 的 order, knot 的数量和位置的选择. Choose order of the spline, the number of knots and their placement. [注意, 在下面的 <code>bs(x, df=7)</code> 中，原本四个结点的三次样条自由度为 8 ，但是 <code>bs()</code> 函数本身默认在基中去掉常数项. 也即, 这里满足 <code>df = degree + len(knots)</code>; df=freedom-1; degree=M-1]</p>
<ul>
<li><code>bs(x, df=7)</code><ul>
<li>Default order <span class="arithmatex"><span class="MathJax_Preview">M=4</span><script type="math/tex">M=4</script></span> cubic spline (也即 degree=3)</li>
<li><span class="arithmatex"><span class="MathJax_Preview">7-3=4</span><script type="math/tex">7-3=4</script></span> interior knots at the <span class="arithmatex"><span class="MathJax_Preview">20_{t h}, 40_{t h}, 60_{t h}, 80_{t h}</span><script type="math/tex">20_{t h}, 40_{t h}, 60_{t h}, 80_{t h}</script></span> percentiles of <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span></li>
<li>Return a <span class="arithmatex"><span class="MathJax_Preview">N \times 8</span><script type="math/tex">N \times 8</script></span> matrix of —— 注意, 返回的矩阵还是根据自由度作为宽的</li>
</ul>
</li>
<li><code>bs (x, degree=1, knots=c(0.2,0.4,0.6))</code><ul>
<li>Piecewise constant</li>
<li>return <span class="arithmatex"><span class="MathJax_Preview">\mathrm{N} \times 4</span><script type="math/tex">\mathrm{N} \times 4</script></span> matrix</li>
</ul>
</li>
</ul>
<h4 id="problems-with-polynomials-and-splines">Problems with polynomials and splines<a class="headerlink" href="#problems-with-polynomials-and-splines" title="Permanent link">&para;</a></h4>
<ul>
<li>Poor fit near boundaries</li>
<li>Extrapolation beyond boundaries 外推</li>
</ul>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-11-54-59.png" /></p>
<h4 id="natural-cubic-splines">Natural Cubic Splines<a class="headerlink" href="#natural-cubic-splines" title="Permanent link">&para;</a></h4>
<p>Natural cubic spline 增加了约束: 要求在边界之外的函数要是线性的, 因此减少了2*2 个参数</p>
<ul>
<li>Assume function is linear beyond the boundary knots</li>
<li>Frees up <span class="arithmatex"><span class="MathJax_Preview">2 \times 2</span><script type="math/tex">2 \times 2</script></span> degrees of freedom</li>
</ul>
<p>Natural cubic spline with <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span> knots - <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span> basis functions</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">N_{1}(X)=1, N_{2}(X)=X, N_{k+2}(X)=d_{k}(X)-d_{K-1}(X)</span><script type="math/tex">N_{1}(X)=1, N_{2}(X)=X, N_{k+2}(X)=d_{k}(X)-d_{K-1}(X)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">d_{k}(X)=\frac{\left(X-\xi_{k}\right)_{+}^{3}-\left(X-\xi_{K}\right)_{+}^{3}}{\xi_{K}-\xi_{k}}</span><script type="math/tex">d_{k}(X)=\frac{\left(X-\xi_{k}\right)_{+}^{3}-\left(X-\xi_{K}\right)_{+}^{3}}{\xi_{K}-\xi_{k}}</script></span></li>
<li>Easy to prove: zero second and third derivatives when <span class="arithmatex"><span class="MathJax_Preview">X \geqslant \xi_{K}</span><script type="math/tex">X \geqslant \xi_{K}</script></span> or <span class="arithmatex"><span class="MathJax_Preview">X \leqslant \xi_{1}</span><script type="math/tex">X \leqslant \xi_{1}</script></span></li>
</ul>
<p>这里使用过之前的三次样条的幂基函数推出来的, 参见 <a href="https://github.com/szcf-weiya/ESL-CN/issues/31">here</a>.</p>
<h5 id="_1">自由度的计算说明<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h5>
<p>下面说明<strong>对于自然三次样条而言, 基函数个数即为结点个数</strong>. 设有 <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> 个结点, 则有 <span class="arithmatex"><span class="MathJax_Preview">K-2</span><script type="math/tex">K-2</script></span> 个内结点, <span class="arithmatex"><span class="MathJax_Preview">(K-2+1)</span><script type="math/tex">(K-2+1)</script></span>
个区域, 每个区域参数为 4 个, 每个内结点减掉 3 个参数, 每个边界点减掉一个参数, 则还剩下</p>
<div class="arithmatex">
<div class="MathJax_Preview">
(K-1) \cdot 4-3(K-2)-2 \times 1=K
</div>
<script type="math/tex; mode=display">
(K-1) \cdot 4-3(K-2)-2 \times 1=K
</script>
</div>
<p>也就是个基函数.</p>
<p>具体地, 假设某内结点 <span class="arithmatex"><span class="MathJax_Preview">\xi</span><script type="math/tex">\xi</script></span> 的左右区域函数分别为</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f_{i}(x)=a_{i} x^{3}+b_{i} x^{2}+c_{i} x+d_{i}, \quad i=1,2,
</div>
<script type="math/tex; mode=display">
f_{i}(x)=a_{i} x^{3}+b_{i} x^{2}+c_{i} x+d_{i}, \quad i=1,2,
</script>
</div>
<p>则自由参数有 8 个 <span class="arithmatex"><span class="MathJax_Preview">\left(a_{i}, b_{i}, c_{i}, d_{i}\right), i=1,2</span><script type="math/tex">\left(a_{i}, b_{i}, c_{i}, d_{i}\right), i=1,2</script></span>, 结点 <span class="arithmatex"><span class="MathJax_Preview">\xi</span><script type="math/tex">\xi</script></span> 处需要满足 “函数值相等”、“阶导相等”、“阶导相等”, 即</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
f_{1}(\xi)=f_{2}(\xi)\\
f_{1}^{\prime}(\xi)=f_{2}^{\prime}(\xi)\\
f_{1}^{\prime \prime}(\xi)=f_{2}^{\prime \prime}(\xi)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
f_{1}(\xi)=f_{2}(\xi)\\
f_{1}^{\prime}(\xi)=f_{2}^{\prime}(\xi)\\
f_{1}^{\prime \prime}(\xi)=f_{2}^{\prime \prime}(\xi)
\end{aligned}
</script>
</div>
<p>相当于减少了 3 个自由参数（换句话说, 有三个参数可以被其他 5 个参数表示出来）。</p>
<p>对于某边界点 <span class="arithmatex"><span class="MathJax_Preview">\xi_{0}</span><script type="math/tex">\xi_{0}</script></span>, 其所在区域的函数为 <span class="arithmatex"><span class="MathJax_Preview">f(x)=a x^{3}+b x^{2}+c x+d</span><script type="math/tex">f(x)=a x^{3}+b x^{2}+c x+d</script></span>, 边界点需要满足 “二阶导为零”的约束, 即</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f^{\prime \prime}\left(\xi_{0}\right)=0,
</div>
<script type="math/tex; mode=display">
f^{\prime \prime}\left(\xi_{0}\right)=0,
</script>
</div>
<p>这减少了 1 个自由参数。</p>
<h4 id="example-south-african-heart-disease-continued">Example: South African Heart Disease (Continued)<a class="headerlink" href="#example-south-african-heart-disease-continued" title="Permanent link">&para;</a></h4>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-12-17-40.png" /></p>
<h3 id="smoothing-splines">Smoothing Splines<a class="headerlink" href="#smoothing-splines" title="Permanent link">&para;</a></h3>
<p>这里我们讨论使用最大结点集合的样条基方法来彻底避免结点选择的问题．拟合的复杂度由正则化来控制．</p>
<p>With <strong>regression splines</strong>, choosing knots is a tricky business. 上面讲的「回归样条」, 这里是光滑样条.</p>
<p>Smoothing Splines</p>
<ul>
<li>avoid knot selection problem by using a maximal set of knots</li>
<li>control for overfitting by <strong>shrinking the coefficients</strong> of the estimated function</li>
</ul>
<p>Smoothing splines can be motivated directly by 1 . natural cubic splines with knots at <span class="arithmatex"><span class="MathJax_Preview">\left(x_{1}, x_{2}, \ldots, x_{n}\right)</span><script type="math/tex">\left(x_{1}, x_{2}, \ldots, x_{n}\right)</script></span> and regularization on parameters or 2. a function minimization perspective. We&rsquo;ll talk about 2 in accordance with the text.</p>
<p>Among all functions <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> with <strong>two continuous derivatives</strong>, find one that minimizes the penalized residual sum of squares</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{RSS}(f, \lambda)=\sum_{i=1}^{N}\left(y_{i}-f\left(x_{i}\right)\right)^{2}+\lambda \int f^{\prime \prime}(t)^{2} d(t)
</div>
<script type="math/tex; mode=display">
\operatorname{RSS}(f, \lambda)=\sum_{i=1}^{N}\left(y_{i}-f\left(x_{i}\right)\right)^{2}+\lambda \int f^{\prime \prime}(t)^{2} d(t)
</script>
</div>
<ul>
<li>the first term quantifies the goodness-of-fit to the data</li>
<li>the second term measures the roughness (curvature in <span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> )</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> : smoothing parameter</li>
</ul>
<p>Special case</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\lambda=0: f</span><script type="math/tex">\lambda=0: f</script></span> can be any function that interpolates the data.</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\lambda=\infty: f</span><script type="math/tex">\lambda=\infty: f</script></span> is simple least squares line</li>
</ul>
<p>With <span class="arithmatex"><span class="MathJax_Preview">\lambda \in(0, \infty)</span><script type="math/tex">\lambda \in(0, \infty)</script></span>, there exists a unique minimizer - a natural cubic spline with knots at the unique values of the <span class="arithmatex"><span class="MathJax_Preview">x_{i}, i=1, \ldots, N</span><script type="math/tex">x_{i}, i=1, \ldots, N</script></span>. 这里的 f 是任意的, 因此是无限维空间; 然而可以证明, 在上面的二阶微分惩罚项下, 存在一个在有限维的唯一最小点, 是一个结点在不重复的 <span class="arithmatex"><span class="MathJax_Preview">x_i, i=1,2,...,N</span><script type="math/tex">x_i, i=1,2,...,N</script></span> 处的自然三次样条. 相关证明见 <a href="https://esl.hohoweiya.xyz/05-Basis-Expansions-and-Regularization/5.4-Smoothing-Splines/index.html">here</a></p>
<p>THEOREM :
Let <span class="arithmatex"><span class="MathJax_Preview">g</span><script type="math/tex">g</script></span> be any differentiable function on <span class="arithmatex"><span class="MathJax_Preview">[a, b]</span><script type="math/tex">[a, b]</script></span> for which <span class="arithmatex"><span class="MathJax_Preview">g\left(x_{i}\right)=z_{i}</span><script type="math/tex">g\left(x_{i}\right)=z_{i}</script></span> for <span class="arithmatex"><span class="MathJax_Preview">i=1, \ldots, n</span><script type="math/tex">i=1, \ldots, n</script></span>. Suppose <span class="arithmatex"><span class="MathJax_Preview">n \geqslant 2</span><script type="math/tex">n \geqslant 2</script></span>, and that <span class="arithmatex"><span class="MathJax_Preview">\tilde{g}</span><script type="math/tex">\tilde{g}</script></span> is the natural cubic spline interpolant to the values <span class="arithmatex"><span class="MathJax_Preview">z_{1}, \ldots, z_{n}</span><script type="math/tex">z_{1}, \ldots, z_{n}</script></span> at points <span class="arithmatex"><span class="MathJax_Preview">x_{1}, \ldots, x_{n}</span><script type="math/tex">x_{1}, \ldots, x_{n}</script></span> with <span class="arithmatex"><span class="MathJax_Preview">a&lt;x_{1}&lt;&lt;x_{n}&lt;b</span><script type="math/tex">a<x_{1}<<x_{n}<b</script></span>.
Then <span class="arithmatex"><span class="MathJax_Preview">\int\left(g^{\prime \prime}\right)^{2} \geqslant \int\left(\tilde{g}^{\prime \prime}\right)^{2}</span><script type="math/tex">\int\left(g^{\prime \prime}\right)^{2} \geqslant \int\left(\tilde{g}^{\prime \prime}\right)^{2}</script></span> with equality only if <span class="arithmatex"><span class="MathJax_Preview">\tilde{g}=g</span><script type="math/tex">\tilde{g}=g</script></span></p>
<p>因此, 可以基于这一结论进行进一步的推导</p>
<p>Since now <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> is a natural cubic spline, <span class="arithmatex"><span class="MathJax_Preview">f(x)=\sum_{j=1}^{N} N_{j}(x) \theta_{j}</span><script type="math/tex">f(x)=\sum_{j=1}^{N} N_{j}(x) \theta_{j}</script></span>
The penalized RSS reduces to:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
R S S(\theta, \lambda)=(y-N \theta)^{\top}(y-N \theta)+\lambda \theta^{\top} \Omega_{N} \theta
</div>
<script type="math/tex; mode=display">
R S S(\theta, \lambda)=(y-N \theta)^{\top}(y-N \theta)+\lambda \theta^{\top} \Omega_{N} \theta
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> is <span class="arithmatex"><span class="MathJax_Preview">n \times n</span><script type="math/tex">n \times n</script></span>, with <span class="arithmatex"><span class="MathJax_Preview">N_{i j}=N_{j}\left(X_{i}\right)</span><script type="math/tex">N_{i j}=N_{j}\left(X_{i}\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\Omega</span><script type="math/tex">\Omega</script></span> is <span class="arithmatex"><span class="MathJax_Preview">n \times n</span><script type="math/tex">n \times n</script></span>, with <span class="arithmatex"><span class="MathJax_Preview">\Omega_{N_{i j}}=\int N_{i}^{\prime \prime}(t) N_{j}^{\prime \prime}(t) d t</span><script type="math/tex">\Omega_{N_{i j}}=\int N_{i}^{\prime \prime}(t) N_{j}^{\prime \prime}(t) d t</script></span></li>
</ul>
<p>因此 [是广义岭回归]</p>
<ul>
<li>The solution: <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}=\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top} y</span><script type="math/tex">\hat{\theta}=\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top} y</script></span></li>
<li>The fitted: <span class="arithmatex"><span class="MathJax_Preview">\hat{y}=N \hat{\theta}=N\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top} y=S_{\lambda} y</span><script type="math/tex">\hat{y}=N \hat{\theta}=N\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top} y=S_{\lambda} y</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">S_{\lambda}:</span><script type="math/tex">S_{\lambda}:</script></span> smoother matrix</li>
</ul>
<h4 id="pre-specify-the-amount-of-smoothing">Pre-specify the Amount of Smoothing<a class="headerlink" href="#pre-specify-the-amount-of-smoothing" title="Permanent link">&para;</a></h4>
<p>我们还没有指出光滑样条的 λ 是怎么选取的．本章的后面我们描述使用自动化方 法，比如交叉验证．在这部分中我们讨论预先确定光滑总量的直观方式．</p>
<p>Assume <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> is fixed, smoothing spline operates through linear operator</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}=N\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top} y=S_{\lambda} y
</div>
<script type="math/tex; mode=display">
\hat{f}=N\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top} y=S_{\lambda} y
</script>
</div>
<ul>
<li>fits are linear in <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> 也即, 线性光滑 (<strong>linear smoother</strong>) 和线性回归一样</li>
<li><strong>smoother matrix</strong> <span class="arithmatex"><span class="MathJax_Preview">S_{\lambda}</span><script type="math/tex">S_{\lambda}</script></span> depends on <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span></li>
</ul>
<p>Compare hat matrix <span class="arithmatex"><span class="MathJax_Preview">H</span><script type="math/tex">H</script></span> and smoother matrix <span class="arithmatex"><span class="MathJax_Preview">S_{\lambda}</span><script type="math/tex">S_{\lambda}</script></span></p>
<ul>
<li>symmetric, positive semidefinite?</li>
<li>idempotent?</li>
<li>rank?</li>
</ul>
<h4 id="smoother-matrix">Smoother Matrix<a class="headerlink" href="#smoother-matrix" title="Permanent link">&para;</a></h4>
<p>上面的平滑矩阵 <span class="arithmatex"><span class="MathJax_Preview">S_{\lambda} = N\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top}</span><script type="math/tex">S_{\lambda} = N\left(N^{\top} N+\lambda \Omega_{N}\right)^{-1} N^{\top}</script></span> 可以写成以下形式, 参见 <a href="https://github.com/szcf-weiya/ESL-CN/issues/35">here</a> ; 这样可以分离出超参数 <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>, 从而进行奇异值分解</p>
<ol>
<li>Re-write <span class="arithmatex"><span class="MathJax_Preview">S_{\lambda}</span><script type="math/tex">S_{\lambda}</script></span> in the <strong>Reinsch form</strong></li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
S_{\lambda}=(I+\lambda K)^{-1}
</div>
<script type="math/tex; mode=display">
S_{\lambda}=(I+\lambda K)^{-1}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">K=N^{-\top} \Omega_{N} N^{-1}</span><script type="math/tex">K=N^{-\top} \Omega_{N} N^{-1}</script></span>, does not depend on <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> : <strong>penalty matrix</strong> since <span class="arithmatex"><span class="MathJax_Preview">P R S S=(y-f)^{\top}(y-f)+\lambda f^{\top} K f</span><script type="math/tex">P R S S=(y-f)^{\top}(y-f)+\lambda f^{\top} K f</script></span></li>
</ul>
<ol start="2">
<li>Eigen decomposition of <span class="arithmatex"><span class="MathJax_Preview">S_{\lambda}</span><script type="math/tex">S_{\lambda}</script></span></li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
S_{\lambda}=\sum_{k=1}^{N} \rho_{k}(\lambda) \mu_{k} \mu_{k}^{\top}
</div>
<script type="math/tex; mode=display">
S_{\lambda}=\sum_{k=1}^{N} \rho_{k}(\lambda) \mu_{k} \mu_{k}^{\top}
</script>
</div>
<ul>
<li>Eigenvalue <span class="arithmatex"><span class="MathJax_Preview">\rho_{k}(\lambda)=\frac{1}{1+\lambda d_{k}}</span><script type="math/tex">\rho_{k}(\lambda)=\frac{1}{1+\lambda d_{k}}</script></span></li>
<li>Eigenvector <span class="arithmatex"><span class="MathJax_Preview">\mu_{k}</span><script type="math/tex">\mu_{k}</script></span></li>
</ul>
<p>Note: <span class="arithmatex"><span class="MathJax_Preview">\mu_{k}</span><script type="math/tex">\mu_{k}</script></span> are also the eigenvector of <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span>, and <span class="arithmatex"><span class="MathJax_Preview">d_{k}</span><script type="math/tex">d_{k}</script></span> are the eigenvalue of <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span></p>
<p><strong>Effective degree of freedom</strong> <span class="arithmatex"><span class="MathJax_Preview">d f_{\lambda}=\operatorname{tr}\left(S_{\lambda}\right)</span><script type="math/tex">d f_{\lambda}=\operatorname{tr}\left(S_{\lambda}\right)</script></span></p>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-13-33-50.png" /></p>
<h4 id="highlights-of-eigen-representation">Highlights of Eigen Representation<a class="headerlink" href="#highlights-of-eigen-representation" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
S_{\lambda}=\sum_{k=1}^{N} \rho_{k}(\lambda) \mu_{k} \mu_{k}^{\top}
</div>
<script type="math/tex; mode=display">
S_{\lambda}=\sum_{k=1}^{N} \rho_{k}(\lambda) \mu_{k} \mu_{k}^{\top}
</script>
</div>
<ul>
<li>The eigenvectors are not affected by <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> 奇异向量与超参无关</li>
<li><span class="arithmatex"><span class="MathJax_Preview">S_{\lambda} y=\sum_{k=1}^{N} \mu_{k} \rho_{k}(\lambda)\left\langle\mu_{k}^{\top} y\right\rangle</span><script type="math/tex">S_{\lambda} y=\sum_{k=1}^{N} \mu_{k} \rho_{k}(\lambda)\left\langle\mu_{k}^{\top} y\right\rangle</script></span>, compare to regression spline with <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> basis? 可以看到, 这里是通对 <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> 关于基 <span class="arithmatex"><span class="MathJax_Preview">\mu_k</span><script type="math/tex">\mu_k</script></span> 进行了分解, 通过 <span class="arithmatex"><span class="MathJax_Preview">\rho_{k}(\lambda)</span><script type="math/tex">\rho_{k}(\lambda)</script></span> 控制了收缩的大小; 这就和上一节中的「回归样条」不一样, 在回归样条中, 组分要么不变要么收缩为 0. —— ．基于这个原因光滑样条 被称作<strong>收缩 (shrinking)光滑器</strong>，而回归样条被称作<strong>投影 (projection) 光滑器</strong>.</li>
<li><span class="arithmatex"><span class="MathJax_Preview">d_{1}=d_{2}=0</span><script type="math/tex">d_{1}=d_{2}=0</script></span>, thus <span class="arithmatex"><span class="MathJax_Preview">\rho_{1}(\lambda)=\rho_{2}(\lambda)=1</span><script type="math/tex">\rho_{1}(\lambda)=\rho_{2}(\lambda)=1</script></span>, linear functions are never shrunk</li>
<li>The eigenvectors have increasing complexity, and the higher the complexity, the more they are shrunk</li>
<li>Reparametrize RSS with new basis <span class="arithmatex"><span class="MathJax_Preview">\mu_{k}, R S S=\|y-U \theta\|+\lambda \theta^{\top} D \theta</span><script type="math/tex">\mu_{k}, R S S=\|y-U \theta\|+\lambda \theta^{\top} D \theta</script></span>, thoughts? 其中 <span class="arithmatex"><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> 列向量为 <span class="arithmatex"><span class="MathJax_Preview">u_k</span><script type="math/tex">u_k</script></span> ，且 <span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span> 为元素为 <span class="arithmatex"><span class="MathJax_Preview">d_k</span><script type="math/tex">d_k</script></span> 的对角矩阵．</li>
<li><span class="arithmatex"><span class="MathJax_Preview">d f_{\lambda}=\operatorname{tr}\left(S_{\lambda}\right)=\sum_{k=1}^{N} \rho_{k}(\lambda)</span><script type="math/tex">d f_{\lambda}=\operatorname{tr}\left(S_{\lambda}\right)=\sum_{k=1}^{N} \rho_{k}(\lambda)</script></span>, compare to regression splines? 在投影光滑器中, 所有的特征值为 1.</li>
</ul>
<h4 id="choosing-lambdalambda">Choosing <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span><a class="headerlink" href="#choosing-lambdalambda" title="Permanent link">&para;</a></h4>
<p>see <a href="https://esl.hohoweiya.xyz/notes/spline/sim-5-9/index.html">here</a></p>
<p>Crucial and tricky&hellip;</p>
<ol>
<li>Fixing the Degrees of Freedom 可以直接认为指定
   - <span class="arithmatex"><span class="MathJax_Preview">d f_{\lambda}=\operatorname{tr}\left(S_{\lambda}\right)</span><script type="math/tex">d f_{\lambda}=\operatorname{tr}\left(S_{\lambda}\right)</script></span> is monotone in <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> for smoothing splines
   - R 中可以采用 <code>smooth.spline(x, y, df=6)</code> 确定光滑程度</li>
<li>Cross validation<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{EPE}\left(\hat{f}_{\lambda}\right)=\sigma^{2}+\operatorname{Var}\left(\hat{f}_{\lambda}\right)+\operatorname{Bias}(\hat{f}(\lambda))^{2}</span><script type="math/tex">\operatorname{EPE}\left(\hat{f}_{\lambda}\right)=\sigma^{2}+\operatorname{Var}\left(\hat{f}_{\lambda}\right)+\operatorname{Bias}(\hat{f}(\lambda))^{2}</script></span></li>
<li>K-fold cross-validation</li>
<li>seek a good compromise between bias and variance</li>
</ul>
</li>
</ol>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-14-16-48.png" /></p>
<h3 id="nonparametric-logistic-regression">Nonparametric Logistic Regression<a class="headerlink" href="#nonparametric-logistic-regression" title="Permanent link">&para;</a></h3>
<p>see <a href="https://esl.hohoweiya.xyz/05-Basis-Expansions-and-Regularization/5.6-Nonparametric-Logistic-Regression/index.html">here</a></p>
<p>Logistic regression with single quantitative input <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\log \frac{\operatorname{Pr}(Y=1 \mid X=x)}{\operatorname{Pr}(Y=0 \mid X=x)}=f(x)
</div>
<script type="math/tex; mode=display">
\log \frac{\operatorname{Pr}(Y=1 \mid X=x)}{\operatorname{Pr}(Y=0 \mid X=x)}=f(x)
</script>
</div>
<ul>
<li>Fitting <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> in a smooth fashion leads to a smooth estimate of the conditional probability <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(Y=1 \mid x)</span><script type="math/tex">\operatorname{Pr}(Y=1 \mid x)</script></span></li>
</ul>
<p>这是之前的似然函数. The likelihood function, where <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(Y=1 \mid X=x)=p\left(x_{i}\right)</span><script type="math/tex">\operatorname{Pr}(Y=1 \mid X=x)=p\left(x_{i}\right)</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
l=\sum_{i=1}^{N}\left\{y_{i} \log p\left(x_{i}\right)+\left(1-y_{i}\right) \log \left(1-p\left(x_{i}\right)\right)\right\}
</div>
<script type="math/tex; mode=display">
l=\sum_{i=1}^{N}\left\{y_{i} \log p\left(x_{i}\right)+\left(1-y_{i}\right) \log \left(1-p\left(x_{i}\right)\right)\right\}
</script>
</div>
<p>since <span class="arithmatex"><span class="MathJax_Preview">p\left(x_{i}\right)=\frac{e^{f\left(x_{i}\right)}}{\left.1+e^{f\left(x_{i}\right)}\right)}</span><script type="math/tex">p\left(x_{i}\right)=\frac{e^{f\left(x_{i}\right)}}{\left.1+e^{f\left(x_{i}\right)}\right)}</script></span>,</p>
<div class="arithmatex">
<div class="MathJax_Preview">
l=\sum_{i=1}^{N}\left\{y_{i} f\left(x_{i}\right)-\log \left(1+e^{f\left(x_{i}\right)}\right)\right\}
</div>
<script type="math/tex; mode=display">
l=\sum_{i=1}^{N}\left\{y_{i} f\left(x_{i}\right)-\log \left(1+e^{f\left(x_{i}\right)}\right)\right\}
</script>
</div>
<h4 id="penalized-log-likelihood">Penalized log-likelihood<a class="headerlink" href="#penalized-log-likelihood" title="Permanent link">&para;</a></h4>
<p>利用光滑化的思想, 加上惩罚项</p>
<div class="arithmatex">
<div class="MathJax_Preview">
l=\sum_{i=1}^{N}\left\{y_{i} f\left(x_{i}\right)-\log \left(1+e^{f\left(x_{i}\right)}\right)-\frac{1}{2} \int\left\{f^{\prime \prime}(t)\right\}^{2} d t\right\}
</div>
<script type="math/tex; mode=display">
l=\sum_{i=1}^{N}\left\{y_{i} f\left(x_{i}\right)-\log \left(1+e^{f\left(x_{i}\right)}\right)-\frac{1}{2} \int\left\{f^{\prime \prime}(t)\right\}^{2} d t\right\}
</script>
</div>
<p>Optimal <span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> is a <strong>finite-dimensional natural spline with knots</strong> at the unique values of <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>.</p>
<p>Represent <span class="arithmatex"><span class="MathJax_Preview">f(x)=\sum_{j=1}^{N} N_{j}(x) \theta_{j}</span><script type="math/tex">f(x)=\sum_{j=1}^{N} N_{j}(x) \theta_{j}</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\frac{\partial l(\theta)}{\partial \theta}=N^{\top}(y-p)-\lambda \Omega \theta</span><script type="math/tex">\frac{\partial l(\theta)}{\partial \theta}=N^{\top}(y-p)-\lambda \Omega \theta</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\frac{\partial^{2} /(\theta)}{\partial \theta \partial \theta^{\top}}=-N^{\top} W N-\lambda \Omega</span><script type="math/tex">\frac{\partial^{2} /(\theta)}{\partial \theta \partial \theta^{\top}}=-N^{\top} W N-\lambda \Omega</script></span></li>
</ul>
<p>Newton-Raphson update</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;\theta^{\text {new }}=\left(N^{\top} W N+\lambda \Omega\right)^{-1} N^{\top} W\left(N \theta^{\text {old }}+W^{-1}(y-p)\right)= \left(N^{\top} W N+\lambda \Omega\right)^{-1} N^{\top} W * z \\
&amp;f^{\text {new }}=N\left(N^{\top} W N+\lambda \Omega\right)^{-1} N^{\top} W\left(f^{\text {old }}+W^{-1}(y-p)\right)=S_{\lambda, W} z
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&\theta^{\text {new }}=\left(N^{\top} W N+\lambda \Omega\right)^{-1} N^{\top} W\left(N \theta^{\text {old }}+W^{-1}(y-p)\right)= \left(N^{\top} W N+\lambda \Omega\right)^{-1} N^{\top} W * z \\
&f^{\text {new }}=N\left(N^{\top} W N+\lambda \Omega\right)^{-1} N^{\top} W\left(f^{\text {old }}+W^{-1}(y-p)\right)=S_{\lambda, W} z
\end{aligned}
</script>
</div>
<p>这里的更新公式的形式很有指导意义．它试图用任何非参（加权）回归算子代替 <span class="arithmatex"><span class="MathJax_Preview">S_{\lambda, W}</span><script type="math/tex">S_{\lambda, W}</script></span>，并且得到一般的非参逻辑斯蒂回归模型的族．尽管这里 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 是一维的，但这个过程可以很自然地推广到高维的 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>．这些拓展是 广义可加模型 (generalized additive models) 的核心，我们将在第 9 章中讨论．</p>
<h3 id="multidimensional-splines">Multidimensional Splines<a class="headerlink" href="#multidimensional-splines" title="Permanent link">&para;</a></h3>
<h4 id="from-1-dim-to-2-dim">From 1-dim to 2-dim<a class="headerlink" href="#from-1-dim-to-2-dim" title="Permanent link">&para;</a></h4>
<p>Suppose <span class="arithmatex"><span class="MathJax_Preview">X \in \mathbb{R}^{2}</span><script type="math/tex">X \in \mathbb{R}^{2}</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{1 j}\left(X_{1}\right), j=1, \ldots, M_{1}</span><script type="math/tex">h_{1 j}\left(X_{1}\right), j=1, \ldots, M_{1}</script></span> are the basis for coordinate <span class="arithmatex"><span class="MathJax_Preview">X_{1}</span><script type="math/tex">X_{1}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{2 k}\left(X_{2}\right), k=1, \ldots, M_{2}</span><script type="math/tex">h_{2 k}\left(X_{2}\right), k=1, \ldots, M_{2}</script></span> are the basis for coordinate <span class="arithmatex"><span class="MathJax_Preview">X_{2}</span><script type="math/tex">X_{2}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">g_{j k}(X)=h_{1 j}\left(X_{1}\right) h_{2 k}\left(X_{2}\right)</span><script type="math/tex">g_{j k}(X)=h_{1 j}\left(X_{1}\right) h_{2 k}\left(X_{2}\right)</script></span> are the <span class="arithmatex"><span class="MathJax_Preview">M_{1} \times M_{2}</span><script type="math/tex">M_{1} \times M_{2}</script></span>-dim <strong>tensor product basis</strong> 张量积基底</li>
</ul>
<p>The new basis representation of the model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
g(X)=\sum_{j=1}^{M_{1}} \sum_{k=1}^{M_{2}} \theta_{j k} g_{j k}(X)
</div>
<script type="math/tex; mode=display">
g(X)=\sum_{j=1}^{M_{1}} \sum_{k=1}^{M_{2}} \theta_{j k} g_{j k}(X)
</script>
</div>
<p>Note, the dimension of the basis grows exponentially fast - curse of dimensionality.</p>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-14-38-35.png" /></p>
<p>而对于一维光滑样条(正则化) 也可以推广. The 1-dim penalty</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\lambda \int f^{\prime \prime}(t) d t
</div>
<script type="math/tex; mode=display">
\lambda \int f^{\prime \prime}(t) d t
</script>
</div>
<p>Natural generalization to 2-dim</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\iint_{\mathbb{R}^{2}}\left\{\left(\frac{\partial^{2} f(x)}{\partial x_{1}^{2}}\right)^{2}+2\left(\frac{\partial^{2} f(x)}{\partial x_{1} \partial x_{2}}\right)^{2}+\left(\frac{\partial f(x)}{\partial x_{2}^{2}}\right)^{2}\right\} d x_{1} d x_{2}
</div>
<script type="math/tex; mode=display">
\iint_{\mathbb{R}^{2}}\left\{\left(\frac{\partial^{2} f(x)}{\partial x_{1}^{2}}\right)^{2}+2\left(\frac{\partial^{2} f(x)}{\partial x_{1} \partial x_{2}}\right)^{2}+\left(\frac{\partial f(x)}{\partial x_{2}^{2}}\right)^{2}\right\} d x_{1} d x_{2}
</script>
</div>
<p>The solution is a smooth two-dimensional surface - <strong>thin plate spline</strong></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\lambda \rightarrow 0, f \rightarrow</span><script type="math/tex">\lambda \rightarrow 0, f \rightarrow</script></span> interpolating function</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\lambda \rightarrow \infty, f \rightarrow</span><script type="math/tex">\lambda \rightarrow \infty, f \rightarrow</script></span> least square plane</li>
<li>For <span class="arithmatex"><span class="MathJax_Preview">\lambda \in(0, \infty)</span><script type="math/tex">\lambda \in(0, \infty)</script></span>, 解可以表示成基函数的线性展开，其中系数可以通过广 义的岭回归得到．<ul>
<li>solution has the form <span class="arithmatex"><span class="MathJax_Preview">f(x)=\beta_{0}+\beta^{\top} x+\sum_{j=1}^{N} \alpha_{j} h_{j}(x)</span><script type="math/tex">f(x)=\beta_{0}+\beta^{\top} x+\sum_{j=1}^{N} \alpha_{j} h_{j}(x)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">h_{j}(x)=\left\|x-x_{j}\right\|^{2} \log \left\|x-x_{j}\right\|</span><script type="math/tex">h_{j}(x)=\left\|x-x_{j}\right\|^{2} \log \left\|x-x_{j}\right\|</script></span>, <strong>radial basis function</strong> 径向基函数</li>
</ul>
</li>
</ul>
<h4 id="d-dim">d-dim<a class="headerlink" href="#d-dim" title="Permanent link">&para;</a></h4>
<p>More generally, when <span class="arithmatex"><span class="MathJax_Preview">X \in \mathbb{R}^{d}</span><script type="math/tex">X \in \mathbb{R}^{d}</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\min \sum_{i=1}^{N}\left\{y_{i}-f\left(x_{i}\right)\right\}^{2}+\lambda J|f|
</div>
<script type="math/tex; mode=display">
\min \sum_{i=1}^{N}\left\{y_{i}-f\left(x_{i}\right)\right\}^{2}+\lambda J|f|
</script>
</div>
<p>Restricted class of multidimensional splines</p>
<div class="arithmatex">
<div class="MathJax_Preview">
J|f|=J\left(f_{1}+f_{2}+\ldots+f_{d}\right)=\sum_{j=1}^{d} \int f_{j}^{\prime \prime}\left(t_{j}\right) d t_{j}
</div>
<script type="math/tex; mode=display">
J|f|=J\left(f_{1}+f_{2}+\ldots+f_{d}\right)=\sum_{j=1}^{d} \int f_{j}^{\prime \prime}\left(t_{j}\right) d t_{j}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> is additive</li>
<li>additive penalty on each of the component functions</li>
</ul>
<h2 id="kernel-smoothing-methods">Kernel Smoothing Methods<a class="headerlink" href="#kernel-smoothing-methods" title="Permanent link">&para;</a></h2>
<p>这章中我们描述一类<strong>回归技巧</strong>, 这类技巧能够通过某种方式实现在定义域 <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{p}</span><script type="math/tex">\mathbb{R}^{p}</script></span> 中估计回归函数 <span class="arithmatex"><span class="MathJax_Preview">f(X)</span><script type="math/tex">f(X)</script></span> 的灵活性, 这种方式是在每个查询点 <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span> 处分别拟合不同但简单的模型. 仅仅使用离目标点 很近的观测点来拟合这个简单的模型, 这种方式得到的估计函数 <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(X)</span><script type="math/tex">\hat{f}(X)</script></span> 在 <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{p}</span><script type="math/tex">\mathbb{R}^{p}</script></span> 是光滑的. 这个局 部化可以通过一个加权的函数或者 核 (kernel) 函数 <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, x_{i}\right)</span><script type="math/tex">K_{\lambda}\left(x_{0}, x_{i}\right)</script></span> 来实现, 核函数是基于 <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> 到 <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span> 的 距离赋予一个权重. 核 <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}</span><script type="math/tex">K_{\lambda}</script></span> 一般地通过参数 <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> 来编号, 参数 <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> 规定了邻域的宽度. 原则上, 这些 基于记忆性 (memory-based) 的方法需要很少或者不需要训练; 所有的工作在 赋值 (evaluation) 阶 段便完成了. 根据训练集唯一需要确定的参数是 <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>. 然而, 该模型是整个训练数据集.</p>
<p>我们也讨论更加一般类别的基于核的技巧, 它们与其他章节中结构化的方法联系在一起了, 这在<strong>密度估计</strong>和<strong>分类</strong>中很有用.</p>
<p>本章中的技巧不应该与最近使用最多的“核方法”混淆. 这章中, 核大多作为局部化的工具. 我们在 <span class="arithmatex"><span class="MathJax_Preview">5.8</span><script type="math/tex">5.8</script></span> 节, <span class="arithmatex"><span class="MathJax_Preview">14.5 .4</span><script type="math/tex">14.5 .4</script></span> 节, <span class="arithmatex"><span class="MathJax_Preview">18.5</span><script type="math/tex">18.5</script></span> 节和第 12 章讨论核方法; 在这些部分, 核在一个高维的（隐式的）特征 空间中计算内积, 而且被用于正规化非线性建模中。我们将在本章的 <span class="arithmatex"><span class="MathJax_Preview">6.7</span><script type="math/tex">6.7</script></span> 节的最后将这些方法联系 起来.</p>
<h3 id="1-dim-kernel-smoother">1-Dim Kernel Smoother<a class="headerlink" href="#1-dim-kernel-smoother" title="Permanent link">&para;</a></h3>
<p>用KNN来显示最直观. 左图是直接用KNN的结果, 可见拟合的函数是非连续的; 右图展示了采用 Epanechnikov quadratic kernel 的平滑结果.</p>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-17-07-32.png" /></p>
<p>Kernel weighted avarage:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}\left(x_{0}\right)=\frac{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) y_{i}}{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)}
</div>
<script type="math/tex; mode=display">
\hat{f}\left(x_{0}\right)=\frac{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) y_{i}}{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)}
</script>
</div>
<p>with the Epanechnikov quadratic kernel</p>
<div class="arithmatex">
<div class="MathJax_Preview">
K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left|x-x_{0}\right|}{\lambda}\right)
</div>
<script type="math/tex; mode=display">
K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left|x-x_{0}\right|}{\lambda}\right)
</script>
</div>
<p>with</p>
<div class="arithmatex">
<div class="MathJax_Preview">
D(t)= \begin{cases}\frac{3}{4}\left(1-t^{2}\right), &amp; \text { if }|t| \leqslant 1 \\ 0, &amp; \text { otherwise }\end{cases}
</div>
<script type="math/tex; mode=display">
D(t)= \begin{cases}\frac{3}{4}\left(1-t^{2}\right), & \text { if }|t| \leqslant 1 \\ 0, & \text { otherwise }\end{cases}
</script>
</div>
<h4 id="kernel-weighted-varying-metric-window-size">Kernel Weighted - Varying Metric Window Size<a class="headerlink" href="#kernel-weighted-varying-metric-window-size" title="Permanent link">&para;</a></h4>
<p>The previous Epanechnikov kernel weighted average</p>
<ul>
<li>constant metric window size <span class="arithmatex"><span class="MathJax_Preview">\lambda=0.2</span><script type="math/tex">\lambda=0.2</script></span></li>
<li>does not change while <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span> moves</li>
<li>KNN adapts to the local density of <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span></li>
</ul>
<p>Kernel with adaptive neighborhood</p>
<div class="arithmatex">
<div class="MathJax_Preview">
K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left|x-x_{0}\right|}{h_{\lambda}\left(x_{0}\right)}\right)
</div>
<script type="math/tex; mode=display">
K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left|x-x_{0}\right|}{h_{\lambda}\left(x_{0}\right)}\right)
</script>
</div>
<ul>
<li>For KNN, <span class="arithmatex"><span class="MathJax_Preview">h_{k}\left(x_{0}\right)=\left|x_{0}-x_{[k]}\right|</span><script type="math/tex">h_{k}\left(x_{0}\right)=\left|x_{0}-x_{[k]}\right|</script></span></li>
<li>where <span class="arithmatex"><span class="MathJax_Preview">x_{[k]}</span><script type="math/tex">x_{[k]}</script></span> is the <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> closest <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> to <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span></li>
<li>Boundary issues with metric neighbourhood and KNN</li>
</ul>
<h4 id="popular-kernels-for-smoothing">Popular Kernels for Smoothing<a class="headerlink" href="#popular-kernels-for-smoothing" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}\left(x_{0}\right)=\frac{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) y_{i}}{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)} \quad K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left|x-x_{0}\right|}{\lambda}\right)
</div>
<script type="math/tex; mode=display">
\hat{f}\left(x_{0}\right)=\frac{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) y_{i}}{\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)} \quad K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left|x-x_{0}\right|}{\lambda}\right)
</script>
</div>
<ul>
<li>Epanechnikov - Compact, non-differentiable at the boundary</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
D(t)= \begin{cases}\frac{3}{4}\left(1-t^{2}\right), &amp; \text { if }|t| \leqslant 1 \\ 0, &amp; \text { otherwise }\end{cases}
</div>
<script type="math/tex; mode=display">
D(t)= \begin{cases}\frac{3}{4}\left(1-t^{2}\right), & \text { if }|t| \leqslant 1 \\ 0, & \text { otherwise }\end{cases}
</script>
</div>
<ul>
<li>Tri-cube kernel - Compact, differentiable at the boudary</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
D(t)= \begin{cases}\left(1-|t|^{3}\right)^{3}, &amp; \text { if }|t| \leqslant 1 \\ 0, &amp; \text { otherwise }\end{cases}
</div>
<script type="math/tex; mode=display">
D(t)= \begin{cases}\left(1-|t|^{3}\right)^{3}, & \text { if }|t| \leqslant 1 \\ 0, & \text { otherwise }\end{cases}
</script>
</div>
<ul>
<li>Gaussian kernel - non-compact, continuously differentiable</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
D(t)=\phi(t)
</div>
<script type="math/tex; mode=display">
D(t)=\phi(t)
</script>
</div>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-17-16-57.png" /></p>
<h4 id="local-linear-regression">Local Linear Regression 局部线性回归<a class="headerlink" href="#local-linear-regression" title="Permanent link">&para;</a></h4>
<p>Boundary Problem: 上面提到的 locally weighted average 局部加权平均的方法, 在边界上会有问题</p>
<ul>
<li>Boundary problems occur due to asymmetry of the kernel</li>
<li>Can happen in the interior region if X is not equally spaced 不仅会发生在边界上上, 还是由于核的不对称性, 其在区间内部也可能发生</li>
</ul>
<p>解决方法: 局部加权线性回归 —— 局部加权线性回归会纠正为一阶误差.</p>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-17-19-48.png" /></p>
<p>Locally weighted regression solves a weighted least squares problem at each target point <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\min_{\alpha\left(x_{0}\right), \beta\left(x_{0}\right)} \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left[y_{i}-\alpha\left(x_{0}\right)-\beta\left(x_{0}\right) x_{i}\right]^{2}
</div>
<script type="math/tex; mode=display">
\min_{\alpha\left(x_{0}\right), \beta\left(x_{0}\right)} \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left[y_{i}-\alpha\left(x_{0}\right)-\beta\left(x_{0}\right) x_{i}\right]^{2}
</script>
</div>
<ul>
<li>Design matrix <span class="arithmatex"><span class="MathJax_Preview">B_{N \times 2}</span><script type="math/tex">B_{N \times 2}</script></span>, with <span class="arithmatex"><span class="MathJax_Preview">i_{t h}</span><script type="math/tex">i_{t h}</script></span> row <span class="arithmatex"><span class="MathJax_Preview">b(x)^{\top}=\left(1, x_{i}\right)</span><script type="math/tex">b(x)^{\top}=\left(1, x_{i}\right)</script></span></li>
<li>Weight matrix <span class="arithmatex"><span class="MathJax_Preview">W_{N \times N}\left(x_{0}\right)</span><script type="math/tex">W_{N \times N}\left(x_{0}\right)</script></span>, with <span class="arithmatex"><span class="MathJax_Preview">i_{t h}</span><script type="math/tex">i_{t h}</script></span> diagonal element <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, x_{i}\right)</span><script type="math/tex">K_{\lambda}\left(x_{0}, x_{i}\right)</script></span></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}\left(x_{0}\right)=b\left(x_{0}\right)^{\top}\left(B^{\top} W\left(x_{0}\right) B\right)^{-1} B^{\top} W\left(x_{0}\right) y=\sum_{i=1}^{N} l_{i}\left(x_{0}\right) y_{i}
</div>
<script type="math/tex; mode=display">
\hat{f}\left(x_{0}\right)=b\left(x_{0}\right)^{\top}\left(B^{\top} W\left(x_{0}\right) B\right)^{-1} B^{\top} W\left(x_{0}\right) y=\sum_{i=1}^{N} l_{i}\left(x_{0}\right) y_{i}
</script>
</div>
<ul>
<li>Estimate is linear in <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">l_{i}\left(x_{0}\right)</span><script type="math/tex">l_{i}\left(x_{0}\right)</script></span> combines the weighting kernel <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, \cdot\right)</span><script type="math/tex">K_{\lambda}\left(x_{0}, \cdot\right)</script></span> and least square operation, also referred to as <strong>equivalent kernel</strong> 等价核</li>
</ul>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-17-27-34.png" /></p>
<p><strong>Automatic Kernel Carpentry</strong> 局部线性回归自动地修改核将偏差矫正到恰好为一阶，这是被称为 自动核作品 (automatic kernel carpentry) 的现象</p>
<p>Local linear regression automatically modifies the kernel to correct bias exactly to first order</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
E\left(\hat{f}\left(x_{0}\right)\right)=\sum_{i=1}^{N} l_{i}\left(x_{0}\right) f\left(x_{i}\right)= \\
f\left(x_{0}\right) \sum_{i=1}^{N} l_{i}\left(x_{0}\right)+f^{\prime}\left(x_{0}\right) \sum_{i=1}^{N}\left(x_{i}-x_{0}\right) l_{i}\left(x_{0}\right)+\frac{f^{\prime \prime}\left(x_{0}\right.}{2} \sum_{i=1}^{N}\left(x_{i}-x_{0}\right)^{2} l_{i}\left(x_{0}\right)+R
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
E\left(\hat{f}\left(x_{0}\right)\right)=\sum_{i=1}^{N} l_{i}\left(x_{0}\right) f\left(x_{i}\right)= \\
f\left(x_{0}\right) \sum_{i=1}^{N} l_{i}\left(x_{0}\right)+f^{\prime}\left(x_{0}\right) \sum_{i=1}^{N}\left(x_{i}-x_{0}\right) l_{i}\left(x_{0}\right)+\frac{f^{\prime \prime}\left(x_{0}\right.}{2} \sum_{i=1}^{N}\left(x_{i}-x_{0}\right)^{2} l_{i}\left(x_{0}\right)+R
\end{gathered}
</script>
</div>
<p>It can be shown that for local linear regression (证明见 <a href="https://github.com/szcf-weiya/ESL-CN/issues/148">here</a>; 但更为简单的事是观察 <span class="arithmatex"><span class="MathJax_Preview">L^TB = b(x_0)</span><script type="math/tex">L^TB = b(x_0)</script></span>, 也即 <span class="arithmatex"><span class="MathJax_Preview">L^T(1,x)=(1,x_0)</span><script type="math/tex">L^T(1,x)=(1,x_0)</script></span>)</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\sum_{i=1}^{N} l_{i}\left(x_{0}\right)=1</span><script type="math/tex">\sum_{i=1}^{N} l_{i}\left(x_{0}\right)=1</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\sum_{i=1}^{N}\left(x_{i}-x_{0}\right) l_{i}\left(x_{0}\right)=0</span><script type="math/tex">\sum_{i=1}^{N}\left(x_{i}-x_{0}\right) l_{i}\left(x_{0}\right)=0</script></span></li>
</ul>
<p>Therefore, the bias only depends on quadratic and higher older terms</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">E \hat{f}\left(x_{0}\right)-f\left(x_{0}\right)=\frac{f^{\prime \prime}(x)}{2} \sum_{i=1}^{N}\left(x_{i}-x_{0}\right)^{2} l_{i}\left(x_{0}\right)+R</span><script type="math/tex">E \hat{f}\left(x_{0}\right)-f\left(x_{0}\right)=\frac{f^{\prime \prime}(x)}{2} \sum_{i=1}^{N}\left(x_{i}-x_{0}\right)^{2} l_{i}\left(x_{0}\right)+R</script></span></li>
<li>Typically small under suitable smoothness assumption</li>
</ul>
<h4 id="local-polynomial-regression">Local Polynomial Regression<a class="headerlink" href="#local-polynomial-regression" title="Permanent link">&para;</a></h4>
<p>Local linear fits bias: trimming the hills and filling the valleys</p>
<p>Local polynomial regression of degree-d</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\min _{\alpha\left(x_{0}\right), \beta_{j}\left(x_{0}\right), j=1, \ldots, d} \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left[y_{i}-\alpha\left(x_{0}\right)-\sum_{j=1}^{d} \beta_{j}\left(x_{0}\right) x_{i}^{j}\right]^{2}
</div>
<script type="math/tex; mode=display">
\min _{\alpha\left(x_{0}\right), \beta_{j}\left(x_{0}\right), j=1, \ldots, d} \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left[y_{i}-\alpha\left(x_{0}\right)-\sum_{j=1}^{d} \beta_{j}\left(x_{0}\right) x_{i}^{j}\right]^{2}
</script>
</div>
<ul>
<li>Able to correct the bias in the regions of curvature</li>
<li>But with increased variance (bias-variance tradeoff)</li>
<li>方差 <span class="arithmatex"><span class="MathJax_Preview">\left\|l\left(x_{0}\right)\right\|^2</span><script type="math/tex">\left\|l\left(x_{0}\right)\right\|^2</script></span> increase with d</li>
</ul>
<p>Whether or not to choose local quadratic regression</p>
<ul>
<li>If interested in <strong>extrapolation</strong> (boundary) <span class="arithmatex"><span class="MathJax_Preview">\rightarrow</span><script type="math/tex">\rightarrow</script></span> local linear</li>
</ul>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-18-22-14.png" /></p>
<h3 id="local-regression-in-rprp">Local Regression in <span class="arithmatex"><span class="MathJax_Preview">R^p</span><script type="math/tex">R^p</script></span><a class="headerlink" href="#local-regression-in-rprp" title="Permanent link">&para;</a></h3>
<p>核光滑和局部回归可以非常自然地推广到二维或更高维空间中．</p>
<p>Convert distance based kernel to radius based kernel</p>
<div class="arithmatex">
<div class="MathJax_Preview">
K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left\|x-x_{0}\right\|}{\lambda}\right)
</div>
<script type="math/tex; mode=display">
K_{\lambda}\left(x_{0}, x\right)=D\left(\frac{\left\|x-x_{0}\right\|}{\lambda}\right)
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\|\cdot\|</span><script type="math/tex">\|\cdot\|</script></span> is the Euclidean norm</li>
<li>Since the Euclidean norm depends on the units in each coordinate, need to <strong>standardize</strong> each <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span></li>
</ul>
<h4 id="problems-with-local-regression-in-mathbbrpmathbbrp">Problems with local regression in <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{p}</span><script type="math/tex">\mathbb{R}^{p}</script></span><a class="headerlink" href="#problems-with-local-regression-in-mathbbrpmathbbrp" title="Permanent link">&para;</a></h4>
<p>Boundary problem</p>
<ul>
<li>More and more points can be found on the boundary as dimension increases</li>
<li>直接修改核来适应二维边界会变得很复杂，特别是对于不规则的边界</li>
<li>Local polynomial regression still helps automatically deal with boundary issues for high dimensions (but not desired)</li>
</ul>
<p>Curse of Dimensionality</p>
<ul>
<li>Impossible to simultaneously maintain localness (low bias) and a sizable sample in the neighbourhood (low variance) without total sample size increasing exponentially in <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span></li>
</ul>
<p>Non-visualizable</p>
<ul>
<li>Goal of getting a smooth fitting function is to visualize the data which is difficult in high dimensions</li>
<li>不够直观, it is quite diﬃcult to interpret the results except at a gross level. From a data analysis perspective, conditional plots are far more useful. 这里的 conditional plots 指的就是下面的例子.</li>
</ul>
<p>臭氧浓度～太阳辐射、温度、风速. 共有三个变量, 控制 Wind 和 Temp 的情况下进行单变量的回归.</p>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-18-35-11.png" /></p>
<h3 id="structured-local-regression-models-in-rprp">Structured Local Regression Models in <span class="arithmatex"><span class="MathJax_Preview">R^p</span><script type="math/tex">R^p</script></span><a class="headerlink" href="#structured-local-regression-models-in-rprp" title="Permanent link">&para;</a></h3>
<p>当维度与样本大小的比率不是很好，则局部回归对我们没有太大帮助，除非我们想要对模型做出一些结构化的假设．这本书的很多部分是关于结构化回归和分类模型的．这里我们关注一些与核方法直接相关的方法．</p>
<p><strong>Sperical kernel</strong> (球面核, 就是没有中间的参数矩阵 A) gives equal weight to each coordinate, while a structured kernel emphasizes weight on certain coordinates using matrix <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
K_{\lambda, A}\left(x_{0}, x\right)=D\left(\frac{\left(x-x_{0}\right)^{\top} A\left(x-x_{0}\right)}{\lambda}\right)
</div>
<script type="math/tex; mode=display">
K_{\lambda, A}\left(x_{0}, x\right)=D\left(\frac{\left(x-x_{0}\right)^{\top} A\left(x-x_{0}\right)}{\lambda}\right)
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> is positive semidefinite 半正定矩阵</li>
<li>Entire coordinates or directions can be downgraded or omitted by imposing appropriate restrictions on <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>.</li>
<li>A - diagonal?</li>
<li>A - low rank?</li>
<li>General forms of <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> not recommended</li>
</ul>
<p>相较于不加限定的任意参数矩阵, 可以对 A 进行一定的限制.</p>
<h4 id="structured-regression-functions">Structured Regression Functions 结构回归函数<a class="headerlink" href="#structured-regression-functions" title="Permanent link">&para;</a></h4>
<p>Suppose now we are trying to fit a regression function <span class="arithmatex"><span class="MathJax_Preview">E(Y \mid X)=f\left(X_{1}, X_{2}, \ldots, X_{p}\right)</span><script type="math/tex">E(Y \mid X)=f\left(X_{1}, X_{2}, \ldots, X_{p}\right)</script></span> in <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{p}</span><script type="math/tex">\mathbb{R}^{p}</script></span>, in which every level of interaction is potentially present</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f\left(X_{1}, X_{2}, \ldots, X_{p}\right)=\alpha+\sum_{j} g_{j}\left(X_{j}\right)+\sum_{k&lt;l} g_{k l}\left(X_{k}, X_{l}\right)+\ldots
</div>
<script type="math/tex; mode=display">
f\left(X_{1}, X_{2}, \ldots, X_{p}\right)=\alpha+\sum_{j} g_{j}\left(X_{j}\right)+\sum_{k<l} g_{k l}\left(X_{k}, X_{l}\right)+\ldots
</script>
</div>
<p>Eliminate some higher-order terms</p>
<ul>
<li>Additive model (only main effects) 忽略交叉项</li>
<li>Second order model (interactions up to degree 2)</li>
</ul>
<p>Estimate low order models - <strong>Iterative backfitting</strong> algorithms 第 9 章中，我们描述了对于拟合这样低阶交叉模型的 迭代向后拟合 (iterative backfiting) 算法</p>
<ul>
<li>Assume all but the <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> term is known</li>
<li>Fit local regression of <span class="arithmatex"><span class="MathJax_Preview">Y-\sum_{j \neq k} g_{j}\left(X_{j}\right)</span><script type="math/tex">Y-\sum_{j \neq k} g_{j}\left(X_{j}\right)</script></span> on <span class="arithmatex"><span class="MathJax_Preview">X_{k}</span><script type="math/tex">X_{k}</script></span></li>
<li>Repeat for each variable until convergence</li>
</ul>
<h4 id="varying-coefficient-models">Varying Coefficient Models 可变参数模型<a class="headerlink" href="#varying-coefficient-models" title="Permanent link">&para;</a></h4>
<p>Suppose <span class="arithmatex"><span class="MathJax_Preview">X \in \mathbb{R}^{p}</span><script type="math/tex">X \in \mathbb{R}^{p}</script></span>, divide <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> into <span class="arithmatex"><span class="MathJax_Preview">\left(X_{1}, X_{2}, \ldots, X_{q}\right)</span><script type="math/tex">\left(X_{1}, X_{2}, \ldots, X_{q}\right)</script></span> with <span class="arithmatex"><span class="MathJax_Preview">q&lt;p</span><script type="math/tex">q<p</script></span> and the remainder collected in vector <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
f(X)=\alpha(Z)+\beta_{1}(Z) X_{1}+\ldots+\beta_{q}(Z) X_{q}
</div>
<script type="math/tex; mode=display">
f(X)=\alpha(Z)+\beta_{1}(Z) X_{1}+\ldots+\beta_{q}(Z) X_{q}
</script>
</div>
<ul>
<li>Given <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span>, model is linear in <span class="arithmatex"><span class="MathJax_Preview">X_{1}, \ldots, X_{q}</span><script type="math/tex">X_{1}, \ldots, X_{q}</script></span></li>
<li>Each coefficient <span class="arithmatex"><span class="MathJax_Preview">\beta_{1}(Z), \ldots, \beta_{q}(Z)</span><script type="math/tex">\beta_{1}(Z), \ldots, \beta_{q}(Z)</script></span> vary with <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span></li>
</ul>
<p>Estimate varying coefficient models - <strong>locally weighted least squares</strong> 通过局部加权最小二乘拟合这个模型</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\min _{\alpha\left(z_{0}\right), \beta\left(z_{0}\right)} \sum_{i=1}^{N} K_{\lambda}\left(z_{0}, z_{i}\right)\left(y_{i}-\alpha\left(z_{0}\right)-x_{1 i} \beta_{1}\left(z_{0}\right)-\ldots-x_{q i} \beta_{q}\left(z_{0}\right)\right)^{2}
</div>
<script type="math/tex; mode=display">
\min _{\alpha\left(z_{0}\right), \beta\left(z_{0}\right)} \sum_{i=1}^{N} K_{\lambda}\left(z_{0}, z_{i}\right)\left(y_{i}-\alpha\left(z_{0}\right)-x_{1 i} \beta_{1}\left(z_{0}\right)-\ldots-x_{q i} \beta_{q}\left(z_{0}\right)\right)^{2}
</script>
</div>
<p>例子: 主动脉半径～年龄; 相关的变量是性别和动脉距离的远近. 在<strong>可变系数模型</strong>中, 该例子的对应关系为 X - (age, gender, depth) , Y - aorta diameter, Z - (gender, depth)</p>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-18-56-53.png" /></p>
<p>根据上面的图, 可以看到主动脉半径确实随着年龄的增长而变厚; 而从不同 depth 的设置上, 可以看到 the relationship fades with distance down the aorta(系数变小). 下面的图直接画出了系数的变化</p>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-19-02-47.png" /></p>
<h3 id="local-likelihood-other-models">Local likelihood &amp; Other Models<a class="headerlink" href="#local-likelihood-other-models" title="Permanent link">&para;</a></h3>
<h4 id="local-likelihood">Local Likelihood<a class="headerlink" href="#local-likelihood" title="Permanent link">&para;</a></h4>
<p>Likelihood - (generalized) linear model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
l(\beta)=\sum_{i=1}^{N} l \left(y_{i}, x_{i}^{\top} \beta\right)
</div>
<script type="math/tex; mode=display">
l(\beta)=\sum_{i=1}^{N} l \left(y_{i}, x_{i}^{\top} \beta\right)
</script>
</div>
<p>Local likelihood - relax the global linear assumption to local linear</p>
<div class="arithmatex">
<div class="MathJax_Preview">
l\left(\beta\left(x_{0}\right)\right)=\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) l \left(y_{i}, x_{i}^{\top} \beta\left(x_{0}\right)\right)
</div>
<script type="math/tex; mode=display">
l\left(\beta\left(x_{0}\right)\right)=\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right) l \left(y_{i}, x_{i}^{\top} \beta\left(x_{0}\right)\right)
</script>
</div>
<h4 id="example-multiclass-logistic-regression-model">Example: Multiclass logistic regression model<a class="headerlink" href="#example-multiclass-logistic-regression-model" title="Permanent link">&para;</a></h4>
<p>Multiclass logistic regression model, <span class="arithmatex"><span class="MathJax_Preview">\mathcal{G}=(1,2, \ldots, J)</span><script type="math/tex">\mathcal{G}=(1,2, \ldots, J)</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{Pr}(G=j \mid X=x)=\frac{e^{\beta_{j 0}+\beta_{j}^{\top} x}}{1+\sum_{k=1}^{J-1} e^{\beta_{k 0}+\beta_{k}^{\top} x}}
</div>
<script type="math/tex; mode=display">
\operatorname{Pr}(G=j \mid X=x)=\frac{e^{\beta_{j 0}+\beta_{j}^{\top} x}}{1+\sum_{k=1}^{J-1} e^{\beta_{k 0}+\beta_{k}^{\top} x}}
</script>
</div>
<p>The <strong>local version</strong> of multiclass logistic regression 修改为局部形式的似然函数</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
l \left(\beta\left(x_{0}\right)\right)=&amp; \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left\{\beta_{g_{i}, 0}\left(x_{0}\right)+\beta_{g_{i}}\left(x_{0}\right)^{\top}\left(x_{i}-x_{0}\right)\right.\\
&amp;\left.-\log \left[1+\sum_{k=1}^{J-1} \exp \left(\beta_{k, 0}\left(x_{0}\right)+\beta_{k}\left(x_{0}\right)^{\top}\left(x_{i}-x_{0}\right)\right)\right]\right\}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
l \left(\beta\left(x_{0}\right)\right)=& \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left\{\beta_{g_{i}, 0}\left(x_{0}\right)+\beta_{g_{i}}\left(x_{0}\right)^{\top}\left(x_{i}-x_{0}\right)\right.\\
&\left.-\log \left[1+\sum_{k=1}^{J-1} \exp \left(\beta_{k, 0}\left(x_{0}\right)+\beta_{k}\left(x_{0}\right)^{\top}\left(x_{i}-x_{0}\right)\right)\right]\right\}
\end{aligned}
</script>
</div>
<p>The fitted posterior probabilities</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{Pr}\left(G=j \mid X=x_{0}\right)=\frac{e^{\hat{\beta}_{j 0}\left(x_{0}\right)}}{1+\sum_{k=1}^{j-1} e^{\hat{\beta}_{k 0}\left(x_{0}\right)}}
</div>
<script type="math/tex; mode=display">
\operatorname{Pr}\left(G=j \mid X=x_{0}\right)=\frac{e^{\hat{\beta}_{j 0}\left(x_{0}\right)}}{1+\sum_{k=1}^{j-1} e^{\hat{\beta}_{k 0}\left(x_{0}\right)}}
</script>
</div>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-19-26-30.png" /></p>
<h3 id="kernel-density-estimation-classification">Kernel Density Estimation &amp; Classiﬁcation<a class="headerlink" href="#kernel-density-estimation-classification" title="Permanent link">&para;</a></h3>
<h4 id="kernel-density-estimation">Kernel Density Estimation<a class="headerlink" href="#kernel-density-estimation" title="Permanent link">&para;</a></h4>
<p><strong>Empirical local estimate</strong> - bumpy
$$
\hat{f}<em 0="0">{X}\left(x</em>\right)=\frac{# x_{i} \in \mathcal{N}\left(x_{0}\right)}{N{\lambda}}
$$</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathcal{N}\left(x_{0}\right)</span><script type="math/tex">\mathcal{N}\left(x_{0}\right)</script></span> - small metric neighbourhood around <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span> of width <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span></li>
</ul>
<p><strong>Parzen estimate</strong> - smooth
$$
\hat{f}<em 0="0">{X}\left(x</em>\right)=\frac{1}{N{\lambda}} \sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)
$$</p>
<ul>
<li>Counts observations with weights that decrease with distance</li>
<li>Choice of <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}</span><script type="math/tex">K_{\lambda}</script></span> : Gaussian Kernel <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, x\right)=\phi\left(\frac{\left|x-x_{0}\right|}{\lambda}\right)</span><script type="math/tex">K_{\lambda}\left(x_{0}, x\right)=\phi\left(\frac{\left|x-x_{0}\right|}{\lambda}\right)</script></span></li>
</ul>
<h4 id="gaussian-kernel-density-estimate">Gaussian kernel density estimate<a class="headerlink" href="#gaussian-kernel-density-estimate" title="Permanent link">&para;</a></h4>
<p>参见 <a href="https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/">https://jakevdp.github.io/blog/2013/12/01/kernel-density-estimation/</a></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}_{X}(x)=\frac{1}{N} \sum_{i=1}^{N} \phi_{\lambda}\left(x-x_{i}\right)=\left(\hat{F} * \phi_{\lambda}\right)(x)
</div>
<script type="math/tex; mode=display">
\hat{f}_{X}(x)=\frac{1}{N} \sum_{i=1}^{N} \phi_{\lambda}\left(x-x_{i}\right)=\left(\hat{F} * \phi_{\lambda}\right)(x)
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\phi_{\lambda}</span><script type="math/tex">\phi_{\lambda}</script></span> denotes Gaussian density with mean 0 and standard deviation <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span></li>
<li>Convolution of the sample empirical distribution <span class="arithmatex"><span class="MathJax_Preview">\hat{F}</span><script type="math/tex">\hat{F}</script></span> (jumpy) with <span class="arithmatex"><span class="MathJax_Preview">\phi_{\lambda}(</span><script type="math/tex">\phi_{\lambda}(</script></span> smooth <span class="arithmatex"><span class="MathJax_Preview">)</span><script type="math/tex">)</script></span></li>
</ul>
<p>Gaussian kernel density estimate in <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{p}</span><script type="math/tex">\mathbb{R}^{p}</script></span> 在高阶空间中的自然推广
$$
\hat{f}<em 0="0">{X}\left(x</em>\right)=\frac{1}{N\left(2 \lambda^{2} \pi\right)^{\frac{p}{2}}} \sum_{i=1}^{N} e^{-\frac{1}{2}\left(\left|x_{i}-x_{0}\right| / \lambda\right)^{2}}
$$</p>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-19-36-47.png" /></p>
<h4 id="convolution">关于卷积 Convolution<a class="headerlink" href="#convolution" title="Permanent link">&para;</a></h4>
<p>按照卷积 [https://en.wikipedia.org/wiki/Convolution]的定义，</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\left(\hat{F} \star \phi_{\lambda}\right)(x)=\sum_{i=1}^{n} \hat{F}\left(x_{i}\right) \phi_{\lambda}\left(x-x_{i}\right)=\frac{1}{N} \sum_{i=1}^{n} \phi_{\lambda}\left(x-x_{i}\right)
</div>
<script type="math/tex; mode=display">
\left(\hat{F} \star \phi_{\lambda}\right)(x)=\sum_{i=1}^{n} \hat{F}\left(x_{i}\right) \phi_{\lambda}\left(x-x_{i}\right)=\frac{1}{N} \sum_{i=1}^{n} \phi_{\lambda}\left(x-x_{i}\right)
</script>
</div>
<p>另外，两个独立随机变量和的分布也可以表示为卷积形式. 具体地，假设 <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> 和 <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> 独立，分布函 数分别为 <span class="arithmatex"><span class="MathJax_Preview">F(x)=P(X \leq x), G(y)=P(Y \leq y)</span><script type="math/tex">F(x)=P(X \leq x), G(y)=P(Y \leq y)</script></span> ，则 <span class="arithmatex"><span class="MathJax_Preview">X+Y</span><script type="math/tex">X+Y</script></span> 的分布函数为</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P(X+Y \leq z)=\int F(z-y) d G(y)=F * G(z) .
</div>
<script type="math/tex; mode=display">
P(X+Y \leq z)=\int F(z-y) d G(y)=F * G(z) .
</script>
</div>
<h4 id="kernel-density-classification">Kernel Density Classification<a class="headerlink" href="#kernel-density-classification" title="Permanent link">&para;</a></h4>
<p>For a J class problem, suppose we can fit nonparametric density estimates <span class="arithmatex"><span class="MathJax_Preview">\hat{f}_{j}(X)</span><script type="math/tex">\hat{f}_{j}(X)</script></span> separately for each class, and we also have estimates of the class priors <span class="arithmatex"><span class="MathJax_Preview">\hat{\pi}_{j}</span><script type="math/tex">\hat{\pi}_{j}</script></span></p>
<p>Then by Bayes Theorem, we can derive the class posterior probabilities</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\operatorname{Pr}}\left(G=j \mid X=x_{0}\right)=\frac{\hat{\pi}_{j} \hat{f}_{j}\left(x_{0}\right)}{\sum_{k=1}^{j} \hat{n}_{k} \hat{t}_{k}\left(x_{0}\right)}
</div>
<script type="math/tex; mode=display">
\hat{\operatorname{Pr}}\left(G=j \mid X=x_{0}\right)=\frac{\hat{\pi}_{j} \hat{f}_{j}\left(x_{0}\right)}{\sum_{k=1}^{j} \hat{n}_{k} \hat{t}_{k}\left(x_{0}\right)}
</script>
</div>
<p>If classification is the ultimate goal</p>
<ul>
<li>Learning the separate class densities well may be unnecessary or misleading</li>
<li>Need only estimate the posterior well near the decision boundary</li>
</ul>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-20-31-24.png" /></p>
<p>主要的差异出现在图 6.14 中右图的高 SBP 区域．这个区域中对于两个类别的数据都是稀疏的，并且因为高斯核密度估计采用度量核，所以密度估计在其他区域中效果很差（高方差）．局部逻辑斯蒂回归方法 （6.20）采用 k-NN 带宽的三次立方核；这有效地拓宽了这个区域中的核，并且利 用局部线性假设来对估计进行光滑（在逻辑斯蒂尺度上）．</p>
<h4 id="naive-bayes-classifier">Naive Bayes Classifier<a class="headerlink" href="#naive-bayes-classifier" title="Permanent link">&para;</a></h4>
<p>Assumes that given class <span class="arithmatex"><span class="MathJax_Preview">G=j</span><script type="math/tex">G=j</script></span>, the features <span class="arithmatex"><span class="MathJax_Preview">X_{k}</span><script type="math/tex">X_{k}</script></span> are independent</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f_{j}(X)=\prod_{k=1}^{p} f_{j k}\left(X_{k}\right)
</div>
<script type="math/tex; mode=display">
f_{j}(X)=\prod_{k=1}^{p} f_{j k}\left(X_{k}\right)
</script>
</div>
<ul>
<li>Useful when <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> is large, when density estimation is unattractive</li>
<li>Each <span class="arithmatex"><span class="MathJax_Preview">f_{j k}</span><script type="math/tex">f_{j k}</script></span> can be estimated using 1-dim kernel density estimates 用一维核密度估计单独的类别条件的边缘密度</li>
<li>If <span class="arithmatex"><span class="MathJax_Preview">X_{j}</span><script type="math/tex">X_{j}</script></span> is discrete, can be estimated by appropriate histogram</li>
<li>Sometimes outperform far more sophisticated models</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
\log \frac{\operatorname{Pr}(G=I \mid X)}{\operatorname{Pr}(G=J \mid X)} &amp;=\log \frac{\pi_{l} f_{l}(X)}{\pi_{J} f_{J}(X)}=\log \frac{\pi_{l} \prod_{k=1}^{p} f_{l k}\left(X_{k}\right)}{\pi_{J} \prod_{k=1}^{p} f_{J k}\left(X_{k}\right)} \\
&amp;=\log \frac{\pi_{l}}{\pi_{J}}+\sum_{k=1}^{p} \log \frac{f_{l k}\left(X_{k}\right)}{f_{J k}\left(X_{k}\right)}=\alpha_{l}+\sum_{k=1}^{p} g_{l k}\left(X_{k}\right)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\log \frac{\operatorname{Pr}(G=I \mid X)}{\operatorname{Pr}(G=J \mid X)} &=\log \frac{\pi_{l} f_{l}(X)}{\pi_{J} f_{J}(X)}=\log \frac{\pi_{l} \prod_{k=1}^{p} f_{l k}\left(X_{k}\right)}{\pi_{J} \prod_{k=1}^{p} f_{J k}\left(X_{k}\right)} \\
&=\log \frac{\pi_{l}}{\pi_{J}}+\sum_{k=1}^{p} \log \frac{f_{l k}\left(X_{k}\right)}{f_{J k}\left(X_{k}\right)}=\alpha_{l}+\sum_{k=1}^{p} g_{l k}\left(X_{k}\right)
\end{aligned}
</script>
</div>
<ul>
<li>这有广义加性模型的形式，更多细节将在 第 9 章描述. 参见 <a href="https://github.com/szcf-weiya/ESL-CN/issues/188">here</a></li>
<li>朴素贝叶斯和广义加性模型间的关系可以类比成线性判别分析和逻辑斯蒂回归</li>
</ul>
<h3 id="radial-basis-function-kernels">Radial Basis Function &amp; Kernels 径向基函数<a class="headerlink" href="#radial-basis-function-kernels" title="Permanent link">&para;</a></h3>
<p>see <a href="https://esl.hohoweiya.xyz/06-Kernel-Smoothing-Methods/6.7-Radial-Basis-Functions-and-Kernels/index.html">here</a></p>
<div class="arithmatex">
<div class="MathJax_Preview">
f(x)=\sum_{j=1}^{M} K_{\lambda_{j}}\left(\xi_{j}, x\right) \beta_{j}=\sum_{j=1}^{M} D\left(\frac{\left\|x-\xi_{j}\right\|}{\lambda_{j}}\right) \beta_{j}
</div>
<script type="math/tex; mode=display">
f(x)=\sum_{j=1}^{M} K_{\lambda_{j}}\left(\xi_{j}, x\right) \beta_{j}=\sum_{j=1}^{M} D\left(\frac{\left\|x-\xi_{j}\right\|}{\lambda_{j}}\right) \beta_{j}
</script>
</div>
<ul>
<li>Treat the kernel function <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}(\xi, x)</span><script type="math/tex">K_{\lambda}(\xi, x)</script></span> as basis functions</li>
<li>Each basis element is indexed by a location parameter <span class="arithmatex"><span class="MathJax_Preview">\xi_{j}</span><script type="math/tex">\xi_{j}</script></span> and a scale parameter <span class="arithmatex"><span class="MathJax_Preview">\lambda_{j}</span><script type="math/tex">\lambda_{j}</script></span></li>
<li>Popular choice for <span class="arithmatex"><span class="MathJax_Preview">D</span><script type="math/tex">D</script></span> - standard Gaussian</li>
</ul>
<p>Approaches for learning parameters <span class="arithmatex"><span class="MathJax_Preview">\left\{\lambda_{j}, \xi_{j}, \beta_{j}\right\}</span><script type="math/tex">\left\{\lambda_{j}, \xi_{j}, \beta_{j}\right\}</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\min_{\left\{\lambda_{j}, \xi_{j}, \beta_{j}\right\}_{1}^{M}} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{M} \beta_{j} \exp \left\{-\frac{\left(x_{i}-\xi_{j}\right)^{\top}\left(x_{i}-\xi_{j}\right)}{\lambda_{j}^{2}}\right\}\right)^{2}</span><script type="math/tex">\min_{\left\{\lambda_{j}, \xi_{j}, \beta_{j}\right\}_{1}^{M}} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{M} \beta_{j} \exp \left\{-\frac{\left(x_{i}-\xi_{j}\right)^{\top}\left(x_{i}-\xi_{j}\right)}{\lambda_{j}^{2}}\right\}\right)^{2}</script></span> [高斯核, 回归. 称为 RBF 网络，这是 S 型神经网络的替代选择, 参见 11章.]</li>
<li>Estimate <span class="arithmatex"><span class="MathJax_Preview">\left\{\lambda_{j}, \xi_{j}\right\}</span><script type="math/tex">\left\{\lambda_{j}, \xi_{j}\right\}</script></span> (often unsupervised) separately from <span class="arithmatex"><span class="MathJax_Preview">\beta_{j}</span><script type="math/tex">\beta_{j}</script></span>. 给定前者，后者的估计是简单的最小二乘问题.</li>
</ul>
<h3 id="mixture-models-for-density-estimation-classification">Mixture Models for Density Estimation &amp; Classiﬁcation<a class="headerlink" href="#mixture-models-for-density-estimation-classification" title="Permanent link">&para;</a></h3>
<h4 id="gaussian-mixture">Gaussian Mixture<a class="headerlink" href="#gaussian-mixture" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
f(x)=\sum_{m=1}^{M} \alpha_{m} \phi\left(x ; \mu_{m}, \Sigma_{m}\right)
</div>
<script type="math/tex; mode=display">
f(x)=\sum_{m=1}^{M} \alpha_{m} \phi\left(x ; \mu_{m}, \Sigma_{m}\right)
</script>
</div>
<ul>
<li>Mixing proportions <span class="arithmatex"><span class="MathJax_Preview">\alpha_{m}</span><script type="math/tex">\alpha_{m}</script></span> with <span class="arithmatex"><span class="MathJax_Preview">\sum_{m} \alpha_{m}=1</span><script type="math/tex">\sum_{m} \alpha_{m}=1</script></span></li>
<li>Mean <span class="arithmatex"><span class="MathJax_Preview">\mu_{m}</span><script type="math/tex">\mu_{m}</script></span>, covariance matrix <span class="arithmatex"><span class="MathJax_Preview">\Sigma_{m}</span><script type="math/tex">\Sigma_{m}</script></span></li>
<li>Usually fit by maximum likelihood using EM algorithm</li>
</ul>
<p>Special cases</p>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">\Sigma_{m}=\sigma_{m} I, f(x)</span><script type="math/tex">\Sigma_{m}=\sigma_{m} I, f(x)</script></span> has the form of a radial basis expansion 径向基 展开的形式</li>
<li>If in addition <span class="arithmatex"><span class="MathJax_Preview">\sigma_{m}=\sigma&gt;0</span><script type="math/tex">\sigma_{m}=\sigma>0</script></span> is fixed and <span class="arithmatex"><span class="MathJax_Preview">M \uparrow N</span><script type="math/tex">M \uparrow N</script></span>, the MLE approaches the kernel density estimate where <span class="arithmatex"><span class="MathJax_Preview">\hat{\alpha}_{m}=\frac{1}{N}</span><script type="math/tex">\hat{\alpha}_{m}=\frac{1}{N}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\hat{\mu}_{m}=x_{m}</span><script type="math/tex">\hat{\mu}_{m}=x_{m}</script></span></li>
</ul>
<p>混合模型同样给出了观测 i 属于组分 m 的概率的估计. Estimate of the probability that the <span class="arithmatex"><span class="MathJax_Preview">i_{t h}</span><script type="math/tex">i_{t h}</script></span> observation belongs to component <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>
$$
\hat{r}<em m="m">{i m}=\frac{\hat{\alpha}</em> \phi\left(x_{i} ; \hat{\mu}<em m="m">{m}, \hat{\Sigma}</em>\right)}{\sum_{k=1}^{M} \hat{\alpha}<em i="i">{k} \phi\left(x</em> ; \hat{\mu}<em k="k">{k}, \hat{\Sigma}</em>\right)}
$$</p>
<p><img alt="" src="../media/ASL%20note2/2021-12-13-22-28-28.png" /></p>
<h2 id="model-assessment-selection">Model Assessment &amp; Selection<a class="headerlink" href="#model-assessment-selection" title="Permanent link">&para;</a></h2>
<h3 id="intro-of-model-assessment-selection">Intro of Model Assessment &amp; Selection<a class="headerlink" href="#intro-of-model-assessment-selection" title="Permanent link">&para;</a></h3>
<h4 id="regression-a-review">Regression - A Review<a class="headerlink" href="#regression-a-review" title="Permanent link">&para;</a></h4>
<ul>
<li>Estimate target variable <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> from a vector of inputs <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>.</li>
<li>A prediction model <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(X)</span><script type="math/tex">\hat{f}(X)</script></span> estimated from training data <span class="arithmatex"><span class="MathJax_Preview">T=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</span><script type="math/tex">T=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</script></span></li>
<li>The loss function <span class="arithmatex"><span class="MathJax_Preview">L(Y, \hat{f}(X))</span><script type="math/tex">L(Y, \hat{f}(X))</script></span> measures the error / loss between <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(X)</span><script type="math/tex">\hat{f}(X)</script></span></li>
<li>Common choices for loss function</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
L(Y, \hat{f}(X))= \begin{cases}(Y-\hat{f}(X))^{2}, &amp; \text { squared error } \\ |Y-\hat{f}(X)|, &amp; \text { absolute error }\end{cases}
</div>
<script type="math/tex; mode=display">
L(Y, \hat{f}(X))= \begin{cases}(Y-\hat{f}(X))^{2}, & \text { squared error } \\ |Y-\hat{f}(X)|, & \text { absolute error }\end{cases}
</script>
</div>
<p>Test Error</p>
<p><strong>Test error</strong>, also called <strong>generalization error</strong></p>
<div class="arithmatex">
<div class="MathJax_Preview">
E r r_{T}=E[L(Y, \hat{f}(X)) \mid T]
</div>
<script type="math/tex; mode=display">
E r r_{T}=E[L(Y, \hat{f}(X)) \mid T]
</script>
</div>
<ul>
<li>Prediction error over an independent test sample</li>
<li><span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> and <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> are drawn randomly from <span class="arithmatex"><span class="MathJax_Preview">p(X, Y)</span><script type="math/tex">p(X, Y)</script></span></li>
<li>The training set <span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> is fixed</li>
</ul>
<p>Expected Prediction Error</p>
<p><strong>Expected prediction error</strong>, also called <strong>expected test error</strong></p>
<div class="arithmatex">
<div class="MathJax_Preview">
E r r=E[L(Y, \hat{f}(X))]=E\left[E r r_{T}\right]
</div>
<script type="math/tex; mode=display">
E r r=E[L(Y, \hat{f}(X))]=E\left[E r r_{T}\right]
</script>
</div>
<ul>
<li>Average over everything that is random</li>
<li>Including the randomness in the training set that produced <span class="arithmatex"><span class="MathJax_Preview">\hat{f}</span><script type="math/tex">\hat{f}</script></span></li>
</ul>
<p>We would be interested in estimating <span class="arithmatex"><span class="MathJax_Preview">Err_T</span><script type="math/tex">Err_T</script></span>, but in most cases it is easier to estimate <span class="arithmatex"><span class="MathJax_Preview">Err</span><script type="math/tex">Err</script></span></p>
<p>Training Error</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\overline{e r r}=\frac{1}{n} \sum_{i=1}^{n} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right)
</div>
<script type="math/tex; mode=display">
\overline{e r r}=\frac{1}{n} \sum_{i=1}^{n} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right)
</script>
</div>
<p>As model complexity increases</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\overline{e r r} \rightarrow 0</span><script type="math/tex">\overline{e r r} \rightarrow 0</script></span></li>
<li>but <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}_{T}</span><script type="math/tex">\operatorname{Err}_{T}</script></span> increases, tendency to overfit</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span> is not a good estimate of <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}_{T}</span><script type="math/tex">\operatorname{Err}_{T}</script></span> or <span class="arithmatex"><span class="MathJax_Preview">E r r</span><script type="math/tex">E r r</script></span></li>
</ul>
<p><img alt="" src="../media/ASL%20note2/2021-12-17-16-53-26.png" /></p>
<ul>
<li>Light Blue Curve: training error <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span></li>
<li>Solid blue curve: expected training error <span class="arithmatex"><span class="MathJax_Preview">E[\overrightarrow{e r r}]</span><script type="math/tex">E[\overrightarrow{e r r}]</script></span></li>
<li>Light red curve: conditional test error Err,</li>
<li>Solid red curve: expected test error Err</li>
</ul>
<h4 id="classification-a-review">Classification - A Review<a class="headerlink" href="#classification-a-review" title="Permanent link">&para;</a></h4>
<ul>
<li>Estimate categorical response <span class="arithmatex"><span class="MathJax_Preview">G \in\{1, \ldots, K\}</span><script type="math/tex">G \in\{1, \ldots, K\}</script></span> from a vector of inputs <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span></li>
<li>Typically model <span class="arithmatex"><span class="MathJax_Preview">p_{k}(X)=\operatorname{Pr}(G=k \mid X)</span><script type="math/tex">p_{k}(X)=\operatorname{Pr}(G=k \mid X)</script></span> and define <span class="arithmatex"><span class="MathJax_Preview">\hat{G}(X)=\operatorname{argmax}_{k} p_{k}(X)</span><script type="math/tex">\hat{G}(X)=\operatorname{argmax}_{k} p_{k}(X)</script></span></li>
</ul>
<p>Common choices of loss function</p>
<ul>
<li>0-1 loss</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
L(G, \hat{G}(X))=\operatorname{lnd}(G \neq \hat{G}(X))
</div>
<script type="math/tex; mode=display">
L(G, \hat{G}(X))=\operatorname{lnd}(G \neq \hat{G}(X))
</script>
</div>
<ul>
<li>log-likelihood (deviance) 负对数似然</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
L(G, \hat{p}(X))=-2 \sum_{k=1}^{K} \operatorname{lnd}(G=k) \log \hat{p}_{k}(X)=-2 \log \hat{p}_{G}(X)
</div>
<script type="math/tex; mode=display">
L(G, \hat{p}(X))=-2 \sum_{k=1}^{K} \operatorname{lnd}(G=k) \log \hat{p}_{k}(X)=-2 \log \hat{p}_{G}(X)
</script>
</div>
<p>这里的 <span class="arithmatex"><span class="MathJax_Preview">−2 × \operatorname{log-likelihood}</span><script type="math/tex">−2 × \operatorname{log-likelihood}</script></span> 值有时也被称为 <strong>偏差 (deviance)</strong>．</p>
<h4 id="ulimate-goal">Ulimate Goal<a class="headerlink" href="#ulimate-goal" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\alpha}=\operatorname{argmin}_{\alpha} E\left[L\left(Y, \hat{f}_{\alpha}(X)\right)\right]
</div>
<script type="math/tex; mode=display">
\hat{\alpha}=\operatorname{argmin}_{\alpha} E\left[L\left(Y, \hat{f}_{\alpha}(X)\right)\right]
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{f}_{\alpha}(x)</span><script type="math/tex">\hat{f}_{\alpha}(x)</script></span> typically has a tuning parameter <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> controlling its complexity</li>
<li>Estimate <span class="arithmatex"><span class="MathJax_Preview">E\left[L\left(Y, \hat{f}_{\alpha}(X)\right)\right]</span><script type="math/tex">E\left[L\left(Y, \hat{f}_{\alpha}(X)\right)\right]</script></span> for different values of <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span></li>
<li>Find the value of <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> that produces the minimum expected prediction error</li>
</ul>
<p>Two Separate Goals</p>
<ul>
<li>Model selection: Estimate the performance of diﬀerent models in order to choose the best one</li>
<li>Model assessment: Having chosen a ﬁnal model, estimate its prediction error (generalization error) on new data.</li>
</ul>
<p>For a <strong>Data-rich</strong> Situation</p>
<ul>
<li>Randomly divide the dataset into 3 parts: Train Validation Test</li>
<li>Common split ratio <span class="arithmatex"><span class="MathJax_Preview">50 \%, 25 \%, 25 \%</span><script type="math/tex">50 \%, 25 \%, 25 \%</script></span></li>
<li>Model selection<ul>
<li>Use training set to fit each model</li>
<li>Use validation set to estimate <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}_{T}</span><script type="math/tex">\operatorname{Err}_{T}</script></span> for each model</li>
<li>Choose model with lowest <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}_{T}</span><script type="math/tex">\operatorname{Err}_{T}</script></span> estimate</li>
</ul>
</li>
<li>Model assessment<ul>
<li>Use the test set - unseen until this stage - to estimate <span class="arithmatex"><span class="MathJax_Preview">E r r_{T}</span><script type="math/tex">E r r_{T}</script></span></li>
</ul>
</li>
</ul>
<p>For a <strong>Data-insufficient</strong> Situation</p>
<p>Approximate the validation step either</p>
<ul>
<li>analytically with approahces such as 分析的手段 (<strong>AIC，BIC，MDL，SRM</strong>)
  1. Akaike Information Criterion
  2. Bayesian Information Criterion
  3. Minimum Description Length
  4. Structural Risk Minimization</li>
<li>with efficient sample re-use 有效的样本重利用（交叉验证和自助法）
  1. cross-validation
  2. the bootstrap</li>
</ul>
<p>Each method also provides estimates of the <span class="arithmatex"><span class="MathJax_Preview">Err</span><script type="math/tex">Err</script></span> or <span class="arithmatex"><span class="MathJax_Preview">Err_T</span><script type="math/tex">Err_T</script></span> of the final chosen model</p>
<h3 id="the-bias-variance-decomposition">The Bias-Variance Decomposition<a class="headerlink" href="#the-bias-variance-decomposition" title="Permanent link">&para;</a></h3>
<p>Assume an additive model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Y=f(X)+\epsilon
</div>
<script type="math/tex; mode=display">
Y=f(X)+\epsilon
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">E(\epsilon)=0</span><script type="math/tex">E(\epsilon)=0</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Var}(\epsilon)=\sigma_{\epsilon}^{2}</span><script type="math/tex">\operatorname{Var}(\epsilon)=\sigma_{\epsilon}^{2}</script></span></p>
<p>The <strong>expected prediction error</strong> of <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(X)</span><script type="math/tex">\hat{f}(X)</script></span> at <span class="arithmatex"><span class="MathJax_Preview">X=x_{0}</span><script type="math/tex">X=x_{0}</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{Err}\left(x_{0}\right)=E\left(\left(Y-\hat{f}\left(x_{0}\right)\right)^{2} \mid X=x_{0}\right)
</div>
<script type="math/tex; mode=display">
\operatorname{Err}\left(x_{0}\right)=E\left(\left(Y-\hat{f}\left(x_{0}\right)\right)^{2} \mid X=x_{0}\right)
</script>
</div>
<p>can be expressed as</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{Err}\left(x_{0}\right) = \sigma_{\epsilon}^{2} + MSE =\operatorname{IrreducibleError}+\operatorname{Bias}^{2}+\text { Variance }
</div>
<script type="math/tex; mode=display">
\operatorname{Err}\left(x_{0}\right) = \sigma_{\epsilon}^{2} + MSE =\operatorname{IrreducibleError}+\operatorname{Bias}^{2}+\text { Variance }
</script>
</div>
<ul>
<li>Irreducible error: <span class="arithmatex"><span class="MathJax_Preview">\sigma_{\epsilon}^{2}</span><script type="math/tex">\sigma_{\epsilon}^{2}</script></span></li>
<li>Bias: <span class="arithmatex"><span class="MathJax_Preview">E\left[f\left(x_{0}\right)-E\left[\hat{f}\left(x_{0}\right)\right]\right]</span><script type="math/tex">E\left[f\left(x_{0}\right)-E\left[\hat{f}\left(x_{0}\right)\right]\right]</script></span></li>
<li>Variance: <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Var}\left[\hat{f}\left(x_{0}\right)\right]=E\left[\hat{f}\left(x_{0}\right)-E \hat{f}\left(x_{0}\right)\right]^{2}</span><script type="math/tex">\operatorname{Var}\left[\hat{f}\left(x_{0}\right)\right]=E\left[\hat{f}\left(x_{0}\right)-E \hat{f}\left(x_{0}\right)\right]^{2}</script></span></li>
</ul>
<p>第一项是目标在真实均值 <span class="arithmatex"><span class="MathJax_Preview">f\left(x_{0}\right)</span><script type="math/tex">f\left(x_{0}\right)</script></span> 处的方差，无论我们对 <span class="arithmatex"><span class="MathJax_Preview">f\left(x_{0}\right)</span><script type="math/tex">f\left(x_{0}\right)</script></span> 的估计有多好，这是不可避免的，除非 <span class="arithmatex"><span class="MathJax_Preview">\sigma_{\varepsilon}^{2}=0</span><script type="math/tex">\sigma_{\varepsilon}^{2}=0</script></span>. (因此, 我们只要关注 <strong>MSE</strong> 即可.) 第二项是偏差的平方，是我们估计的均值与真实的均值间的偏差量；最后一项是方差，是估计的 <span class="arithmatex"><span class="MathJax_Preview">\hat{f}\left(x_{0}\right)</span><script type="math/tex">\hat{f}\left(x_{0}\right)</script></span> 在其均值处的平方偏差的期 望值. 一般地，我们建立的模型 <span class="arithmatex"><span class="MathJax_Preview">\hat{f}</span><script type="math/tex">\hat{f}</script></span> 越复杂， (平方) 偏差越低但是方差越大.</p>
<h4 id="k-nearest-neighbor-regression-fit">K-Nearest-Neighbor Regression Fit<a class="headerlink" href="#k-nearest-neighbor-regression-fit" title="Permanent link">&para;</a></h4>
<p>Assuming <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> are fixed,</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{Err}\left(x_{0}\right)=\sigma_{\epsilon}^{2}+\left[f\left(x_{0}\right)-\frac{1}{k} \sum_{l=1}^{k} f\left(x_{(l)}\right)\right]^{2}+\frac{\sigma_{\epsilon}^{2}}{k}
</div>
<script type="math/tex; mode=display">
\operatorname{Err}\left(x_{0}\right)=\sigma_{\epsilon}^{2}+\left[f\left(x_{0}\right)-\frac{1}{k} \sum_{l=1}^{k} f\left(x_{(l)}\right)\right]^{2}+\frac{\sigma_{\epsilon}^{2}}{k}
</script>
</div>
<ul>
<li>Complexity of model is inversely related with <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span></li>
<li>As <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> increases, the variance decreases</li>
<li>As <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> increases, the squared bias increases</li>
</ul>
<p>这里为了简单我们假设训练输入 <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> 为固定的，则随机性来自 <span class="arithmatex"><span class="MathJax_Preview">y_i</span><script type="math/tex">y_i</script></span> ．邻居的个数 <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> 与模型复杂度负相关．对于较小的 k ，估计出的 <span class="arithmatex"><span class="MathJax_Preview">\hat{f}_k (x)</span><script type="math/tex">\hat{f}_k (x)</script></span> 可以对潜在的函数 <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> 可能有更好的适应性．当 k 增大，偏差 —— <span class="arithmatex"><span class="MathJax_Preview">f(x_0)</span><script type="math/tex">f(x_0)</script></span> 与 k-最近邻中的 <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> 平均值的差的平方—— 一般会增大，而方差会降低．</p>
<h4 id="linear-model-least-square-fit">Linear Model - least square fit<a class="headerlink" href="#linear-model-least-square-fit" title="Permanent link">&para;</a></h4>
<p>Now assume a linear model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}_{p}(x)=x^{\top} \hat{\beta}
</div>
<script type="math/tex; mode=display">
\hat{f}_{p}(x)=x^{\top} \hat{\beta}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}</span><script type="math/tex">\hat{\beta}</script></span> is <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>-dimensional and fit by least squares, then</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{Err}\left(x_{0}\right)=\sigma_{\epsilon}^{2}+\left[f\left(x_{0}\right)-E\left(\hat{f}_{p}\left(x_{0}\right)\right)\right]^{2}+\left\|h\left(x_{0}\right)\right\|^{2} \sigma_{\epsilon}^{2}
</div>
<script type="math/tex; mode=display">
\operatorname{Err}\left(x_{0}\right)=\sigma_{\epsilon}^{2}+\left[f\left(x_{0}\right)-E\left(\hat{f}_{p}\left(x_{0}\right)\right)\right]^{2}+\left\|h\left(x_{0}\right)\right\|^{2} \sigma_{\epsilon}^{2}
</script>
</div>
<p>with <span class="arithmatex"><span class="MathJax_Preview">h\left(x_{0}\right)=X\left(X^{\top} X\right)^{-1} x_{0}</span><script type="math/tex">h\left(x_{0}\right)=X\left(X^{\top} X\right)^{-1} x_{0}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\hat{f}_{p}\left(x_{0}\right)=x_{0}^{\top}\left(X^{\top} X\right)^{-1} X^{\top} y</span><script type="math/tex">\hat{f}_{p}\left(x_{0}\right)=x_{0}^{\top}\left(X^{\top} X\right)^{-1} X^{\top} y</script></span></p>
<p>注意到这里的方差随着 <span class="arithmatex"><span class="MathJax_Preview">x_0</span><script type="math/tex">x_0</script></span> 变化， 但是它的平均为 <span class="arithmatex"><span class="MathJax_Preview">(p/N)\sigma_\epsilon^2</span><script type="math/tex">(p/N)\sigma_\epsilon^2</script></span>.这是因为: 若记 <span class="arithmatex"><span class="MathJax_Preview">\mathbf{X}=\left[x_{1}^{\prime}, x_{2}^{\prime}, \ldots, x_{N}^{\prime}\right]^{\prime}</span><script type="math/tex">\mathbf{X}=\left[x_{1}^{\prime}, x_{2}^{\prime}, \ldots, x_{N}^{\prime}\right]^{\prime}</script></span>, 则</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
\sum_{i=1}^{N}\left\|\mathbf{h}\left(x_{i}\right)\right\|^{2} &amp;=\sum_{i=1}^{N} \mathbf{h}^{T}\left(x_{i}\right) \mathbf{h}\left(x_{i}\right) \\
&amp;=\sum_{i=1}^{N} x_{i}^{T}\left(\mathbf{X}^{\mathbf{T}} \mathbf{X}\right)^{-1} x_{i} \\
&amp;=\operatorname{trace}\left[\mathbf{X}\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T}\right] \\
&amp;=\operatorname{trace}\left[\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \mathbf{X}\right] \\
&amp;=p
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\sum_{i=1}^{N}\left\|\mathbf{h}\left(x_{i}\right)\right\|^{2} &=\sum_{i=1}^{N} \mathbf{h}^{T}\left(x_{i}\right) \mathbf{h}\left(x_{i}\right) \\
&=\sum_{i=1}^{N} x_{i}^{T}\left(\mathbf{X}^{\mathbf{T}} \mathbf{X}\right)^{-1} x_{i} \\
&=\operatorname{trace}\left[\mathbf{X}\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T}\right] \\
&=\operatorname{trace}\left[\left(\mathbf{X}^{T} \mathbf{X}\right)^{-1} \mathbf{X}^{T} \mathbf{X}\right] \\
&=p
\end{aligned}
</script>
</div>
<p>因此有</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{1}{N} \sum_{i=1}^{N} \operatorname{Err}\left(x_{i}\right)=\sigma_{\varepsilon}^{2}+\frac{1}{N} \sum_{i=1}^{N}\left[f\left(x_{i}\right)-\mathrm{E} \hat{f}\left(x_{i}\right)\right]^{2}+\frac{p}{N} \sigma_{\varepsilon}^{2}
</div>
<script type="math/tex; mode=display">
\frac{1}{N} \sum_{i=1}^{N} \operatorname{Err}\left(x_{i}\right)=\sigma_{\varepsilon}^{2}+\frac{1}{N} \sum_{i=1}^{N}\left[f\left(x_{i}\right)-\mathrm{E} \hat{f}\left(x_{i}\right)\right]^{2}+\frac{p}{N} \sigma_{\varepsilon}^{2}
</script>
</div>
<p>这称作 <strong>样本内 (in-sample) 误差</strong>. 这里模型复杂度直接与参数个数 <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> 有关.</p>
<h4 id="linear-model-ridge-fit">Linear Model - Ridge Fit<a class="headerlink" href="#linear-model-ridge-fit" title="Permanent link">&para;</a></h4>
<p>Still assume a linear model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}_{p, \alpha}(x)=x^{\top} \hat{\beta}_{\alpha}
</div>
<script type="math/tex; mode=display">
\hat{f}_{p, \alpha}(x)=x^{\top} \hat{\beta}_{\alpha}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}_{\alpha}</span><script type="math/tex">\hat{\beta}_{\alpha}</script></span> is <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span>-dimensional and fit via ridge regression, then</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{Err}\left(x_{0}\right)=\sigma_{\epsilon}^{2}+\left[f\left(x_{0}\right)-E\left(\hat{f}_{p, \alpha}\left(x_{0}\right)\right)\right]^{2}+\left|h_{\alpha}\left(x_{0}\right)\right|^{2} \sigma_{\epsilon}^{2}
</div>
<script type="math/tex; mode=display">
\operatorname{Err}\left(x_{0}\right)=\sigma_{\epsilon}^{2}+\left[f\left(x_{0}\right)-E\left(\hat{f}_{p, \alpha}\left(x_{0}\right)\right)\right]^{2}+\left|h_{\alpha}\left(x_{0}\right)\right|^{2} \sigma_{\epsilon}^{2}
</script>
</div>
<p>with <span class="arithmatex"><span class="MathJax_Preview">h_{\alpha}\left(x_{0}\right)=X\left(X^{\top} X+\alpha I\right)^{-1} x_{0}</span><script type="math/tex">h_{\alpha}\left(x_{0}\right)=X\left(X^{\top} X+\alpha I\right)^{-1} x_{0}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\hat{f}_{p, \alpha}\left(x_{0}\right)=x_{0}^{\top}\left(X^{\top} X+\alpha I\right)^{-1} X^{\top} y</span><script type="math/tex">\hat{f}_{p, \alpha}\left(x_{0}\right)=x_{0}^{\top}\left(X^{\top} X+\alpha I\right)^{-1} X^{\top} y</script></span>
Therefore, this regression fit model has a different bias and variance to the least square fit</p>
<h4 id="linear-model-fine-decomposition-of-bias">Linear Model - Fine Decomposition of Bias<a class="headerlink" href="#linear-model-fine-decomposition-of-bias" title="Permanent link">&para;</a></h4>
<p>对于线性模型族比如岭回归，我们可以更精细地分解偏差．</p>
<p>Let <span class="arithmatex"><span class="MathJax_Preview">\beta_{*}</span><script type="math/tex">\beta_{*}</script></span> denote the parameters of the best-fitting linear approx to <span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> :</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\beta_{*}=\operatorname{argmin}_{\beta} E\left[\left(f(X)-X^{\top} \beta\right)^{2}\right]
</div>
<script type="math/tex; mode=display">
\beta_{*}=\operatorname{argmin}_{\beta} E\left[\left(f(X)-X^{\top} \beta\right)^{2}\right]
</script>
</div>
<p>Now re-write the averaged squared bias 偏差平方的平均</p>
<div class="arithmatex">
<div class="MathJax_Preview">
E_{x_{0}}\left[\left(f\left(x_{0}\right)-E\left[\hat{f}_{\alpha}\left(x_{0}\right)\right]\right)^{2}\right]
</div>
<script type="math/tex; mode=display">
E_{x_{0}}\left[\left(f\left(x_{0}\right)-E\left[\hat{f}_{\alpha}\left(x_{0}\right)\right]\right)^{2}\right]
</script>
</div>
<p>as</p>
<div class="arithmatex">
<div class="MathJax_Preview">
E_{x_{0}}\left[\left(f\left(x_{0}\right)-x_{0}^{\top} \beta_{*}\right)^{2}\right]+E_{x_{0}}\left[\left(x_{0}^{\top} \beta_{*}-E\left[x_{0}^{\top} \hat{\beta}_{\alpha}\right]\right)^{2}\right]
</div>
<script type="math/tex; mode=display">
E_{x_{0}}\left[\left(f\left(x_{0}\right)-x_{0}^{\top} \beta_{*}\right)^{2}\right]+E_{x_{0}}\left[\left(x_{0}^{\top} \beta_{*}-E\left[x_{0}^{\top} \hat{\beta}_{\alpha}\right]\right)^{2}\right]
</script>
</div>
<p>where</p>
<ul>
<li>First term: Ave[Model Bias] <span class="arithmatex"><span class="MathJax_Preview">^{2}</span><script type="math/tex">^{2}</script></span> 模型偏差</li>
<li>Second term: Ave[Estimation Bias] <span class="arithmatex"><span class="MathJax_Preview">^{2}</span><script type="math/tex">^{2}</script></span> 估计偏差</li>
</ul>
<p>Estimation bias is zero for least square estimate, and positive for ridge regression estimate. 对于通过普通最小二乘拟合的线性模型，估计量的偏差为 0．对于约束的拟合， 比如岭回归，它是正的，而且我们用减小方差的好处进行交易．模型偏差只可能通过将线性模型类扩大为更广的模型类才能降低，或者通过在模型中加入变量的交叉项以及变换项（通过变量的变换得到的）来降低．</p>
<p><img alt="" src="../media/ASL%20note2/2021-12-17-20-14-51.png" /></p>
<p>上图给了一个形象的例子, 展现了通过进行 shrink 或者 regularization, 虽然模型的偏差变大了, 如果方差减小更多也是值得的.</p>
<h4 id="example-1">Example 1<a class="headerlink" href="#example-1" title="Permanent link">&para;</a></h4>
<p>Set-up</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">n=80</span><script type="math/tex">n=80</script></span> observations and <span class="arithmatex"><span class="MathJax_Preview">p=20</span><script type="math/tex">p=20</script></span> predictors</li>
<li><span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> is uniformly distributed in <span class="arithmatex"><span class="MathJax_Preview">[0,1]^{20}</span><script type="math/tex">[0,1]^{20}</script></span> 在这样一个超立方体中均匀分布</li>
<li>Use squared error loss to measure Err for the regression task</li>
<li>Use 0-1 loss to measure Err for the classification task</li>
</ul>
<p>下图中左边的设置: 数据分布如下, 分类和回归任务均采用 KNN</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Y= \begin{cases}0 &amp; \text { if } X_{1} \leqslant 0.5 \\ 1 &amp; \text { if } X_{1}&gt;0.5\end{cases}
</div>
<script type="math/tex; mode=display">
Y= \begin{cases}0 & \text { if } X_{1} \leqslant 0.5 \\ 1 & \text { if } X_{1}>0.5\end{cases}
</script>
</div>
<p>右边的设置: 分布如下, 对于分类和回归任务均采用 best subset linear regression of size <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
Y= \begin{cases}1 &amp; \text { if } \sum_{j=1}^{10} X_{j} \leqslant 5 \\ 0 &amp; \text { otherwise }\end{cases}
</div>
<script type="math/tex; mode=display">
Y= \begin{cases}1 & \text { if } \sum_{j=1}^{10} X_{j} \leqslant 5 \\ 0 & \text { otherwise }\end{cases}
</script>
</div>
<p><img alt="" src="../media/ASL%20note2/2021-12-17-20-25-22.png" /></p>
<p>图中: 预测误差（红色）、平方误差（绿色）和方差（蓝色）.</p>
<p>在回归任务的(上面两张图), 预测误差是平方误差和方差之和; 但在分类任务(下面两张图)则不是, 这是因为分类的预测误差用的损失函数不一样. (比如给定一个点, 类别1的真实概率为0.9, 模型预测为0.6, 此时的偏差平方 <span class="arithmatex"><span class="MathJax_Preview">(0.6-0.9)^2</span><script type="math/tex">(0.6-0.9)^2</script></span> 较大, 但是预测误差为零). 总而言之, 「偏差方差间的权衡在 0-1 损失的表现与在平方误差损失的表现不一样」. 反过来意味着调整参数的最优选择可能在两种设定下本质上不同．正如后面章节中描述的那样，<strong>我们应该将调整参数的选择建立于对预测误差的估计之上</strong>．</p>
<h3 id="optimism-of-training-error-rate">Optimism of Training Error Rate<a class="headerlink" href="#optimism-of-training-error-rate" title="Permanent link">&para;</a></h3>
<h4 id="optimism-of-overlinetext-err-overlinetext-err">Optimism of <span class="arithmatex"><span class="MathJax_Preview">\overline{\text { err }}</span><script type="math/tex">\overline{\text { err }}</script></span><a class="headerlink" href="#optimism-of-overlinetext-err-overlinetext-err" title="Permanent link">&para;</a></h4>
<p>区分 <strong>训练误差</strong> <span class="arithmatex"><span class="MathJax_Preview">\overline{err}</span><script type="math/tex">\overline{err}</script></span> 和 <strong>泛化/预测误差</strong> generalization/prediction error <span class="arithmatex"><span class="MathJax_Preview">Err_{T}</span><script type="math/tex">Err_{T}</script></span></p>
<p>Suppose <span class="arithmatex"><span class="MathJax_Preview">\left(x_{0}, y_{0}\right)</span><script type="math/tex">\left(x_{0}, y_{0}\right)</script></span> is a new test data point, drawn from the joint distribution of the data. 注意下面期望误差中, training set <span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> 是固定的, <span class="arithmatex"><span class="MathJax_Preview">X_{0}, Y_{0}</span><script type="math/tex">X_{0}, Y_{0}</script></span> new test data point.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
\overline{e r r}=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right) \\
E r r_{T}=E_{X_{0}, Y_{0}}\left(L\left(Y_{0}, \hat{f}\left(X_{0}\right)\right) \mid T\right)
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
\overline{e r r}=\frac{1}{N} \sum_{i=1}^{N} L\left(y_{i}, \hat{f}\left(x_{i}\right)\right) \\
E r r_{T}=E_{X_{0}, Y_{0}}\left(L\left(Y_{0}, \hat{f}\left(X_{0}\right)\right) \mid T\right)
\end{gathered}
</script>
</div>
<p>Training error <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}&lt;&lt;\operatorname{Err}_{T}</span><script type="math/tex">\overline{e r r}<<\operatorname{Err}_{T}</script></span> as it uses <span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span> for both fitting and assessment.</p>
<p>这里的 <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span> 是训练误差, 泛化误差 <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}_{T}</span><script type="math/tex">\operatorname{Err}_{T}</script></span> 可以看成是 <strong>样本外 (extra-sample) 误差</strong> (因为测试输入向量不需要与训练输入向量一致). 下面定义样本内误差, 从而理解训练误差 <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span> 是乐观估计.</p>
<p>Define <strong>in-sample error</strong> as 定义样本内误差 [考虑到了噪声项]</p>
<div class="arithmatex">
<div class="MathJax_Preview">
E r r_{i n}=\frac{1}{n} \sum_{i=1}^{n} E_{Y^{\prime}}\left[L\left(y_{i}^{\prime}, \hat{f}\left(x_{i}\right)\right) \mid T\right]
</div>
<script type="math/tex; mode=display">
E r r_{i n}=\frac{1}{n} \sum_{i=1}^{n} E_{Y^{\prime}}\left[L\left(y_{i}^{\prime}, \hat{f}\left(x_{i}\right)\right) \mid T\right]
</script>
</div>
<p>where the expectation is over new responses <span class="arithmatex"><span class="MathJax_Preview">y_{i}^{\prime}</span><script type="math/tex">y_{i}^{\prime}</script></span> at each training point <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> 也即, 在每一个样本点 <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> 处, 都观测一系列新的响应变量, 求期望.</p>
<p>Define the <strong>optimism</strong> as</p>
<div class="arithmatex">
<div class="MathJax_Preview">
o p=E r r_{i n}-\overline{e r r}
</div>
<script type="math/tex; mode=display">
o p=E r r_{i n}-\overline{e r r}
</script>
</div>
<p>The <strong>average optimism</strong></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\omega=E_{y}[o p]
</div>
<script type="math/tex; mode=display">
\omega=E_{y}[o p]
</script>
</div>
<p>where</p>
<ul>
<li>the training input vectors are held fixed</li>
<li>the expectation is over the <strong>training output values</strong> [因为有随机噪声的存在]</li>
</ul>
<p>For squared error, 0-1, and other loss functions, one can show quite generally that</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\omega=\frac{2}{n} \sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)
</div>
<script type="math/tex; mode=display">
\omega=\frac{2}{n} \sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)
</script>
</div>
<blockquote>
<p>补充说明: Here the predictors in the training set are fixed, and the expectation is over the training set outcome values; hence we have used the notation <span class="arithmatex"><span class="MathJax_Preview">\mathrm{E}_{\mathbf{y}}</span><script type="math/tex">\mathrm{E}_{\mathbf{y}}</script></span> instead of <span class="arithmatex"><span class="MathJax_Preview">\mathrm{E}_{\mathcal{T}}</span><script type="math/tex">\mathrm{E}_{\mathcal{T}}</script></span>. We can usually estimate only the expected error <span class="arithmatex"><span class="MathJax_Preview">\omega</span><script type="math/tex">\omega</script></span> rather than <span class="arithmatex"><span class="MathJax_Preview">op</span><script type="math/tex">op</script></span>, in the same way that we can estimate the expected error <span class="arithmatex"><span class="MathJax_Preview">Err</span><script type="math/tex">Err</script></span> rather than the conditional error <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}_{\mathcal{T}}</span><script type="math/tex">\operatorname{Err}_{\mathcal{T}}</script></span>.</p>
</blockquote>
<p>对于这些概念的理解和公式证明参见 <a href="https://github.com/szcf-weiya/ESL-CN/issues/27">here</a> 的习题. 总而言之, 训练误差仅和样本相关, 也即一个 <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> 其响应值是固定的; 然而对于同一个样本点, 由于噪声的存在其真实的响应值满足一个分布, 这里的样本内误差 <span class="arithmatex"><span class="MathJax_Preview">Err_{in}</span><script type="math/tex">Err_{in}</script></span> 就是考虑了这一特点. 一般而言, 样本内误差会比训练误差更大. 而 average optimism 则是在此基础上, 考虑了训练样本的分布, 对于 y 的分布进行了求期望.</p>
<ul>
<li>The harder we fit the model, the stronger <span class="arithmatex"><span class="MathJax_Preview">y_{i}</span><script type="math/tex">y_{i}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\hat{y}_{i}</span><script type="math/tex">\hat{y}_{i}</script></span> are associated, and the larger <span class="arithmatex"><span class="MathJax_Preview">\omega</span><script type="math/tex">\omega</script></span></li>
<li>Large <span class="arithmatex"><span class="MathJax_Preview">\omega \rightarrow</span><script type="math/tex">\omega \rightarrow</script></span> greater optimism of <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span></li>
</ul>
<p>通过对于 op 的分析, 可以对于方差-偏差的分解有更好的理解. 模型的复杂度越高, 样本点 <span class="arithmatex"><span class="MathJax_Preview">y_{i}</span><script type="math/tex">y_{i}</script></span> 和拟合值 <span class="arithmatex"><span class="MathJax_Preview">\hat{y}_{i}</span><script type="math/tex">\hat{y}_{i}</script></span> 的相关性越高, 则 average optimism  <span class="arithmatex"><span class="MathJax_Preview">\omega</span><script type="math/tex">\omega</script></span> 也越高.</p>
<p>In summary, we have the <strong>important relation</strong></p>
<div class="arithmatex">
<div class="MathJax_Preview">
E_{y}\left[E r r_{i n}\right]=E_{y}[\overline{e r r}]+\frac{2}{n} \sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)
</div>
<script type="math/tex; mode=display">
E_{y}\left[E r r_{i n}\right]=E_{y}[\overline{e r r}]+\frac{2}{n} \sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)
</script>
</div>
<h4 id="how-to-estimate-prediction-error">How to Estimate Prediction Error<a class="headerlink" href="#how-to-estimate-prediction-error" title="Permanent link">&para;</a></h4>
<p>Option 1 [估计样本内误差]</p>
<ul>
<li>Estimate the optimism and add it to <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">C_{p}, A I C, B I C</span><script type="math/tex">C_{p}, A I C, B I C</script></span> work in this way for a special class of estimates that are linear in their parameters</li>
<li>Can use in-sample error for model selection but not a good estimate of Err</li>
</ul>
<p>Option 2 [直接估计样本外误差]</p>
<ul>
<li>Use cross-validation or bootstrap as direct estimates of the extra-sample <span class="arithmatex"><span class="MathJax_Preview">Err</span><script type="math/tex">Err</script></span> 本章后面描述的交叉验证以及自助法是对 样本外 (extra-sample) 误差 <span class="arithmatex"><span class="MathJax_Preview">Err</span><script type="math/tex">Err</script></span> 直接估计的方法．这些一般工具可以用于任意损失函数以及非线性自适应拟合技巧．</li>
</ul>
<p>样本内误差通常不是直接感兴趣的，因为特征的未来值不可能与它们训练集值一 致．但是为了模型之间的比较，样本内误差是很方便的，并且经常能够有效地进行模型选择．原因在于误差的相对（而不是绝对）大小是我们所关心的．</p>
<h3 id="estimates-of-err_inerr_in">Estimates of <span class="arithmatex"><span class="MathJax_Preview">Err_{in}</span><script type="math/tex">Err_{in}</script></span><a class="headerlink" href="#estimates-of-err_inerr_in" title="Permanent link">&para;</a></h3>
<h4 id="err_inerr_in-estimate-c_pc_p-statistic"><span class="arithmatex"><span class="MathJax_Preview">Err_{in}</span><script type="math/tex">Err_{in}</script></span> Estimate: <span class="arithmatex"><span class="MathJax_Preview">C_{p}</span><script type="math/tex">C_{p}</script></span> Statistic<a class="headerlink" href="#err_inerr_in-estimate-c_pc_p-statistic" title="Permanent link">&para;</a></h4>
<p>If <span class="arithmatex"><span class="MathJax_Preview">\hat{y}_{i}</span><script type="math/tex">\hat{y}_{i}</script></span> is obtained by a linear fit with <span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span> inputs, then</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)=d \sigma_{\epsilon}^{2}
</div>
<script type="math/tex; mode=display">
\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)=d \sigma_{\epsilon}^{2}
</script>
</div>
<p>for the additive error model <span class="arithmatex"><span class="MathJax_Preview">Y=f(X)+\epsilon</span><script type="math/tex">Y=f(X)+\epsilon</script></span></p>
<p>So (holds approximately when <span class="arithmatex"><span class="MathJax_Preview">N \rightarrow \infty</span><script type="math/tex">N \rightarrow \infty</script></span> )</p>
<div class="arithmatex">
<div class="MathJax_Preview">
E_{y}\left[E r r_{i n}\right]=E_{y}[\overline{e r r}]+2 \frac{d}{n} \sigma_{\epsilon}^{2}
</div>
<script type="math/tex; mode=display">
E_{y}\left[E r r_{i n}\right]=E_{y}[\overline{e r r}]+2 \frac{d}{n} \sigma_{\epsilon}^{2}
</script>
</div>
<p>Adapting this expression leads to the <span class="arithmatex"><span class="MathJax_Preview">C_{p}</span><script type="math/tex">C_{p}</script></span> statistic</p>
<div class="arithmatex">
<div class="MathJax_Preview">
C_{p}=\overline{e r r}+2 \frac{d}{n} \hat{\sigma}_{\epsilon}^{2}
</div>
<script type="math/tex; mode=display">
C_{p}=\overline{e r r}+2 \frac{d}{n} \hat{\sigma}_{\epsilon}^{2}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\hat{\sigma}_{\epsilon}^{2}</span><script type="math/tex">\hat{\sigma}_{\epsilon}^{2}</script></span> is an estimate of the <strong>noise variance</strong></p>
<h4 id="akaike-information-criterion">Akaike Information Criterion<a class="headerlink" href="#akaike-information-criterion" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
A I C=-\frac{2}{n} \log l i k+2 \frac{d}{n}
</div>
<script type="math/tex; mode=display">
A I C=-\frac{2}{n} \log l i k+2 \frac{d}{n}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\log l i k=\sum_{i=1}^{n} \log P_{\hat{\theta}}\left(y_{i}\right)</span><script type="math/tex">\log l i k=\sum_{i=1}^{n} \log P_{\hat{\theta}}\left(y_{i}\right)</script></span>, and <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}</span><script type="math/tex">\hat{\theta}</script></span> is the MLE of <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">-\frac{2}{n} \log l i k</span><script type="math/tex">-\frac{2}{n} \log l i k</script></span> rewards the fit between model and the data</li>
<li><span class="arithmatex"><span class="MathJax_Preview">2 \frac{d}{n}</span><script type="math/tex">2 \frac{d}{n}</script></span> is the penalty for including extra predictors in the model</li>
</ul>
<p>AIC can be seen as an estimate of <span class="arithmatex"><span class="MathJax_Preview">E r r_{i n}</span><script type="math/tex">E r r_{i n}</script></span> in this case with a <strong>log-likelihood loss</strong></p>
<p>另外, 在 随机项正态性假设下, 可知基于 RSS 的估计和 AIC 等价, 参见 <a href="https://en.wikipedia.org/wiki/Akaike_information_criterion#Comparison_with_least_squares">wiki</a>. 即有形式 <span class="arithmatex"><span class="MathJax_Preview">AIC = 2k + n \log(RSS)</span><script type="math/tex">AIC = 2k + n \log(RSS)</script></span></p>
<p><img alt="" src="../media/ASL%20note2/2021-12-22-10-55-39.png" /></p>
<p>注意,</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)=d \sigma_{\epsilon}^{2}
</div>
<script type="math/tex; mode=display">
\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)=d \sigma_{\epsilon}^{2}
</script>
</div>
<p>对于含有加性误差的线性模型在平方误差损失下是精确成立的，在对数似然损失下是近似成立的．特别地，这个法则一般地对于 0-1 损失是不成立的.</p>
<h4 id="effective-of-paramaters">Eﬀective # of Paramaters<a class="headerlink" href="#effective-of-paramaters" title="Permanent link">&para;</a></h4>
<p>For regularized fitting need to generalize the concept of number of parameters, why?</p>
<p>Consider regularized linear fitting - ridge, smoothing splines</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{y}=S y
</div>
<script type="math/tex; mode=display">
\hat{y}=S y
</script>
</div>
<p>where</p>
<ol>
<li><span class="arithmatex"><span class="MathJax_Preview">y=\left(y_{1}, y_{2}, \ldots, y_{n}\right)</span><script type="math/tex">y=\left(y_{1}, y_{2}, \ldots, y_{n}\right)</script></span> is the vector of training outputs</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{y}=\left(\hat{y}_{1}, \ldots, \hat{y}_{n}\right)</span><script type="math/tex">\hat{y}=\left(\hat{y}_{1}, \ldots, \hat{y}_{n}\right)</script></span> is the vector of predictions</li>
<li><span class="arithmatex"><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> is an <span class="arithmatex"><span class="MathJax_Preview">n \times n</span><script type="math/tex">n \times n</script></span> matrix, which depends on <span class="arithmatex"><span class="MathJax_Preview">x_{1}, \ldots, x_{n}</span><script type="math/tex">x_{1}, \ldots, x_{n}</script></span> but not <span class="arithmatex"><span class="MathJax_Preview">y_{1}, \ldots, y_{n}</span><script type="math/tex">y_{1}, \ldots, y_{n}</script></span></li>
</ol>
<p>Define the <strong>effective number of parameters</strong> as 有效参数个数, 也被称作 有效自由度 (effective degrees-of-freedom)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
d f(S)=\operatorname{trace}(S)
</div>
<script type="math/tex; mode=display">
d f(S)=\operatorname{trace}(S)
</script>
</div>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> is an <strong>orthogonal-projection</strong> matrix onto a basis set spanned by <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> features, then <span class="arithmatex"><span class="MathJax_Preview">\operatorname{trace}(S)=M</span><script type="math/tex">\operatorname{trace}(S)=M</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{tr}(S)</span><script type="math/tex">\operatorname{tr}(S)</script></span> - the correct quantity to replace <span class="arithmatex"><span class="MathJax_Preview">d</span><script type="math/tex">d</script></span> as the number of parameters in the <span class="arithmatex"><span class="MathJax_Preview">C_{p}</span><script type="math/tex">C_{p}</script></span> statistic. 在 <span class="arithmatex"><span class="MathJax_Preview">C_{p}</span><script type="math/tex">C_{p}</script></span> 统计量中的 d 应该是这里所定义的「有效自由度」</li>
<li>If <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> arises from an additive error model <span class="arithmatex"><span class="MathJax_Preview">Y=f(X)+\epsilon</span><script type="math/tex">Y=f(X)+\epsilon</script></span> with <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Var}(\epsilon)=\sigma_{\epsilon}^{2}</span><script type="math/tex">\operatorname{Var}(\epsilon)=\sigma_{\epsilon}^{2}</script></span>, then [上面有效自由度的定义基于线性变换的矩阵, 而在加性随机项假设下, 可以得到更一般性的定义] 证明 see <a href="https://github.com/szcf-weiya/ESL-CN/issues/195">here</a></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)=\operatorname{trace}(S) \sigma_{\epsilon}^{2}
</div>
<script type="math/tex; mode=display">
\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)=\operatorname{trace}(S) \sigma_{\epsilon}^{2}
</script>
</div>
<p>So the more general definitin of dof is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
d y(\hat{y})=\frac{\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)}{\sigma_{\epsilon}^{2}}
</div>
<script type="math/tex; mode=display">
d y(\hat{y})=\frac{\sum_{i=1}^{n} \operatorname{Cov}\left(\hat{y}_{i}, y_{i}\right)}{\sigma_{\epsilon}^{2}}
</script>
</div>
<h4 id="bayesian-approach-bic">Bayesian Approach &amp; BIC<a class="headerlink" href="#bayesian-approach-bic" title="Permanent link">&para;</a></h4>
<p>Generic Form of BIC</p>
<p><strong>Bayesian Information Criterion</strong> (BIC)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
B I C=-2 \log l i k+(\log N) \cdot d
</div>
<script type="math/tex; mode=display">
B I C=-2 \log l i k+(\log N) \cdot d
</script>
</div>
<ul>
<li>BIC statistic <span class="arithmatex"><span class="MathJax_Preview">\left(\times \frac{1}{2}\right)</span><script type="math/tex">\left(\times \frac{1}{2}\right)</script></span> is also known as the Schwarz criterion (SBC)</li>
</ul>
<p>Under Gaussian model with known variance <span class="arithmatex"><span class="MathJax_Preview">\sigma_{\epsilon}^{2}</span><script type="math/tex">\sigma_{\epsilon}^{2}</script></span>, for squared error loss</p>
<div class="arithmatex">
<div class="MathJax_Preview">
-2 \log \operatorname{lik} \propto \sum_{i} \frac{\left(y_{i}-\hat{f}\left(x_{i}\right)\right)^{2}}{\sigma_{\epsilon}^{2}}=\frac{N \cdot \overline{e r r}}{\sigma_{\epsilon}^{2}}
</div>
<script type="math/tex; mode=display">
-2 \log \operatorname{lik} \propto \sum_{i} \frac{\left(y_{i}-\hat{f}\left(x_{i}\right)\right)^{2}}{\sigma_{\epsilon}^{2}}=\frac{N \cdot \overline{e r r}}{\sigma_{\epsilon}^{2}}
</script>
</div>
<p>As a result, BIC can be re-written as</p>
<div class="arithmatex">
<div class="MathJax_Preview">
B I C=\frac{n}{\sigma_{\epsilon}^{2}}\left(\overline{e r r}+\log (N) \frac{d}{N} \sigma_{\epsilon}^{2}\right)
</div>
<script type="math/tex; mode=display">
B I C=\frac{n}{\sigma_{\epsilon}^{2}}\left(\overline{e r r}+\log (N) \frac{d}{N} \sigma_{\epsilon}^{2}\right)
</script>
</div>
<ul>
<li>BIC is proportional to AIC, with factor 2 replaced by <span class="arithmatex"><span class="MathJax_Preview">\log N</span><script type="math/tex">\log N</script></span> [因此可知当 <span class="arithmatex"><span class="MathJax_Preview">N&gt;e^2=7.4</span><script type="math/tex">N>e^2=7.4</script></span> 时, BIC 对于模型参数的惩罚更大]</li>
</ul>
<h5 id="derivation-of-bic">Derivation of BIC<a class="headerlink" href="#derivation-of-bic" title="Permanent link">&para;</a></h5>
<p>Starting point:</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\left\{M_{1}, \ldots, M_{m}\right\}</span><script type="math/tex">\left\{M_{1}, \ldots, M_{m}\right\}</script></span>, a set of candidate models and their corresponding parameters <span class="arithmatex"><span class="MathJax_Preview">\theta_{1}, \ldots, \theta_{m}</span><script type="math/tex">\theta_{1}, \ldots, \theta_{m}</script></span></li>
</ul>
<p>Goal:</p>
<ul>
<li>Choose the best model <span class="arithmatex"><span class="MathJax_Preview">M_{i}</span><script type="math/tex">M_{i}</script></span></li>
</ul>
<p>How</p>
<ul>
<li>Training data <span class="arithmatex"><span class="MathJax_Preview">Z=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</span><script type="math/tex">Z=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</script></span></li>
<li>Priors <span class="arithmatex"><span class="MathJax_Preview">p\left(\theta_{m} \mid M_{m}\right)</span><script type="math/tex">p\left(\theta_{m} \mid M_{m}\right)</script></span></li>
<li>Derive the Posterior of model <span class="arithmatex"><span class="MathJax_Preview">M_{m}</span><script type="math/tex">M_{m}</script></span></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
P\left(M_{m} \mid Z\right) &amp; \propto P\left(M_{m}\right) p\left(Z \mid M_{m}\right) \\
&amp; \propto P\left(M_{m}\right) \int p\left(Z \mid \theta_{m}, M_{m}\right) p\left(\theta_{m} \mid M_{m}\right) d \theta_{m}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
P\left(M_{m} \mid Z\right) & \propto P\left(M_{m}\right) p\left(Z \mid M_{m}\right) \\
& \propto P\left(M_{m}\right) \int p\left(Z \mid \theta_{m}, M_{m}\right) p\left(\theta_{m} \mid M_{m}\right) d \theta_{m}
\end{aligned}
</script>
</div>
<p>In comparision of two models <span class="arithmatex"><span class="MathJax_Preview">M_{m}</span><script type="math/tex">M_{m}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">M_{1}</span><script type="math/tex">M_{1}</script></span> 为了比较两个模型, 构造 后验 odds (选择概率较大的模型)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{\operatorname{Pr}\left(M_{m} \mid Z\right)}{\operatorname{Pr}\left(M_{l} \mid Z\right)}=\frac{\operatorname{Pr}\left(M_{m}\right)}{\operatorname{Pr}\left(M_{l}\right)} \times \frac{\operatorname{Pr}\left(Z \mid M_{m}\right)}{\operatorname{Pr}\left(Z \mid M_{l}\right)}
</div>
<script type="math/tex; mode=display">
\frac{\operatorname{Pr}\left(M_{m} \mid Z\right)}{\operatorname{Pr}\left(M_{l} \mid Z\right)}=\frac{\operatorname{Pr}\left(M_{m}\right)}{\operatorname{Pr}\left(M_{l}\right)} \times \frac{\operatorname{Pr}\left(Z \mid M_{m}\right)}{\operatorname{Pr}\left(Z \mid M_{l}\right)}
</script>
</div>
<ul>
<li>Posterior odds <span class="arithmatex"><span class="MathJax_Preview">=</span><script type="math/tex">=</script></span> prior odds <span class="arithmatex"><span class="MathJax_Preview">* \operatorname{BF}(Z)</span><script type="math/tex">* \operatorname{BF}(Z)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">B F(Z)=\frac{\operatorname{Pr}\left(Z \mid M_{m}\right)}{\operatorname{Pr}\left(Z \mid M_{l}\right)}</span><script type="math/tex">B F(Z)=\frac{\operatorname{Pr}\left(Z \mid M_{m}\right)}{\operatorname{Pr}\left(Z \mid M_{l}\right)}</script></span> is called <strong>Bayes factor</strong> 贝叶斯因子</li>
<li>Indicates the contribution of the data toward the posterior odds 数据对于后验 odds 的贡献</li>
</ul>
<p>The posterior of model <span class="arithmatex"><span class="MathJax_Preview">M_{m}</span><script type="math/tex">M_{m}</script></span> is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(M_{m} \mid Z\right) \propto P\left(M_{m}\right) \int p\left(Z \mid \theta_{m}, M_{m}\right) p\left(\theta_{m} \mid M_{m}\right) d \theta_{m}
</div>
<script type="math/tex; mode=display">
P\left(M_{m} \mid Z\right) \propto P\left(M_{m}\right) \int p\left(Z \mid \theta_{m}, M_{m}\right) p\left(\theta_{m} \mid M_{m}\right) d \theta_{m}
</script>
</div>
<ul>
<li>Usually assume uniform prior <span class="arithmatex"><span class="MathJax_Preview">P\left(M_{m}\right)=1 / M</span><script type="math/tex">P\left(M_{m}\right)=1 / M</script></span></li>
<li>Approximate <span class="arithmatex"><span class="MathJax_Preview">\int p\left(Z \mid \theta_{m}, M_{m}\right) p\left(\theta_{m} \mid M_{m}\right) d \theta_{m}</span><script type="math/tex">\int p\left(Z \mid \theta_{m}, M_{m}\right) p\left(\theta_{m} \mid M_{m}\right) d \theta_{m}</script></span> by <strong>simplification and Laplace approximation</strong></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\log P\left(Z \mid M_{m}\right)=\log P\left(Z \mid \hat{\theta}_{m}, M_{m}\right)-\frac{d_{m}}{2} \log N+O(1)
</div>
<script type="math/tex; mode=display">
\log P\left(Z \mid M_{m}\right)=\log P\left(Z \mid \hat{\theta}_{m}, M_{m}\right)-\frac{d_{m}}{2} \log N+O(1)
</script>
</div>
<ul>
<li>where <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}_{m}</span><script type="math/tex">\hat{\theta}_{m}</script></span> is MLE and <span class="arithmatex"><span class="MathJax_Preview">d_{m}</span><script type="math/tex">d_{m}</script></span> is # of free parameters in <span class="arithmatex"><span class="MathJax_Preview">M_{m}</span><script type="math/tex">M_{m}</script></span></li>
</ul>
<p>If we define the loss function as <span class="arithmatex"><span class="MathJax_Preview">-2 \log \operatorname{Pr}\left(Z \mid \hat{\theta}_{m}, M_{m}\right)</span><script type="math/tex">-2 \log \operatorname{Pr}\left(Z \mid \hat{\theta}_{m}, M_{m}\right)</script></span>, then it&rsquo;s equivalent to BIC</p>
<h5 id="aic-vs-bic">AIC vs BIC<a class="headerlink" href="#aic-vs-bic" title="Permanent link">&para;</a></h5>
<p>If <span class="arithmatex"><span class="MathJax_Preview">M_{\text {true }} \in\left\{M_{1}, \ldots, M_{M}\right\}</span><script type="math/tex">M_{\text {true }} \in\left\{M_{1}, \ldots, M_{M}\right\}</script></span> then as <span class="arithmatex"><span class="MathJax_Preview">N \rightarrow \infty</span><script type="math/tex">N \rightarrow \infty</script></span></p>
<ul>
<li>BIC will select <span class="arithmatex"><span class="MathJax_Preview">M_{\text {true }}</span><script type="math/tex">M_{\text {true }}</script></span> with probability 1</li>
<li>AIC will not, tends to choose models which are too complex</li>
</ul>
<p>However, when <span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> is small</p>
<ul>
<li>BIC often choose models which are too simple</li>
</ul>
<p>In addition, for BIC</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\frac{e^{-\frac{1}{2} B I C_{m}}}{\sum_{l=1}^{M} e^{-\frac{1}{2} B I C_{l}}}</span><script type="math/tex">\frac{e^{-\frac{1}{2} B I C_{m}}}{\sum_{l=1}^{M} e^{-\frac{1}{2} B I C_{l}}}</script></span> [对 <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> 个元素的模型集合进行计算 BIC 准则, 可以得到每个模型的后验概率即为此]</li>
<li>Estimate not only the best model, but also their relative merits</li>
</ul>
<h3 id="cross-validation">Cross Validation<a class="headerlink" href="#cross-validation" title="Permanent link">&para;</a></h3>
<h4 id="k-fold-cross-validation">K-Fold Cross Validation<a class="headerlink" href="#k-fold-cross-validation" title="Permanent link">&para;</a></h4>
<p>General Approach</p>
<ul>
<li>Split the data into <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span> roughly equal-size parts</li>
<li>For the <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> part, calculate the prediction error of the model fit using the other K-1 parts</li>
<li>Do this for <span class="arithmatex"><span class="MathJax_Preview">k=1,2, \ldots, K</span><script type="math/tex">k=1,2, \ldots, K</script></span> and combine the <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span> estimates of the <strong>prediction error</strong></li>
</ul>
<p>When and Why</p>
<ul>
<li>It is applied when labelled training data is relatively sparse 适合训练数据较少的情况</li>
<li>This method directly estimates <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}=E[L(Y, \hat{f}(X))]</span><script type="math/tex">\operatorname{Err}=E[L(Y, \hat{f}(X))]</script></span> 直接估计了样本外误差</li>
</ul>
<p>具体而言</p>
<ul>
<li>The mapping <span class="arithmatex"><span class="MathJax_Preview">\kappa: 1, \ldots, n \rightarrow 1, \ldots, K</span><script type="math/tex">\kappa: 1, \ldots, n \rightarrow 1, \ldots, K</script></span> indicates observation <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> belongs to partition <span class="arithmatex"><span class="MathJax_Preview">\kappa(i)</span><script type="math/tex">\kappa(i)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{f}^{-k}(x)</span><script type="math/tex">\hat{f}^{-k}(x)</script></span> is the function fitted with the <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> part of the data removed</li>
<li>Cross-validation estimate the prediction error</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
C V(\hat{f})=\frac{1}{n} \sum_{i=1}^{n} L\left(y_{i}, \hat{f}^{-\kappa(i)}\left(x_{i}\right)\right.
</div>
<script type="math/tex; mode=display">
C V(\hat{f})=\frac{1}{n} \sum_{i=1}^{n} L\left(y_{i}, \hat{f}^{-\kappa(i)}\left(x_{i}\right)\right.
</script>
</div>
<ul>
<li>Typical choices for <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span> are 5 or 10</li>
<li>The case <span class="arithmatex"><span class="MathJax_Preview">K=n</span><script type="math/tex">K=n</script></span> is shown as <strong>leave-one-out cross-validation</strong> 留一法 (<strong>LOOCV</strong>)</li>
</ul>
<p>这样的话, 我么利用了训练数据对于泛化误差有了一个估计. 那么如何利用 CV 来进行模型选择? 我们直接选择在 CV 的估计中泛化误差最小的那个模型.</p>
<p>Candidate models <span class="arithmatex"><span class="MathJax_Preview">f(x, \alpha)</span><script type="math/tex">f(x, \alpha)</script></span> indexed by a parameter <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> <span class="arithmatex"><span class="MathJax_Preview">\hat{f}^{-k}(x, \alpha)</span><script type="math/tex">\hat{f}^{-k}(x, \alpha)</script></span> is the <span class="arithmatex"><span class="MathJax_Preview">\alpha_{t h}</span><script type="math/tex">\alpha_{t h}</script></span> model fit with <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> part of the data removed. The define</p>
<div class="arithmatex">
<div class="MathJax_Preview">
C V(\hat{f}, \alpha)=\frac{1}{n} \sum_{i=1}^{n} L\left(y_{i}, \hat{f}^{-\kappa(i)}\left(x_{i}, \alpha\right)\right.
</div>
<script type="math/tex; mode=display">
C V(\hat{f}, \alpha)=\frac{1}{n} \sum_{i=1}^{n} L\left(y_{i}, \hat{f}^{-\kappa(i)}\left(x_{i}, \alpha\right)\right.
</script>
</div>
<p>Choose the model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\alpha}=\operatorname{argmin}_{\alpha} C V(\hat{f}, \alpha)
</div>
<script type="math/tex; mode=display">
\hat{\alpha}=\operatorname{argmin}_{\alpha} C V(\hat{f}, \alpha)
</script>
</div>
<h5 id="what-does-cross-validation-estimate">What Does Cross Validation Estimate<a class="headerlink" href="#what-does-cross-validation-estimate" title="Permanent link">&para;</a></h5>
<p>Intuition says</p>
<ul>
<li>When <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}=5</span><script type="math/tex">\mathrm{K}=5</script></span> or <span class="arithmatex"><span class="MathJax_Preview">10, C V(\hat{f}) \approx \operatorname{Err}</span><script type="math/tex">10, C V(\hat{f}) \approx \operatorname{Err}</script></span> as training sets for each fold are fairly different</li>
<li>When <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}=\mathrm{n}, C V(\hat{f}) \approx \operatorname{Err}_{T}</span><script type="math/tex">\mathrm{K}=\mathrm{n}, C V(\hat{f}) \approx \operatorname{Err}_{T}</script></span> as training sets for each fold are almost identical</li>
</ul>
<p>However, according to theory and simulation experiments</p>
<ul>
<li>Cross validation really only effectively estimates Err [参见 7.12 节]</li>
</ul>
<h5 id="what-values-of-mathrmkmathrmk">What Values of <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span><a class="headerlink" href="#what-values-of-mathrmkmathrmk" title="Permanent link">&para;</a></h5>
<p>When <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}=\mathrm{n}</span><script type="math/tex">\mathrm{K}=\mathrm{n}</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">C V(\hat{f})</span><script type="math/tex">C V(\hat{f})</script></span> is approximately an unbiased estimate of <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}</span><script type="math/tex">\operatorname{Err}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">C V(\hat{f})</span><script type="math/tex">C V(\hat{f})</script></span> has high variance as the <span class="arithmatex"><span class="MathJax_Preview">\mathrm{n}</span><script type="math/tex">\mathrm{n}</script></span> training sets are similar</li>
<li>Computational burden high (except for a few exceptions)</li>
</ul>
<p>When <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}=5</span><script type="math/tex">\mathrm{K}=5</script></span> (lowish)</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">C V(\hat{f})</span><script type="math/tex">C V(\hat{f})</script></span> has low variance</li>
<li><span class="arithmatex"><span class="MathJax_Preview">C V(\hat{f})</span><script type="math/tex">C V(\hat{f})</script></span> is potentially an upward biased estimate of <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Err}</span><script type="math/tex">\operatorname{Err}</script></span> 偏差较大</li>
</ul>
<p><img alt="" src="../media/ASL%20note2/2021-12-22-12-03-08.png" /></p>
<h4 id="cross-validation-right-wrong">Cross Validation - Right \&amp; Wrong<a class="headerlink" href="#cross-validation-right-wrong" title="Permanent link">&para;</a></h4>
<p>Task: classification problem with a large number of predictors</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">N=50</span><script type="math/tex">N=50</script></span> samples in two equal-sized classes</li>
<li><span class="arithmatex"><span class="MathJax_Preview">p=5000</span><script type="math/tex">p=5000</script></span> quantitative predictors (standard Gaussian) independent of class labels</li>
<li>True test error rate of any classifier <span class="arithmatex"><span class="MathJax_Preview">50 \%</span><script type="math/tex">50 \%</script></span></li>
</ul>
<p>Strategy 1</p>
<ol>
<li>Screen the predictors: Find a subset of good predictors that are correlated with the class labels</li>
<li>Build a classifier: based on the subset of good predictors</li>
<li>Perform cross-validation: estimate the unknown tuning parameters and Err of the final model</li>
</ol>
<p>Strategy 2</p>
<ol>
<li>Divide the samples into <span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span> groups randomly</li>
<li>For each fold <span class="arithmatex"><span class="MathJax_Preview">k=1, \ldots, K</span><script type="math/tex">k=1, \ldots, K</script></span>
   - Find a subset of good predictors using all the samples minus the <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> fold
   - Build a classifier using all the samples minus the <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> fold
   - Use the classifier to predict the labels for the samples in the <span class="arithmatex"><span class="MathJax_Preview">k_{t h}</span><script type="math/tex">k_{t h}</script></span> fold</li>
<li>The error estimates from step 2 are then accumulated over all <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> folds to produce the cross-validation estimate of prediction error</li>
</ol>
<p><img alt="" src="../media/ASL%20note2/2021-12-22-14-07-53.png" /></p>
<p>一般地，在多步建模过程中，交叉验证必须应用到整个模型步骤的序列中．特别地，<strong>“丢弃”样本必须在任何选择或者过滤之前</strong>．有一个条件：初始非监督筛选步骤可以在丢弃样本之前完成．举个例子，开始交叉验证前，我们可以选择 1000 个在 50 个样本上有着最大方差的预测变量．因为这个过滤不涉及到类别，所以它不会给预测变量不公平的好处．</p>
<h3 id="bootstrap-method">Bootstrap Method<a class="headerlink" href="#bootstrap-method" title="Permanent link">&para;</a></h3>
<p>The Bootstrap
Given a training set <span class="arithmatex"><span class="MathJax_Preview">X=\left(z_{1}, z_{2}, \ldots, z_{n}\right)</span><script type="math/tex">X=\left(z_{1}, z_{2}, \ldots, z_{n}\right)</script></span> with each <span class="arithmatex"><span class="MathJax_Preview">z_{i}=\left(x_{i}, y_{i}\right)</span><script type="math/tex">z_{i}=\left(x_{i}, y_{i}\right)</script></span></p>
<p>The <strong>bootstrap</strong> idea is
For <span class="arithmatex"><span class="MathJax_Preview">b=1,2, \ldots, B</span><script type="math/tex">b=1,2, \ldots, B</script></span></p>
<ol>
<li>Randomly draw <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> samples <strong>with replacement</strong> from <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> to get <span class="arithmatex"><span class="MathJax_Preview">Z^{*b}</span><script type="math/tex">Z^{*b}</script></span> that is</li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
Z^{* b}=\left(z_{b_{1}}, z_{b_{2}}, \ldots, z_{b_{n}}\right) \text { with } b_{i} \in 1,2, \ldots, n
</div>
<script type="math/tex; mode=display">
Z^{* b}=\left(z_{b_{1}}, z_{b_{2}}, \ldots, z_{b_{n}}\right) \text { with } b_{i} \in 1,2, \ldots, n
</script>
</div>
<ol start="2">
<li>Refit the model using <span class="arithmatex"><span class="MathJax_Preview">Z^{*b}</span><script type="math/tex">Z^{*b}</script></span> to get <span class="arithmatex"><span class="MathJax_Preview">S\left(Z^{* b}\right)</span><script type="math/tex">S\left(Z^{* b}\right)</script></span></li>
</ol>
<p>Examine the behavior of the <span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span> fits</p>
<div class="arithmatex">
<div class="MathJax_Preview">
S\left(Z^{*1}\right), S\left(Z^{* 2}\right), \ldots, S\left(Z^{* B}\right)
</div>
<script type="math/tex; mode=display">
S\left(Z^{*1}\right), S\left(Z^{* 2}\right), \ldots, S\left(Z^{* B}\right)
</script>
</div>
<p><img alt="" src="../media/ASL%20note2/2021-12-22-15-16-34.png" /></p>
<p><strong>Can Estimate Any Aspect of the Distribution</strong> of <span class="arithmatex"><span class="MathJax_Preview">S(Z)</span><script type="math/tex">S(Z)</script></span></p>
<p>For example its variance</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\operatorname{Var}}[S(Z)]=\frac{1}{B-1} \sum_{b=1}^{B}\left(S\left(Z^{* b}\right)-\bar{S}^{*}\right)^{2}
</div>
<script type="math/tex; mode=display">
\hat{\operatorname{Var}}[S(Z)]=\frac{1}{B-1} \sum_{b=1}^{B}\left(S\left(Z^{* b}\right)-\bar{S}^{*}\right)^{2}
</script>
</div>
<p>where</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\bar{S}^{*}=\frac{1}{B} \sum_{b=1}^{B} S\left(Z^{* b}\right)
</div>
<script type="math/tex; mode=display">
\bar{S}^{*}=\frac{1}{B} \sum_{b=1}^{B} S\left(Z^{* b}\right)
</script>
</div>
<h4 id="attempt-1">Attempt 1<a class="headerlink" href="#attempt-1" title="Permanent link">&para;</a></h4>
<p>$$
\hat{E r r r}<em b="1">{b o o t}=\frac{1}{B} \frac{1}{n} \sum</em>^{B} \sum_{i=1}^{n} L\left(y_{i}, \hat{f}^{*b}\left(x_{i}\right)\right)
$$
where <span class="arithmatex"><span class="MathJax_Preview">\hat{f}^{*b}\left(x_{i}\right)</span><script type="math/tex">\hat{f}^{*b}\left(x_{i}\right)</script></span> is the predicted value at <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> using the model computed from <span class="arithmatex"><span class="MathJax_Preview">Z^{* b}</span><script type="math/tex">Z^{* b}</script></span></p>
<ul>
<li>Is this a good estimate or not? Why</li>
<li>Overlap between the training and testing sets</li>
<li>How could we do better?</li>
<li>Mimic cross validation</li>
</ul>
<p>这种方式中, 事实上二层的损失函数算的是样本内误差, 因此会低估预测误差.</p>
<h4 id="attempt-2-leave-one-out-bootstrap">Attempt 2: leave-one-out bootstrap<a class="headerlink" href="#attempt-2-leave-one-out-bootstrap" title="Permanent link">&para;</a></h4>
<p>因此, 这里的想法是使得训练和测试集分离.</p>
<p>$$
\hat{E r r}^{(1)}=\frac{1}{n} \sum_{i=1}^{n} \frac{1}{\left|C^{-i}\right|} \sum_{b \in C^{-i}} L\left(y_{i}, \hat{f}^{* b}\left(x_{i}\right)\right)
$$
where <span class="arithmatex"><span class="MathJax_Preview">C^{-i}</span><script type="math/tex">C^{-i}</script></span> is the set of bootstrap samples <span class="arithmatex"><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> not containing observation <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span></p>
<ul>
<li>Either make <span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span> large enough so <span class="arithmatex"><span class="MathJax_Preview">\left|C^{-i}\right|&gt;0</span><script type="math/tex">\left|C^{-i}\right|>0</script></span> for all <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> or</li>
<li>Omit observation <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> from testing if <span class="arithmatex"><span class="MathJax_Preview">\left|C^{-i}\right|=0</span><script type="math/tex">\left|C^{-i}\right|=0</script></span></li>
</ul>
<p>Pros</p>
<ol>
<li>avoids the overfitting problems of <span class="arithmatex"><span class="MathJax_Preview">\bar{Err}_{boot}</span><script type="math/tex">\bar{Err}_{boot}</script></span></li>
</ol>
<p>Cons</p>
<ol>
<li>Has the training-set-size bias of corss-validation 会有在讨论交叉验证中提到的 训练集大小偏差 (training-set-size) 问题</li>
<li>Probability that observation <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> in bootstrap sample <span class="arithmatex"><span class="MathJax_Preview">Z^{*b}</span><script type="math/tex">Z^{*b}</script></span></li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(i \in Z^{* b}\right)=1-\left(1-\frac{1}{n}\right)^{n} \approx 1-e^{-1}=0.632
</div>
<script type="math/tex; mode=display">
P\left(i \in Z^{* b}\right)=1-\left(1-\frac{1}{n}\right)^{n} \approx 1-e^{-1}=0.632
</script>
</div>
<ol start="3">
<li>Therefore the average number of distinct observations in <span class="arithmatex"><span class="MathJax_Preview">Z^{* b}</span><script type="math/tex">Z^{* b}</script></span> is <span class="arithmatex"><span class="MathJax_Preview">0.632 n</span><script type="math/tex">0.632 n</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\overline{E r r}^{(1)}</span><script type="math/tex">\overline{E r r}^{(1)}</script></span> bias similar to two fold cross validation (usually upward) 向上的偏差</li>
</ol>
<h4 id="attempt-3-the-632-estimator-to-alleviate-bias">Attempt 3: The .632 estimator To Alleviate Bias<a class="headerlink" href="#attempt-3-the-632-estimator-to-alleviate-bias" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
\overline{E r r}^{(.632)}=.368 \overline{e r r}+.632 \overline{E r r}^{(1)}
</div>
<script type="math/tex; mode=display">
\overline{E r r}^{(.632)}=.368 \overline{e r r}+.632 \overline{E r r}^{(1)}
</script>
</div>
<ul>
<li>Compromise between the training error <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span> and the leave-one-out bootstrap estimate</li>
<li>Derivation not easy</li>
<li>The constant <span class="arithmatex"><span class="MathJax_Preview">.632</span><script type="math/tex">.632</script></span> relates to <span class="arithmatex"><span class="MathJax_Preview">P\left(i \in Z^{* b}\right)</span><script type="math/tex">P\left(i \in Z^{* b}\right)</script></span></li>
</ul>
<p>However, .632 estimator does not do well if predictor overfits</p>
<h4 id="estimate-the-degree-of-overfitting">Estimate the Degree of Overfitting<a class="headerlink" href="#estimate-the-degree-of-overfitting" title="Permanent link">&para;</a></h4>
<p><strong>No-information error rate</strong> 无信息误差率</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\gamma}=\frac{1}{n^{2}} \sum_{i=1}^{n} \sum_{j=1}^{n} L\left(y_{i}, \hat{f}\left(x_{j}\right)\right)
</div>
<script type="math/tex; mode=display">
\hat{\gamma}=\frac{1}{n^{2}} \sum_{i=1}^{n} \sum_{j=1}^{n} L\left(y_{i}, \hat{f}\left(x_{j}\right)\right)
</script>
</div>
<ul>
<li>Estimate of the error rate of <span class="arithmatex"><span class="MathJax_Preview">\hat{f}</span><script type="math/tex">\hat{f}</script></span> if <strong>inputs and outputs were independent</strong> 输入变量与类别标签独立时的预测规则的误差率</li>
<li>Note the prediction rule <span class="arithmatex"><span class="MathJax_Preview">\hat{f}</span><script type="math/tex">\hat{f}</script></span>, is evaluated on all possible combinations of targets <span class="arithmatex"><span class="MathJax_Preview">y_{i}</span><script type="math/tex">y_{i}</script></span> and predictors <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span></li>
</ul>
<p><strong>Relative overfitting rate</strong> 相对过拟合率</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{R}=\frac{\overline{Err^{(1)}}-\overline{e r r}}{\hat{\gamma}-\overline{e r r}}
</div>
<script type="math/tex; mode=display">
\hat{R}=\frac{\overline{Err^{(1)}}-\overline{e r r}}{\hat{\gamma}-\overline{e r r}}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">0 \leqslant \hat{R} \leqslant 1</span><script type="math/tex">0 \leqslant \hat{R} \leqslant 1</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{R}=0 \rightarrow</span><script type="math/tex">\hat{R}=0 \rightarrow</script></span> no overfitting</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{R}=1 \rightarrow</span><script type="math/tex">\hat{R}=1 \rightarrow</script></span> overfitting or no-information value <span class="arithmatex"><span class="MathJax_Preview">\hat{\gamma}-\overline{e r r}</span><script type="math/tex">\hat{\gamma}-\overline{e r r}</script></span></li>
</ul>
<h4 id="attempt-4-the-632632-estimator">Attempt 4: The <span class="arithmatex"><span class="MathJax_Preview">.632+</span><script type="math/tex">.632+</script></span> estimator<a class="headerlink" href="#attempt-4-the-632632-estimator" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{E r r}^{(.632+)}=(1-\hat{\omega}) \overline{e r r}+\hat{\omega} \hat{E r r}^{(1)}
</div>
<script type="math/tex; mode=display">
\hat{E r r}^{(.632+)}=(1-\hat{\omega}) \overline{e r r}+\hat{\omega} \hat{E r r}^{(1)}
</script>
</div>
<p>with</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\omega}=\frac{.632}{1-.632 \hat{R}}
</div>
<script type="math/tex; mode=display">
\hat{\omega}=\frac{.632}{1-.632 \hat{R}}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">.632 \leqslant \hat{\omega} \leqslant 1</span><script type="math/tex">.632 \leqslant \hat{\omega} \leqslant 1</script></span> as <span class="arithmatex"><span class="MathJax_Preview">\hat{R}</span><script type="math/tex">\hat{R}</script></span> ranges from 0 to 1</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{E r r}^{(.632+)}</span><script type="math/tex">\hat{E r r}^{(.632+)}</script></span> ranges from <span class="arithmatex"><span class="MathJax_Preview">\hat{E r r}^{(.632)}</span><script type="math/tex">\hat{E r r}^{(.632)}</script></span> to <span class="arithmatex"><span class="MathJax_Preview">\hat{E r r}^{(1)}</span><script type="math/tex">\hat{E r r}^{(1)}</script></span> 大致来讲，它得到舍一自助法和训练误差率之间的权衡，权衡依赖于模型过拟合的程度</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{E r r}^{(.632+)}</span><script type="math/tex">\hat{E r r}^{(.632+)}</script></span> is a compromise between <span class="arithmatex"><span class="MathJax_Preview">\hat{E r r}^{(.632)}</span><script type="math/tex">\hat{E r r}^{(.632)}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\overline{e r r}</span><script type="math/tex">\overline{e r r}</script></span> that depends on the amount of overfitting</li>
<li>Again, the derivation is non-trivial</li>
</ul>
<p><img alt="" src="../media/ASL%20note2/2021-12-22-16-20-10.png" /></p>
<p>10-Fold Cross Validation \&amp; .632+ Bootstrap</p>
<p>In the above cases</p>
<ul>
<li>AlC, cross validation or bootstrap yields a model fairly close to the best available</li>
<li>AIC (BIC) overestimate Err by <span class="arithmatex"><span class="MathJax_Preview">38 \%, 37 \%, 51 \%</span><script type="math/tex">38 \%, 37 \%, 51 \%</script></span>, and <span class="arithmatex"><span class="MathJax_Preview">30 \%</span><script type="math/tex">30 \%</script></span></li>
<li>Cross validation (bootstrap) overestimate Err by <span class="arithmatex"><span class="MathJax_Preview">1 \%, 4 \%, 0 \%</span><script type="math/tex">1 \%, 4 \%, 0 \%</script></span>, and <span class="arithmatex"><span class="MathJax_Preview">4 \%</span><script type="math/tex">4 \%</script></span></li>
</ul>
<p>However, for many adaptive, nonlinear techniques (like trees), estimation of the effective number of parameters is very difficult, making AIC impractical.</p>

              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../ASL%20note/" class="md-footer__link md-footer__link--prev" aria-label="Previous: ASL1" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              ASL1
            </div>
          </div>
        </a>
      
      
        
        <a href="../ASL%20note3/" class="md-footer__link md-footer__link--next" aria-label="Next: ASL3" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              ASL3
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2021-2022 Easonshi
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://github.com/Lightblues" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://lightblues.github.io/" target="_blank" rel="noopener" title="lightblues.github.io" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><path d="M280.37 148.26 96 300.11V464a16 16 0 0 0 16 16l112.06-.29a16 16 0 0 0 15.92-16V368a16 16 0 0 1 16-16h64a16 16 0 0 1 16 16v95.64a16 16 0 0 0 16 16.05L464 480a16 16 0 0 0 16-16V300L295.67 148.26a12.19 12.19 0 0 0-15.3 0zM571.6 251.47 488 182.56V44.05a12 12 0 0 0-12-12h-56a12 12 0 0 0-12 12v72.61L318.47 43a48 48 0 0 0-61 0L4.34 251.47a12 12 0 0 0-1.6 16.9l25.5 31A12 12 0 0 0 45.15 301l235.22-193.74a12.19 12.19 0 0 1 15.3 0L530.9 301a12 12 0 0 0 16.9-1.6l25.5-31a12 12 0 0 0-1.7-16.93z"/></svg>
    </a>
  
    
    
    <a href="mailto:oldcitystal@gmail.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M464 64H48C21.49 64 0 85.49 0 112v288c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V112c0-26.51-21.49-48-48-48zm0 48v40.805c-22.422 18.259-58.168 46.651-134.587 106.49-16.841 13.247-50.201 45.072-73.413 44.701-23.208.375-56.579-31.459-73.413-44.701C106.18 199.465 70.425 171.067 48 152.805V112h416zM48 400V214.398c22.914 18.251 55.409 43.862 104.938 82.646 21.857 17.205 60.134 55.186 103.062 54.955 42.717.231 80.509-37.199 103.053-54.947 49.528-38.783 82.032-64.401 104.947-82.653V400H48z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.top"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing", "select.version.title": "Select version"}, "search": "../../assets/javascripts/workers/search.cefbb252.min.js"}</script>
    
    
      <script src="../../assets/javascripts/bundle.17f42bbf.min.js"></script>
      
        <script src="../../js/baidu-tongji.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>