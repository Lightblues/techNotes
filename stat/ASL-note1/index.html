
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="O'ver curious">
      
      
        <meta name="author" content="Easonshi">
      
      
        <link rel="canonical" href="https://lightblues.github.io/techNotes/stat/ASL-note1/">
      
      
        <link rel="prev" href="../ASL-mindmap/">
      
      
        <link rel="next" href="../ASL-note2/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.30">
    
    
      
        <title>ASL1 - techNotes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#asl-note" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="techNotes" class="md-header__button md-logo" aria-label="techNotes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            techNotes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ASL1
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/lightblues/techNotes" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    lightblues/techNotes
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../code/CS/git-note/" class="md-tabs__link">
          
  
  Code

        </a>
      </li>
    
  

    
  

      
        
  
  
    
  
  
    
    
      
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../" class="md-tabs__link">
          
  
  Stat

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../Linux/Linux/" class="md-tabs__link">
          
  
  Linux

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../os/EFI/" class="md-tabs__link">
          
  
  OS

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../NLP/Lexicon-%E8%AF%8D%E6%B1%87%E5%A2%9E%E5%BC%BA/" class="md-tabs__link">
          
  
  NLP

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="https://blog.easonsi.site" class="md-tabs__link">
        
  
    
  
  üîóBlog

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="techNotes" class="md-nav__button md-logo" aria-label="techNotes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    techNotes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/lightblues/techNotes" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    lightblues/techNotes
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Code
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Code
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    CS
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            CS
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/CS/git-note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Git
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Python
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Python
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/Python/Python-note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ËØæÁ®ãÁ¨îËÆ∞
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    JavaScripe
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            JavaScripe
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/js-mindmap/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mindmap
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/js-note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    js note
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/jQuery-note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jQuery
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/node-js-note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Node.js
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Stat
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Stat
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" checked>
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="">
            
  
  <span class="md-ellipsis">
    È´òÁ≠âÁªüËÆ°Â≠¶‰π†
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            È´òÁ≠âÁªüËÆ°Â≠¶‰π†
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    index
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-mindmap/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ASL mindmap
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    ASL1
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    ASL1
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      ÂºïÂÖ•
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regression-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Regression \&amp; Classification
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      ‰∏§ÁßçÁÆÄÂçïÁöÑÈ¢ÑÊµãÊñπÊ≥ï
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#statistical-decision-theory" class="md-nav__link">
    <span class="md-ellipsis">
      Statistical Decision Theory
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#curse-of-dimensionality" class="md-nav__link">
    <span class="md-ellipsis">
      Curse of Dimensionality
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      ÁªüËÆ°Ê®°ÂûãÔºåÁõëÁù£Â≠¶‰π†ÂíåÂáΩÊï∞ÈÄºËøë
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#restricted-estimators" class="md-nav__link">
    <span class="md-ellipsis">
      ÈôêÂà∂ÊÄß‰º∞ËÆ° Restricted Estimators
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-selection-and-the-biasvariance-tradeoff" class="md-nav__link">
    <span class="md-ellipsis">
      Model Selection and the Bias‚ÄìVariance Tradeoff
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-methods-for-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Linear Methods for Regression
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Linear Methods for Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction-of-linear-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction of Linear Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recap-of-linear-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Recap of Linear Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#subset-selection" class="md-nav__link">
    <span class="md-ellipsis">
      Subset Selection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shrinkage-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Shrinkage methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods-using-derived-input-directions" class="md-nav__link">
    <span class="md-ellipsis">
      Methods Using Derived Input Directions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#some-algebra" class="md-nav__link">
    <span class="md-ellipsis">
      Some Algebra
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Some Algebra">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#eigen-decomposition-square-matrix" class="md-nav__link">
    <span class="md-ellipsis">
      Eigen decomposition (square matrix)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singular-value-decomposition-any-matrix-m-times-nm-times-n" class="md-nav__link">
    <span class="md-ellipsis">
      Singular value decomposition (any matrix m \times nm \times n )
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pca-principle-component-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      PCA (Principle Component Analysis)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-methods-for-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Linear Methods for Classification
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Linear Methods for Classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#linear-regression-of-an-indicator-matrix" class="md-nav__link">
    <span class="md-ellipsis">
      Linear Regression of an Indicator Matrix
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linear-discriminant-analysis-lda" class="md-nav__link">
    <span class="md-ellipsis">
      Linear Discriminant Analysis (LDA)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lda-practicalities" class="md-nav__link">
    <span class="md-ellipsis">
      LDA Practicalities
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logistic-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Logistic Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#naive-bayes" class="md-nav__link">
    <span class="md-ellipsis">
      Naive Bayes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#seprating-hyperplanes" class="md-nav__link">
    <span class="md-ellipsis">
      Seprating Hyperplanes Áï•
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-note2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ASL2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-note3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ASL3
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-note4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ASL4
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    ÁÆóÊ≥ïÂØºËÆ∫
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            ÁÆóÊ≥ïÂØºËÆ∫
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-MinCut-2SAT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    basic
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-prob-Markov-Chebyshev-Hoeffding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ê¶ÇÁéáÂü∫Á°Ä
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-computation-%E8%AE%A1%E7%AE%97%E7%90%86%E8%AE%BA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ËÆ°ÁÆóÁêÜËÆ∫
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-graph-Adjacency-Laplacian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ÂõæÁõ∏ÂÖ≥, Laplacian
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-differential-privacy-%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Â∑ÆÂàÜÈöêÁßÅ
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../optimization-%E5%87%B8%E4%BC%98%E5%8C%96/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Âá∏‰ºòÂåñ
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Linux
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Linux
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/Linux/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    General
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/tutor-%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83%E7%9B%B8%E5%85%B3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ÂÆûÈ™åÁéØÂ¢ÉÁõ∏ÂÖ≥
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/Linux-maintain/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Á≥ªÁªüÁª¥Êä§
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/Linux-configure-make-install-%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ÁºñËØëÂÆâË£ÖËΩØ‰ª∂
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    OS
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            OS
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/EFI/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    EFI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
        
          
          <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    OSs
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_2">
            <span class="md-nav__icon md-icon"></span>
            OSs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/macOS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    macOS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/macOS-intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    macOS-intro
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/macOS-softwares/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    macOS ËΩØ‰ª∂ÂàóË°®
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Hackintosh/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hackintosh
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Arch/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Arch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/CentOS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CentOS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Ubuntu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ubuntu
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Windows/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Windows
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    softwares
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            softwares
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/vscode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VSCode
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/vscode-markdown/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VSCode Markdown
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    NLP
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            NLP
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/Lexicon-%E8%AF%8D%E6%B1%87%E5%A2%9E%E5%BC%BA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lexicon ËØçÊ±áÂ¢ûÂº∫
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/Prompt-%E6%8F%90%E7%A4%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt ÊèêÁ§∫ËØ≠Ë®ÄÊ®°Âûã
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/ABSA-Sentiment-Analysis-%E5%9F%BA%E4%BA%8E%E6%96%B9%E9%9D%A2%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ABSA
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://blog.easonsi.site" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    üîóBlog
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    <span class="md-ellipsis">
      ÂºïÂÖ•
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regression-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Regression \&amp; Classification
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    <span class="md-ellipsis">
      ‰∏§ÁßçÁÆÄÂçïÁöÑÈ¢ÑÊµãÊñπÊ≥ï
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#statistical-decision-theory" class="md-nav__link">
    <span class="md-ellipsis">
      Statistical Decision Theory
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#curse-of-dimensionality" class="md-nav__link">
    <span class="md-ellipsis">
      Curse of Dimensionality
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    <span class="md-ellipsis">
      ÁªüËÆ°Ê®°ÂûãÔºåÁõëÁù£Â≠¶‰π†ÂíåÂáΩÊï∞ÈÄºËøë
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#restricted-estimators" class="md-nav__link">
    <span class="md-ellipsis">
      ÈôêÂà∂ÊÄß‰º∞ËÆ° Restricted Estimators
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-selection-and-the-biasvariance-tradeoff" class="md-nav__link">
    <span class="md-ellipsis">
      Model Selection and the Bias‚ÄìVariance Tradeoff
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-methods-for-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Linear Methods for Regression
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Linear Methods for Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction-of-linear-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Introduction of Linear Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recap-of-linear-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Recap of Linear Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#subset-selection" class="md-nav__link">
    <span class="md-ellipsis">
      Subset Selection
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shrinkage-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Shrinkage methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods-using-derived-input-directions" class="md-nav__link">
    <span class="md-ellipsis">
      Methods Using Derived Input Directions
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#some-algebra" class="md-nav__link">
    <span class="md-ellipsis">
      Some Algebra
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Some Algebra">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#eigen-decomposition-square-matrix" class="md-nav__link">
    <span class="md-ellipsis">
      Eigen decomposition (square matrix)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singular-value-decomposition-any-matrix-m-times-nm-times-n" class="md-nav__link">
    <span class="md-ellipsis">
      Singular value decomposition (any matrix m \times nm \times n )
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pca-principle-component-analysis" class="md-nav__link">
    <span class="md-ellipsis">
      PCA (Principle Component Analysis)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-methods-for-classification" class="md-nav__link">
    <span class="md-ellipsis">
      Linear Methods for Classification
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Linear Methods for Classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#linear-regression-of-an-indicator-matrix" class="md-nav__link">
    <span class="md-ellipsis">
      Linear Regression of an Indicator Matrix
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linear-discriminant-analysis-lda" class="md-nav__link">
    <span class="md-ellipsis">
      Linear Discriminant Analysis (LDA)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lda-practicalities" class="md-nav__link">
    <span class="md-ellipsis">
      LDA Practicalities
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logistic-regression" class="md-nav__link">
    <span class="md-ellipsis">
      Logistic Regression
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#naive-bayes" class="md-nav__link">
    <span class="md-ellipsis">
      Naive Bayes
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#seprating-hyperplanes" class="md-nav__link">
    <span class="md-ellipsis">
      Seprating Hyperplanes Áï•
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


<h1 id="asl-note">ASL note<a class="headerlink" href="#asl-note" title="Permanent link">&para;</a></h1>
<p>ÂèÇËßÅ</p>
<ul>
<li>ESL, <a href="https://esl.hohoweiya.xyz/">‰∏≠ÊñáÁâà</a> ‚≠êÔ∏è</li>
<li><a href="https://jozeelin.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">Jozee&rsquo;sÊäÄÊúØÂçöÂÆ¢|Êú∫Âô®Â≠¶‰π†</a> ‚≠êÔ∏è</li>
</ul>
<p>ÂÖ∂‰ªñ</p>
<ul>
<li><a href="https://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/">Êú∫Âô®Â≠¶‰π†ÔºàÂÖ≠ÔºâË¥ùÂè∂ÊñØÂàÜÁ±ªÂô®</a>ÔºõBlog <a href="https://baidinghub.github.io/">https://baidinghub.github.io/</a> ÂæàËµû</li>
<li><a href="https://weirping.github.io/blog/Bayesian-Probabilities-in-ML.html">Âè∂ÊñØÁ∫øÊÄßÂõûÂΩí‰∏éË¥ùÂè∂ÊñØÈÄªËæëÂõûÂΩí</a></li>
<li>PRML Á¨îËÆ∞ <a href="http://www.datakit.cn/category/#PRML">http://www.datakit.cn/category/#PRML</a></li>
<li><a href="https://keson96.github.io/2017/04/07/2017-04-07-Bayesian-Model-Comparison/">Ë¥ùÂè∂ÊñØÊ®°ÂûãÊØîËæÉ(Bayesian Model Comparison)</a></li>
</ul>
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h2>
<p>Outline</p>
<ul>
<li>Variable Types and Terminology</li>
<li>Two Simple Approaches to Prediction<ul>
<li>Least Squares</li>
<li>Nearest-Neighbour Methods</li>
<li>Comparison</li>
</ul>
</li>
<li>Statistical Decision Theory</li>
<li>Curse of Dimensionality</li>
<li>Statistical Models, Supervised Learning \&amp; Function Approximation</li>
<li>Class of Restricted Estimators</li>
<li>Bias Variance Tradeoff</li>
</ul>
<h3 id="_1">ÂºïÂÖ•<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h3>
<p>Cases:</p>
<ul>
<li>Email spam</li>
<li>Prostate Cancer (continuous)</li>
<li>Handwritten Digit Recognition</li>
<li>DNA Expression Microarrays (cluster)</li>
</ul>
<h3 id="regression-classification">Regression \&amp; Classification<a class="headerlink" href="#regression-classification" title="Permanent link">&para;</a></h3>
<p>Inputs - predictors, independent variables, features&hellip;
Outputs - outcome, response, dependent variable, label&hellip;</p>
<ul>
<li>Regression problems - outputs being quantitative measurement<ul>
<li>Measurements close in value are also close in nature</li>
<li>Prostate cancer example</li>
</ul>
</li>
<li>Classification problems - outputs being qualitative<ul>
<li>No explicit ordering in the outputs</li>
<li>Email spam, Handwritten digits recognition</li>
</ul>
</li>
<li>Special case - ordered categorical outputs<ul>
<li>e.g., {small, medium, large}</li>
<li>difference between medium and small need not be the same as that between large and medium</li>
</ul>
</li>
</ul>
<h3 id="_2">‰∏§ÁßçÁÆÄÂçïÁöÑÈ¢ÑÊµãÊñπÊ≥ï<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h3>
<h4 id="least-squares">Least Squares<a class="headerlink" href="#least-squares" title="Permanent link">&para;</a></h4>
<p>Given a vector of inputs <span class="arithmatex"><span class="MathJax_Preview">X^{\top}=\left(X_{1}, X_{2}, \ldots, X_{P}\right)</span><script type="math/tex">X^{\top}=\left(X_{1}, X_{2}, \ldots, X_{P}\right)</script></span>, predict the output <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> (scalar) by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{Y}=X^{\top} \hat{\beta}
</div>
<script type="math/tex; mode=display">
\hat{Y}=X^{\top} \hat{\beta}
</script>
</div>
<p>Note: in this course, we will always assume <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> includes a constant variable <span class="arithmatex"><span class="MathJax_Preview">1, \beta</span><script type="math/tex">1, \beta</script></span> include the intercept <span class="arithmatex"><span class="MathJax_Preview">\beta_{0}</span><script type="math/tex">\beta_{0}</script></span></p>
<p>Question: How do we fit the linear model to a set of training data <span class="arithmatex"><span class="MathJax_Preview">\left\{\left(X_{1}, y_{1}\right),\left(X_{2}, y_{2}\right), \ldots,\left(X_{N}, y_{N}\right)\right\} ?</span><script type="math/tex">\left\{\left(X_{1}, y_{1}\right),\left(X_{2}, y_{2}\right), \ldots,\left(X_{N}, y_{N}\right)\right\} ?</script></span></p>
<p>Answer: Many different methods, most popular - least squares</p>
<ul>
<li>Pick <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> that minimize the residual sum of squares (RSS)</li>
<li><span class="arithmatex"><span class="MathJax_Preview">R S S(\beta)=\sum_{i=1}^{N}\left(y_{i}-x_{i}^{\top} \beta\right)^{2}</span><script type="math/tex">R S S(\beta)=\sum_{i=1}^{N}\left(y_{i}-x_{i}^{\top} \beta\right)^{2}</script></span></li>
<li>In matrix notation <span class="arithmatex"><span class="MathJax_Preview">R S S(\beta)=(y-X \beta)^{\top}(y-X \beta)</span><script type="math/tex">R S S(\beta)=(y-X \beta)^{\top}(y-X \beta)</script></span></li>
<li>Quandratic function of <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>, so its minimum always exists (but may not be unique!)</li>
<li>Differentiating w.r.t. <span class="arithmatex"><span class="MathJax_Preview">\beta \rightarrow X^{\top}(y-X \beta)=0</span><script type="math/tex">\beta \rightarrow X^{\top}(y-X \beta)=0</script></span></li>
<li>If <span class="arithmatex"><span class="MathJax_Preview">X^{\top} X</span><script type="math/tex">X^{\top} X</script></span> is nonsingular, the unique solution <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}=\left(X^{\top} X\right)^{-1} X^{\top} y</span><script type="math/tex">\hat{\beta}=\left(X^{\top} X\right)^{-1} X^{\top} y</script></span></li>
</ul>
<p>Fitted value at <span class="arithmatex"><span class="MathJax_Preview">x_{i}: \hat{y}_{i}=x_{i}^{\top} \hat{\beta}</span><script type="math/tex">x_{i}: \hat{y}_{i}=x_{i}^{\top} \hat{\beta}</script></span></p>
<p>Predicted value at an arbituary input <span class="arithmatex"><span class="MathJax_Preview">x_{0}: \hat{y}_{0}=x_{0}^{\top} \hat{\beta}</span><script type="math/tex">x_{0}: \hat{y}_{0}=x_{0}^{\top} \hat{\beta}</script></span></p>
<h4 id="nearest-neighbour-methods">Nearest-Neighbour Methods<a class="headerlink" href="#nearest-neighbour-methods" title="Permanent link">&para;</a></h4>
<p>Given a vector of inputs <span class="arithmatex"><span class="MathJax_Preview">X^{\top}=\left(X_{1}, X_{2}, \ldots, X_{p}\right)</span><script type="math/tex">X^{\top}=\left(X_{1}, X_{2}, \ldots, X_{p}\right)</script></span>, predict the output <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> (scalar) by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{Y}(x)=\frac{1}{k} \sum_{x_{i} \in N_{k}(x)} y_{i}
</div>
<script type="math/tex; mode=display">
\hat{Y}(x)=\frac{1}{k} \sum_{x_{i} \in N_{k}(x)} y_{i}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">N_{k}(x)</span><script type="math/tex">N_{k}(x)</script></span> is the neighborhood of <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> defined by the <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> closest points <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> in the training sample</p>
<p>Question: How to define &ldquo;closest&rdquo;?</p>
<ul>
<li>Can be defined on different metrics.</li>
<li>For the moment, assume Euclidean distance</li>
</ul>
<h4 id="least-square-vs-knn">Least Square vs KNN (ÈÉΩÂèØ‰ª•ÂÆåÊàêÂàÜÁ±ªÂíåÂõûÂΩí‰ªªÂä°)<a class="headerlink" href="#least-square-vs-knn" title="Permanent link">&para;</a></h4>
<p>ÊØîËæÉÁ∫øÊÄßÊ®°ÂûãÂíåkNN</p>
<ul>
<li>Linear<ul>
<li>Smooth and stable decision boundry</li>
<li>Linear assumption</li>
<li>High bias and low variance</li>
</ul>
</li>
<li>kNN<ul>
<li>Wiggly and unstable decision boundry</li>
<li>Assumption free</li>
<li>Low bias and high variance (kNN ÁöÑÊúâÊïàÂèÇÊï∞Êï∞Èáè‰∏∫ <span class="arithmatex"><span class="MathJax_Preview">N/k</span><script type="math/tex">N/k</script></span>, ‰∏ÄËà¨Â§ß‰∫éÁ∫øÊÄßÊ®°ÂûãÁöÑÂèÇÊï∞Èáè)</li>
</ul>
</li>
</ul>
<p><img alt="" src="../media/ASL-note1/2022-01-06-15-19-40.png" /></p>
<p>ËøôÈáåÂ±ïÁ§∫‰∫ÜÂú®‰∏Ä‰∏™Êï∞ÊçÆÈõÜ‰∏ä(Êõ¥ÈÄÇÂêà kNN)ÁöÑË°®Áé∞, Á¥´Ëâ≤‰∏∫ Bayes ÈîôËØØÁéá, ‰∏§‰∏™ÊñπÂùóÊòØÁ∫øÊÄßÊ®°ÂûãÁöÑË°®Áé∞, ËÄå‰∏§Êù°ÊäòÁ∫øÂàôÊòØ kNN Âú®ÈÄâÊã©‰∏çÂêåÂèÇÊï∞Êó∂ÁöÑË°®Áé∞.</p>
<h4 id="variants-of-l-sl-s-and-knn">Variants of <span class="arithmatex"><span class="MathJax_Preview">L S</span><script type="math/tex">L S</script></span> and KNN<a class="headerlink" href="#variants-of-l-sl-s-and-knn" title="Permanent link">&para;</a></h4>
<ul>
<li>Kernal methods<ul>
<li>Weights decrease smoothly as distance further from the target point</li>
<li>In high-dimensional space, distance kernels are modified to emphasize on some variables more than others</li>
</ul>
</li>
<li>Local regression<ul>
<li>Fit linear models by locally weighted least squares</li>
</ul>
</li>
<li>Basis expansion<ul>
<li>Allow more complex inputs</li>
</ul>
</li>
</ul>
<h3 id="statistical-decision-theory">Statistical Decision Theory<a class="headerlink" href="#statistical-decision-theory" title="Permanent link">&para;</a></h3>
<h4 id="loss-function">Loss Function<a class="headerlink" href="#loss-function" title="Permanent link">&para;</a></h4>
<ul>
<li>Squared loss<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathcal{L}(Y, f(x))=(Y-f(x))^{2}</span><script type="math/tex">\mathcal{L}(Y, f(x))=(Y-f(x))^{2}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">f(x)=\operatorname{argmin}_{c}\left(E_{Y \mid X}(Y-c)^{2} \mid X=x\right)=E(Y \mid X=x)</span><script type="math/tex">f(x)=\operatorname{argmin}_{c}\left(E_{Y \mid X}(Y-c)^{2} \mid X=x\right)=E(Y \mid X=x)</script></span></li>
</ul>
</li>
<li>L1 loss<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathcal{L}(Y, f(x))=|Y-f(x)|</span><script type="math/tex">\mathcal{L}(Y, f(x))=|Y-f(x)|</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\rightarrow f(x)=\operatorname{median}(Y \mid X=x)</span><script type="math/tex">\rightarrow f(x)=\operatorname{median}(Y \mid X=x)</script></span></li>
</ul>
</li>
<li>0-1 loss<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\rightarrow \mathcal{L}(G, G(x))=\sum_{k} \mathcal{L}\left(G_{k}-\hat{G}(x)\right) \operatorname{Pr}\left(G_{k} \mid X\right)</span><script type="math/tex">\rightarrow \mathcal{L}(G, G(x))=\sum_{k} \mathcal{L}\left(G_{k}-\hat{G}(x)\right) \operatorname{Pr}\left(G_{k} \mid X\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{G}(x)=\operatorname{argmax}_{G \in \mathcal{G}} \operatorname{Pr}(G \mid X=x)</span><script type="math/tex">\hat{G}(x)=\operatorname{argmax}_{G \in \mathcal{G}} \operatorname{Pr}(G \mid X=x)</script></span></li>
</ul>
</li>
</ul>
<h4 id="squared-loss">Squared Loss<a class="headerlink" href="#squared-loss" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Expected prediction error</strong><ul>
<li><span class="arithmatex"><span class="MathJax_Preview">E P E(f)=E(Y-f(x))^{2}=\int(y-f(x))^{2} \operatorname{Pr}(d x, d y)</span><script type="math/tex">E P E(f)=E(Y-f(x))^{2}=\int(y-f(x))^{2} \operatorname{Pr}(d x, d y)</script></span></li>
<li>Ê≥®ÊÑèËøôÈáåÊòØÂØπ‰∫é X Âíå Y ÈÉΩÂèñÊúüÊúõ</li>
</ul>
</li>
<li>By conditioning on <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>, we can write EPE as <span class="arithmatex"><span class="MathJax_Preview">E P E(f)=E_{X} E_{Y \mid X}\left[(y-f(x))^{2} \mid x\right]</span><script type="math/tex">E P E(f)=E_{X} E_{Y \mid X}\left[(y-f(x))^{2} \mid x\right]</script></span></li>
<li>It suffices to minimize <span class="arithmatex"><span class="MathJax_Preview">E P E</span><script type="math/tex">E P E</script></span> pointwise<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">f(x)=\operatorname{argmin}_{c} E_{Y \mid X}\left[(Y-c)^{2} \mid X=x\right]</span><script type="math/tex">f(x)=\operatorname{argmin}_{c} E_{Y \mid X}\left[(Y-c)^{2} \mid X=x\right]</script></span></li>
<li>Êàë‰ª¨Âè™ÈúÄË¶ÅÈ¢ÑÊµãÂáΩÊï∞ <strong>ÈÄêÁÇπÊúÄÂ∞è</strong> Âç≥ÂèØ</li>
</ul>
</li>
<li>The solution is Êù°‰ª∂ÊúüÊúõ<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">f(x)=E(Y \mid X=x)</span><script type="math/tex">f(x)=E(Y \mid X=x)</script></span></li>
<li>‰πüË¢´Áß∞‰Ωú ÂõûÂΩí (regression) ÂáΩÊï∞</li>
</ul>
</li>
</ul>
<p>For least squares</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
f(x) &amp;=E(Y \mid X=x)=X \beta \\
f(x) &amp;=X \hat{\beta}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
f(x) &=E(Y \mid X=x)=X \beta \\
f(x) &=X \hat{\beta}
\end{aligned}
</script>
</div>
<p>For k-nearest-neighbour</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}(x)=\operatorname{Ave}\left(y_{i} \mid x_{i} \in N_{k}(x)\right)
</div>
<script type="math/tex; mode=display">
\hat{f}(x)=\operatorname{Ave}\left(y_{i} \mid x_{i} \in N_{k}(x)\right)
</script>
</div>
<ul>
<li>Expectation is approximated by averaging over sample data</li>
<li>Conditioning at a point is relaxed to conditioning on some region &ldquo;close&rdquo; to the target point</li>
<li>For large training set and large <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>, approximation works better (‰ΩÜÊ≠£Â¶Ç‰∏ã‰∏ÄËäÇÊâÄËØ¥, ‰ºöÊúâÁª¥Â∫¶ÁÅæÈöæÁöÑÈóÆÈ¢ò)</li>
</ul>
<h4 id="0-1-loss">0-1 LOSS<a class="headerlink" href="#0-1-loss" title="Permanent link">&para;</a></h4>
<ul>
<li>Expected prediction error for categorical output <span class="arithmatex"><span class="MathJax_Preview">G</span><script type="math/tex">G</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">E P E=E[L(G, \hat{G}(x))]</span><script type="math/tex">E P E=E[L(G, \hat{G}(x))]</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">L(k, l)</span><script type="math/tex">L(k, l)</script></span> denotes the price paid for classifying an observation belonging to <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> as <span class="arithmatex"><span class="MathJax_Preview">l</span><script type="math/tex">l</script></span>.</li>
<li>By conditioning on <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>, we can write EPE as</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
E P E=E_{x} \sum_{k=1}^{k} L\left[G_{k}, \hat{G}(x)\right] \operatorname{Pr}\left(G_{k} \mid X=x\right)
</div>
<script type="math/tex; mode=display">
E P E=E_{x} \sum_{k=1}^{k} L\left[G_{k}, \hat{G}(x)\right] \operatorname{Pr}\left(G_{k} \mid X=x\right)
</script>
</div>
<ul>
<li>ÂêåÊ†∑‰πüÊòØÈÄêÁÇπÊúÄÂ∞èÂåñ, So <span class="arithmatex"><span class="MathJax_Preview">\hat{G}(x)=\operatorname{argmin}_{G \in \mathcal{G}} \sum_{k=1}^{k} L\left(G_{k}, g\right) \operatorname{Pr}\left(G_{k} \mid X=x\right)</span><script type="math/tex">\hat{G}(x)=\operatorname{argmin}_{G \in \mathcal{G}} \sum_{k=1}^{k} L\left(G_{k}, g\right) \operatorname{Pr}\left(G_{k} \mid X=x\right)</script></span></li>
<li>For 0-1 loss, <span class="arithmatex"><span class="MathJax_Preview">\hat{G}(X)=G_{k}</span><script type="math/tex">\hat{G}(X)=G_{k}</script></span> if <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}\left(G_{k} \mid X=x\right)=\max _{g} \operatorname{Pr}(g \mid X=x)</span><script type="math/tex">\operatorname{Pr}\left(G_{k} \mid X=x\right)=\max _{g} \operatorname{Pr}(g \mid X=x)</script></span></li>
<li>In words, classify to the most probable class.</li>
</ul>
<p>ÂΩìÊù°‰ª∂ÂàÜÂ∏ÉÂ∑≤Áü•ÁöÑÊÉÖÂÜµ‰∏ã, ÁêÜÊÉ≥ÊÉÖÂÜµÂ∞±ÊòØÊåâÁÖßÂêéÈ™åÊ¶ÇÁéáËøõË°å‰º∞ËÆ°, Áß∞‰∏∫ <strong>Ë¥ùÂè∂ÊñØÂàÜÁ±ªÂô®</strong> (Bayes classifier); Ë¥ùÂè∂ÊñØÂàÜÁ±ªÁöÑËØØÂ∑ÆÁéáË¢´Áß∞‰Ωú Ë¥ùÂè∂ÊñØÁéá (Bayes rate).</p>
<p>For least squares (indicator variable regression)</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">G(x)</span><script type="math/tex">G(x)</script></span> is approximated by <span class="arithmatex"><span class="MathJax_Preview">X \hat{\beta}</span><script type="math/tex">X \hat{\beta}</script></span></li>
<li>in practice problems can occur</li>
</ul>
<p>For k-nearest-neighbour</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{G}(x)</span><script type="math/tex">\hat{G}(x)</script></span> - majority vote in a neighbourhood</li>
<li>Conditional probability at a point is relaxed to conditional probability within a neighborhood of a point</li>
<li>Probabilities are estimated by training-sample proportions.</li>
</ul>
<h3 id="curse-of-dimensionality">Curse of Dimensionality<a class="headerlink" href="#curse-of-dimensionality" title="Permanent link">&para;</a></h3>
<p>ÂØπÁª¥Êï∞ÁÅæÈöæÁöÑÁõ¥ËßÇÁêÜËß£</p>
<ul>
<li>Êï∞ÊçÆÂàÜÂ∏ÉÁ®ÄÁñè: ËÄÉËôëÂú® p Áª¥ÂÆΩÂ∫¶‰∏∫ 1 Á´ãÊñπ‰Ωì‰∏≠ÂùáÂåÄÂàÜÂ∏ÉÁöÑÊÉÖÂÜµ, Â¶ÇÊûúÂú®‰∏Ä‰∏™Â∞èÁöÑÁ´ãÊñπ‰Ωì‰∏≠ÂåÖÊã¨ 10% ÁöÑÊï∞ÊçÆ, ÂΩì p=10 Êó∂ÈúÄË¶ÅÁöÑÁ´ãÊñπ‰ΩìÂ§ßÂ∞è‰∏∫ 0.8 (ÂÖ∂ÂÆûÂ∞±ÊòØ, Ë¶ÅË¶ÜÁõñ p Áª¥ÁöÑÊï∞ÊçÆÁÇπË¶ÅÂæàÂ§ß)</li>
<li>Êï∞ÊçÆÂæàÂ§ö‰ºöÂá∫Áé∞Âú®ËæπÁïåÈôÑËøë: ‰æãÂ≠êÊòØÂú®‰∏Ä‰∏™ p Áª¥ÁêÉ‰Ωì‰∏≠, ÂêÑÊï∞ÊçÆÁÇπË∑ùÁ¶ªÁêÉÂøÉÁöÑÂπ≥ÂùáË∑ùÁ¶ªÈöèÁùÄÁª¥Â∫¶ÁöÑÂ¢ûÂä†ËÄåÂ¢ûÂä†, ‰πüÂç≥Â§öËÄÉËôëËæπÁïå; ËøôÊ†∑, Â∞±‰∏çËÉΩÈááÁî®ÂÜÖÂ∑ÆËÄåÈúÄË¶ÅÂ§ñÊé®. (‰πüÂèØÁêÜËß£‰∏∫Êï∞ÊçÆÁ®ÄÁñè?)</li>
</ul>
<h3 id="_3">ÁªüËÆ°Ê®°ÂûãÔºåÁõëÁù£Â≠¶‰π†ÂíåÂáΩÊï∞ÈÄºËøë<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<p>Êàë‰ª¨ÁöÑÁõÆÊ†áÊòØÂØªÊâæÂáΩÊï∞ <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> ÁöÑ‰∏Ä‰∏™ÊúâÁî®ÁöÑËøë‰ºº <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(x)</span><script type="math/tex">\hat{f}(x)</script></span>, ÂáΩÊï∞ <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> Ëï¥Âê´ÁùÄËæìÂÖ•‰∏éËæìÂá∫‰πãÈó¥ÁöÑÈ¢ÑÊµãÂÖ≥Á≥ª. Âú®ÂâçÈù¢ÁªüËÆ°Âà§Âà´ÁêÜËÆ∫ÁöÑÁ´†ËäÇÁöÑÁêÜËÆ∫ÂáÜÂ§á‰∏≠, ÂØπ‰∫éÂÆöÈáèÁöÑÂìçÂ∫î, Êàë‰ª¨ÁúãÂà∞Âπ≥ÊñπËØØÂ∑ÆÊçüÂ§±ÂºïÂØºÊàë‰ª¨ÂæóÂà∞‰∫ÜÂõûÂΩíÂáΩÊï∞ <span class="arithmatex"><span class="MathJax_Preview">f(X)=\mathrm{E}(Y \mid X=x)</span><script type="math/tex">f(X)=\mathrm{E}(Y \mid X=x)</script></span>. ÊúÄËøëÈÇªÊñπÊ≥ïÂèØ‰ª•ÁúãÊàêÊòØÂØπÊù°‰ª∂ÊúüÊúõÁöÑÁõ¥Êé•‰º∞ËÆ°, ‰ΩÜÊòØÊàë‰ª¨ÂèØ‰ª•ÁúãÂà∞Ëá≥Â∞ëÂú®‰∏§‰∏™ÊñπÈù¢ÂÆÉ‰ª¨‰∏çËµ∑‰ΩúÁî®</p>
<ul>
<li>Â¶ÇÊûúËæìÂÖ•Á©∫Èó¥ÁöÑÁª¥Êï∞È´ò, ÂàôÊúÄËøëÈÇª‰∏çÂøÖÁ¶ªÁõÆÊ†áÁÇπËøë, ËÄå‰∏îÂèØËÉΩÂØºËá¥Â§ßÁöÑËØØÂ∑Æ</li>
<li>Â¶ÇÊûúÁü•ÈÅìÂ≠òÂú®ÁâπÊÆäÁöÑÁªìÊûÑ, ÂèØ‰ª•Áî®Êù•Èôç‰Ωé‰º∞ËÆ°ÁöÑÂÅèÂ∑Æ‰∏éÊñπÂ∑Æ.</li>
</ul>
<p>Êàë‰ª¨È¢ÑÂÖàÁî®‰∫ÜÂÖ≥‰∫é <span class="arithmatex"><span class="MathJax_Preview">f(X)</span><script type="math/tex">f(X)</script></span> ÁöÑÂÖ∂ÂÆÉÁ±ªÂà´ÁöÑÊ®°Âûã, Âú®ÂæàÂ§öÊÉÖÂΩ¢‰∏ãÊòØ‰∏∫‰∫ÜËß£ÂÜ≥Áª¥Êï∞ÈóÆÈ¢òËÄåÁâπÂà´ËÆæËÆ°ÁöÑ, Áé∞Âú®Êàë‰ª¨ËÆ®ËÆ∫ÊääÂÆÉ‰ª¨ÂêàÂπ∂Ëøõ‰∏Ä‰∏™È¢ÑÊµãÈóÆÈ¢òÁöÑÊ°ÜÊû∂.</p>
<h4 id="statistical-models">Statistical Models<a class="headerlink" href="#statistical-models" title="Permanent link">&para;</a></h4>
<ul>
<li>Goal<ul>
<li>Predict the relationship between inputs and outputs with <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span></li>
<li>Approximate <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> using <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(x)</span><script type="math/tex">\hat{f}(x)</script></span></li>
</ul>
</li>
<li>Quantitative output<ul>
<li>Squared error loss <span class="arithmatex"><span class="MathJax_Preview">\rightarrow f(x)=E(Y \mid X=x)</span><script type="math/tex">\rightarrow f(x)=E(Y \mid X=x)</script></span></li>
<li>Choice for <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(x)</span><script type="math/tex">\hat{f}(x)</script></span> : additive error model, nearest neighbour, &hellip;</li>
</ul>
</li>
<li>Qualitative output<ul>
<li>Additive error models typically not used (additive ËØØÂ∑ÆÊ®°Âûã: Âú®ÂÆûÈôÖÁöÑ f ‰∏≠Êúâ‰∏Ä‰∏™ËØØÂ∑ÆÈ°π) Âú®ÂàÜÁ±ªÈóÆÈ¢ò‰∏≠‰∏ÄËà¨‰∏çÂÅöËøô‰∏™ÂÅáËÆæ, Âõ†Ê≠§Áõ¥Êé•ÂØπ‰∫éÊù°‰ª∂Ê¶ÇÁéáÂª∫Ê®°</li>
<li>Directly model <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(G \mid X)</span><script type="math/tex">\operatorname{Pr}(G \mid X)</script></span> : logistic regression, probit regression, <span class="arithmatex"><span class="MathJax_Preview">\ldots</span><script type="math/tex">\ldots</script></span></li>
</ul>
</li>
</ul>
<h4 id="function-approximation">Function Approximation<a class="headerlink" href="#function-approximation" title="Permanent link">&para;</a></h4>
<p>‰∏Ä‰∫õÂü∫Êú¨ÁöÑÊ®°Âûã</p>
<p>Common approximations ( <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> denotes parameters)</p>
<ul>
<li>Linear model <span class="arithmatex"><span class="MathJax_Preview">f(x)=X^{\top} \beta</span><script type="math/tex">f(x)=X^{\top} \beta</script></span></li>
<li>Basis expansion <span class="arithmatex"><span class="MathJax_Preview">f_{\theta}(x)=\sum_{k=1}^{K} h(x) \theta_{k}</span><script type="math/tex">f_{\theta}(x)=\sum_{k=1}^{K} h(x) \theta_{k}</script></span><ul>
<li><span class="arithmatex"><span class="MathJax_Preview">h(x)</span><script type="math/tex">h(x)</script></span> are transformations of <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span></li>
<li>Polynomial and trigonometric expansions <span class="arithmatex"><span class="MathJax_Preview">x_{1}^{2}, x_{1} x_{2}^{2}, \cos \left(x_{1}\right)</span><script type="math/tex">x_{1}^{2}, x_{1} x_{2}^{2}, \cos \left(x_{1}\right)</script></span></li>
<li>Sigmoid transformation <span class="arithmatex"><span class="MathJax_Preview">\frac{1}{1+\exp \left(-x^{\top} \beta_{k}\right)}</span><script type="math/tex">\frac{1}{1+\exp \left(-x^{\top} \beta_{k}\right)}</script></span></li>
</ul>
</li>
</ul>
<p>Parameter estimate</p>
<ul>
<li>Least squares<ul>
<li>Closed form for linear model</li>
<li>May require iterative methods or numerical optimization</li>
</ul>
</li>
<li>Maximum likelihood estimation - more general<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathcal{L}(\theta)=\sum_{i=1}^{N} \log P r_{\theta}\left(y_{i}\right)</span><script type="math/tex">\mathcal{L}(\theta)=\sum_{i=1}^{N} \log P r_{\theta}\left(y_{i}\right)</script></span> <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}=\operatorname{argmax}_{\theta} \mathcal{L}(\theta)</span><script type="math/tex">\hat{\theta}=\operatorname{argmax}_{\theta} \mathcal{L}(\theta)</script></span></li>
</ul>
</li>
</ul>
<h5 id="mle-normal-distribution">‰æãÂ≠ê: MLE - Normal Distribution<a class="headerlink" href="#mle-normal-distribution" title="Permanent link">&para;</a></h5>
<p>Normal distribution</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f\left(x \mid \mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
</div>
<script type="math/tex; mode=display">
f\left(x \mid \mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
</script>
</div>
<p>Normal likelihood</p>
<ul>
<li>Assume <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(Y \mid X, \beta)=\mathcal{N}\left(X \beta, \sigma^{2}\right)</span><script type="math/tex">\operatorname{Pr}(Y \mid X, \beta)=\mathcal{N}\left(X \beta, \sigma^{2}\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">I(\beta)=-\frac{N}{2} \log (2 \pi)-N \log (\sigma)-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{N}\left(y_{i}-X_{i} \beta\right)^{2}</span><script type="math/tex">I(\beta)=-\frac{N}{2} \log (2 \pi)-N \log (\sigma)-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{N}\left(y_{i}-X_{i} \beta\right)^{2}</script></span></li>
<li>Maximizing <span class="arithmatex"><span class="MathJax_Preview">I(\theta)</span><script type="math/tex">I(\theta)</script></span> only involves the last term <span class="arithmatex"><span class="MathJax_Preview">-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{N}\left(y_{i}-f_{\theta}\left(x_{i}\right)\right)^{2}</span><script type="math/tex">-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{N}\left(y_{i}-f_{\theta}\left(x_{i}\right)\right)^{2}</script></span>, which is equivalent to minimizing RSS</li>
</ul>
<p>ÁªìËÆ∫: Âú®Ê≠£ÊÄÅÂàÜÂ∏ÉÊù°‰ª∂‰∏ã, MLE Á≠â‰ª∑‰∫é ÊúÄÂ∞èÂåñRSS.</p>
<h5 id="mle-multinomial-distribution">‰æãÂ≠ê: MLE - Multinomial Distribution<a class="headerlink" href="#mle-multinomial-distribution" title="Permanent link">&para;</a></h5>
<p>Multinomial distribution</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(X_{1}=x_{1}, \ldots, X_{m}=x_{m}\right)=\frac{n !}{\prod_{i=1}^{m} x_{i} !} \prod_{i=1}^{m} \theta_{i}^{x_{i}}
</div>
<script type="math/tex; mode=display">
P\left(X_{1}=x_{1}, \ldots, X_{m}=x_{m}\right)=\frac{n !}{\prod_{i=1}^{m} x_{i} !} \prod_{i=1}^{m} \theta_{i}^{x_{i}}
</script>
</div>
<p>Multinomial likelihood</p>
<ul>
<li>Assume <span class="arithmatex"><span class="MathJax_Preview">X_{1}, \ldots, X_{m}</span><script type="math/tex">X_{1}, \ldots, X_{m}</script></span> are counts in cells up to <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>, each cell has a different probability <span class="arithmatex"><span class="MathJax_Preview">\theta_{1}, \ldots, \theta_{m}</span><script type="math/tex">\theta_{1}, \ldots, \theta_{m}</script></span></li>
<li>constraints: <span class="arithmatex"><span class="MathJax_Preview">\sum_{i=1}^{m} X_{i}=n, \sum_{i=1}^{m} \theta_{i}=1</span><script type="math/tex">\sum_{i=1}^{m} X_{i}=n, \sum_{i=1}^{m} \theta_{i}=1</script></span></li>
<li>The joint probability of <span class="arithmatex"><span class="MathJax_Preview">\left(X_{1}, \ldots, X_{m}\right)</span><script type="math/tex">\left(X_{1}, \ldots, X_{m}\right)</script></span> is multinomial</li>
<li><span class="arithmatex"><span class="MathJax_Preview">I\left(\theta_{1}, \ldots, \theta_{m}\right)=\log n !-\sum_{i=1}^{m} \log x_{i}+\sum_{i=1}^{m} x_{i} \log \theta_{i}</span><script type="math/tex">I\left(\theta_{1}, \ldots, \theta_{m}\right)=\log n !-\sum_{i=1}^{m} \log x_{i}+\sum_{i=1}^{m} x_{i} \log \theta_{i}</script></span></li>
<li>Introduce Lagrange multipliers, <span class="arithmatex"><span class="MathJax_Preview">I\left(\theta_{1}, \ldots, \theta_{m}\right)=\log n !-\sum_{i=1}^{m} \log x_{i}+\sum_{i=1}^{m} x_{i} \log \theta_{i}+\lambda\left(1-\sum_{i=1}^{m} \theta_{i}\right)</span><script type="math/tex">I\left(\theta_{1}, \ldots, \theta_{m}\right)=\log n !-\sum_{i=1}^{m} \log x_{i}+\sum_{i=1}^{m} x_{i} \log \theta_{i}+\lambda\left(1-\sum_{i=1}^{m} \theta_{i}\right)</script></span></li>
<li>solution <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}_{i}=\frac{x_{i}}{n}</span><script type="math/tex">\hat{\theta}_{i}=\frac{x_{i}}{n}</script></span>, that is, the sample proportion</li>
</ul>
<h3 id="restricted-estimators">ÈôêÂà∂ÊÄß‰º∞ËÆ° Restricted Estimators<a class="headerlink" href="#restricted-estimators" title="Permanent link">&para;</a></h3>
<p>‰ªãÁªç‰∫Ü‰∏âÁ±ªÊñπÊ≥ï</p>
<h4 id="roughness-penalty-and-bayesian-methods">Roughness Penalty and Bayesian Methods<a class="headerlink" href="#roughness-penalty-and-bayesian-methods" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
P R S S(f ; \lambda)=R S S(f)+\lambda J(f)
</div>
<script type="math/tex; mode=display">
P R S S(f ; \lambda)=R S S(f)+\lambda J(f)
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathrm{J}(\mathrm{f})</span><script type="math/tex">\mathrm{J}(\mathrm{f})</script></span> will be large for functions <span class="arithmatex"><span class="MathJax_Preview">\mathrm{f}</span><script type="math/tex">\mathrm{f}</script></span> that vary too rapidly over small regions of input space.</li>
<li>smoothing parameter <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> controls the amount of penalty</li>
<li>e.g., <span class="arithmatex"><span class="MathJax_Preview">P R S S(f ; \lambda)=\sum_{i=1}^{N}\left(y_{i}-f\left(x_{i}\right)\right)^{2}+\lambda \int\left[f(x)^{\prime \prime}\right]^{2}</span><script type="math/tex">P R S S(f ; \lambda)=\sum_{i=1}^{N}\left(y_{i}-f\left(x_{i}\right)\right)^{2}+\lambda \int\left[f(x)^{\prime \prime}\right]^{2}</script></span><ul>
<li>Solution - cubic smoothing spline ‰∏äÈù¢ÊÉ©ÁΩöÈ°πÁöÑËß£ÊòØ <strong>‰∏âÊ¨°ÂÖâÊªëÊ†∑Êù°</strong></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\lambda=0</span><script type="math/tex">\lambda=0</script></span> Êó∂, Ê≤°ÊúâÊÉ©ÁΩö, ÂèØ‰ª•‰ΩøÁî®‰ªªÊÑèÊèíÂÄºÂáΩÊï∞; <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> ‰∏∫Êó†Á©∑Êó∂, ‰ªÖ‰ªÖÂÖÅËÆ∏ x ÁöÑÁ∫øÊÄßÂáΩÊï∞.</li>
</ul>
</li>
</ul>
<p><span class="arithmatex"><span class="MathJax_Preview">J(f)</span><script type="math/tex">J(f)</script></span> reflects our <strong>prior belief</strong> that <span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> exhibit a certain type of smooth behavior</p>
<ul>
<li>Bayesian analogy Ê≠£ÂàôÈ°πË°®Ëææ‰∫ÜÊàë‰ª¨ÁöÑ prior belief, Á°ÆÂÆûÂèØ‰ª•Â•óËøõË¥ùÂè∂ÊñØÁöÑÊ®°Âûã‰∏≠</li>
<li>Penalty J - log-prior</li>
<li>PRSS <span class="arithmatex"><span class="MathJax_Preview">(f ; \lambda)</span><script type="math/tex">(f ; \lambda)</script></span> - log-posterior distribution</li>
<li>Minimizing <span class="arithmatex"><span class="MathJax_Preview">P R S S(f ; \lambda)</span><script type="math/tex">P R S S(f ; \lambda)</script></span> - finding the posterior mode.</li>
<li>Êàë‰ª¨Â∞ÜÂú® <code>Á¨¨ 5 Á´†</code> ‰∏≠ËÆ®ËÆ∫Á≤óÁ≥ôÊÉ©ÁΩöÊñπÊ≥ïÂπ∂Âú® <code>Á¨¨ 8 Á´†</code> ‰∏≠ËÆ®ËÆ∫Ë¥ùÂè∂ÊñØËåÉÂºè</li>
</ul>
<h4 id="kernel-methods-and-local-regression">Kernel Methods and Local Regression<a class="headerlink" href="#kernel-methods-and-local-regression" title="Permanent link">&para;</a></h4>
<p>Ëøô‰∫õÊñπÊ≥ïÂèØ‰ª•ËÆ§‰∏∫ÊòØÈÄöËøáÁ°ÆÂÆöÂ±ÄÈÉ®ÈÇªÂüüÁöÑÊú¨Ë¥®Êù•ÊòæÂºèÁªôÂá∫ÂõûÂΩíÂáΩÊï∞ÁöÑ‰º∞ËÆ°ÊàñÊù°‰ª∂ÊúüÊúõÔºåÂπ∂‰∏îÂ±û‰∫éÂ±ÄÈÉ®ÊãüÂêàÂæóÂæàÂ•ΩÁöÑËßÑÂàôÂáΩÊï∞Á±ªÔºé</p>
<p>The local neighborhood is specified by a kernel function <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, x\right)</span><script type="math/tex">K_{\lambda}\left(x_{0}, x\right)</script></span></p>
<ul>
<li>Assign weight <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, x\right)</span><script type="math/tex">K_{\lambda}\left(x_{0}, x\right)</script></span> to points <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> in a region around <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span></li>
<li>e.g., <strong>Gaussian kernal</strong> <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, x\right)=\frac{1}{\lambda} \exp \left[-\frac{\left\|x-x_{0}\right\|^{2}}{2 \lambda}\right]</span><script type="math/tex">K_{\lambda}\left(x_{0}, x\right)=\frac{1}{\lambda} \exp \left[-\frac{\left\|x-x_{0}\right\|^{2}}{2 \lambda}\right]</script></span> È´òÊñØÊ†∏</li>
<li>Weights die exponentialy with squared Euclidean distance from <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span>.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
R S S\left(f_{\theta}, x_{0}\right)=\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left(y_{i}-f_{\theta}\left(x_{i}\right)\right)^{2}
</div>
<script type="math/tex; mode=display">
R S S\left(f_{\theta}, x_{0}\right)=\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left(y_{i}-f_{\theta}\left(x_{i}\right)\right)^{2}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">f_{\hat{\theta}}\left(x_{0}\right)</span><script type="math/tex">f_{\hat{\theta}}\left(x_{0}\right)</script></span> is the local regression estimate of <span class="arithmatex"><span class="MathJax_Preview">f\left(x_{0}\right)</span><script type="math/tex">f\left(x_{0}\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}</span><script type="math/tex">\hat{\theta}</script></span> minimizes <span class="arithmatex"><span class="MathJax_Preview">R S S\left(f_{\theta}, x_{0}\right)</span><script type="math/tex">R S S\left(f_{\theta}, x_{0}\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">f_{\theta}(x)=\theta_{0} \rightarrow</span><script type="math/tex">f_{\theta}(x)=\theta_{0} \rightarrow</script></span> <strong>Nadaraya-Watson estimate</strong></li>
<li><span class="arithmatex"><span class="MathJax_Preview">f_{\theta}(x)=\theta_{0}+\theta_{1} x \rightarrow</span><script type="math/tex">f_{\theta}(x)=\theta_{0}+\theta_{1} x \rightarrow</script></span> local linear regression model. Â±ÄÈÉ®Á∫øÊÄßÂõûÂΩí</li>
</ul>
<p>ÊúÄËøëÈÇªÊñπÊ≥ïÂèØ‰ª•ÁúãÊàêÊòØÊüê‰∏™Êõ¥Âä†‰æùËµñÊï∞ÊçÆÁöÑÂ∫¶ÈáèÁöÑÊ†∏ÊñπÊ≥ï. Nearest Neighbour ÊâÄÂØπÂ∫îÁöÑÊ†∏‰∏∫ <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, x\right)=I\left(\left\|x-x_{0}\right\| \leqslant\left\|x_{k}-x_{0}\right\|\right)</span><script type="math/tex">K_{\lambda}\left(x_{0}, x\right)=I\left(\left\|x-x_{0}\right\| \leqslant\left\|x_{k}-x_{0}\right\|\right)</script></span></p>
<p>‰∏∫‰∫ÜÈÅøÂÖçÁª¥Êï∞ÁÅæÈöæÔºåËøô‰∫õÊñπÊ≥ïÂú®È´òÁª¥ÊÉÖÂΩ¢‰∏ãË¶ÅÂÅö‰øÆÊ≠£ÔºéÂ∞ÜÂú® <code>Á¨¨ 6 Á´†</code> ËÆ®ËÆ∫‰∏çÂêåÁöÑÊîπÁºñÔºé</p>
<h4 id="basis-functions-and-dictionary-methods">Basis Functions and Dictionary Methods<a class="headerlink" href="#basis-functions-and-dictionary-methods" title="Permanent link">&para;</a></h4>
<p>Linear basis expansion <span class="arithmatex"><span class="MathJax_Preview">f_{\theta}(x)=\sum_{m=1}^{M}\left(\theta_{m} h_{m}(x)\right)</span><script type="math/tex">f_{\theta}(x)=\sum_{m=1}^{M}\left(\theta_{m} h_{m}(x)\right)</script></span></p>
<ul>
<li>Piecewise polynomial of degree <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> at <span class="arithmatex"><span class="MathJax_Preview">M-K</span><script type="math/tex">M-K</script></span> knots (ËßÅ 5.2 ËäÇ)</li>
<li>ÂæÑÂêëÂü∫ÂáΩÊï∞ <strong>Radial basis functions</strong> <span class="arithmatex"><span class="MathJax_Preview">f_{\theta}(x)=\sum_{m=1}^{M} K_{\lambda_{m}}\left(\mu_{m}, x\right) \theta_{m}</span><script type="math/tex">f_{\theta}(x)=\sum_{m=1}^{M} K_{\lambda_{m}}\left(\mu_{m}, x\right) \theta_{m}</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">\mu_{m}</span><script type="math/tex">\mu_{m}</script></span> are centroids (ËßÅ 6.7 ËäÇ)</li>
</ul>
<p>Neural Network</p>
<ul>
<li>ÂçïÂ±ÇÁöÑÂêëÂâçÂèçÈ¶àÁöÑÂ∏¶ÊúâÁ∫øÊÄßËæìÂá∫ÊùÉÈáçÁöÑÁ•ûÁªèÁΩëÁªúÊ®°ÂûãÂèØ‰ª•ËÆ§‰∏∫ÊòØ‰∏ÄÁßçËá™ÈÄÇÂ∫îÁöÑÂü∫ÂáΩÊï∞ÊñπÊ≥ï</li>
<li><span class="arithmatex"><span class="MathJax_Preview">f_{\theta}(x)=\sum_{i=1}^{M} \beta_{m} \sigma\left(\alpha_{m}^{\top} x+b_{m}\right)</span><script type="math/tex">f_{\theta}(x)=\sum_{i=1}^{M} \beta_{m} \sigma\left(\alpha_{m}^{\top} x+b_{m}\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\sigma(x)=\frac{1}{1+e^{(-x)}}</span><script type="math/tex">\sigma(x)=\frac{1}{1+e^{(-x)}}</script></span> activation function</li>
<li>Direction <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> and bias <span class="arithmatex"><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> need to be computed</li>
</ul>
<p>Dictionary methods</p>
<ul>
<li>Functions are adaptively chosen Ëøô‰∫õËá™ÈÄÇÂ∫îÈÄâÊã©Âü∫ÂáΩÊï∞ÁöÑÊñπÊ≥ï‰πüË¢´Áß∞‰Ωú Â≠óÂÖ∏ (dictionary) ÊñπÊ≥ï</li>
<li>A possibly infinite set or dictionary <span class="arithmatex"><span class="MathJax_Preview">\mathcal{D}</span><script type="math/tex">\mathcal{D}</script></span> of candidate basis functions models employ some kind of search mechanism.</li>
</ul>
<h3 id="model-selection-and-the-biasvariance-tradeoff">Model Selection and the Bias‚ÄìVariance Tradeoff<a class="headerlink" href="#model-selection-and-the-biasvariance-tradeoff" title="Permanent link">&para;</a></h3>
<h4 id="model-complexity">Model complexity<a class="headerlink" href="#model-complexity" title="Permanent link">&para;</a></h4>
<p>‰∏äÈù¢ËÆ®ËÆ∫ÁöÑÊâÄÊúâÊ®°Âûã‰ª•ÂèäÂÖ∂‰ªñÂ∞ÜË¶ÅÂú®ÂêéÈù¢Á´†ËäÇËÆ®ËÆ∫ÁöÑÊ®°ÂûãÈÉΩÊúâ‰∏Ä‰∏™ ÂÖâÊªë (smoothing) Êàñ Â§çÊùÇÊÄß (complexity) ÂèÇÊï∞ÈúÄË¶ÅÁ°ÆÂÆö</p>
<ul>
<li>The multiplier of the penalty <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span></li>
<li>Width of the kernel</li>
<li>Number of basis functions</li>
</ul>
<p>Decomposition of EPE (expected prediction error) ËøôÈáåÂÅáËÆæ‰∫Ü x ÊòØÂõ∫ÂÆöÁöÑËÄåÈùûÈöèÊú∫ÁöÑ (‰πüÂç≥ pointwise ÊúÄÂ∞è)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
E P E\left(x_{0}\right)&amp;=E\left[\left(y_{0}-\hat{f}\left(x_{0}\right)\right)^{2} \mid X=x_{0}\right] \\
&amp;=E\left[\left(y_{0}-f\left(x_{0}\right)+f\left(x_{0}\right)-E \hat{f}\left(x_{0}\right)+E \hat{f}\left(x_{0}\right)-\hat{f}\left(x_{0}\right)\right)^{2} \mid X=x_{0}\right] \\
&amp;=E\left[y_{0}-f\left(x_{0}\right)\right]^{2}+E\left[f\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right) \right]^{2}+E\left[\hat{f}\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right)\right]^{2}.
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
E P E\left(x_{0}\right)&=E\left[\left(y_{0}-\hat{f}\left(x_{0}\right)\right)^{2} \mid X=x_{0}\right] \\
&=E\left[\left(y_{0}-f\left(x_{0}\right)+f\left(x_{0}\right)-E \hat{f}\left(x_{0}\right)+E \hat{f}\left(x_{0}\right)-\hat{f}\left(x_{0}\right)\right)^{2} \mid X=x_{0}\right] \\
&=E\left[y_{0}-f\left(x_{0}\right)\right]^{2}+E\left[f\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right) \right]^{2}+E\left[\hat{f}\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right)\right]^{2}.
\end{aligned}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">E\left[y_{0}-f\left(x_{0}\right)\right]^{2}</span><script type="math/tex">E\left[y_{0}-f\left(x_{0}\right)\right]^{2}</script></span> irreducible error, beyond our control ‰∏çÂèØÁ∫¶ÂáèÁöÑ</li>
<li><span class="arithmatex"><span class="MathJax_Preview">E\left[f\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right)\right]^{2}</span><script type="math/tex">E\left[f\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right)\right]^{2}</script></span> Bias squared, decrease with complexity ÂÅèÂ∑ÆÈÉ®ÂàÜ</li>
<li><span class="arithmatex"><span class="MathJax_Preview">E\left[\hat{f}\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right)\right]^{2}</span><script type="math/tex">E\left[\hat{f}\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right)\right]^{2}</script></span> Variance, increase with complexity ÊñπÂ∑ÆÈÉ®ÂàÜ</li>
</ul>
<p>ÂêéÈù¢ÁöÑ‰∏§È°πÊûÑÊàê‰∫Ü <strong>ÂùáÊñπËØØÂ∑Æ</strong> (mean squared error)</p>
<p><img alt="" src="../media/ASL-note1/2022-01-06-17-25-01.png" /></p>
<p>‰∏ÄËà¨Âú∞, Êàë‰ª¨ÈÄâÊã©Ê®°ÂûãÂ§çÊùÇÂ∫¶‰ΩøÂÅèÂ∑Æ‰∏éÊñπÂ∑ÆËææÂà∞ÂùáË°°‰ªéËÄå‰ΩøÊµãËØïËØØÂ∑ÆÊúÄÂ∞è„ÄÇÊµãËØïËØØÂ∑ÆÁöÑ‰∏Ä‰∏™ÊòéÊòæÁöÑ ‰º∞ËÆ°ÊòØ ËÆ≠ÁªÉËØØÂ∑Æ (training error) <span class="arithmatex"><span class="MathJax_Preview">\frac{1}{N} \sum_{i}\left(y_{i}-\hat{y}_{i}\right)^{2}</span><script type="math/tex">\frac{1}{N} \sum_{i}\left(y_{i}-\hat{y}_{i}\right)^{2}</script></span>. ‰∏çÂπ∏ÁöÑÊòØ, ËÆ≠ÁªÉËØØÂ∑Æ‰∏çÊòØÊµãËØïËØØÂ∑ÆÁöÑËâØÂ•Ω‰º∞ËÆ°, Âõ†‰∏∫Ëøô‰∏çËÉΩËß£ÈáäÊ®°ÂûãÂ§çÊùÇÂ∫¶.</p>
<p>Âú® <code>Á¨¨ 7 Á´†</code> (7.2, 7.3) ‰∏≠ÔºåÊàë‰ª¨ËÄÉËôë‰º∞ËÆ°È¢ÑÊµãÊñπÊ≥ïÁöÑÊµãËØïËØØÂ∑ÆÁöÑÂêÑÁßçÊñπÂºèÔºåÂπ∂Âõ†Ê≠§Âú®ÁªôÂÆöÁöÑÈ¢ÑÊµãÊñπÊ≥ïÂíåËÆ≠ÁªÉÈõÜ‰∏ãÔºå‰º∞ËÆ°Âá∫ÊúÄ‰ºòÁöÑÊ®°ÂûãÂ§çÊùÇÂ∫¶Ôºé</p>
<h2 id="linear-methods-for-regression">Linear Methods for Regression<a class="headerlink" href="#linear-methods-for-regression" title="Permanent link">&para;</a></h2>
<p>Outline</p>
<ul>
<li>Intro</li>
<li>Linear Regression \&amp; Least Squares</li>
<li>Subset Selection</li>
<li>Shrinkage methods</li>
<li>Methods Using Derived Input Directions</li>
</ul>
<h3 id="introduction-of-linear-methods">Introduction of Linear Methods<a class="headerlink" href="#introduction-of-linear-methods" title="Permanent link">&para;</a></h3>
<p>Advantages of Linear Models</p>
<ul>
<li>Simple and often provide an adequate and interpretable description of how the inputs affect the output.</li>
<li>Sometimes outperform fancier nonlinear models, especially in situations with small numbers of training cases, low signal-to-noise ratio or sparse data.</li>
<li>Can be applied to transformations of the inputs and this considerably expands their scope.</li>
</ul>
<h3 id="recap-of-linear-regression">Recap of Linear Regression<a class="headerlink" href="#recap-of-linear-regression" title="Permanent link">&para;</a></h3>
<div class="arithmatex">
<div class="MathJax_Preview">
f(X)=\beta_{0}+\sum_{j=1}^{p} X_{j} \beta_{j}
</div>
<script type="math/tex; mode=display">
f(X)=\beta_{0}+\sum_{j=1}^{p} X_{j} \beta_{j}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\beta_{j}</span><script type="math/tex">\beta_{j}</script></span> unknown parameter</li>
<li>Sources of <span class="arithmatex"><span class="MathJax_Preview">X_{j}</span><script type="math/tex">X_{j}</script></span><ul>
<li>quantitative inputs</li>
<li>transformations</li>
<li>basis expansions</li>
<li>dummy coding of qualitative variables</li>
<li>interactions between variables</li>
</ul>
</li>
<li>Model is linear in <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span></li>
</ul>
<h4 id="estimation">Estimation<a class="headerlink" href="#estimation" title="Permanent link">&para;</a></h4>
<p>Training data:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\left(x_{1}, y_{1}\right), \ldots,\left(x_{N}, y_{N}\right)
</div>
<script type="math/tex; mode=display">
\left(x_{1}, y_{1}\right), \ldots,\left(x_{N}, y_{N}\right)
</script>
</div>
<p>Feature measurements for the <span class="arithmatex"><span class="MathJax_Preview">i_{t h}</span><script type="math/tex">i_{t h}</script></span> case: <span class="arithmatex"><span class="MathJax_Preview">x_{i}=\left(x_{i 1}, x_{i 2}, \ldots, x_{i p}\right)^{\top}:</span><script type="math/tex">x_{i}=\left(x_{i 1}, x_{i 2}, \ldots, x_{i p}\right)^{\top}:</script></span>
Least squares:
choose <span class="arithmatex"><span class="MathJax_Preview">\beta=\left(\beta_{0}, \beta_{1}, \ldots, \beta_{p}\right)^{\top}</span><script type="math/tex">\beta=\left(\beta_{0}, \beta_{1}, \ldots, \beta_{p}\right)^{\top}</script></span> to minimize the Residual Sum of Squares.
<span class="arithmatex"><span class="MathJax_Preview">R S S=\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}</span><script type="math/tex">R S S=\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}</script></span></p>
<h4 id="matrix-notation">Matrix notation<a class="headerlink" href="#matrix-notation" title="Permanent link">&para;</a></h4>
<p>ËÆ∞‰∏∫Áü©ÈòµÂΩ¢Âºè, ÂèØ‰ª•ÂæóÂà∞Ëß£ÊûêËß£. (Âú® X ÂàóÊª°Áß©ÁöÑÊÉÖÂÜµ‰∏ã)</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">X: N \times(p+1)</span><script type="math/tex">X: N \times(p+1)</script></span> matrix</li>
<li><span class="arithmatex"><span class="MathJax_Preview">Y: N</span><script type="math/tex">Y: N</script></span> vector</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
R S S(\beta)=(y-X \beta)^{\top}(y-X \beta)
</div>
<script type="math/tex; mode=display">
R S S(\beta)=(y-X \beta)^{\top}(y-X \beta)
</script>
</div>
<ul>
<li>Quadratic function of <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span></li>
<li>Differentiating w.r.t. <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span></li>
</ul>
<p><span class="arithmatex"><span class="MathJax_Preview">\frac{\partial R S S}{\partial \beta}=-2 X^{\top}(y-X \beta)</span><script type="math/tex">\frac{\partial R S S}{\partial \beta}=-2 X^{\top}(y-X \beta)</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\frac{\partial^{2} R S S}{\partial \beta \partial \beta^{\top}}=2 X^{\top} X</span><script type="math/tex">\frac{\partial^{2} R S S}{\partial \beta \partial \beta^{\top}}=2 X^{\top} X</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\beta}=\left(X^{\top} X\right)^{-1} X^{\top} y
</div>
<script type="math/tex; mode=display">
\hat{\beta}=\left(X^{\top} X\right)^{-1} X^{\top} y
</script>
</div>
<ul>
<li>Assume <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> has full column rank</li>
<li>Fitted value <span class="arithmatex"><span class="MathJax_Preview">\hat{y}=X \hat{\beta}=X\left(X^{\top} X\right)^{-1} X^{\top} y</span><script type="math/tex">\hat{y}=X \hat{\beta}=X\left(X^{\top} X\right)^{-1} X^{\top} y</script></span></li>
<li>Hat matrix <span class="arithmatex"><span class="MathJax_Preview">H=X\left(X^{\top} X\right)^{-1} X^{\top}</span><script type="math/tex">H=X\left(X^{\top} X\right)^{-1} X^{\top}</script></span></li>
</ul>
<p><img alt="" src="../media/ASL-note1/2021-11-29-16-20-17.png" /></p>
<h4 id="sampling-properties-of-hatbetahatbeta">Sampling Properties of <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}</span><script type="math/tex">\hat{\beta}</script></span><a class="headerlink" href="#sampling-properties-of-hatbetahatbeta" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\beta}=\left(X^{\top} X\right)^{-1} X^{\top} y
</div>
<script type="math/tex; mode=display">
\hat{\beta}=\left(X^{\top} X\right)^{-1} X^{\top} y
</script>
</div>
<ul>
<li>ÂèÇÊï∞‰º∞ËÆ°ÁöÑÂçèÊñπÂ∑ÆÁü©Èòµ(covariance matrix, variance-covariance matrix, dispersion matrix Ëøô‰∫õÂêçÂ≠óÈÉΩÊòØ) <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Var}(\hat{\beta})=\left(X^{\top} X\right)^{-1} \sigma^{2}</span><script type="math/tex">\operatorname{Var}(\hat{\beta})=\left(X^{\top} X\right)^{-1} \sigma^{2}</script></span></li>
<li>‰º∞ËÆ°ÊñπÂ∑Æ <span class="arithmatex"><span class="MathJax_Preview">\hat{\sigma}^{2}=\frac{1}{N-p-1} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}</span><script type="math/tex">\hat{\sigma}^{2}=\frac{1}{N-p-1} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}</script></span></li>
</ul>
<p>Additional assumptions Âä†‰∏äÊ≠£ÊÄÅÊÄßÂÅáËÆæ</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">y=\beta_{0}+\sum_{j=1}^{p} X_{j} \beta_{j}+\epsilon</span><script type="math/tex">y=\beta_{0}+\sum_{j=1}^{p} X_{j} \beta_{j}+\epsilon</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)</span><script type="math/tex">\epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)</script></span></li>
</ul>
<p>Âú®Ëøô‰∏ÄÂÅáËÆæ‰∏ã, Êàë‰ª¨ÂèØ‰ª•ÂæóÂà∞ÂèÇÊï∞‰º∞ËÆ°ÁöÑÂàÜÂ∏É: Sampling distribution of <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}, \hat{\sigma}^{2}</span><script type="math/tex">\hat{\beta}, \hat{\sigma}^{2}</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\beta} \sim \mathcal{N}\left(\beta,\left(X^{\top} X\right)^{-1} \sigma^{2}\right)</span><script type="math/tex">\hat{\beta} \sim \mathcal{N}\left(\beta,\left(X^{\top} X\right)^{-1} \sigma^{2}\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">(N-p-1) \hat{\sigma}^{2} \sim \sigma^{2} \chi_{N-p-1}^{2}</span><script type="math/tex">(N-p-1) \hat{\sigma}^{2} \sim \sigma^{2} \chi_{N-p-1}^{2}</script></span></li>
</ul>
<p>‰ªéËÄå‰πãÂêéÂèØ‰ª•ËøõË°åÂÅáËÆæÊ£ÄÈ™åÂíåÁΩÆ‰ø°Âå∫Èó¥.</p>
<h4 id="hypothesis-testing-single-parameter">Hypothesis Testing - single parameter<a class="headerlink" href="#hypothesis-testing-single-parameter" title="Permanent link">&para;</a></h4>
<p>When <span class="arithmatex"><span class="MathJax_Preview">\sigma^{2}</span><script type="math/tex">\sigma^{2}</script></span> is known (rare)</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">v_{j}</span><script type="math/tex">v_{j}</script></span> is the <span class="arithmatex"><span class="MathJax_Preview">j_{t h}</span><script type="math/tex">j_{t h}</script></span> diagonal element of <span class="arithmatex"><span class="MathJax_Preview">\left(X^{\top} X\right)^{-1}</span><script type="math/tex">\left(X^{\top} X\right)^{-1}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">z_{j}=\frac{\hat{\beta}_{j}}{\sigma \sqrt{v_{j}}}</span><script type="math/tex">z_{j}=\frac{\hat{\beta}_{j}}{\sigma \sqrt{v_{j}}}</script></span></li>
<li>Under <span class="arithmatex"><span class="MathJax_Preview">H_{0}, z_{j} \sim \mathcal{N}(0,1)</span><script type="math/tex">H_{0}, z_{j} \sim \mathcal{N}(0,1)</script></span></li>
</ul>
<p>When <span class="arithmatex"><span class="MathJax_Preview">\sigma^{2}</span><script type="math/tex">\sigma^{2}</script></span> is unknown (most common)</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">z_{j}=\frac{\hat{\beta}_{j}}{\hat{\sigma} \sqrt{v_{j}}}</span><script type="math/tex">z_{j}=\frac{\hat{\beta}_{j}}{\hat{\sigma} \sqrt{v_{j}}}</script></span></li>
<li>Under <span class="arithmatex"><span class="MathJax_Preview">H_{0}, z_{j} \sim t(N-p-1)</span><script type="math/tex">H_{0}, z_{j} \sim t(N-p-1)</script></span></li>
</ul>
<p>The differences between <span class="arithmatex"><span class="MathJax_Preview">t(N-p-1)</span><script type="math/tex">t(N-p-1)</script></span> and standard normal become negligible as the sample size increases</p>
<h4 id="hypothesis-testing-group-of-coefficients">Hypothesis Testing - group of coefficients<a class="headerlink" href="#hypothesis-testing-group-of-coefficients" title="Permanent link">&para;</a></h4>
<p>Bigger model</p>
<ul>
<li>residual sum-of-squares for the least squares fit <span class="arithmatex"><span class="MathJax_Preview">R S S_{1}</span><script type="math/tex">R S S_{1}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">p_{1}+1</span><script type="math/tex">p_{1}+1</script></span> parameters</li>
</ul>
<p>Nested smaller model</p>
<ul>
<li>residual sum-of-squares for the least squares fit <span class="arithmatex"><span class="MathJax_Preview">R S S_{0}</span><script type="math/tex">R S S_{0}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">p_{0}+1</span><script type="math/tex">p_{0}+1</script></span> parameters</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
F=\frac{\left(R S S_{0}-R S S_{1}\right) /\left(P_{1}-P_{0}\right)}{R S S_{1} /\left(N-P_{1}-1\right)}
</div>
<script type="math/tex; mode=display">
F=\frac{\left(R S S_{0}-R S S_{1}\right) /\left(P_{1}-P_{0}\right)}{R S S_{1} /\left(N-P_{1}-1\right)}
</script>
</div>
<ul>
<li><strong>Measures the change in RSS per additional parameter</strong> in the bigger model Ë°°Èáè‰∫ÜÂ§ßÊ®°Âûã‰∏≠ÊØè‰∏™Â¢ûÂä†ÁöÑÁ≥ªÊï∞ÂØπ‰∫éRSSÁöÑÊîπÂèò, Âπ∂‰∏îÁî®ÊñπÂ∑ÆÁöÑ‰º∞ËÆ°ÂÄºËøõË°åÊ†áÂáÜÂåñ</li>
<li>Under <span class="arithmatex"><span class="MathJax_Preview">H_{0}, F \sim \mathcal{F}_{p_{1}-p_{0}, N-p_{1}-1}</span><script type="math/tex">H_{0}, F \sim \mathcal{F}_{p_{1}-p_{0}, N-p_{1}-1}</script></span></li>
</ul>
<h4 id="confidence-interval-region">Confidence Interval / Region<a class="headerlink" href="#confidence-interval-region" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex"><span class="MathJax_Preview">\beta_{j}: 1-2 \alpha</span><script type="math/tex">\beta_{j}: 1-2 \alpha</script></span> confidence interval ÂÖ∂‰∏≠ <span class="arithmatex"><span class="MathJax_Preview">z^{1-\alpha}</span><script type="math/tex">z^{1-\alpha}</script></span> ‰∏∫Ê≠£ÊÄÅÂàÜÂ∏ÉÁöÑ <span class="arithmatex"><span class="MathJax_Preview">{1-\alpha}</span><script type="math/tex">{1-\alpha}</script></span> ÂàÜ‰ΩçÊï∞</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\left(\hat{\beta}_{j}-z^{1-\alpha} v_{j}^{1 / 2} \hat{\sigma}, \hat{\beta}_{j}+z^{1-\alpha} v_{j}^{1 / 2} \hat{\sigma}\right)
</div>
<script type="math/tex; mode=display">
\left(\hat{\beta}_{j}-z^{1-\alpha} v_{j}^{1 / 2} \hat{\sigma}, \hat{\beta}_{j}+z^{1-\alpha} v_{j}^{1 / 2} \hat{\sigma}\right)
</script>
</div>
<p>Entire parameter vector <span class="arithmatex"><span class="MathJax_Preview">\beta: 1-2 \alpha</span><script type="math/tex">\beta: 1-2 \alpha</script></span> confidence region</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathcal{C}_{\beta}=\left\{\beta \mid(\hat{\beta}-\beta)^{\top} X^{\top} X(\hat{\beta}-\beta) \leqslant \hat{\sigma}^{2} {\chi_{p+1}^{2}}^{1-2 \alpha}\right\}
</div>
<script type="math/tex; mode=display">
\mathcal{C}_{\beta}=\left\{\beta \mid(\hat{\beta}-\beta)^{\top} X^{\top} X(\hat{\beta}-\beta) \leqslant \hat{\sigma}^{2} {\chi_{p+1}^{2}}^{1-2 \alpha}\right\}
</script>
</div>
<h4 id="gauss-markov-theorem">Gauss Markov Theorem<a class="headerlink" href="#gauss-markov-theorem" title="Permanent link">&para;</a></h4>
<p>Why least squares?</p>
<p><strong>Gauss Markov Theorem</strong>: least squares estimates of the parameters <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> have the smallest variance among all linear unbiased estimates</p>
<p><strong>Gauss-Markov ÂÆöÁêÜ</strong>: Âú®ÂèÇÊï∞ <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> ÁöÑÊâÄÊúâÊó†ÂÅè‰º∞ËÆ°‰∏≠, ÊúÄÂ∞è‰∫å‰πò‰º∞ËÆ°ÊòØÂÖ∂‰∏≠ÊñπÂ∑ÆÊúÄÂ∞èÁöÑ.</p>
<p>In mathematical term: suppose <span class="arithmatex"><span class="MathJax_Preview">\theta=\alpha^{\top} \beta</span><script type="math/tex">\theta=\alpha^{\top} \beta</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}=\alpha^{\top} \hat{\beta}</span><script type="math/tex">\hat{\theta}=\alpha^{\top} \hat{\beta}</script></span> is unbiased for <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span></li>
<li>Consider any other linear estimator <span class="arithmatex"><span class="MathJax_Preview">\tilde{\theta}=c^{\top} y</span><script type="math/tex">\tilde{\theta}=c^{\top} y</script></span> such that <span class="arithmatex"><span class="MathJax_Preview">E\left(c^{\top} y\right)=\alpha^{\top} \beta</span><script type="math/tex">E\left(c^{\top} y\right)=\alpha^{\top} \beta</script></span></li>
<li>According to Gauss Markov Theorem <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Var}\left(\alpha^{\top} \hat{\beta}\right)&lt;\operatorname{Var}\left(c^{\top} y\right)</span><script type="math/tex">\operatorname{Var}\left(\alpha^{\top} \hat{\beta}\right)<\operatorname{Var}\left(c^{\top} y\right)</script></span></li>
</ul>
<p>ËôΩÁÑ∂ Gauss-Markov ÂÆöÁêÜ ËØ¥Êòé‰∫ÜÂèÇÊï∞‰º∞ËÆ°Âú®Êó†ÂÅè‰º∞ËÆ°‰∏≠ÊòØÊúÄÂ•ΩÁöÑ, ‰ΩÜÊ†πÊçÆ‰∏ãÈù¢ÁöÑ‰º∞ËÆ°ÂÄºÁöÑÂùáÊñπËØØÂ∑ÆÂàÜËß£, ÂèØÁü•ÂÖ∂Áî±‰º∞ËÆ°ÁöÑÊñπÂ∑ÆÂíå‰º∞ËÆ°ÂÅèÂ∑ÆÂÖ±ÂêåÂÜ≥ÂÆö ‚Äî‚Äî Âõ†Ê≠§ÂºïÂÖ•‰∫Ü‰∏ãÈù¢ÁöÑ shrink ÊñπÊ≥ï.</p>
<p><strong>Mean squared error</strong> of an estimator <span class="arithmatex"><span class="MathJax_Preview">\tilde{\theta}</span><script type="math/tex">\tilde{\theta}</script></span> in estimating <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> :</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{MSE}(\tilde{\theta})=E(\tilde{\theta}-\theta)^{2}=E(\tilde{\theta}-E(\tilde{\theta})+E(\tilde{\theta})-\theta)^{2}=\operatorname{Var}(\tilde{\theta})+\operatorname{Bias}(\tilde{\theta})^{2}
</div>
<script type="math/tex; mode=display">
\operatorname{MSE}(\tilde{\theta})=E(\tilde{\theta}-\theta)^{2}=E(\tilde{\theta}-E(\tilde{\theta})+E(\tilde{\theta})-\theta)^{2}=\operatorname{Var}(\tilde{\theta})+\operatorname{Bias}(\tilde{\theta})^{2}
</script>
</div>
<p>How about biased estimator (best subset, shrinkage)?</p>
<ul>
<li>Might have smaller MSE by trading a little bias for a larger reduction in variance.</li>
</ul>
<p>&ldquo;All models are wrong, some are useful&rdquo;</p>
<ul>
<li>Picking the right model amounts to creating the right balance between bias and variance.</li>
</ul>
<h4 id="multiple-linear-regression">Multiple Linear Regression<a class="headerlink" href="#multiple-linear-regression" title="Permanent link">&para;</a></h4>
<p>Â§öÂÖÉÁ∫øÊÄßÂõûÂΩíÂíå‰∏ÄÂÖÉÁöÑÂÖ≥Á≥ª: ÂΩìÊâÄÊúâËá™ÂèòÈáèÈÉΩÊòØÊ≠£‰∫§ÁöÑÊÉÖÂÜµ‰∏ã, ÂæóÂà∞ÁöÑÁ≥ªÊï∞ÈÉΩÊòØ‰∏ÄÊ†∑ÁöÑ. ËÄåÂØπ‰∫é‰∏çÊ≠£‰∫§ÁöÑÊÉÖÂÜµ, ÂèØ‰ª•ÈááÁî® Gram-Schmidt Ê≠£‰∫§Âåñ, ÂΩìÁÑ∂Âú®‰∏§‰∏™ÂèòÈáèÁõ∏ÂÖ≥ÊÄßËæÉÈ´òÁöÑÊÉÖÂÜµ‰∏ã, ‰º∞ËÆ°‰ºö‰∏çÁ®≥ÂÆö.</p>
<p><span class="arithmatex"><span class="MathJax_Preview">y_{i}=\beta_{0}+\sum_{j=1}^{p} x_{i j} \beta_{j}</span><script type="math/tex">y_{i}=\beta_{0}+\sum_{j=1}^{p} x_{i j} \beta_{j}</script></span> where <span class="arithmatex"><span class="MathJax_Preview">p&gt;1</span><script type="math/tex">p>1</script></span></p>
<p>When <span class="arithmatex"><span class="MathJax_Preview">x_{1}, x_{2}, \ldots, x_{p}</span><script type="math/tex">x_{1}, x_{2}, \ldots, x_{p}</script></span> are orthogonal <span class="arithmatex"><span class="MathJax_Preview">\left(\left\langle x_{j}, x_{k}\right\rangle=0\right.</span><script type="math/tex">\left(\left\langle x_{j}, x_{k}\right\rangle=0\right.</script></span> for all <span class="arithmatex"><span class="MathJax_Preview">\left.j \neq k\right)</span><script type="math/tex">\left.j \neq k\right)</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}_{j}=\frac{\left\langle x_{j}, y\right\rangle}{\left\langle x_{j}, x_{j}\right\rangle}</span><script type="math/tex">\hat{\beta}_{j}=\frac{\left\langle x_{j}, y\right\rangle}{\left\langle x_{j}, x_{j}\right\rangle}</script></span>, same as univariate solution</li>
</ul>
<p>When <span class="arithmatex"><span class="MathJax_Preview">x_{1}, x_{2}, \ldots, x_{p}</span><script type="math/tex">x_{1}, x_{2}, \ldots, x_{p}</script></span> are not orthogonal</p>
<ul>
<li>Orthogonalize <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span> to get residual vector <span class="arithmatex"><span class="MathJax_Preview">z_{j}, j=1, \ldots, p</span><script type="math/tex">z_{j}, j=1, \ldots, p</script></span></li>
<li>Regress <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> on the residual <span class="arithmatex"><span class="MathJax_Preview">z_{p}</span><script type="math/tex">z_{p}</script></span> to give the estimate <span class="arithmatex"><span class="MathJax_Preview">\beta_{p}</span><script type="math/tex">\beta_{p}</script></span></li>
<li>Also known as <strong>Gram-Schmidt</strong> procedure</li>
<li>Note: if <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span> is highly correlated with <span class="arithmatex"><span class="MathJax_Preview">x_{k}, z_{j}</span><script type="math/tex">x_{k}, z_{j}</script></span> will be close to zero, and <span class="arithmatex"><span class="MathJax_Preview">\beta_{j}</span><script type="math/tex">\beta_{j}</script></span> will be very unstable.</li>
</ul>
<h4 id="multiple-outputs-y_1-y_2-ldots-y_ky_1-y_2-ldots-y_k">Multiple Outputs <span class="arithmatex"><span class="MathJax_Preview">Y_{1}, Y_{2}, \ldots, Y_{K}</span><script type="math/tex">Y_{1}, Y_{2}, \ldots, Y_{K}</script></span><a class="headerlink" href="#multiple-outputs-y_1-y_2-ldots-y_ky_1-y_2-ldots-y_k" title="Permanent link">&para;</a></h4>
<p>ÂØπ‰∫éÂ§öÂÖÉÁöÑËæìÂá∫ËÄåË®Ä, Ëã•ÈöèÊú∫È°πÁõ∏‰∫íÁã¨Á´ã, ÂàôÂíå‰∏ÄÂÖÉY‰∏ÄËá¥.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Y_{k}=\beta_{0 k}+\sum_{j=1}^{p} X_{j} \beta_{j k}+\epsilon_{k}=f_{k}(X)+\epsilon_{k}
</div>
<script type="math/tex; mode=display">
Y_{k}=\beta_{0 k}+\sum_{j=1}^{p} X_{j} \beta_{j k}+\epsilon_{k}=f_{k}(X)+\epsilon_{k}
</script>
</div>
<p>or in matrix notation <span class="arithmatex"><span class="MathJax_Preview">Y=X B+E</span><script type="math/tex">Y=X B+E</script></span></p>
<p>If errors <span class="arithmatex"><span class="MathJax_Preview">\epsilon=\left(\epsilon_{1}, \epsilon_{2}, \ldots, \epsilon_{k}\right)</span><script type="math/tex">\epsilon=\left(\epsilon_{1}, \epsilon_{2}, \ldots, \epsilon_{k}\right)</script></span> are i.i.d.
‰∏çÂêåÁöÑ <span class="arithmatex"><span class="MathJax_Preview">y_i</span><script type="math/tex">y_i</script></span> ‰πãÈó¥ËØØÂ∑ÆÈ°πÁã¨Á´ã, ‰πüÂç≥ÂèØÂàÜÂà´ÁúãÊàêÁã¨Á´ãÁöÑ LR.</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">R S S(B)=\sum_{k=1}^{K} \sum_{i=1}^{N}\left(y_{i k}-f_{k}\left(x_{i}\right)\right)^{2}=\operatorname{tr}\left[(Y-X B)^{\top}(Y-X B)\right]</span><script type="math/tex">R S S(B)=\sum_{k=1}^{K} \sum_{i=1}^{N}\left(y_{i k}-f_{k}\left(x_{i}\right)\right)^{2}=\operatorname{tr}\left[(Y-X B)^{\top}(Y-X B)\right]</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{B}=\operatorname{argmin}_{B} R S S(B)=\left(X^{\top} X\right)^{-1} X^{\top} Y</span><script type="math/tex">\hat{B}=\operatorname{argmin}_{B} R S S(B)=\left(X^{\top} X\right)^{-1} X^{\top} Y</script></span></li>
<li>Same form as univariate output</li>
</ul>
<p>If errors <span class="arithmatex"><span class="MathJax_Preview">\epsilon=\left(\epsilon_{1}, \epsilon_{2}, \ldots, \epsilon_{k}\right)</span><script type="math/tex">\epsilon=\left(\epsilon_{1}, \epsilon_{2}, \ldots, \epsilon_{k}\right)</script></span> are correlated</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">R S S(B, \Sigma)=\sum_{i=1}^{N}\left(y_{i}-f\left(x_{i}\right)\right)^{\top} \Sigma^{-1}\left(y_{i}-f\left(x_{i}\right)\right)</span><script type="math/tex">R S S(B, \Sigma)=\sum_{i=1}^{N}\left(y_{i}-f\left(x_{i}\right)\right)^{\top} \Sigma^{-1}\left(y_{i}-f\left(x_{i}\right)\right)</script></span> Ê≥®ÊÑèËøôÈáåÁî®‰∫ÜËØØÂ∑ÆÁöÑÂçèÊñπÂ∑ÆÁü©ÈòµËøõË°åËÆæÁΩÆ‰∏çÂêå‰º∞ËÆ°ËØØÂ∑Æ‰πãÈó¥ÁöÑÁ≥ªÊï∞, Âè´ÂÅö „ÄåÂ§öÈáçÂèòÈáèÂä†ÊùÉÂáÜÂàô„Äç, ÂèØÁî±„ÄåÂ§öÂèòÈáèÈ´òÊñØÂÆöÁêÜ„ÄçÊé®Âá∫.</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{B}=\operatorname{argmin}_{B} R S S(B, \Sigma)=\left(X^{\top} X\right)^{-1} X^{\top} Y</span><script type="math/tex">\hat{B}=\operatorname{argmin}_{B} R S S(B, \Sigma)=\left(X^{\top} X\right)^{-1} X^{\top} Y</script></span> Ê≥®ÊÑèËøôÈáåÁöÑÂΩ¢ÂºèÂíå‰∏äÈù¢ËØØÂ∑ÆÈ°πÁã¨Á´ãÊó∂‰∏ÄËá¥</li>
<li>Note: If <span class="arithmatex"><span class="MathJax_Preview">\Sigma_{i}</span><script type="math/tex">\Sigma_{i}</script></span> vary among observations, results no longer hold</li>
</ul>
<h3 id="subset-selection">Subset Selection<a class="headerlink" href="#subset-selection" title="Permanent link">&para;</a></h3>
<p>‰∏§ÁÇπÁêÜÁî±: È¢ÑÊµãÁ≤æÁ°ÆÊÄß, Ê®°ÂûãÁöÑÂèØËß£ÈáäÊÄß</p>
<ul>
<li>Prediction accuracy<ul>
<li>least squares estimates often have low bias but large variance.</li>
<li>Prediction accuracy can sometimes be improved by shrinking or setting some coeÔ¨Écients to zero.</li>
</ul>
</li>
<li>Interpretability<ul>
<li>Determine a smaller subset that exhibit the strongest effects.</li>
<li>SacriÔ¨Åce some of the small details in order to get the ‚Äùbig picture‚Äù</li>
</ul>
</li>
</ul>
<h4 id="forward-backward-stepwise-selection">Forward, Backward, Stepwise Selection<a class="headerlink" href="#forward-backward-stepwise-selection" title="Permanent link">&para;</a></h4>
<p>Áõ∏ËæÉ‰∫éÈááÁî®FÁªüËÆ°ÈáèÊù•ÈÄâÊã©„ÄåÊòæËëóÊÄß„ÄçÈ°πÁÑ∂ÂêéÂà†Èô§ÈùûÊòæËëóÈ°π, ÂÆÉÁöÑÈóÆÈ¢òÂú®‰∫éÊ≤°ÊúâËÄÉËôë„ÄåÂ§öÈáçÊ£ÄÈ™å„ÄçÈóÆÈ¢ò, Âõ†Ê≠§ Forward/Backward-stepwise selection Êõ¥Â•Ω.</p>
<p>ÊúâÁêÜÊÉ≥ÁöÑÊÉÖÂÜµ, Ëá™ÁÑ∂ÊòØ‰ªéÊâÄÊúâÂèØËÉΩÁöÑÂ≠êÈõÜ‰∏≠ÈÄâÊã©ÊúÄ‰ºòÁöÑ, ‰ΩÜÊòØËÆ°ÁÆóÈáèËøáÂ§ß.</p>
<ul>
<li>Forward<ul>
<li>Starts with the intercept</li>
<li>Sequentially adds the predictor that most improves the Ô¨Åt</li>
<li>Must determine model size k</li>
<li>ÂâçÂêëÈÄâÊã©ÂèØ‰ª•ÁêÜËß£‰∏∫‰∏ÄÁßçË¥™ÂøÉÁÆóÊ≥ï. 1. Âú® computational‰∏ä‰ª£‰ª∑ËæÉÂ∞è; 2. ÂâçÂêëÈÄêÊ≠•ÊòØ‰∏ÄÁßçÊúâ„ÄåÊõ¥Â§öÁ∫¶Êùü„ÄçÁöÑÊêúÁ¥¢, ‰ºöÊúâÊõ¥‰ΩéÁöÑÊñπÂ∑Æ, ‰ΩÜÂèØËÉΩ‰ºöÊúâÊõ¥È´òÁöÑËØØÂ∑Æ.</li>
</ul>
</li>
<li>backward<ul>
<li>Starts with the full model</li>
<li>Sequentially deletes the predictor that has the least impact on Ô¨Åt</li>
<li>Candidate for dropping is the variable with the smallest Z-score</li>
</ul>
</li>
<li>Stepwise<ul>
<li>Consider both forward and backward moves at each step</li>
<li>Select the ‚Äúbest‚Äù of the two</li>
<li>R <code>step</code> function: at each step add or drop based on AIC</li>
</ul>
</li>
</ul>
<p>‰ºòÁº∫ÁÇπ</p>
<ul>
<li>Pros<ul>
<li>Interpretable</li>
<li>Possibly lower prediction error than the full model.</li>
</ul>
</li>
<li>Cons<ul>
<li>Discrete process - variables either retained or discarded Áõ∏ËæÉ‰∫é‰∏ãÈù¢ÁöÑ shrink ÊñπÊ≥ï</li>
<li>Often exhibits high variance Áî±‰∫éÂèòÈáèÂ∞ë‰∫Ü, Âπ∂‰∏îÊòØÁ¶ªÊï£Á∫¶Êùü</li>
</ul>
</li>
</ul>
<h3 id="shrinkage-methods">Shrinkage methods<a class="headerlink" href="#shrinkage-methods" title="Permanent link">&para;</a></h3>
<p>‰∏äÈù¢ÁöÑËá™Â∑±ÈÄâÊã©ÂèØ‰ª•ÂæóÂà∞‰∏Ä‰∏™Ëß£ÈáäÊÄßÊõ¥Âº∫ÁöÑÊ®°Âûã, (È¢ÑÊµãËØØÂ∑ÆÂèØËÉΩÊØîÂÖ®Ê®°ÂûãÂ•Ω); ÁÑ∂ËÄåÁî±‰∫éËøôÊòØ‰∏Ä‰∏™Á¶ªÊï£ÁöÑËøáÁ®ã(ÂèòÈáèÈÄâÂÖ•ÊàñËÄÖ‰∏¢ÂºÉ), Âõ†Ê≠§ÂèØËÉΩË°®Áé∞‰∏∫È´òÊñπÂ∑Æ; ËÄåËøôÈáåÁöÑshrinkage ÊñπÊ≥ï, Êõ¥Âä†ËøûÁª≠, Âõ†Ê≠§‰∏ç‰ºöÂèóÂà∞ high variability ÁöÑÂΩ±Âìç.</p>
<h4 id="ridge-regression">Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permanent link">&para;</a></h4>
<p>Shrinks the regression coefficients by imposing a penalty on their size.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\beta}^{\text {ridge }}=\operatorname{argmin}_{\beta}\left\{\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}\right\}
</div>
<script type="math/tex; mode=display">
\hat{\beta}^{\text {ridge }}=\operatorname{argmin}_{\beta}\left\{\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}\right\}
</script>
</div>
<p>which is equivalent to (Âà©Áî® Lagrange ‰πòÂ≠êÊ≥ï)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\beta}^{\text {ridge }}=\operatorname{argmin}_{\beta} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}
 \text{subject to} \sum_{j=1}^{p} \beta_{j}^{2} \leqslant t
</div>
<script type="math/tex; mode=display">
\hat{\beta}^{\text {ridge }}=\operatorname{argmin}_{\beta} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}
 \text{subject to} \sum_{j=1}^{p} \beta_{j}^{2} \leqslant t
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\beta_{0}</span><script type="math/tex">\beta_{0}</script></span> is <strong>left out</strong> of the penalty term</li>
<li><strong>Normalize</strong> inputs before solving for solutions. Ê≥®ÊÑè, ÂØπ‰∫éÂ≤≠ÂõûÂΩí, ÂØπËæìÂÖ•ÊåâÁÖßÊØî‰æãËøõË°åÁº©Êîæ, ÂæóÂà∞ÁöÑÁªìÊûúÊòØ‰∏çÂêåÁöÑ, Âõ†Ê≠§Âú®‰∏äÈù¢ÁöÑÊ±ÇËß£ÂâçÈúÄË¶ÅËßÅËøõË°åÊ†áÂáÜÂåñ. ÂèØ‰ª•ËØÅÊòé, ÁªèËøáÂØπ‰∫éÂèòÈáè‰∏≠ÂøÉÂåñ‰πãÂêé, ÂèØ‰ª•ÂàÜËß£‰∏∫Ê±ÇÊà™Ë∑ùÂíåÂÖ∂‰ªñÁ≥ªÊï∞‰∏§ÈÉ®ÂàÜ, Âõ†Ê≠§ÂèØ‰ª•ÂÜôÊàê‰∏ãÈù¢ÁöÑÂΩ¢Âºè(ËøôÈáåÁöÑÂèÇÊï∞Áª¥Â∫¶‰∏∫ p ËÄåÈùû p+1 ‰∫Ü).</li>
<li>In matrix notation <span class="arithmatex"><span class="MathJax_Preview">R S S(\lambda)=(y-X \beta)^{\top}(y-X \beta)+\lambda \beta^{\top} \beta</span><script type="math/tex">R S S(\lambda)=(y-X \beta)^{\top}(y-X \beta)+\lambda \beta^{\top} \beta</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {ridge }}=\left(X^{\top} X+\lambda I\right)^{-1} X^{\top} y</span><script type="math/tex">\hat{\beta}^{\text {ridge }}=\left(X^{\top} X+\lambda I\right)^{-1} X^{\top} y</script></span></li>
</ul>
<h5 id="estimation-gaussian">Estimation - Gaussian<a class="headerlink" href="#estimation-gaussian" title="Permanent link">&para;</a></h5>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\beta}^{\text {ridge }}=\left(X^{\top} X+\lambda I\right)^{-1} X^{\top} y
</div>
<script type="math/tex; mode=display">
\hat{\beta}^{\text {ridge }}=\left(X^{\top} X+\lambda I\right)^{-1} X^{\top} y
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {ridge }}</span><script type="math/tex">\hat{\beta}^{\text {ridge }}</script></span> is again a linear function of <span class="arithmatex"><span class="MathJax_Preview">\mathrm{y}</span><script type="math/tex">\mathrm{y}</script></span></li>
<li>Makes it nonsingular even if <span class="arithmatex"><span class="MathJax_Preview">X^{\top} X</span><script type="math/tex">X^{\top} X</script></span> is not of full rank</li>
<li>When <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> are orthogonal, <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {ridge }}</span><script type="math/tex">\hat{\beta}^{\text {ridge }}</script></span> are just a scaled version of <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{O L S}</span><script type="math/tex">\hat{\beta}^{O L S}</script></span> Âú®Ê≠£‰∫§ËæìÂÖ•ÁöÑÊÉÖÂΩ¢‰∏ãÔºåÂ≤≠ÂõûÂΩí‰º∞ËÆ°‰ªÖ‰ªÖÊòØÊúÄÂ∞è‰∫å‰πò‰º∞ËÆ°ÁöÑÁº©Â∞èÁâàÊú¨, Âç≥ <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {ridge}}=\frac{1}{1+\lambda}\hat{\beta}^{O L S}</span><script type="math/tex">\hat{\beta}^{\text {ridge}}=\frac{1}{1+\lambda}\hat{\beta}^{O L S}</script></span></li>
<li>Equivalent Bayesian context - <strong>Gaussian prior</strong> for <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> ÂèØ‰ª•ËØÅÊòéÂ≤≠ÂõûÂΩíÁ≠â‰ª∑‰∫éÂèÇÊï∞Âú®È´òÊñØÂÖàÈ™å‰∏ãÁöÑÂêéÈ™å‰º∞ËÆ°, ‰∏îÊª°Ë∂≥ÂÖ≥Á≥ª <span class="arithmatex"><span class="MathJax_Preview">\lambda = \sigma^2 / \tau^2</span><script type="math/tex">\lambda = \sigma^2 / \tau^2</script></span>, ÂÖ∂‰∏≠ <span class="arithmatex"><span class="MathJax_Preview">\sigma^2</span><script type="math/tex">\sigma^2</script></span> ‰∏∫Âô™Â£∞È°πÊñπÂ∑Æ, <span class="arithmatex"><span class="MathJax_Preview">\tau^2</span><script type="math/tex">\tau^2</script></span> ‰∏∫priorÈ´òÊñØÊñπÂ∑Æ.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
X \hat{\beta}^{\text {ridge }}=\sum_{j=1}^{p} \mu_{j} \frac{d_{j}^{2}}{d_{j}^{2}+\lambda} \mu_{j}^{\top} y
</div>
<script type="math/tex; mode=display">
X \hat{\beta}^{\text {ridge }}=\sum_{j=1}^{p} \mu_{j} \frac{d_{j}^{2}}{d_{j}^{2}+\lambda} \mu_{j}^{\top} y
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">u_{j}</span><script type="math/tex">u_{j}</script></span> are the columns of <span class="arithmatex"><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> where <span class="arithmatex"><span class="MathJax_Preview">X=U D V^{\top}</span><script type="math/tex">X=U D V^{\top}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">d_{j}</span><script type="math/tex">d_{j}</script></span> are the singular values of <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span></li>
<li>Greater amount of shrinkage is applied to the coordinates of basis vectors with smaller <span class="arithmatex"><span class="MathJax_Preview">d_{j}</span><script type="math/tex">d_{j}</script></span> ÂØπ‰∫éÊñπÂ∑ÆË∂äÂ∞èÁöÑ‰∏ªÊàêÂàÜ, ridge ÁöÑÊî∂Áº©Ë∂äÂéâÂÆ≥</li>
<li>effective degrees of freedom <span class="arithmatex"><span class="MathJax_Preview">d f(\lambda)=\sum_{j=1}^{p} \frac{d_{j}^{2}}{d_{j}^{2}+\lambda}</span><script type="math/tex">d f(\lambda)=\sum_{j=1}^{p} \frac{d_{j}^{2}}{d_{j}^{2}+\lambda}</script></span></li>
</ul>
<p>ËßÇÂØü‰∏äÈù¢ÁöÑÂºèÂ≠ê, <span class="arithmatex"><span class="MathJax_Preview">\mu_{j}^{\top} y</span><script type="math/tex">\mu_{j}^{\top} y</script></span> Â∞ÜyÊäïÂΩ±Âà∞ X ÁöÑÂêÑ‰∏™Â•áÂºÇÂêëÈáè(‰∏ªÊàêÂàÜ), ÁÑ∂ÂêéÊ†πÊçÆÂØπÂ∫îÁöÑÂ•áÂºÇÂÄº(<span class="arithmatex"><span class="MathJax_Preview">X^TX</span><script type="math/tex">X^TX</script></span>ÁöÑÁâπÂæÅÂÄº) ËøõË°åÊî∂Áº©, Ê†πÊçÆÂàÜÂºèÂèØÁü•Â•áÂºÇÂÄºË∂äÂ∞èÁöÑÊî∂Áº©Ë∂äÂéâÂÆ≥.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned} \mathrm{df}(\lambda) &amp;=\operatorname{tr}\left[\mathbf{X}\left(\mathbf{X}^{\mathbf{T}} \mathbf{X}+\lambda \mathbf{I}\right)^{-1} \mathbf{X}^{T}\right] \\ &amp;=\operatorname{tr}\left(\mathbf{H}_{\lambda}\right) \\ &amp;=\sum_{j=1}^{p} \frac{d_{j}^{2}}{d_{j}^{2}+\lambda} \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \mathrm{df}(\lambda) &=\operatorname{tr}\left[\mathbf{X}\left(\mathbf{X}^{\mathbf{T}} \mathbf{X}+\lambda \mathbf{I}\right)^{-1} \mathbf{X}^{T}\right] \\ &=\operatorname{tr}\left(\mathbf{H}_{\lambda}\right) \\ &=\sum_{j=1}^{p} \frac{d_{j}^{2}}{d_{j}^{2}+\lambda} \end{aligned}
</script>
</div>
<p>ÊòØÂ≤≠ÂõûÂΩíÁöÑÊúâÊïàËá™Áî±Â∫¶, <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> ÁöÑÈÄíÂáèÂáΩÊï∞.</p>
<p><img alt="" src="../media/ASL-note1/2021-11-29-17-38-39.png" /></p>
<h4 id="the-lasso">The LASSO<a class="headerlink" href="#the-lasso" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex"><span class="MathJax_Preview">L_{2}</span><script type="math/tex">L_{2}</script></span> penalty <span class="arithmatex"><span class="MathJax_Preview">\sum_{j=1}^{p} \beta_{j}^{2}</span><script type="math/tex">\sum_{j=1}^{p} \beta_{j}^{2}</script></span> is replaced by <span class="arithmatex"><span class="MathJax_Preview">L_{1}</span><script type="math/tex">L_{1}</script></span> penalty <span class="arithmatex"><span class="MathJax_Preview">\sum_{j=1}^{p}\left|\beta_{j}\right|</span><script type="math/tex">\sum_{j=1}^{p}\left|\beta_{j}\right|</script></span>.
<span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {lasso }}=\operatorname{argmin}_{\beta}\left\{\frac{1}{2} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|\right\}</span><script type="math/tex">\hat{\beta}^{\text {lasso }}=\operatorname{argmin}_{\beta}\left\{\frac{1}{2} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|\right\}</script></span>
Which is equivalent to
<span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {lasso }}=\operatorname{argmin}_{\beta} \frac{1}{2} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}</span><script type="math/tex">\hat{\beta}^{\text {lasso }}=\operatorname{argmin}_{\beta} \frac{1}{2} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}</script></span> subject to <span class="arithmatex"><span class="MathJax_Preview">\sum_{j=1}^{p}\left|\beta_{j}\right| \leqslant t</span><script type="math/tex">\sum_{j=1}^{p}\left|\beta_{j}\right| \leqslant t</script></span></p>
<ul>
<li>Non-linear in y</li>
<li>No closed form expression</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {lasso }}</span><script type="math/tex">\hat{\beta}^{\text {lasso }}</script></span> becomes <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {OLS }}</span><script type="math/tex">\hat{\beta}^{\text {OLS }}</script></span> when <span class="arithmatex"><span class="MathJax_Preview">t&gt;\sum_{j=1}^{p}\left|\beta_{j}^{\text {OLS }}\right|</span><script type="math/tex">t>\sum_{j=1}^{p}\left|\beta_{j}^{\text {OLS }}\right|</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {lasso }}=0</span><script type="math/tex">\hat{\beta}^{\text {lasso }}=0</script></span> when <span class="arithmatex"><span class="MathJax_Preview">t \rightarrow 0</span><script type="math/tex">t \rightarrow 0</script></span></li>
<li>Continuous subset selection.</li>
</ul>
<h4 id="comparisons-orthonormal-inputs">Comparisons - Orthonormal Inputs<a class="headerlink" href="#comparisons-orthonormal-inputs" title="Permanent link">&para;</a></h4>
<p>Âú® X Ê≠£‰∫§ÁöÑÊÉÖÂÜµ, ‰∏âËÄÖÈÉΩÊúâÊòæÂºèËß£.</p>
<p><img alt="" src="../media/ASL-note1/2021-12-09-15-42-21.png" /></p>
<ul>
<li>Best Subset<ul>
<li>Drop all variables with coefficients smaller than the M-th</li>
<li>&ldquo;hard-thresholding.</li>
</ul>
</li>
<li>Ridge<ul>
<li>Proportional shrinkage</li>
</ul>
</li>
<li>LASSO<ul>
<li>translates by a constant factor <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>, truncating at zero.</li>
<li>&ldquo;soft thresholding&rdquo;</li>
</ul>
</li>
</ul>
<h4 id="comparisons-nonorthogonal-inputs">Comparisons - Nonorthogonal Inputs<a class="headerlink" href="#comparisons-nonorthogonal-inputs" title="Permanent link">&para;</a></h4>
<p><img alt="" src="../media/ASL-note1/2021-12-09-15-45-11.png" /></p>
<h4 id="generalization-of-ridge-and-lasso">Generalization of Ridge and LASSO<a class="headerlink" href="#generalization-of-ridge-and-lasso" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
\tilde{\beta}=\operatorname{argmin}_{\beta}\left\{\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|^{q}\right\}, q \geqslant 0
</div>
<script type="math/tex; mode=display">
\tilde{\beta}=\operatorname{argmin}_{\beta}\left\{\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|^{q}\right\}, q \geqslant 0
</script>
</div>
<p><img alt="" src="../media/ASL-note1/2021-12-09-15-48-58.png" /></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">q=0</span><script type="math/tex">q=0</script></span>, subset selection</li>
<li><span class="arithmatex"><span class="MathJax_Preview">0 \leqslant q&lt;1</span><script type="math/tex">0 \leqslant q<1</script></span>, non-convex constraint regions, difficult optimization <span class="arithmatex"><span class="MathJax_Preview">q=1</span><script type="math/tex">q=1</script></span>, LASSO</li>
<li><span class="arithmatex"><span class="MathJax_Preview">1&lt;q&lt;2</span><script type="math/tex">1<q<2</script></span> : &ldquo;compromise&rdquo; between Ridge and LASSO, but not desirable</li>
<li><span class="arithmatex"><span class="MathJax_Preview">q=2</span><script type="math/tex">q=2</script></span>, ridge</li>
</ul>
<p>Elastic net: <span class="arithmatex"><span class="MathJax_Preview">\lambda \sum_{j=1}^{p}\left(\alpha \beta_{j}^{2}+(1-\alpha)\left|\beta_{j}\right|\right), 0 \leqslant \alpha \leqslant 1</span><script type="math/tex">\lambda \sum_{j=1}^{p}\left(\alpha \beta_{j}^{2}+(1-\alpha)\left|\beta_{j}\right|\right), 0 \leqslant \alpha \leqslant 1</script></span></p>
<h4 id="least-angle-regression">Least Angle Regression<a class="headerlink" href="#least-angle-regression" title="Permanent link">&para;</a></h4>
<p>ÂèÇËßÅ <a href="https://blog.csdn.net/guofei_fly/article/details/103845342">ÊúÄÂ∞èËßíÂõûÂΩíÁÆóÊ≥ïÔºàLARSÔºâ</a> ÂõæÁ§∫ÂæàÊ∏ÖÊ•ö</p>
<p>LARÔºàLeast angle regressionÔºåÊúÄÂ∞èËßíÂõûÂΩíÔºâÔºåÁî±EfronÁ≠âÔºà2004ÔºâÊèêÂá∫„ÄÇËøôÊòØ‰∏ÄÁßçÈùûÂ∏∏ÊúâÊïàÁöÑÊ±ÇËß£LASSOÁöÑÁÆóÊ≥ïÔºåÂèØ‰ª•ÂæóÂà∞LASSOÁöÑËß£ÁöÑË∑ØÂæÑ„ÄÇ</p>
<p>Similar to forward stepwise, but only enters &ldquo;as much&rdquo; of a predictor as it deserves.</p>
<ul>
<li>Identifies the variable most correlated with the response</li>
<li>Moves the coef continuously toward its least-squares value</li>
<li>As soon as another variable &ldquo;catches up&rdquo;, stop</li>
<li>The second variable joins the active set</li>
<li>Repeat&hellip;</li>
</ul>
<p>Least Angle Regression - Algorithm</p>
<ul>
<li><strong>Standardize</strong> the predictors to have mean zero and unit norm. Start with the residual <span class="arithmatex"><span class="MathJax_Preview">r=y-\bar{y}, \beta_{1}, \beta_{2}, \ldots, \beta_{p}=0</span><script type="math/tex">r=y-\bar{y}, \beta_{1}, \beta_{2}, \ldots, \beta_{p}=0</script></span>.</li>
<li>Find the predictor <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span> most <strong>correlated</strong> with <span class="arithmatex"><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span></li>
<li>Move <span class="arithmatex"><span class="MathJax_Preview">\beta_{j}</span><script type="math/tex">\beta_{j}</script></span> from 0 towards its least-squares coefficient <span class="arithmatex"><span class="MathJax_Preview">\left\langle x_{j}, r\right\rangle</span><script type="math/tex">\left\langle x_{j}, r\right\rangle</script></span> , until some other competitor <span class="arithmatex"><span class="MathJax_Preview">x_{k}</span><script type="math/tex">x_{k}</script></span> has as much correlation with the current residual as does <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span></li>
<li>Move <span class="arithmatex"><span class="MathJax_Preview">\beta_{j}</span><script type="math/tex">\beta_{j}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\beta_{k}</span><script type="math/tex">\beta_{k}</script></span> in the direction defined by their <strong>joint least squares coefficient</strong> of the current residual on <span class="arithmatex"><span class="MathJax_Preview">\left(x_{j}, x_{k}\right)</span><script type="math/tex">\left(x_{j}, x_{k}\right)</script></span> until some other competitor <span class="arithmatex"><span class="MathJax_Preview">x_{l}</span><script type="math/tex">x_{l}</script></span> has as much correlation with the current residual</li>
<li>Continue in this way until all <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> predictors have been entered. After <span class="arithmatex"><span class="MathJax_Preview">\min (N-1, p)</span><script type="math/tex">\min (N-1, p)</script></span> steps, we arrive at the full least-squares solution.</li>
</ul>
<p><img alt="" src="../media/ASL-note1/2021-12-09-15-58-56.png" /></p>
<p><img alt="" src="../media/ASL-note1/2021-12-09-15-59-21.png" /></p>
<h3 id="methods-using-derived-input-directions">Methods Using Derived Input Directions<a class="headerlink" href="#methods-using-derived-input-directions" title="Permanent link">&para;</a></h3>
<p>Âõæ 3.18 ÊòæÁ§∫‰∫Ü‰∏çÂêåÊñπÊ≥ï‰∏ãÂΩìÂÆÉ‰ª¨ÊÉ©ÁΩöÂèÇÊï∞ÊîπÂèòÊó∂ÁöÑÁ≥ªÊï∞Êõ≤Á∫øÔºé‰∏äÂõæ œÅ = 0.5Ôºå‰∏ãÂõæ œÅ = ‚àí0.5ÔºéÂ≤≠ÂõûÂΩíÂíå lasso ÁöÑÊÉ©ÁΩöÂèÇÊï∞Âú®‰∏Ä‰∏™ËøûÁª≠ÁöÑÂå∫ÂüüÂÜÖÂèòÂåñÔºåËÄåÊúÄ‰ºòÂ≠êÈõÜÔºåPLS Âíå PCR Âè™Ë¶Å‰∏§‰∏™Á¶ªÊï£ÁöÑÊ≠•È™§‰æøËææÂà∞‰∫ÜÊúÄÂ∞è‰∫å‰πòËß£ÔºéÂú®‰∏äÈù¢ÁöÑÂõæ‰∏≠Ôºå‰ªéÈõ∂ÁÇπÂºÄÂßãÔºåÂ≤≠ÂõûÂΩíÊï¥‰ΩìÊî∂Áº©ÂèÇÊï∞Áõ¥Âà∞ÊúÄÂêéÊî∂Áº©Âà∞ÊúÄÂ∞è‰∫å‰πòÔºéÂ∞ΩÁÆ° PLS Âíå PCR ÊòØÁ¶ªÊï£ ÁöÑ‰∏îÊõ¥Âä†ÊûÅÁ´ØÔºå‰ΩÜÂÆÉ‰ª¨ÊòæÁ§∫‰∫ÜÁ±ª‰ººÂ≤≠ÂõûÂΩíÁöÑË°å‰∏∫ÔºéÊúÄ‰ºòÂ≠êÈõÜË∂ÖÂá∫Ëß£ÁÑ∂ÂêéÂõûÊ∫ØÔºé <strong>lasso ÁöÑË°å‰∏∫ÊòØÂÖ∂‰ªñÊñπÊ≥ïÁöÑËøáÊ∏°</strong>ÔºéÂΩìÁõ∏ÂÖ≥Á≥ªÊï∞‰∏∫Ë¥üÊï∞Êó∂Ôºà‰∏ãÂõæÔºâÔºåPLS Âíå PCR ÂÜç‰∏ÄÊ¨°Â§ßËá¥Âú∞Ë∑üÈöèÂ≤≠ÂõûÂΩíÁöÑË∑ØÂæÑÔºåËÄåÊâÄÊúâÁöÑÊñπÊ≥ïÈÉΩÊõ¥Âä†Áõ∏‰ººÔºé</p>
<p>ÊØîËæÉ‰∏çÂêåÊñπÊ≥ïÁöÑÊî∂Áº©Ë°å‰∏∫ÊòØÂæàÊúâË∂£ÁöÑÔºéÂ≤≠ÂõûÂΩíÂØπÊâÄÊúâÊñπÂêëÈÉΩÊúâÊî∂Áº©‰ΩÜÂú®‰ΩéÊñπÂ∑ÆÊñπÂêëÊî∂Áº©Á®ãÂ∫¶Êõ¥ÂéâÂÆ≥Ôºé‰∏ªÊàêÂàÜÂõûÂΩíÂ∞Ü M ‰∏™È´òÊñπÂ∑ÆÁöÑÊñπÂêëÂçïÁã¨ÂèñÂá∫Êù•ÔºåÁÑ∂Âêé‰∏¢ÊéâÂâ©‰∏ãÁöÑÔºéÊúâË∂£ÁöÑÊòØÔºåÂèØ‰ª•ËØÅÊòé<strong>ÂÅèÊúÄÂ∞è‰∫å‰πò‰πüË∂ãÂêë‰∫éÊî∂Áº©‰ΩéÊñπÂ∑ÆÁöÑÊñπÂêë</strong>Ôºå‰ΩÜÊòØÂÆûÈôÖ‰∏ä‰ºö‰ΩøÂæóÊüê‰∫õÈ´òÊñπÂ∑ÆÊñπÂêëËÜ®ËÉÄÔºéËøô‰ΩøÂæó PLS Á®çÂæÆ‰∏çÂ§™Á®≥ÂÆöÔºåÂõ†Ê≠§Áõ∏ÊØî‰∫éÂ≤≠ÂõûÂΩí‰ºöÊúâËæÉÂ§ßÁöÑÈ¢ÑÊµãËØØÂ∑ÆÔºéÊï¥‰∏™Á†îÁ©∂Áî± Frank and Friedman (1993) ÁªôÂá∫Ôºé‰ªñ‰ª¨ÊÄªÁªìÂà∞<strong>ÂØπ‰∫éÊúÄÂ∞èÂåñÈ¢ÑÊµãËØØÂ∑ÆÔºåÂ≤≠ÂõûÂΩí‰∏ÄËà¨ÊØîÂèòÈáèÂ≠êÈõÜÈÄâÊã©„ÄÅ‰∏ªÊàêÂàÜÂõûÂΩíÂíåÂÅèÊúÄÂ∞è‰∫å‰πòÊõ¥Â•Ω</strong>ÔºéÁÑ∂ËÄåÔºåÁõ∏ÂØπ‰∫éÂêé‰∏§ÁßçÊñπÊ≥ïÁöÑÊèêÈ´òÂè™ÊòØÂæàÂ∞èÁöÑÔºé</p>
<p>ÊÄªÁªì‰∏Ä‰∏ãÔºåPLSÔºåPCR ‰ª•ÂèäÂ≤≠ÂõûÂΩíË∂ãÂêë‰∫éË°®Áé∞‰∏ÄËá¥ÔºéÂ≤≠ÂõûÂΩíÂèØËÉΩ‰ºöÊõ¥Â•ΩÔºåÂõ†‰∏∫ÂÆÉÊî∂Áº©ÂæóÂæàÂÖâÊªëÔºå‰∏çÂÉèÁ¶ªÊï£Ê≠•È™§‰∏≠‰∏ÄÊ†∑Ôºé<strong>Lasso ‰ªã‰∫éÂ≤≠ÂõûÂΩíÂíåÊúÄ‰ºòÂ≠êÈõÜÂõûÂΩí‰∏≠Èó¥ÔºåÂπ∂‰∏îÊúâ‰∏§ËÄÖÁöÑÈÉ®ÂàÜÊÄßË¥®</strong>Ôºé</p>
<p><img alt="" src="../media/ASL-note1/2021-12-09-16-36-40.png" /></p>
<h4 id="principle-components-regression-pcr">Principle Components Regression ‰∏ªÊàêÂàÜÂõûÂΩí PCR<a class="headerlink" href="#principle-components-regression-pcr" title="Permanent link">&para;</a></h4>
<p>Eigen decomposition of <span class="arithmatex"><span class="MathJax_Preview">X^{\top} X: X^{\top} X=V D^{2} V^{\top}</span><script type="math/tex">X^{\top} X: X^{\top} X=V D^{2} V^{\top}</script></span></p>
<ul>
<li>Eigen vector <span class="arithmatex"><span class="MathJax_Preview">v_{j}</span><script type="math/tex">v_{j}</script></span> (column of <span class="arithmatex"><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> ): principal components directions</li>
<li><span class="arithmatex"><span class="MathJax_Preview">z_{j}=X v_{j}:</span><script type="math/tex">z_{j}=X v_{j}:</script></span> principle components</li>
<li><span class="arithmatex"><span class="MathJax_Preview">z_{1}</span><script type="math/tex">z_{1}</script></span> has the largest sample variance amongst all normalized linear combinations of the columns of <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>.</li>
<li>Derived input columns <span class="arithmatex"><span class="MathJax_Preview">\left(z_{1}, z_{2}, \ldots, z_{M}\right)</span><script type="math/tex">\left(z_{1}, z_{2}, \ldots, z_{M}\right)</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">M \leqslant p</span><script type="math/tex">M \leqslant p</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{y}_{(M)}^{p c r}=\bar{y} \mathbb{I}+\sum_{m=1}^{M} \hat{\theta}_{m} z_{m}</span><script type="math/tex">\hat{y}_{(M)}^{p c r}=\bar{y} \mathbb{I}+\sum_{m=1}^{M} \hat{\theta}_{m} z_{m}</script></span></li>
</ul>
<p><img alt="" src="../media/ASL-note1/2021-12-09-16-09-16.png" /></p>
<ul>
<li>ÂΩì <span class="arithmatex"><span class="MathJax_Preview">M=p</span><script type="math/tex">M=p</script></span> Êó∂Â∞±ÊòØ‰∏ÄËà¨ÁöÑÊúÄÂ∞è‰∫å‰πò‰º∞ËÆ°, ËÄåÂØπ‰∫é <span class="arithmatex"><span class="MathJax_Preview">M&lt;p</span><script type="math/tex">M<p</script></span> Êàë‰ª¨ÂæóÂà∞‰∏Ä‰∏™ÈôçÁª¥ÁöÑÂõûÂΩíÈóÆÈ¢ò.</li>
<li>Á±ª‰ºº‰∏äÈù¢ÁöÑ Ridge, ÈÉΩÊòØÂØπ‰∫é PC ËøõË°åÊìç‰Ωú. Ridge ‰∏≠ÂØπ‰∫é‰∏ªÊàêÂàÜÁ≥ªÊï∞ËøõË°å‰∫ÜÊî∂Áº©, ËÄåPCR‰∏¢Êéâ‰∫Ü p-M ‰∏™ÊúÄÂ∞èÁöÑÁâπÂæÅÂÄºÂàÜÈáè.</li>
</ul>
<h4 id="partial-least-squares">Partial Least Squares ÂÅèÊúÄÂ∞è‰∫å‰πò<a class="headerlink" href="#partial-least-squares" title="Permanent link">&para;</a></h4>
<p>Âü∫Êú¨ÁöÑÊÄùË∑ØÂ∞±ÊòØ, ËÆ°ÁÆóÊØè‰∏Ä‰∏™ <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> ÊñπÂêë‰∏äÁöÑÂõûÂΩíÁ≥ªÊï∞, ÁÑ∂ÂêéÂ∞ÜËøô‰∫õÊñπÂêëÊ±áÊÄªËµ∑Êù•ÂæóÂà∞‰∏Ä‰∏™ËôöÊãüÁöÑÂèòÈáèËøõË°åÂõûÂΩí; ÁÑ∂ÂêéÂ∞ÜÊâÄÊúâÁöÑ <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> ÂéªÈô§ËØ•ÊñπÂêë‰∏äÁöÑÂàÜÈáè(Ê≠£‰∫§Âåñ), ÁªßÁª≠, Áõ¥Âà∞ÊâæÂà∞ M ‰∏™.</p>
<p>Use the output <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> (in addition to <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> ) in construction of the derived input</p>
<ul>
<li>Compute <span class="arithmatex"><span class="MathJax_Preview">\hat{\phi}_{1 j}=\left\langle x_{j}, y\right\rangle</span><script type="math/tex">\hat{\phi}_{1 j}=\left\langle x_{j}, y\right\rangle</script></span> for each <span class="arithmatex"><span class="MathJax_Preview">j=1, \ldots, p</span><script type="math/tex">j=1, \ldots, p</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">z_{1}=\sum_{j} \hat{\phi}_{1 j} x_{j}</span><script type="math/tex">z_{1}=\sum_{j} \hat{\phi}_{1 j} x_{j}</script></span> : <strong>first partial least squares direction</strong></li>
<li>Output <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> is regressed on <span class="arithmatex"><span class="MathJax_Preview">z_{1}</span><script type="math/tex">z_{1}</script></span> giving coefficient <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}_{1}</span><script type="math/tex">\hat{\theta}_{1}</script></span> ÊâÄË∞ì„Äåy is regressed on x„ÄçÂ∞±ÊòØÁî®xÊù•È¢ÑÊµãy</li>
<li>Orthogonalize <span class="arithmatex"><span class="MathJax_Preview">x_{1}, \ldots, x_{p}</span><script type="math/tex">x_{1}, \ldots, x_{p}</script></span> w.r.t. <span class="arithmatex"><span class="MathJax_Preview">z_{1}</span><script type="math/tex">z_{1}</script></span></li>
<li>Continue until <span class="arithmatex"><span class="MathJax_Preview">M \leqslant p</span><script type="math/tex">M \leqslant p</script></span> directions have been obtained</li>
</ul>
<p>Note: Each <span class="arithmatex"><span class="MathJax_Preview">z_{m}</span><script type="math/tex">z_{m}</script></span> is weighted by the strength of their univariate effect on <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span></p>
<p>Partial Least Squares - Algorithm</p>
<ul>
<li>Standardize each <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span> to have mean zero and variance one. Set <span class="arithmatex"><span class="MathJax_Preview">\hat{y}^{(0)}=\bar{y}, x_{j}^{(0)}=x_{j}, j=1, \ldots, p .</span><script type="math/tex">\hat{y}^{(0)}=\bar{y}, x_{j}^{(0)}=x_{j}, j=1, \ldots, p .</script></span></li>
<li>For <span class="arithmatex"><span class="MathJax_Preview">m=1,2, \ldots, p</span><script type="math/tex">m=1,2, \ldots, p</script></span>
  1. <span class="arithmatex"><span class="MathJax_Preview">z_{m}=\sum_{j=1}^{p} \hat{\phi}_{m j} x_{j}^{(m-1)}</span><script type="math/tex">z_{m}=\sum_{j=1}^{p} \hat{\phi}_{m j} x_{j}^{(m-1)}</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">\hat{\phi}_{m j}=\left\langle x_{j}^{(m-1)}, y\right\rangle</span><script type="math/tex">\hat{\phi}_{m j}=\left\langle x_{j}^{(m-1)}, y\right\rangle</script></span>
  2. <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}_{m}=\frac{z_{m, y}}{z_{m}, z_{m}}</span><script type="math/tex">\hat{\theta}_{m}=\frac{z_{m, y}}{z_{m}, z_{m}}</script></span>
  3. <span class="arithmatex"><span class="MathJax_Preview">\hat{y}^{(m)}=\hat{y}^{(m-1)}+\hat{\theta}_{m} z_{m}</span><script type="math/tex">\hat{y}^{(m)}=\hat{y}^{(m-1)}+\hat{\theta}_{m} z_{m}</script></span>
  4. Orthogonalize each <span class="arithmatex"><span class="MathJax_Preview">x_{j}^{(m-1)}</span><script type="math/tex">x_{j}^{(m-1)}</script></span> w.r.t. <span class="arithmatex"><span class="MathJax_Preview">z_{m}: x_{j}^{(m)}=x_{j}^{(m-1)} - \frac{z_{m}, x_{j}^{(m-1)}}{z_{m}, z_{m}} z_{m}</span><script type="math/tex">z_{m}: x_{j}^{(m)}=x_{j}^{(m-1)} - \frac{z_{m}, x_{j}^{(m-1)}}{z_{m}, z_{m}} z_{m}</script></span>, <span class="arithmatex"><span class="MathJax_Preview">j=1, \ldots, p</span><script type="math/tex">j=1, \ldots, p</script></span></li>
<li>Output the sequence of fitted vectors <span class="arithmatex"><span class="MathJax_Preview">\left\{\hat{y}^{(m)}\right\}</span><script type="math/tex">\left\{\hat{y}^{(m)}\right\}</script></span>, recover coef <span class="arithmatex"><span class="MathJax_Preview">\hat{y}^{(m)}=X \hat{\beta}^{P L S}(m)</span><script type="math/tex">\hat{y}^{(m)}=X \hat{\beta}^{P L S}(m)</script></span></li>
</ul>
<h4 id="comparison-pcr-pls">Comparison - PCR \&amp; PLS<a class="headerlink" href="#comparison-pcr-pls" title="Permanent link">&para;</a></h4>
<p>PCR: seeks directions that have high variance</p>
<ul>
<li>The <span class="arithmatex"><span class="MathJax_Preview">m_{\text {th }}</span><script type="math/tex">m_{\text {th }}</script></span> principal component direction <span class="arithmatex"><span class="MathJax_Preview">\nu_{m}</span><script type="math/tex">\nu_{m}</script></span> solves</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\max _{\alpha} \operatorname{Var}(X \alpha)</span><script type="math/tex">\max _{\alpha} \operatorname{Var}(X \alpha)</script></span></li>
<li>subject to <span class="arithmatex"><span class="MathJax_Preview">\|\alpha\|=1, \alpha^{\top} S \nu_{l}=0, I=1, \ldots, m-1</span><script type="math/tex">\|\alpha\|=1, \alpha^{\top} S \nu_{l}=0, I=1, \ldots, m-1</script></span></li>
</ul>
<p>PLS: seeks directions that have high variance \&amp; high correlation with the output</p>
<ul>
<li>The <span class="arithmatex"><span class="MathJax_Preview">m_{t h}</span><script type="math/tex">m_{t h}</script></span> PLS direction <span class="arithmatex"><span class="MathJax_Preview">\hat{\phi}_{m}</span><script type="math/tex">\hat{\phi}_{m}</script></span> solves</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\max _{\alpha} \operatorname{Corr}^{2}(y, X \alpha) \operatorname{Var}(X \alpha)</span><script type="math/tex">\max _{\alpha} \operatorname{Corr}^{2}(y, X \alpha) \operatorname{Var}(X \alpha)</script></span></li>
<li>subject to <span class="arithmatex"><span class="MathJax_Preview">\|\alpha\|=1, \alpha^{\top} S \hat{\phi}_{1}=0, I=1, \ldots, m-1</span><script type="math/tex">\|\alpha\|=1, \alpha^{\top} S \hat{\phi}_{1}=0, I=1, \ldots, m-1</script></span></li>
</ul>
<p>Note: the variance aspect tends to dominate, so PLS behaves much like ridge and <span class="arithmatex"><span class="MathJax_Preview">P C R</span><script type="math/tex">P C R</script></span></p>
<h2 id="some-algebra">Some Algebra<a class="headerlink" href="#some-algebra" title="Permanent link">&para;</a></h2>
<p>ÂÖ≥‰∫éÂ•áÂºÇÂÄºÂàÜËß£, ÂèÇËßÅ <a href="https://zhuanlan.zhihu.com/p/26306568">Â•áÂºÇÂÄºÂàÜËß£ÁöÑÊè≠ÁßòÔºà‰∏ÄÔºâÔºöÁü©ÈòµÁöÑÂ•áÂºÇÂÄºÂàÜËß£ËøáÁ®ã</a></p>
<ul>
<li>ÊñπÈòµÁöÑÂØπËßíÂåñÂàÜËß£/ÁâπÂæÅÂàÜËß£ <span class="arithmatex"><span class="MathJax_Preview">A=U\Lambda U^{-1}</span><script type="math/tex">A=U\Lambda U^{-1}</script></span>, ÂÖ∂‰∏≠UÁöÑÊØè‰∏ÄÂàóÊòØÁâπÂæÅÂêëÈáè<ul>
<li>ÂÆûÂØπÁß∞ÈòµÁöÑÁâπÂæÅÂÄºÈÉΩÊòØÂÆûÊï∞, ‰∏îÊúâN‰∏™Á∫øÊÄßÊó†ÂÖ≥ÁöÑÁâπÂæÅÂêëÈáè, Âπ∂‰∏îËøô‰∫õÁâπÂæÅÂêëÈáèÂèØ‰ª•<strong>Ê≠£‰∫§Âçï‰ΩçÂåñ</strong>‰∏∫‰∏ÄÁªÑÊ≠£‰∫§Âçï‰ΩçÂêëÈáè, Âõ†Ê≠§ÂèØÂàÜËß£‰∏∫ <span class="arithmatex"><span class="MathJax_Preview">A=Q\Lambda Q^{-1}= Q\Lambda Q</span><script type="math/tex">A=Q\Lambda Q^{-1}= Q\Lambda Q</script></span> ÂÖ∂‰∏≠Q‰∏∫Ê≠£‰∫§Èòµ (Áü©ÈòµAÁöÑ <strong>ÂØπÁß∞ÂØπËßíÂåñÂàÜËß£</strong>)</li>
</ul>
</li>
<li>Â•áÂºÇÂÄºÂàÜËß£ <span class="arithmatex"><span class="MathJax_Preview">A=P\Sigma Q</span><script type="math/tex">A=P\Sigma Q</script></span><ul>
<li>ËøôÈáåÁöÑPÂíåQÂàÜÂà´ÊòØÂ∑¶Â•áÂºÇÂêëÈáè, ÂàÜÂà´ÊòØ <span class="arithmatex"><span class="MathJax_Preview">AA^T=P\Lambda_1 P^T, A^TA=Q\Lambda_2 Q^T</span><script type="math/tex">AA^T=P\Lambda_1 P^T, A^TA=Q\Lambda_2 Q^T</script></span>, ËøôÈáå‰∏§‰∏™ÁâπÂæÅÂÄºÂØπËßíÈòµ‰∏≠, ÈùûÈõ∂ÁâπÂæÅÂÄºÁõ∏Âêå (Ê≥®ÊÑèÂà∞ AB Âíå BA ÁöÑÁâπÂæÅÂÄºÁõ∏Âêå)</li>
<li>Â•áÂºÇÂÄºÂíåÁâπÂæÅÂÄºÂÖ≥Á≥ª: <span class="arithmatex"><span class="MathJax_Preview">AA^T=P\Sigma^2 P^T</span><script type="math/tex">AA^T=P\Sigma^2 P^T</script></span>, Âõ†Ê≠§Êúâ <span class="arithmatex"><span class="MathJax_Preview">\Sigma^2=\Lambda_1</span><script type="math/tex">\Sigma^2=\Lambda_1</script></span>, ‰πüÂç≥Â•áÂºÇÂÄºÊòØ <span class="arithmatex"><span class="MathJax_Preview">AA^T</span><script type="math/tex">AA^T</script></span> ÁöÑÁâπÂæÅÂÄºÁöÑÂπ≥ÊñπÊ†π (Ê≥®ÊÑè <span class="arithmatex"><span class="MathJax_Preview">AA^T</span><script type="math/tex">AA^T</script></span> ‰∏ÄÂÆöÊòØ<strong>Ê≠£ÂÆö</strong>ÁöÑ)</li>
</ul>
</li>
<li>ÈôÑ: „ÄåÊ≠£ÂÆöÈòµ„ÄçÊòØÂú®ÂÆûÂØπÁß∞ÈòµÁöÑÂü∫Á°Ä‰∏äÂÆö‰πâÁöÑ</li>
</ul>
<h3 id="eigen-decomposition-square-matrix">Eigen decomposition (square matrix)<a class="headerlink" href="#eigen-decomposition-square-matrix" title="Permanent link">&para;</a></h3>
<ul>
<li>A: eigenvalues <span class="arithmatex"><span class="MathJax_Preview">\lambda_{1}, \ldots, \lambda_{k}</span><script type="math/tex">\lambda_{1}, \ldots, \lambda_{k}</script></span>, eigenvectors <span class="arithmatex"><span class="MathJax_Preview">\nu_{1}, \ldots, \nu_{k}</span><script type="math/tex">\nu_{1}, \ldots, \nu_{k}</script></span></li>
<li>Let <span class="arithmatex"><span class="MathJax_Preview">D \equiv\left[\begin{array}{c}\lambda_{1}, 0, \ldots, 0 \\ 0, \lambda_{2}, \ldots, 0 \\ \ldots, \ldots, \ldots, \ldots \\ 0,0, \ldots, \lambda_{k}\end{array}\right], P \equiv\left[\nu_{1}, \nu_{2}, \ldots, \nu_{k}\right]</span><script type="math/tex">D \equiv\left[\begin{array}{c}\lambda_{1}, 0, \ldots, 0 \\ 0, \lambda_{2}, \ldots, 0 \\ \ldots, \ldots, \ldots, \ldots \\ 0,0, \ldots, \lambda_{k}\end{array}\right], P \equiv\left[\nu_{1}, \nu_{2}, \ldots, \nu_{k}\right]</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">A=P D P^{-1}, A^{2}=P D^{2} P^{-1}, \ldots, A^{n}=P D^{n} P^{-1}</span><script type="math/tex">A=P D P^{-1}, A^{2}=P D^{2} P^{-1}, \ldots, A^{n}=P D^{n} P^{-1}</script></span></li>
</ul>
<h3 id="singular-value-decomposition-any-matrix-m-times-nm-times-n">Singular value decomposition (any matrix <span class="arithmatex"><span class="MathJax_Preview">m \times n</span><script type="math/tex">m \times n</script></span> )<a class="headerlink" href="#singular-value-decomposition-any-matrix-m-times-nm-times-n" title="Permanent link">&para;</a></h3>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">A=U \Sigma V^{\top}</span><script type="math/tex">A=U \Sigma V^{\top}</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">U_{m \times n}, \Sigma_{n \times n}</span><script type="math/tex">U_{m \times n}, \Sigma_{n \times n}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">V_{n \times n}</span><script type="math/tex">V_{n \times n}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> is orthonormal and span the column space of <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>,</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\Sigma</span><script type="math/tex">\Sigma</script></span> is diagonal with singular values of <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> has eigen vectors of <span class="arithmatex"><span class="MathJax_Preview">A^{\top} A</span><script type="math/tex">A^{\top} A</script></span> in the column</li>
</ul>
<p>‰πüÂèØ‰ª•ÂÜôÊàê‰∏ãÈù¢ÁöÑÂΩ¢Âºè:</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">U \in \mathbb{R}^{m \times m}</span><script type="math/tex">U \in \mathbb{R}^{m \times m}</script></span> orthonormal, <span class="arithmatex"><span class="MathJax_Preview">U U^{\top}=I</span><script type="math/tex">U U^{\top}=I</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">V \in \mathbb{R}^{n \times n}</span><script type="math/tex">V \in \mathbb{R}^{n \times n}</script></span> orthonormal, <span class="arithmatex"><span class="MathJax_Preview">V V^{\top}=I</span><script type="math/tex">V V^{\top}=I</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\Sigma \in \mathbb{R}^{m \times n}</span><script type="math/tex">\Sigma \in \mathbb{R}^{m \times n}</script></span> rectangular diagonal, <span class="arithmatex"><span class="MathJax_Preview">\Sigma=\operatorname{diag}\left(\sigma_{1}, \sigma_{2}, \ldots, \sigma_{p}\right)</span><script type="math/tex">\Sigma=\operatorname{diag}\left(\sigma_{1}, \sigma_{2}, \ldots, \sigma_{p}\right)</script></span> with <span class="arithmatex"><span class="MathJax_Preview">\sigma_{1} \geq \sigma_{2} \geq \ldots \geq \sigma_{p} \geq 0, p=\min (m, n)</span><script type="math/tex">\sigma_{1} \geq \sigma_{2} \geq \ldots \geq \sigma_{p} \geq 0, p=\min (m, n)</script></span></li>
<li>( <span class="arithmatex"><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> : left singular vector, <span class="arithmatex"><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> : right singular vector, <span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> &lsquo;s: singular values)</li>
</ul>
<p><strong>Theorem</strong>: <strong>ÁªôÂÆö‰∏Ä‰∏™ÂÆûÊï∞Áü©Èòµ, ÊÄªËÉΩÊâæÂà∞ÂÖ∂ SVD ÂàÜËß£</strong>, Êª°Ë∂≥ <span class="arithmatex"><span class="MathJax_Preview">\Sigma</span><script type="math/tex">\Sigma</script></span> ‰∏∫ÂØπËßíÈòµÂπ∂‰∏îÂØπËßíÁ∫øÂÖÉÁ¥†ÈùûË¥ü. (Proof is not required)
If <span class="arithmatex"><span class="MathJax_Preview">A \in \mathbb{R}^{m \times n}</span><script type="math/tex">A \in \mathbb{R}^{m \times n}</script></span> is a real matrix, <span class="arithmatex"><span class="MathJax_Preview">\exists</span><script type="math/tex">\exists</script></span> orthonormal matrix <span class="arithmatex"><span class="MathJax_Preview">U \in \mathbb{R}^{m \times m}</span><script type="math/tex">U \in \mathbb{R}^{m \times m}</script></span>, orthornormal matrix <span class="arithmatex"><span class="MathJax_Preview">V \in \mathbb{R}^{n \times n}</span><script type="math/tex">V \in \mathbb{R}^{n \times n}</script></span> and rectangular diagonal matrix <span class="arithmatex"><span class="MathJax_Preview">\Sigma \in \mathbb{R}^{m \times n}</span><script type="math/tex">\Sigma \in \mathbb{R}^{m \times n}</script></span> with descending and non-negative diagonal elements, such that</p>
<div class="arithmatex">
<div class="MathJax_Preview">
A=U \Sigma V^{\top}
</div>
<script type="math/tex; mode=display">
A=U \Sigma V^{\top}
</script>
</div>
<p><img alt="" src="../media/ASL-note1/2021-11-29-17-51-22.png" /></p>
<h4 id="compact-svd">Compact SVD<a class="headerlink" href="#compact-svd" title="Permanent link">&para;</a></h4>
<p>If <span class="arithmatex"><span class="MathJax_Preview">A \in \mathbb{R}^{m \times n}</span><script type="math/tex">A \in \mathbb{R}^{m \times n}</script></span> is a real matrix with <span class="arithmatex"><span class="MathJax_Preview">\operatorname{rank}(A)=r \leq \min (m, n)</span><script type="math/tex">\operatorname{rank}(A)=r \leq \min (m, n)</script></span>, compact SVD:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
A=U_{r} \Sigma_{r} V_{r}^{\top}
</div>
<script type="math/tex; mode=display">
A=U_{r} \Sigma_{r} V_{r}^{\top}
</script>
</div>
<p><span class="arithmatex"><span class="MathJax_Preview">U_{r} \in \mathbb{R}^{m \times r}:</span><script type="math/tex">U_{r} \in \mathbb{R}^{m \times r}:</script></span> first <span class="arithmatex"><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span> columns of <span class="arithmatex"><span class="MathJax_Preview">U \in \mathbb{R}^{m \times m}</span><script type="math/tex">U \in \mathbb{R}^{m \times m}</script></span>
<span class="arithmatex"><span class="MathJax_Preview">V_{r} \in \mathbb{R}^{n \times r}:</span><script type="math/tex">V_{r} \in \mathbb{R}^{n \times r}:</script></span> first <span class="arithmatex"><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span> columns of <span class="arithmatex"><span class="MathJax_Preview">V \in \mathbb{R}^{n \times n}</span><script type="math/tex">V \in \mathbb{R}^{n \times n}</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\Sigma_{r} \in \mathbb{R}^{r \times r}:</span><script type="math/tex">\Sigma_{r} \in \mathbb{R}^{r \times r}:</script></span> first <span class="arithmatex"><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span> diagonal elements of <span class="arithmatex"><span class="MathJax_Preview">\Sigma \in \mathbb{R}^{m \times n}</span><script type="math/tex">\Sigma \in \mathbb{R}^{m \times n}</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\operatorname{rank}\left(\Sigma_{r}\right)=\operatorname{rank}(A)</span><script type="math/tex">\operatorname{rank}\left(\Sigma_{r}\right)=\operatorname{rank}(A)</script></span></p>
<h4 id="truncated-svd">Truncated SVD<a class="headerlink" href="#truncated-svd" title="Permanent link">&para;</a></h4>
<p>If <span class="arithmatex"><span class="MathJax_Preview">A \in \mathbb{R}^{m \times n}</span><script type="math/tex">A \in \mathbb{R}^{m \times n}</script></span> is a real matrix with <span class="arithmatex"><span class="MathJax_Preview">\operatorname{rank}(A)=r \leq \min (m, n), 0&lt;k&lt;r</span><script type="math/tex">\operatorname{rank}(A)=r \leq \min (m, n), 0<k<r</script></span>, truncated SVD:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
A \approx U_{k} \Sigma_{k} V_{k}^{\top}
</div>
<script type="math/tex; mode=display">
A \approx U_{k} \Sigma_{k} V_{k}^{\top}
</script>
</div>
<p><span class="arithmatex"><span class="MathJax_Preview">U_{k} \in \mathbb{R}^{m \times k}</span><script type="math/tex">U_{k} \in \mathbb{R}^{m \times k}</script></span> : first <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> columns of <span class="arithmatex"><span class="MathJax_Preview">U \in \mathbb{R}^{m \times m}</span><script type="math/tex">U \in \mathbb{R}^{m \times m}</script></span>
<span class="arithmatex"><span class="MathJax_Preview">V_{k} \in \mathbb{R}^{n \times k}</span><script type="math/tex">V_{k} \in \mathbb{R}^{n \times k}</script></span> : first <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> columns of <span class="arithmatex"><span class="MathJax_Preview">V \in \mathbb{R}^{n \times n}</span><script type="math/tex">V \in \mathbb{R}^{n \times n}</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\Sigma_{k} \in \mathbb{R}^{k \times k}:</span><script type="math/tex">\Sigma_{k} \in \mathbb{R}^{k \times k}:</script></span> first <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> diagonal elements of <span class="arithmatex"><span class="MathJax_Preview">\Sigma \in \mathbb{R}^{m \times n}</span><script type="math/tex">\Sigma \in \mathbb{R}^{m \times n}</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\operatorname{rank}\left(\Sigma_{k}\right)&lt;\operatorname{rank}(A)</span><script type="math/tex">\operatorname{rank}\left(\Sigma_{k}\right)<\operatorname{rank}(A)</script></span></p>
<h4 id="geometry-of-svd">Geometry of SVD<a class="headerlink" href="#geometry-of-svd" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex"><span class="MathJax_Preview">A \in \mathbb{R}^{m \times n}:</span><script type="math/tex">A \in \mathbb{R}^{m \times n}:</script></span> a linear transformation from <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{n}</span><script type="math/tex">\mathbb{R}^{n}</script></span> to <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{m}</span><script type="math/tex">\mathbb{R}^{m}</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
T: \quad x \rightarrow A x \quad\left(U \Sigma V^{\top} x\right)
</div>
<script type="math/tex; mode=display">
T: \quad x \rightarrow A x \quad\left(U \Sigma V^{\top} x\right)
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">x \in \mathbb{R}^{n}, A x \in \mathbb{R}^{m}</span><script type="math/tex">x \in \mathbb{R}^{n}, A x \in \mathbb{R}^{m}</script></span></li>
<li>A linear transformation can be decomposed into 3 steps: rotation, stretch/scale, rotation again<ul>
<li>coordinates rotated by <span class="arithmatex"><span class="MathJax_Preview">V^{\top}</span><script type="math/tex">V^{\top}</script></span> (orthonormal basis in <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{n}</span><script type="math/tex">\mathbb{R}^{n}</script></span> )</li>
<li>coordinates stretched or scaled by <span class="arithmatex"><span class="MathJax_Preview">\Sigma\left(\sigma_{1}, \sigma_{2}, \ldots, \sigma_{n}\right)</span><script type="math/tex">\Sigma\left(\sigma_{1}, \sigma_{2}, \ldots, \sigma_{n}\right)</script></span></li>
<li>coordinates rotated by <span class="arithmatex"><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> (orthonormal basis in <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{m}</span><script type="math/tex">\mathbb{R}^{m}</script></span> )</li>
</ul>
</li>
</ul>
<p><img alt="" src="../media/ASL-note1/2021-12-04-10-01-01.png" /></p>
<h4 id="characteristics-of-svd">Characteristics of SVD<a class="headerlink" href="#characteristics-of-svd" title="Permanent link">&para;</a></h4>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\begin{aligned} A^{\top} A &amp;=\left(U \Sigma V^{\top}\right)^{\top}\left(U \Sigma V^{\top}\right)=V\left(\Sigma^{\top} \Sigma\right) V^{\top}, \\ A A^{\top} &amp;=\left(U \Sigma V^{\top}\right)\left(U \Sigma V^{\top}\right)^{\top}=U\left(\Sigma \Sigma^{\top}\right) U^{\top} \end{aligned}</span><script type="math/tex">\begin{aligned} A^{\top} A &=\left(U \Sigma V^{\top}\right)^{\top}\left(U \Sigma V^{\top}\right)=V\left(\Sigma^{\top} \Sigma\right) V^{\top}, \\ A A^{\top} &=\left(U \Sigma V^{\top}\right)\left(U \Sigma V^{\top}\right)^{\top}=U\left(\Sigma \Sigma^{\top}\right) U^{\top} \end{aligned}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">A V=U \Sigma U^{\top} V=U \Sigma</span><script type="math/tex">A V=U \Sigma U^{\top} V=U \Sigma</script></span>, i.e., <span class="arithmatex"><span class="MathJax_Preview">A \nu_{j}=\sigma_{j} \mu_{j}, j=1,2, \ldots, n</span><script type="math/tex">A \nu_{j}=\sigma_{j} \mu_{j}, j=1,2, \ldots, n</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">A^{\top} U=V \Sigma^{\top}</span><script type="math/tex">A^{\top} U=V \Sigma^{\top}</script></span>, i.e., <span class="arithmatex"><span class="MathJax_Preview">A^{\top} \mu_{j}=\sigma_{j} \nu_{j}, j=1,2, \ldots, n</span><script type="math/tex">A^{\top} \mu_{j}=\sigma_{j} \nu_{j}, j=1,2, \ldots, n</script></span> and <span class="arithmatex"><span class="MathJax_Preview">A^{\top} \mu_{j}=0</span><script type="math/tex">A^{\top} \mu_{j}=0</script></span>, <span class="arithmatex"><span class="MathJax_Preview">j=n+1, n+2, \ldots, m</span><script type="math/tex">j=n+1, n+2, \ldots, m</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\sigma_{1}, \sigma_{2}, \ldots, \sigma_{n}</span><script type="math/tex">\sigma_{1}, \sigma_{2}, \ldots, \sigma_{n}</script></span> is unique, but <span class="arithmatex"><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> and <span class="arithmatex"><span class="MathJax_Preview">V^{\top}</span><script type="math/tex">V^{\top}</script></span> is not Ê≥®ÊÑèÂà∞, Â•áÂºÇÂêëÈáèÊòØ‰∏çÂîØ‰∏ÄÁöÑ</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{rank}(A)=\operatorname{rank}(\Sigma)=\mathbb{I}\left(\sigma_{i}&gt;0\right)</span><script type="math/tex">\operatorname{rank}(A)=\operatorname{rank}(\Sigma)=\mathbb{I}\left(\sigma_{i}>0\right)</script></span></li>
</ul>
<h4 id="matrix-approximation-and-svd">Matrix Approximation and SVD<a class="headerlink" href="#matrix-approximation-and-svd" title="Permanent link">&para;</a></h4>
<p>SVDÂíåÁü©ÈòµËøë‰ºº: ÂØπ‰∫é‰∏Ä‰∏™Áü©ÈòµA, Ë¶ÅÊ±ÇÂÖ∂Âú®FËåÉÊï∞ÊÑè‰πâ‰∏ãÁöÑ rank-k ÁöÑËøë‰ºº, ËøôÁßçËøë‰ººÁöÑ‰∏ãÁïå‰∏∫ <span class="arithmatex"><span class="MathJax_Preview">\left(\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}</span><script type="math/tex">\left(\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}</script></span>, Âπ∂‰∏îÂèØ‰ª•ÈÄöËøáSVDÁöÑÂâç k ‰∏™Â•áÂºÇÂÄºÁÆóÂá∫Êù•.</p>
<p><span class="arithmatex"><span class="MathJax_Preview">A=\left[a_{i j}\right]_{m \times n}</span><script type="math/tex">A=\left[a_{i j}\right]_{m \times n}</script></span> with <span class="arithmatex"><span class="MathJax_Preview">\|A\|_{F}=\left(\sum_{i=1}^{m} \sum_{j=1}^{n}\left(a_{i j}\right)^{2}\right)^{\frac{1}{2}}=\left(\sigma_{1}^{2}+\sigma_{2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}</span><script type="math/tex">\|A\|_{F}=\left(\sum_{i=1}^{m} \sum_{j=1}^{n}\left(a_{i j}\right)^{2}\right)^{\frac{1}{2}}=\left(\sigma_{1}^{2}+\sigma_{2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}</script></span>, if <span class="arithmatex"><span class="MathJax_Preview">\operatorname{rank}(A)=r</span><script type="math/tex">\operatorname{rank}(A)=r</script></span> and <span class="arithmatex"><span class="MathJax_Preview">A=U \Sigma V^{\top}</span><script type="math/tex">A=U \Sigma V^{\top}</script></span>, denote <span class="arithmatex"><span class="MathJax_Preview">\mathcal{M}</span><script type="math/tex">\mathcal{M}</script></span> the set of all matrix in <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{m \times n}</span><script type="math/tex">\mathbb{R}^{m \times n}</script></span> with rank at most <span class="arithmatex"><span class="MathJax_Preview">k, 0&lt;k&lt;r</span><script type="math/tex">k, 0<k<r</script></span>, if <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> with <span class="arithmatex"><span class="MathJax_Preview">\operatorname{ran} k(X)=k</span><script type="math/tex">\operatorname{ran} k(X)=k</script></span> satisfies</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\|A-X\|_{F}=\min_{S \in \mathcal{M}}\|A-S\|_{F}
</div>
<script type="math/tex; mode=display">
\|A-X\|_{F}=\min_{S \in \mathcal{M}}\|A-S\|_{F}
</script>
</div>
<p>then</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\|A-X\|_{F}=\left(\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}
</div>
<script type="math/tex; mode=display">
\|A-X\|_{F}=\left(\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}
</script>
</div>
<p>Specifically, if <span class="arithmatex"><span class="MathJax_Preview">A^{\prime}=U \Sigma^{\prime} V^{\top}</span><script type="math/tex">A^{\prime}=U \Sigma^{\prime} V^{\top}</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">\Sigma^{\prime}=\left\{\begin{array}{cc}\Sigma_{k} &amp; 0 \\ 0 &amp; 0\end{array}\right\}</span><script type="math/tex">\Sigma^{\prime}=\left\{\begin{array}{cc}\Sigma_{k} & 0 \\ 0 & 0\end{array}\right\}</script></span>, then</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\left\|A-A^{\prime}\right\|_{F}=\left(\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}=\min _{S \in \mathcal{M}}\|A-S\|_{F}
</div>
<script type="math/tex; mode=display">
\left\|A-A^{\prime}\right\|_{F}=\left(\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}=\min _{S \in \mathcal{M}}\|A-S\|_{F}
</script>
</div>
<h3 id="pca-principle-component-analysis">PCA (Principle Component Analysis)<a class="headerlink" href="#pca-principle-component-analysis" title="Permanent link">&para;</a></h3>
<p>ÂèÇËßÅ <a href="https://jozeelin.github.io/2019/08/28/pca/">PCA‰∏ªÊàêÂàÜÂàÜÊûê</a></p>
<ul>
<li>È¶ñÂÖàÂØπÁªôÂÆöÊï∞ÊçÆËøõË°åËßÑËåÉÂåñÔºå‰ΩøÂæóÊï∞ÊçÆÊØè‰∏ÄÂèòÈáèÁöÑÂπ≥ÂùáÂÄº‰∏∫0ÔºåÊñπÂ∑Æ‰∏∫1</li>
<li>ÂØπÊï∞ÊçÆËøõË°åÊ≠£‰∫§ÂèòÊç¢„ÄÇÂéüÊù•Áî±Á∫øÊÄßÁõ∏ÂÖ≥ÂèòÈáèË°®Á§∫ÁöÑÊï∞ÊçÆÔºåÈÄöËøáÊ≠£‰∫§ÂèòÊç¢ÂèòÊàêÁî±Ëã•Âπ≤‰∏™Á∫øÊÄßÊó†ÂÖ≥ÁöÑÊñ∞ÂèòÈáèË°®Á§∫ÁöÑÊï∞ÊçÆ„ÄÇÊñ∞ÂèòÈáèÊòØÂèØËÉΩÁöÑÊ≠£‰∫§ÂèòÊç¢‰∏≠ÂèòÈáèÁöÑÊñπÂ∑ÆÁöÑÂíå(‰ø°ÊÅØ‰øùÂ≠ò)ÊúÄÂ§ßÁöÑÔºåÊñπÂ∑ÆË°®Á§∫Âú®Êñ∞ÂèòÈáè‰∏äÁöÑ‰ø°ÊÅØÁöÑÂ§ßÂ∞è„ÄÇ</li>
</ul>
<p>Êï∞ÊçÆÈõÜÂêà‰∏≠ÁöÑÊ†∑Êú¨Áî±ÂÆûÊï∞Á©∫Èó¥(Ê≠£‰∫§ÂùêÊ†áÁ≥ª)‰∏≠ÁöÑÁÇπË°®Á§∫ÔºåÁ©∫Èó¥ÁöÑ‰∏Ä‰∏™ÂùêÊ†áËΩ¥Ë°®Á§∫‰∏Ä‰∏™ÂèòÈáèÔºåËßÑËåÉÂåñÂ§ÑÁêÜÂêéÂæóÂà∞ÁöÑÊï∞ÊçÆÂàÜÂ∏ÉÂú®ÂéüÁÇπÈôÑËøë„ÄÇÂØπÂéüÂùêÊ†áÁ≥ª‰∏≠ÁöÑÊï∞ÊçÆËøõË°å‰∏ªÊàêÂàÜÂàÜÊûêÁ≠â‰ª∑‰∫éËøõË°åÂùêÊ†áÁ≥ªÊóãËΩ¨ÂèòÊç¢ÔºåÂ∞ÜÊï∞ÊçÆÊäïÂΩ±Âà∞Êñ∞ÂùêÊ†áÁ≥ªÁöÑÂùêÊ†áËΩ¥‰∏äÔºõÊñ∞ÂùêÊ†áÁ≥ªÁöÑÁ¨¨‰∏ÄÂùêÊ†áËΩ¥„ÄÅÁ¨¨‰∫åÂùêÊ†áËΩ¥Á≠âÂàÜÂà´Ë°®Á§∫Á¨¨‰∏Ä‰∏ªÊàêÂàÜ„ÄÅÁ¨¨‰∫å‰∏ªÊàêÂàÜÁ≠â„ÄÇÊï∞ÊçÆÂú®ÊØè‰∏ÄËΩ¥‰∏äÁöÑÂùêÊ†áÂÄºÁöÑÂπ≥ÊñπË°®Á§∫Áõ∏Â∫îÂèòÈáèÁöÑÊñπÂ∑ÆÔºõÂπ∂‰∏îÔºåËøô‰∏™ÂùêÊ†áÁ≥ªÊòØÂú®ÊâÄÊúâÂèØËÉΩÁöÑÊñ∞ÁöÑÂùêÊ†áÁ≥ª‰∏≠ÔºåÂùêÊ†áËΩ¥‰∏äÁöÑÊñπÂ∑ÆÁöÑÂíåÊúÄÂ§ßÁöÑ„ÄÇ</p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-10-47-10.png" /></p>
<ul>
<li>Âú®Êï∞ÊçÆÊÄª‰Ωì‰∏äËøõË°å‰∏ªÊàêÂàÜÂàÜÊûêÁß∞‰∏∫ÊÄª‰Ωì‰∏ªÊàêÂàÜÂàÜÊûê„ÄÇ</li>
<li>Âú®ÊúâÈôêÊ†∑Êú¨‰∏äËøõË°å‰∏ªÊàêÂàÜÂàÜÊûêÁß∞‰∏∫Ê†∑Êú¨‰∏ªÊàêÂàÜÂàÜÊûê„ÄÇ</li>
</ul>
<h4 id="population-pca">Population PCA<a class="headerlink" href="#population-pca" title="Permanent link">&para;</a></h4>
<p>Suppose <span class="arithmatex"><span class="MathJax_Preview">x=\left(x_{1}, x_{2}, \ldots, x_{m}\right)^{\top}</span><script type="math/tex">x=\left(x_{1}, x_{2}, \ldots, x_{m}\right)^{\top}</script></span> is a <span class="arithmatex"><span class="MathJax_Preview">\mathrm{m}</span><script type="math/tex">\mathrm{m}</script></span>-dim random vector, with</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;\mu=E(x)=\left(\mu_{1}, \mu_{2}, \ldots, \mu_{m}\right)^{\top} \\
&amp;\Sigma=\operatorname{cov}(x, x)=E\left[(x-\mu)(x-\mu)^{\top}\right]
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&\mu=E(x)=\left(\mu_{1}, \mu_{2}, \ldots, \mu_{m}\right)^{\top} \\
&\Sigma=\operatorname{cov}(x, x)=E\left[(x-\mu)(x-\mu)^{\top}\right]
\end{aligned}
</script>
</div>
<p>Consider a linear transformation from <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> to <span class="arithmatex"><span class="MathJax_Preview">y=\left(y_{1}, y_{2}, \ldots, y_{m}\right)^{\top}</span><script type="math/tex">y=\left(y_{1}, y_{2}, \ldots, y_{m}\right)^{\top}</script></span>, with <span class="arithmatex"><span class="MathJax_Preview">\alpha_{i}^{\top}=\left(\alpha_{1 i}, \alpha_{2 i}, \ldots, \alpha_{m i}\right), i=1,2, \ldots, m</span><script type="math/tex">\alpha_{i}^{\top}=\left(\alpha_{1 i}, \alpha_{2 i}, \ldots, \alpha_{m i}\right), i=1,2, \ldots, m</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
y_{i}=\alpha_{i}^{\top} x=\alpha_{1 i} x_{1}+\alpha_{2 i} x_{2}+\ldots+\alpha_{m i} x_{m}
</div>
<script type="math/tex; mode=display">
y_{i}=\alpha_{i}^{\top} x=\alpha_{1 i} x_{1}+\alpha_{2 i} x_{2}+\ldots+\alpha_{m i} x_{m}
</script>
</div>
<p>Then we know that</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">E\left(y_{i}\right)=\alpha_{i}^{\top} \mu</span><script type="math/tex">E\left(y_{i}\right)=\alpha_{i}^{\top} \mu</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{var}\left(y_{i}\right)=\alpha_{i}^{\top} \Sigma \alpha_{i}</span><script type="math/tex">\operatorname{var}\left(y_{i}\right)=\alpha_{i}^{\top} \Sigma \alpha_{i}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{cov}\left(y_{i}, y_{j}\right)=\alpha_{i}^{\top} \Sigma \alpha_{j}</span><script type="math/tex">\operatorname{cov}\left(y_{i}, y_{j}\right)=\alpha_{i}^{\top} \Sigma \alpha_{j}</script></span></li>
</ul>
<p>ÈóÆÈ¢òÂÆö‰πâ:</p>
<p>Given a linear transformation</p>
<div class="arithmatex">
<div class="MathJax_Preview">
y_{i}=\alpha_{i}^{\top} x=\alpha_{1 i} x_{1}+\alpha_{2 i} x_{2}+\ldots+\alpha_{m i} x_{m}
</div>
<script type="math/tex; mode=display">
y_{i}=\alpha_{i}^{\top} x=\alpha_{1 i} x_{1}+\alpha_{2 i} x_{2}+\ldots+\alpha_{m i} x_{m}
</script>
</div>
<p>if</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\alpha_{i}^{\top} \alpha_{i}=1</span><script type="math/tex">\alpha_{i}^{\top} \alpha_{i}=1</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{cov}\left(y_{i}, y_{j}\right)=0</span><script type="math/tex">\operatorname{cov}\left(y_{i}, y_{j}\right)=0</script></span> for <span class="arithmatex"><span class="MathJax_Preview">i \neq j</span><script type="math/tex">i \neq j</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">y_{1}</span><script type="math/tex">y_{1}</script></span> has the largest variance among all linear transformations of <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>; <span class="arithmatex"><span class="MathJax_Preview">y_{2}</span><script type="math/tex">y_{2}</script></span> has the largest variance among all linear transformations that are uncorrelated with <span class="arithmatex"><span class="MathJax_Preview">y_{1}, \ldots</span><script type="math/tex">y_{1}, \ldots</script></span></li>
</ul>
<p>Then, we call <span class="arithmatex"><span class="MathJax_Preview">y_{1}, y_{2}, \ldots, y_{m}</span><script type="math/tex">y_{1}, y_{2}, \ldots, y_{m}</script></span> the first, second, &hellip;, <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>-th principle component of <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>.</p>
<h4 id="theorem-of-pca">Theorem of PCA<a class="headerlink" href="#theorem-of-pca" title="Permanent link">&para;</a></h4>
<p><strong>ÂÆöÁêÜ</strong>: ÂØπ‰∫éÈöèÊú∫ÂêëÈáèxÊù•ËØ¥, ÂÖ∂Á¨¨ k ‰∏™PCÊ≠£ÊòØÁî±ÂÖ∂ÂçèÊñπÂ∑ÆÁü©ÈòµÁöÑÁ¨¨ k ‰∏™Â•áÂºÇÂêëÈáè (ÂØπÁß∞ÂçäÊ≠£ÂÆöÈòµÁöÑÁâπÂæÅÂíåÂ•áÂºÇÂÄºÂàÜËß£Â∫îËØ•ÊòØ‰∏ÄÊ†∑ÁöÑ), Âπ∂‰∏îËøô‰∏™ PC ÁöÑÊñπÂ∑ÆÊ≠£ÊòØÁ¨¨ k ‰∏™Â•áÂºÇÂÄº(ÁâπÂæÅÂÄº).</p>
<p>Suppose <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>-dim random variable with <strong>covariance matrix</strong> <span class="arithmatex"><span class="MathJax_Preview">\Sigma</span><script type="math/tex">\Sigma</script></span>, <span class="arithmatex"><span class="MathJax_Preview">\lambda_{1} \geq \lambda_{2} \geq \ldots \geq \lambda_{m} \geq 0</span><script type="math/tex">\lambda_{1} \geq \lambda_{2} \geq \ldots \geq \lambda_{m} \geq 0</script></span> are the eigenvalues of <span class="arithmatex"><span class="MathJax_Preview">\Sigma</span><script type="math/tex">\Sigma</script></span>, and <span class="arithmatex"><span class="MathJax_Preview">\alpha_{1}, \alpha_{2}, \ldots, \alpha_{m}</span><script type="math/tex">\alpha_{1}, \alpha_{2}, \ldots, \alpha_{m}</script></span> are the corresponding eigenvectors, then the <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>-th principle component of <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is given by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
y_{k}=\alpha_{k}^{\top} x=\alpha_{1 k} x_{1}+\alpha_{2 k} x_{2}+\ldots+\alpha_{m k} x_{m}
</div>
<script type="math/tex; mode=display">
y_{k}=\alpha_{k}^{\top} x=\alpha_{1 k} x_{1}+\alpha_{2 k} x_{2}+\ldots+\alpha_{m k} x_{m}
</script>
</div>
<p>for <span class="arithmatex"><span class="MathJax_Preview">k=1,2, \ldots, m</span><script type="math/tex">k=1,2, \ldots, m</script></span>, and the variance of <span class="arithmatex"><span class="MathJax_Preview">y_{k}</span><script type="math/tex">y_{k}</script></span> is given by (the <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>-th eigen value of <span class="arithmatex"><span class="MathJax_Preview">\Sigma</span><script type="math/tex">\Sigma</script></span> )</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{var}\left(y_{k}\right)=\alpha_{k}^{\top} \Sigma \alpha_{k}=\lambda_{k}
</div>
<script type="math/tex; mode=display">
\operatorname{var}\left(y_{k}\right)=\alpha_{k}^{\top} \Sigma \alpha_{k}=\lambda_{k}
</script>
</div>
<p>Ëã•ÁâπÂæÅÂÄºÊúâÈáçÊ†πÔºåÂØπÂ∫îÁöÑÁâπÂæÅÂêëÈáèÁªÑÊàêmÁª¥Á©∫Èó¥ <span class="arithmatex"><span class="MathJax_Preview">\mathcal{R}^m</span><script type="math/tex">\mathcal{R}^m</script></span> ÁöÑ‰∏Ä‰∏™Â≠êÁ©∫Èó¥ÔºåÂ≠êÁ©∫Èó¥ÁöÑÁª¥Êï∞Á≠â‰∫éÈáçÊ†πÊï∞ÔºåÂú®Â≠êÁ©∫Èó¥‰ªªÂèñ‰∏Ä‰∏™Ê≠£‰∫§ÂùêÊ†áÁ≥ªÔºåËøô‰∏™ÂùêÊ†áÁ≥ªÁöÑÂçï‰ΩçÂêëÈáèÂ∞±ÂèØ‰Ωú‰∏∫ÁâπÂæÅÂêëÈáè„ÄÇËøôÊòØÂùêÊ†áÁ≥ªÂèñÊ≥ï‰∏çÂîØ‰∏Ä„ÄÇ</p>
<p>[mÁª¥ÈöèÊú∫ÂèòÈáè <span class="arithmatex"><span class="MathJax_Preview">y=(y_1,y_2,‚Ä¶,y_m)^T</span><script type="math/tex">y=(y_1,y_2,‚Ä¶,y_m)^T</script></span> ÁöÑÂàÜÈáè‰æùÊ¨°ÊòØx ÁöÑÁ¨¨‰∏Ä‰∏ªÊàêÂàÜÂà∞Á¨¨m‰∏ªÊàêÂàÜÁöÑÂÖÖË¶ÅÊù°‰ª∂ÊòØ]</p>
<p>The components of <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>-dim random variable <span class="arithmatex"><span class="MathJax_Preview">y=\left(y_{1}, y_{2}, \ldots, y_{m}\right)^{\top}</span><script type="math/tex">y=\left(y_{1}, y_{2}, \ldots, y_{m}\right)^{\top}</script></span> are the first, second, &hellip;, <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>-th principle component of <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> iff</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">y=A^{\top} x</span><script type="math/tex">y=A^{\top} x</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> is orthogonal with <span class="arithmatex"><span class="MathJax_Preview">A=\left\{\begin{array}{cccc}a_{11} &amp; a_{12} &amp; \ldots &amp; a_{1 m} \\ a_{21} &amp; a_{22} &amp; \ldots &amp; a_{2 m} \\ \ldots &amp; \ldots &amp; \ldots &amp; \ldots \\ a_{m 1} &amp; a_{m 2} &amp; \ldots &amp; a_{m m}\end{array}\right\}</span><script type="math/tex">A=\left\{\begin{array}{cccc}a_{11} & a_{12} & \ldots & a_{1 m} \\ a_{21} & a_{22} & \ldots & a_{2 m} \\ \ldots & \ldots & \ldots & \ldots \\ a_{m 1} & a_{m 2} & \ldots & a_{m m}\end{array}\right\}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{cov}(y)=\Lambda=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}\right)</span><script type="math/tex">\operatorname{cov}(y)=\Lambda=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}\right)</script></span>, with <span class="arithmatex"><span class="MathJax_Preview">\lambda_{1} \geq \lambda_{2} \geq \ldots \geq \lambda_{m}</span><script type="math/tex">\lambda_{1} \geq \lambda_{2} \geq \ldots \geq \lambda_{m}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\lambda_{k}</span><script type="math/tex">\lambda_{k}</script></span> is the <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>-th eigenvalue of <span class="arithmatex"><span class="MathJax_Preview">\Sigma_{1}</span><script type="math/tex">\Sigma_{1}</script></span>, <span class="arithmatex"><span class="MathJax_Preview">\alpha_{k}</span><script type="math/tex">\alpha_{k}</script></span> is the corresponding eigenvector, for <span class="arithmatex"><span class="MathJax_Preview">k=1,2, \ldots, m</span><script type="math/tex">k=1,2, \ldots, m</script></span></li>
</ul>
<h4 id="characteristics-of-pca">Characteristics of PCA<a class="headerlink" href="#characteristics-of-pca" title="Permanent link">&para;</a></h4>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{cov}(y)=\Lambda=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}\right)</span><script type="math/tex">\operatorname{cov}(y)=\Lambda=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}\right)</script></span> ÂêÑ‰∏™PCÁöÑÊñπÂ∑ÆÂ∞±ÊòØÂçèÊñπÂ∑ÆÈòµÁöÑÂêÑ‰∏™Â•áÂºÇÂÄº/ÁâπÂæÅÂÄº</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\sum_{i=1}^{m} \lambda_{i}=\sum_{i=1}^{m} \sigma_{i i}</span><script type="math/tex">\sum_{i=1}^{m} \lambda_{i}=\sum_{i=1}^{m} \sigma_{i i}</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">\sigma_{i i}</span><script type="math/tex">\sigma_{i i}</script></span> is the variance of <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span>  ÊÄª‰Ωì‰∏ªÊàêÂàÜyÁöÑÊñπÂ∑Æ‰πãÂíåÁ≠â‰∫éÈöèÊú∫ÂèòÈáèxÁöÑÊñπÂ∑Æ‰πãÂíå<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\sum_{i=1}^{m} \operatorname{var}\left(x_{i}\right) = \operatorname{tr}\left(\Sigma^{\top}\right)=\operatorname{tr}\left(A \Lambda A^{\top}\right)=\operatorname{tr}\left(A^{\top} \Lambda A\right) =\operatorname{tr}(\Lambda)=\sum_{i=1}^{m} \lambda_{i}=\sum_{i=1}^{m} \operatorname{var}\left(y_{i}\right)</span><script type="math/tex">\sum_{i=1}^{m} \operatorname{var}\left(x_{i}\right) = \operatorname{tr}\left(\Sigma^{\top}\right)=\operatorname{tr}\left(A \Lambda A^{\top}\right)=\operatorname{tr}\left(A^{\top} \Lambda A\right) =\operatorname{tr}(\Lambda)=\sum_{i=1}^{m} \lambda_{i}=\sum_{i=1}^{m} \operatorname{var}\left(y_{i}\right)</script></span></li>
</ul>
</li>
<li>factor loading: correlation coefficient between <span class="arithmatex"><span class="MathJax_Preview">y_{k}</span><script type="math/tex">y_{k}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span>, <span class="arithmatex"><span class="MathJax_Preview">\rho\left(y_{k}, x_{i}\right)=\frac{\sqrt{\lambda_{k}} \alpha_{i k}}{\sqrt{\sigma_{i i}}}</span><script type="math/tex">\rho\left(y_{k}, x_{i}\right)=\frac{\sqrt{\lambda_{k}} \alpha_{i k}}{\sqrt{\sigma_{i i}}}</script></span> Á¨¨k‰∏™‰∏ªÊàêÂàÜ <span class="arithmatex"><span class="MathJax_Preview">y_k</span><script type="math/tex">y_k</script></span> ‰∏éÂèòÈáèxiÁöÑÁõ∏ÂÖ≥Á≥ªÊï∞ <span class="arithmatex"><span class="MathJax_Preview">œÅ(y_k,x_i)</span><script type="math/tex">œÅ(y_k,x_i)</script></span> Áß∞‰∏∫<strong>Âõ†Â≠êË¥üËç∑Èáè</strong>(factor loading)ÔºåÂÆÉË°®Á§∫Á¨¨k‰∏™‰∏ªÊàêÂàÜ <span class="arithmatex"><span class="MathJax_Preview">y_k</span><script type="math/tex">y_k</script></span> ‰∏éÂèòÈáè <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> ÁöÑÁõ∏ÂÖ≥ÊÄß<ul>
<li>Ê≥®ÊÑèÂà∞ <span class="arithmatex"><span class="MathJax_Preview">\operatorname{cov}(y_k, x_i) = \operatorname{cov}\left(\alpha_{k}^{\top} x, e_{i}^{\top} x\right)=\alpha_{k}^{\top} \Sigma e_{i}=e_{i}^{\top} \Sigma \alpha_{k}=\lambda_{k} e_{i}^{\top} \alpha_{k}=\lambda_{k} \alpha_{i k}</span><script type="math/tex">\operatorname{cov}(y_k, x_i) = \operatorname{cov}\left(\alpha_{k}^{\top} x, e_{i}^{\top} x\right)=\alpha_{k}^{\top} \Sigma e_{i}=e_{i}^{\top} \Sigma \alpha_{k}=\lambda_{k} e_{i}^{\top} \alpha_{k}=\lambda_{k} \alpha_{i k}</script></span></li>
</ul>
</li>
<li>sum of factor loading for <span class="arithmatex"><span class="MathJax_Preview">y_{k}</span><script type="math/tex">y_{k}</script></span> over <span class="arithmatex"><span class="MathJax_Preview">x: \sum_{i=1}^{m} \sigma_{i i} \rho^{2}\left(y_{k}, x_{i}\right)=\lambda_{k}</span><script type="math/tex">x: \sum_{i=1}^{m} \sigma_{i i} \rho^{2}\left(y_{k}, x_{i}\right)=\lambda_{k}</script></span> Á¨¨k‰∏™‰∏ªÊàêÂàÜ <span class="arithmatex"><span class="MathJax_Preview">y_k</span><script type="math/tex">y_k</script></span> ‰∏ém‰∏™ÂèòÈáèÁöÑÂõ†Â≠êË¥üËç∑Èáè<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\sum_{i=1}^{m} \sigma_{i i} \rho^{2}\left(y_{k}, x_{i}\right)=\sum_{i=1}^{m} \lambda_{k} \alpha_{i j}^{2}=\lambda_{k} \alpha_{k}^{\top} \alpha_{k}=\lambda_{k}</span><script type="math/tex">\sum_{i=1}^{m} \sigma_{i i} \rho^{2}\left(y_{k}, x_{i}\right)=\sum_{i=1}^{m} \lambda_{k} \alpha_{i j}^{2}=\lambda_{k} \alpha_{k}^{\top} \alpha_{k}=\lambda_{k}</script></span></li>
<li>‰πüÂç≥, ‰ª•xÁöÑÂêÑ‰∏™ÂàÜÈáèÁöÑÊñπÂ∑Æ‰Ωú‰∏∫Á≥ªÊï∞, ÂèØ‰ª•ÂæóÂà∞<strong>Á¨¨ k ‰∏™PCÁöÑÊñπÂ∑ÆÂíåÂõ†Â≠êË¥üËç∑Èáè</strong>ÁöÑÂÖ≥Á≥ª</li>
</ul>
</li>
<li>sum of factor loading for <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> over <span class="arithmatex"><span class="MathJax_Preview">y: \sum_{k=1}^{m} \rho^{2}\left(y_{k}, x_{i}\right)=1</span><script type="math/tex">y: \sum_{k=1}^{m} \rho^{2}\left(y_{k}, x_{i}\right)=1</script></span><ul>
<li>m‰∏™‰∏ªÊàêÂàÜ‰∏éÁ¨¨i‰∏™ÂèòÈáè <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> ÁöÑÂõ†Â≠êË¥üËç∑Èáè, ‰πüÂç≥ÊâÄÊúâÁöÑ‰∏ªÊàêÂàÜÂèØ‰ª•Ëß£Èáä <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> ÁöÑÂÖ®ÈÉ®ÊñπÂ∑Æ</li>
</ul>
</li>
</ul>
<p>How many principle components to choose?</p>
<ul>
<li>variance contribution of <span class="arithmatex"><span class="MathJax_Preview">y_{k}: \eta_{k}=\frac{\lambda_{k}}{\sum_{i=1}^{m} \lambda_{i}}</span><script type="math/tex">y_{k}: \eta_{k}=\frac{\lambda_{k}}{\sum_{i=1}^{m} \lambda_{i}}</script></span></li>
<li>cumulative variance contribution of <span class="arithmatex"><span class="MathJax_Preview">y_{1}, y_{2}, \ldots, y_{k}: \frac{\sum_{i=1}^{k} \lambda_{i}}{\sum_{i=1}^{m} \lambda_{i}}</span><script type="math/tex">y_{1}, y_{2}, \ldots, y_{k}: \frac{\sum_{i=1}^{k} \lambda_{i}}{\sum_{i=1}^{m} \lambda_{i}}</script></span><ul>
<li>ÈÄöÂ∏∏Âèñk‰ΩøÂæóÁ¥ØËÆ°<strong>ÊñπÂ∑ÆË¥°ÁåÆÁéá</strong>ËææÂà∞Êüê‰∏™ÈòàÂÄºÔºåÊØîÂ¶Ç70%ÔΩû80%‰ª•‰∏ä„ÄÇÁ¥ØËÆ°ÊñπÂ∑ÆË¥°ÁåÆÁéáÂèçÊò†‰∫Ü‰∏ªÊàêÂàÜ‰øùÁïô‰ø°ÊÅØÁöÑÊØî‰æã„ÄÇ</li>
<li>usually choose <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> such that <span class="arithmatex"><span class="MathJax_Preview">\frac{\sum_{i=1}^{k} \lambda_{i}}{\sum_{i=1}^{m} \lambda_{i}}&gt;70 \%</span><script type="math/tex">\frac{\sum_{i=1}^{k} \lambda_{i}}{\sum_{i=1}^{m} \lambda_{i}}>70 \%</script></span> or <span class="arithmatex"><span class="MathJax_Preview">80 \%</span><script type="math/tex">80 \%</script></span></li>
<li>cumulative variance contribution reflects the contribution of <span class="arithmatex"><span class="MathJax_Preview">y_{1}, \ldots, y_{k}</span><script type="math/tex">y_{1}, \ldots, y_{k}</script></span> to <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> but not specifically to <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span></li>
</ul>
</li>
<li>contribution to <span class="arithmatex"><span class="MathJax_Preview">x_{i}: \nu_{i}=\rho^{2}\left(x_{i},\left(y_{1}, \ldots, y_{k}\right)\right)=\sum_{j=1}^{k} \frac{\lambda_{j} \alpha_{i j}^{2}}{\sigma_{i i}}</span><script type="math/tex">x_{i}: \nu_{i}=\rho^{2}\left(x_{i},\left(y_{1}, \ldots, y_{k}\right)\right)=\sum_{j=1}^{k} \frac{\lambda_{j} \alpha_{i j}^{2}}{\sigma_{i i}}</script></span></li>
</ul>
<p>Note: <strong>normalize variables before PCA</strong> Âú®ËøõË°åPCA‰πãÂâçÈúÄË¶ÅÂÖàËøõË°åÂΩí‰∏ÄÂåñ</p>
<ul>
<li>Âú®ÂÆûÈôÖÈóÆÈ¢ò‰∏≠Ôºå‰∏çÂêåÂèòÈáèÂèØËÉΩÊúâ‰∏çÂêåÁöÑÈáèÁ∫≤ÔºåÁõ¥Êé•Ê±Ç‰∏ªÊàêÂàÜÊúâÊó∂‰ºö‰∫ßÁîü‰∏çÂêàÁêÜÁöÑÁªìÊûú„ÄÇ‰∏∫‰∫ÜÊ∂àÈô§Ëøô‰∏™ÂΩ±ÂìçÔºåÈúÄË¶ÅÂØπÂêÑ‰∏™ÈöèÊú∫ÂèòÈáèÂÆûÊñΩËßÑËåÉÂåñÔºå‰ΩøÂÖ∂ÂùáÂÄº‰∏∫0ÔºåÊñπÂ∑Æ‰∏∫1.<ul>
<li>ÊòæÁÑ∂Ôºå<strong>ËßÑËåÉÂåñÈöèÊú∫ÂèòÈáèÁöÑÂçèÊñπÂ∑ÆÁü©ÈòµÂ∞±ÊòØÁõ∏ÂÖ≥Áü©ÈòµR</strong>„ÄÇ</li>
<li>ÂØπÁÖßÊÄª‰Ωì‰∏ªÊàêÂàÜÁöÑÊÄßË¥®, ÂèØ‰ª•ÂæóÂà∞ËßÑËåÉÂåñÈöèÊú∫ÂèòÈáèÁöÑÊÄª‰Ωì‰∏ªÊàêÂàÜÂàÜÊûêÁöÑÁ±ª‰ººÊÄßË¥®</li>
<li>Ê≥®ÊÑèÂà∞Ê≠§Êó∂ÁöÑ <span class="arithmatex"><span class="MathJax_Preview">\sigma_{ii}=1</span><script type="math/tex">\sigma_{ii}=1</script></span></li>
</ul>
</li>
</ul>
<h4 id="pca">PCA ‰æãÂ≠ê<a class="headerlink" href="#pca" title="Permanent link">&para;</a></h4>
<p><img alt="" src="../media/ASL-note1/2021-12-04-11-18-33.png" /></p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-11-18-47.png" /></p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-11-19-08.png" /></p>
<ul>
<li>ÂØπ‰∫é correlation matrix ËøõË°åÁâπÂæÅÂàÜËß£</li>
<li>Ê†πÊçÆÂõ†Â≠êË¥üËç∑ÈáèÈÄâÂèñÂêàÈÄÇÁöÑ k, ÂØπÂ∫îÁöÑÁâπÂæÅÂêëÈáèÂ∞±ÊòØ PC ÁöÑÊùÉÈáç</li>
<li>ÁÑ∂ÂêéËøòÂèØ‰ª•ËÆ°ÁÆóÂØπ‰∫éxÁöÑÂêÑÂàÜÈáèÁöÑ Factor loading</li>
</ul>
<h2 id="linear-methods-for-classification">Linear Methods for Classification<a class="headerlink" href="#linear-methods-for-classification" title="Permanent link">&para;</a></h2>
<h3 id="linear-regression-of-an-indicator-matrix">Linear Regression of an Indicator Matrix<a class="headerlink" href="#linear-regression-of-an-indicator-matrix" title="Permanent link">&para;</a></h3>
<p>Â∞±ÊòØÁî®Á∫øÊÄßÂõûÂΩíÊù•Ëß£ÂÜ≥ÂàÜÁ±ªÈóÆÈ¢ò. ÈÄöËøáÂØπ‰∫é 0-1 ÊåáÊ†áËøõË°åÂõûÂΩí, ÂæóÂà∞ÂêÑ‰∏™Á±ªÂà´‰∏ãÁöÑÊøÄÊ¥ªÂÄº, ÁÑ∂ÂêéÂèñÂÖ∂‰∏≠ÊúÄÂ§ßÁöÑ(ÊàñËÄÖÂΩí‰∏ÄÂåñ‰∏∫ÂàÜÂ∏É).</p>
<p>Training data <span class="arithmatex"><span class="MathJax_Preview">\left\{\left(x_{i}, g_{i}\right)\right\}_{i=1}^{n}</span><script type="math/tex">\left\{\left(x_{i}, g_{i}\right)\right\}_{i=1}^{n}</script></span>, where each <span class="arithmatex"><span class="MathJax_Preview">x_{i} \in \mathbb{R}^{p}</span><script type="math/tex">x_{i} \in \mathbb{R}^{p}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">g_{i} \in\{1, \ldots, K\}</span><script type="math/tex">g_{i} \in\{1, \ldots, K\}</script></span>
For each <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> construct a linear discriminant <span class="arithmatex"><span class="MathJax_Preview">\delta_{k}(x)</span><script type="math/tex">\delta_{k}(x)</script></span> via:</p>
<ol>
<li>For <span class="arithmatex"><span class="MathJax_Preview">i=1, \ldots, n</span><script type="math/tex">i=1, \ldots, n</script></span>, set</li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
y_{i k}= \begin{cases}0, &amp; \text { if } g_{i} \neq k \\ 1, &amp; \text { if } g_{i}=k\end{cases}
</div>
<script type="math/tex; mode=display">
y_{i k}= \begin{cases}0, & \text { if } g_{i} \neq k \\ 1, & \text { if } g_{i}=k\end{cases}
</script>
</div>
<ol start="2">
<li>Compute <span class="arithmatex"><span class="MathJax_Preview">\left(\hat{\beta}_{0 k}, \hat{\beta}_{k}\right)=\operatorname{argmin}_{\beta_{0 k}, \beta_{k}} \sum_{i=1}^{n}\left(y_{i k}-\beta_{0 k}-\beta_{k}^{\top} x_{i}\right)^{2}</span><script type="math/tex">\left(\hat{\beta}_{0 k}, \hat{\beta}_{k}\right)=\operatorname{argmin}_{\beta_{0 k}, \beta_{k}} \sum_{i=1}^{n}\left(y_{i k}-\beta_{0 k}-\beta_{k}^{\top} x_{i}\right)^{2}</script></span></li>
<li>Define <span class="arithmatex"><span class="MathJax_Preview">\delta_{k}(x)=\hat{\beta}_{0 k}+\hat{\beta}_{k}^{\top} x</span><script type="math/tex">\delta_{k}(x)=\hat{\beta}_{0 k}+\hat{\beta}_{k}^{\top} x</script></span>
Classify a new point <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> with
<span class="arithmatex"><span class="MathJax_Preview">G(x)=\operatorname{argmax}_{k} \delta_{k}(x)</span><script type="math/tex">G(x)=\operatorname{argmax}_{k} \delta_{k}(x)</script></span></li>
</ol>
<p>ÂèçÊÄù: ËøôÈáåÊòØÁî®Êù•‰º∞ËÆ°Êù°‰ª∂Ê¶ÇÁéá <span class="arithmatex"><span class="MathJax_Preview">E(Y_k‚à£ X = x) = Pr(G = k ‚à£ X = x)</span><script type="math/tex">E(Y_k‚à£ X = x) = Pr(G = k ‚à£ X = x)</script></span></p>
<ul>
<li>ËøôÈáåÁöÑËøë‰ººÂ•ΩÂêó?</li>
<li>ÊØîÂ¶ÇÁ∫øÊÄßÂõûÂΩíÁöÑËæìÂá∫ËåÉÂõ¥‰∏çÂú® [0, 1] ‰∏≠</li>
</ul>
<p>‰∏ãÈù¢Áªô‰∫Ü‰∏Ä‰∏™Âèç‰æã, Ê≥®ÊÑèÂà∞Áî±‰∫éÁ∫øÊÄßÂõûÂΩíÂæóÂà∞ÁöÑÁªìÊûú‰∏çËÉΩÂæàÂ•ΩÂæóËøë‰ººÂêéÈ™åÂàÜÂ∏É, ‰ºöÂ±èËîΩÊéâ‰∏Ä‰∫õÁ±ªÂà´.</p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-12-51-17.png" /></p>
<p>‰∏ãÈù¢ÁöÑÂõæÊòØÂêÑ‰∏™Á∫øÊÄßÂõûÂΩíÁöÑËæìÂá∫ÂÄº(ÂùêÊ†áËøõË°å‰∫ÜÂèòÊç¢), ÂèØ‰ª•ÁúãÂà∞Á¨¨‰∫åÊ≤°ÊúâÊòæËëóÁöÑÊ¶ÇÁéá. ËÄåÂè≥ÂõæÂä†ÂÖ•‰∫Ü‰∫åÊ¨°È°π, ÊïàÊûúÂæóÂà∞‰∫ÜÊèêÂçá. ÁÑ∂ËÄåËøôÂè™ÊòØÁ±ªÂà´Êï∞ 3 ÁöÑÊÉÖÂÜµ, ÂØπ‰∫é kÂàÜÁ±ªËÄåË®Ä‰ºöÂá∫Áé∞‰∫§ÂèâÈ°πÊåáÊï∞Â¢ûÈïøÁöÑÊÉÖÂÜµ.</p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-12-51-43.png" /></p>
<h3 id="linear-discriminant-analysis-lda">Linear Discriminant Analysis (LDA)<a class="headerlink" href="#linear-discriminant-analysis-lda" title="Permanent link">&para;</a></h3>
<p>Model each <span class="arithmatex"><span class="MathJax_Preview">f_{k}(x)</span><script type="math/tex">f_{k}(x)</script></span> as a multivariate Gaussian</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f_{k}(x)=\frac{1}{(2 \pi)^{p / 2} \sqrt{\left|\Sigma_{k}\right|}} \exp \left\{\frac{1}{2}\left(x-\mu_{k}\right)^{\top} \Sigma_{k}^{-1}\left(x-\mu_{k}\right)\right\}
</div>
<script type="math/tex; mode=display">
f_{k}(x)=\frac{1}{(2 \pi)^{p / 2} \sqrt{\left|\Sigma_{k}\right|}} \exp \left\{\frac{1}{2}\left(x-\mu_{k}\right)^{\top} \Sigma_{k}^{-1}\left(x-\mu_{k}\right)\right\}
</script>
</div>
<p>Linear Discriminant Analysis (LDA) arises in the special case when</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\Sigma_{k}=\Sigma \text { for all } k
</div>
<script type="math/tex; mode=display">
\Sigma_{k}=\Sigma \text { for all } k
</script>
</div>
<p>ËÄÉËôë‰∏§‰∏™Á±ªÂà´ kÂíål</p>
<p><span class="arithmatex"><span class="MathJax_Preview">\log \frac{\operatorname{Pr}(G=k \mid X=x)}{\operatorname{Pr}(G=l \mid X=x)}=\log \frac{f_{k}(x)}{f_{l}(x)}+\log \frac{\pi_{k}}{\pi_{l}}=</span><script type="math/tex">\log \frac{\operatorname{Pr}(G=k \mid X=x)}{\operatorname{Pr}(G=l \mid X=x)}=\log \frac{f_{k}(x)}{f_{l}(x)}+\log \frac{\pi_{k}}{\pi_{l}}=</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\log \frac{\pi_{k}}{\pi_{l}}-\frac{1}{2}\left(\mu_{k}+\mu_{l}\right)^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{l}\right)+x^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{l}\right)</span><script type="math/tex">\log \frac{\pi_{k}}{\pi_{l}}-\frac{1}{2}\left(\mu_{k}+\mu_{l}\right)^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{l}\right)+x^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{l}\right)</script></span>
The equal covariance matrices allow <span class="arithmatex"><span class="MathJax_Preview">x^{\top} \Sigma_{k}^{-1} x</span><script type="math/tex">x^{\top} \Sigma_{k}^{-1} x</script></span> and <span class="arithmatex"><span class="MathJax_Preview">x^{\top} \Sigma_{l}^{-1} x</span><script type="math/tex">x^{\top} \Sigma_{l}^{-1} x</script></span> cancel out</p>
<p>‰∏äÂºèÂÆö‰πâ‰∫Ü‰∏§‰∏™Á±ªÂà´ÁöÑÂàÜÁ±ªËæπÁïå. ÂÆûÈôÖ‰∏ä, ÂØπ‰∫é‰∏Ä‰∏™Á±ªÂà´ k, ÂèØ‰ª•ÁÆóÂá∫‰∏Ä‰∏™„ÄåÂàÜÊï∞„Äç, ÂàÜÁ±ªÂô®ÁöÑËæìÂá∫Â∞±ÊòØÂàÜÊï∞ÊúÄÈ´òÁöÑÈÇ£‰∏Ä‰∏™.</p>
<p>From the log odds, we see that the linear discriminant functions</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\delta_{k}(x)=x^{\top} \Sigma^{-1} \mu_{k}-\frac{1}{2} \mu_{k}^{\top} \Sigma^{-1} \mu_{k}+\log \pi_{k}
</div>
<script type="math/tex; mode=display">
\delta_{k}(x)=x^{\top} \Sigma^{-1} \mu_{k}-\frac{1}{2} \mu_{k}^{\top} \Sigma^{-1} \mu_{k}+\log \pi_{k}
</script>
</div>
<p>are an equivalent description of the decision rule with</p>
<div class="arithmatex">
<div class="MathJax_Preview">
G(x)=\operatorname{argmax}_{k} \delta_{k}(x)
</div>
<script type="math/tex; mode=display">
G(x)=\operatorname{argmax}_{k} \delta_{k}(x)
</script>
</div>
<h3 id="lda-practicalities">LDA Practicalities<a class="headerlink" href="#lda-practicalities" title="Permanent link">&para;</a></h3>
<p>In practice we do not know the parameters of the Gaussian distributions, and will need to estimate them using our training data</p>
<p>ÂÆûÈôÖ‰∏ä, Êàë‰ª¨ÈúÄË¶ÅÈÄöËøáÊï∞ÊçÆÊù•‰º∞ËÆ°‰∏äÈù¢ÁöÑÊ†∑Êú¨ÂùáÂÄºÂíåÂçèÊñπÂ∑ÆÁü©Èòµ. (Ê≥®ÊÑèËøôÈáåÊòØ„ÄåÂ§öÂàÜÁ±ª„ÄçÁöÑÊ†∑Êú¨ÊñπÂ∑Æ, ÊâÄ‰ª•ÂàÜÊØç‰∏∫ <span class="arithmatex"><span class="MathJax_Preview">n-K</span><script type="math/tex">n-K</script></span>?)</p>
<p>Let <span class="arithmatex"><span class="MathJax_Preview">n_{k}</span><script type="math/tex">n_{k}</script></span> be the number of class <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> observations then</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
\hat{\pi}_{k} &amp;=\frac{n_{k}}{n} \\
\hat{\mu}_{k} &amp;=\sum_{g_{i}=k} \frac{x_{i}}{n_{k}} \\
\hat{\Sigma} &amp;=\sum_{k=1}^{K} \sum_{g_{i}=k} \frac{\left(x_{i}-\hat{\mu}_{k}\right)\left(x_{i}-\hat{\mu}_{k}\right)^{\top}}{n-K}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{\pi}_{k} &=\frac{n_{k}}{n} \\
\hat{\mu}_{k} &=\sum_{g_{i}=k} \frac{x_{i}}{n_{k}} \\
\hat{\Sigma} &=\sum_{k=1}^{K} \sum_{g_{i}=k} \frac{\left(x_{i}-\hat{\mu}_{k}\right)\left(x_{i}-\hat{\mu}_{k}\right)^{\top}}{n-K}
\end{aligned}
</script>
</div>
<p>Note: for <span class="arithmatex"><span class="MathJax_Preview">k=2</span><script type="math/tex">k=2</script></span> there is a simple correspondence between LDA and linear regression of indicator matrix, which is not true for <span class="arithmatex"><span class="MathJax_Preview">k&gt;2</span><script type="math/tex">k>2</script></span>.</p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-13-19-42.png" /></p>
<h4 id="when-sigma_ksigma_k-not-equal-qda-quadratic-discriminant-analysis">When <span class="arithmatex"><span class="MathJax_Preview">\Sigma_{k}</span><script type="math/tex">\Sigma_{k}</script></span> Not Equal: QDA (Quadratic Discriminant Analysis)<a class="headerlink" href="#when-sigma_ksigma_k-not-equal-qda-quadratic-discriminant-analysis" title="Permanent link">&para;</a></h4>
<p>LDA ÊòØÂÅáËÆæ‰∫ÜÂêÑÁ±ªÂà´ÊúâÁõ∏ÂêåÁöÑÊñπÂ∑Æ</p>
<p>If the <span class="arithmatex"><span class="MathJax_Preview">\Sigma_{k}</span><script type="math/tex">\Sigma_{k}</script></span> are not assumed to be equal, then the quadratic terms remain and we get quandratic discriminant functions <span class="arithmatex"><span class="MathJax_Preview">Q D A</span><script type="math/tex">Q D A</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\delta_{k}(x)=-0.5 \log \left|\Sigma_{k}\right|-0.5\left(x-\mu_{k}\right)^{\top} \Sigma_{k}^{-1}\left(x-\mu_{k}\right)+\log \pi_{k}
</div>
<script type="math/tex; mode=display">
\delta_{k}(x)=-0.5 \log \left|\Sigma_{k}\right|-0.5\left(x-\mu_{k}\right)^{\top} \Sigma_{k}^{-1}\left(x-\mu_{k}\right)+\log \pi_{k}
</script>
</div>
<p>In this case, the decision boundary between classes are described by a <strong>quadratic equation</strong> <span class="arithmatex"><span class="MathJax_Preview">\left\{x: \delta_{k}(x)=\delta_{l}(x)\right\}</span><script type="math/tex">\left\{x: \delta_{k}(x)=\delta_{l}(x)\right\}</script></span></p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-13-26-31.png" /></p>
<h4 id="rda-regularized-discriminant-analysis">RDA: Regularized Discriminant Analysis<a class="headerlink" href="#rda-regularized-discriminant-analysis" title="Permanent link">&para;</a></h4>
<p>Âπ≥Ë°°LDAÂíåQDA</p>
<p><span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}_{k}(\alpha)=\alpha \hat{\Sigma}_{k}+(1-\alpha) \hat{\Sigma}, \alpha \in[0,1]</span><script type="math/tex">\hat{\Sigma}_{k}(\alpha)=\alpha \hat{\Sigma}_{k}+(1-\alpha) \hat{\Sigma}, \alpha \in[0,1]</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}_{k}(\alpha)</span><script type="math/tex">\hat{\Sigma}_{k}(\alpha)</script></span> Regularized covariance matrices</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}</span><script type="math/tex">\hat{\Sigma}</script></span>: pooled covariance matrix</li>
</ul>
<p>Can also replace <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}</span><script type="math/tex">\hat{\Sigma}</script></span> by <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}(\gamma)=\gamma \hat{\Sigma}+(1-\gamma) \hat{\sigma}^{2} I, \gamma \in[0,1]</span><script type="math/tex">\hat{\Sigma}(\gamma)=\gamma \hat{\Sigma}+(1-\gamma) \hat{\sigma}^{2} I, \gamma \in[0,1]</script></span></p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-13-33-12.png" /></p>
<h4 id="computations-for-lda-qda">Computations for LDA &amp; QDA<a class="headerlink" href="#computations-for-lda-qda" title="Permanent link">&para;</a></h4>
<p>Diagonalizing <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}</span><script type="math/tex">\hat{\Sigma}</script></span> or <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}_{k}</span><script type="math/tex">\hat{\Sigma}_{k}</script></span> by eigen decomposition <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}_{k}=U_{k} D_{k} U_{k}^{\top}</span><script type="math/tex">\hat{\Sigma}_{k}=U_{k} D_{k} U_{k}^{\top}</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">U_{k}</span><script type="math/tex">U_{k}</script></span> is <span class="arithmatex"><span class="MathJax_Preview">p \times p</span><script type="math/tex">p \times p</script></span> orthornormal, and <span class="arithmatex"><span class="MathJax_Preview">D_{k}</span><script type="math/tex">D_{k}</script></span> is a diagonal matrix of positive eigenvalues <span class="arithmatex"><span class="MathJax_Preview">d_{k l}</span><script type="math/tex">d_{k l}</script></span>. The ingredients for <span class="arithmatex"><span class="MathJax_Preview">\delta_{k}(x)</span><script type="math/tex">\delta_{k}(x)</script></span> are</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;(x-\hat{\mu}_{k})^{\top} \hat{\Sigma}_{k}^{-1}\left(x-\hat{\mu}_{k}\right)=\left[U_{k}^{\top}\left(x-\hat{\mu}_{k}\right)\right]^{\top} D_{k}^{-1}\left[U_{k}^{\top}\left(x-\hat{\mu}_{k}\right)\right] \\
&amp;\log \left|\hat{\Sigma}_{k}\right|=\sum_{l} \log d_{k l}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&(x-\hat{\mu}_{k})^{\top} \hat{\Sigma}_{k}^{-1}\left(x-\hat{\mu}_{k}\right)=\left[U_{k}^{\top}\left(x-\hat{\mu}_{k}\right)\right]^{\top} D_{k}^{-1}\left[U_{k}^{\top}\left(x-\hat{\mu}_{k}\right)\right] \\
&\log \left|\hat{\Sigma}_{k}\right|=\sum_{l} \log d_{k l}
\end{aligned}
</script>
</div>
<p>Therefore, LDA can be implemented by:</p>
<ul>
<li><strong>Sphere</strong> the data with respect to the common covariance estimate <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}: X^{\star} \leftarrow D^{-\frac{1}{2}} U^{\top} X</span><script type="math/tex">\hat{\Sigma}: X^{\star} \leftarrow D^{-\frac{1}{2}} U^{\top} X</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}=U D U^{\top}</span><script type="math/tex">\hat{\Sigma}=U D U^{\top}</script></span>. The common covariance estimate of <span class="arithmatex"><span class="MathJax_Preview">X^{\star}</span><script type="math/tex">X^{\star}</script></span> will now be the identity ÁêÉÈù¢Âåñ? ‰ªéËÄå‰ΩøÂæóÂèòÊç¢ÂêéÁöÑ <span class="arithmatex"><span class="MathJax_Preview">X^*</span><script type="math/tex">X^*</script></span> ÂçèÊñπÂ∑Æ‰∏∫Âçï‰ΩçÈòµ</li>
<li>Classify to the closet class <strong>centroid</strong> in the transformed space, modulo the effect of the class prior probabilities <span class="arithmatex"><span class="MathJax_Preview">\pi_{k}</span><script type="math/tex">\pi_{k}</script></span></li>
</ul>
<h4 id="reduced-rank-lda">Reduced Rank LDA<a class="headerlink" href="#reduced-rank-lda" title="Permanent link">&para;</a></h4>
<p>ÂØπ‰∫éÂú®pÁª¥Á©∫Èó¥ÁöÑÊï∞ÊçÆ, K‰∏™Á±ªÂà´ÁöÑ‰∏≠ÂøÉËá≥Â§öÂº†Êàê K-1 Áª¥Â≠êÁ©∫Èó¥; ËÄåÊàë‰ª¨Âú®ËøõË°åÂà§ÂÆöÁöÑÊó∂ÂÄô, ÂÖ∂ÂÆûÁúãÁöÑÊòØÂà∞Ëøô‰∫õÁ±ªÂà´‰∏≠ÂøÉÁöÑË∑ùÁ¶ª, Âõ†Ê≠§ÂÖ∂ÂÆûÂèØ‰ª• <strong>ÈôçÁª¥</strong> Âà∞Ëøô‰∏™Â≠êÁ©∫Èó¥‰∏ä.</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span> <strong>centroids</strong> in <span class="arithmatex"><span class="MathJax_Preview">\mathrm{p}</span><script type="math/tex">\mathrm{p}</script></span>-dimensional input space lie in an affine subspace of dimension <span class="arithmatex"><span class="MathJax_Preview">\leqslant K-1</span><script type="math/tex">\leqslant K-1</script></span></li>
<li>if <span class="arithmatex"><span class="MathJax_Preview">p \gg K</span><script type="math/tex">p \gg K</script></span>, there is a big drop in dimension</li>
<li>To locate the closest centroid can ignore the directions orthogonal to this subspace.</li>
<li>Therefore, can project <span class="arithmatex"><span class="MathJax_Preview">X^{*}</span><script type="math/tex">X^{*}</script></span> onto this centroid-spanning subspace <span class="arithmatex"><span class="MathJax_Preview">H_{K-1}</span><script type="math/tex">H_{K-1}</script></span> and make comparisons there</li>
<li>LDA thus performs dimensionality reduction and one need only consider the data in a subspace of dimension at most K-1.</li>
</ul>
<p>Ëøõ‰∏ÄÊ≠•, What About a Subspace of Dim <span class="arithmatex"><span class="MathJax_Preview">L&lt;K-1</span><script type="math/tex">L<K-1</script></span></p>
<p>Which subspace of <span class="arithmatex"><span class="MathJax_Preview">\operatorname{dim} L&lt;K-1</span><script type="math/tex">\operatorname{dim} L<K-1</script></span> should we project onto for optimal LDA?</p>
<p>Finding the sequences of optimal subspaces for LDA involves:</p>
<ul>
<li>Compute the <span class="arithmatex"><span class="MathJax_Preview">K \times p</span><script type="math/tex">K \times p</script></span> matrix of class centroids <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> and the common covariance matrix <span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> (for <strong>within-class covariance</strong>)</li>
<li>Compute <span class="arithmatex"><span class="MathJax_Preview">M^{*}=M W^{-\frac{1}{2}}</span><script type="math/tex">M^{*}=M W^{-\frac{1}{2}}</script></span></li>
<li>Compute <span class="arithmatex"><span class="MathJax_Preview">B^{\star}</span><script type="math/tex">B^{\star}</script></span>, the covariance matrix of <span class="arithmatex"><span class="MathJax_Preview">M^{\star}</span><script type="math/tex">M^{\star}</script></span> (B for <strong>between-class covariance</strong>), and its eigen-decomposition <span class="arithmatex"><span class="MathJax_Preview">B^{\star}=V^{\star} D_{B} V^{* \top}</span><script type="math/tex">B^{\star}=V^{\star} D_{B} V^{* \top}</script></span></li>
<li>The columns <span class="arithmatex"><span class="MathJax_Preview">v_{1}^{\star}</span><script type="math/tex">v_{1}^{\star}</script></span> of <span class="arithmatex"><span class="MathJax_Preview">V^{*}</span><script type="math/tex">V^{*}</script></span> in sequence from first to last define the coordinates of the optimal subspaces.</li>
<li><span class="arithmatex"><span class="MathJax_Preview">D_{\text {th }}</span><script type="math/tex">D_{\text {th }}</script></span> discriminant variable <span class="arithmatex"><span class="MathJax_Preview">Z_{l}=v_{l}^{\top} X, v_{l}=W^{-\frac{1}{2}} v_{l}^{\star}</span><script type="math/tex">Z_{l}=v_{l}^{\top} X, v_{l}=W^{-\frac{1}{2}} v_{l}^{\star}</script></span></li>
</ul>
<p>(Áúã‰∏äÂéªÂ•ΩÂ§çÊùÇ, „Äå‰∏ªË¶ÅÊÄùÊÉ≥ÊòØÂÖàÊ±Ç <span class="arithmatex"><span class="MathJax_Preview">W^{-\frac{1}{2}}BW^{-\frac{1}{2}}</span><script type="math/tex">W^{-\frac{1}{2}}BW^{-\frac{1}{2}}</script></span> ÁöÑÁâπÂæÅÂêëÈáè <span class="arithmatex"><span class="MathJax_Preview">v_l^*</span><script type="math/tex">v_l^*</script></span>, Âàô <span class="arithmatex"><span class="MathJax_Preview">W^{-1}B</span><script type="math/tex">W^{-1}B</script></span> ÁöÑÁâπÂæÅÂêëÈáè‰∏∫ <span class="arithmatex"><span class="MathJax_Preview">W^{-\frac{1}{2}}v_l^*</span><script type="math/tex">W^{-\frac{1}{2}}v_l^*</script></span>„Äç, ÁªìÂêà Ex4.1 ÁöÑËØÅÊòéËøáÁ®ã <a href="https://github.com/szcf-weiya/ESL-CN/issues/142">https://github.com/szcf-weiya/ESL-CN/issues/142</a>)</p>
<h4 id="fisher-criterion">Fisher Criterion Âè¶‰∏Ä‰∏™ËßíÂ∫¶<a class="headerlink" href="#fisher-criterion" title="Permanent link">&para;</a></h4>
<p>Fisher ÁöÑÂáÜÂàô: ÂØªÊâæÁ∫øÊÄßÁªÑÂêà <span class="arithmatex"><span class="MathJax_Preview">Z = a^T X</span><script type="math/tex">Z = a^T X</script></span> ‰ΩøÂæóÁªÑÈó¥ÊñπÂ∑ÆÁõ∏ÂØπ‰∫éÁªÑÂÜÖÊñπÂ∑ÆÊúÄÂ§ßÂåñÔºé</p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-14-14-19.png" /></p>
<ul>
<li>W: pooled covariance about the means</li>
<li>B: covariance of the class means</li>
<li>Then for the projected data <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> ÁªÑÂÜÖÂíåÁªÑÈó¥ÊñπÂ∑Æ<ul>
<li>The between-class variance of <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> is <span class="arithmatex"><span class="MathJax_Preview">a^{\top} B a</span><script type="math/tex">a^{\top} B a</script></span></li>
<li>The within-class variance of <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> is <span class="arithmatex"><span class="MathJax_Preview">a^{\top} W a</span><script type="math/tex">a^{\top} W a</script></span></li>
</ul>
</li>
</ul>
<p>Ë°•ÂÖÖ: ‰∏ãÈù¢ÊòØ <strong>ÁªÑÂÜÖÂíåÁªÑÈó¥ÊñπÂ∑Æ</strong> ÁöÑÂÆö‰πâ. B, W, T ÁöÑËá™Áî±Â∫¶ÂàÜÂà´‰∏∫ K-1, N-K, N-1. ÊòìËØÅ <span class="arithmatex"><span class="MathJax_Preview">T=B+W</span><script type="math/tex">T=B+W</script></span></p>
<p><span class="arithmatex"><span class="MathJax_Preview">\begin{aligned} \mathbf{B} &amp;=\sum_{i=1}^{K} n_{i}\left(\bar{x}_{i}-\bar{x}\right)\left(\bar{x}_{i}-\bar{x}\right)^{\prime} \\ \mathbf{W} &amp;=\sum_{i=1}^{K} \sum_{j=1}^{n_{i}}\left(x_{i j}-\bar{x}_{i}\right)\left(x_{i j}-\bar{x}_{i}\right)^{\prime} \\ \mathbf{T} &amp;=\sum_{i=1}^{K} \sum_{j=1}^{n_{i}}\left(x_{i j}-\bar{x}\right)\left(x_{i j}-\bar{x}\right)^{\prime} \end{aligned}</span><script type="math/tex">\begin{aligned} \mathbf{B} &=\sum_{i=1}^{K} n_{i}\left(\bar{x}_{i}-\bar{x}\right)\left(\bar{x}_{i}-\bar{x}\right)^{\prime} \\ \mathbf{W} &=\sum_{i=1}^{K} \sum_{j=1}^{n_{i}}\left(x_{i j}-\bar{x}_{i}\right)\left(x_{i j}-\bar{x}_{i}\right)^{\prime} \\ \mathbf{T} &=\sum_{i=1}^{K} \sum_{j=1}^{n_{i}}\left(x_{i j}-\bar{x}\right)\left(x_{i j}-\bar{x}\right)^{\prime} \end{aligned}</script></span></p>
<p><strong>Fisher&rsquo;s criterion</strong>:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\max _{a} \frac{a^{\top} B a}{a^{\top} W a}
</div>
<script type="math/tex; mode=display">
\max _{a} \frac{a^{\top} B a}{a^{\top} W a}
</script>
</div>
<p>or equivalently</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\max _{a} a^{\top} B a</span><script type="math/tex">\max _{a} a^{\top} B a</script></span> subject to <span class="arithmatex"><span class="MathJax_Preview">a^{\top} W a=1</span><script type="math/tex">a^{\top} W a=1</script></span></li>
</ul>
<p><span class="arithmatex"><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> is given by the largest eigenvalue of <span class="arithmatex"><span class="MathJax_Preview">W^{-1} B</span><script type="math/tex">W^{-1} B</script></span>, which is identical to <span class="arithmatex"><span class="MathJax_Preview">v=W^{-1 / 2} v_{1}^{*}</span><script type="math/tex">v=W^{-1 / 2} v_{1}^{*}</script></span> defined before. (ÂèÇËÄÉ‰∏äÈù¢ÁöÑ Ex4.1)</p>
<h4 id="lda">LDAÊÄªÁªì<a class="headerlink" href="#lda" title="Permanent link">&para;</a></h4>
<p>To summarize the developments so far:</p>
<ul>
<li>Gaussian classification with common covariances leads to linear decision boundaries. Classification can be achieved by sphering the data with respect to <span class="arithmatex"><span class="MathJax_Preview">\mathbf{W}</span><script type="math/tex">\mathbf{W}</script></span>, and classifying to the <strong>closest centroid</strong> (modulo <span class="arithmatex"><span class="MathJax_Preview">\left.\log \pi_{k}\right)</span><script type="math/tex">\left.\log \pi_{k}\right)</script></span> in the sphered space.</li>
<li>Since only the relative distances to the centroids count, one can confine the data to <strong>the subspace spanned by the centroids</strong> in the sphered space.</li>
<li>This subspace can be further <strong>decomposed into successively optimal subspaces</strong> in term of centroid separation. This decomposition is identical to the decomposition due to <strong>Fisher</strong>. Â≠êÁ©∫Èó¥ÂèØ‰ª•Ëøõ‰∏ÄÊ≠•ÂàÜËß£‰∏∫ÂÖ≥‰∫éÂΩ¢ÂøÉÂàÜÁ¶ªÁöÑÊúÄ‰ºòÂ≠êÁ©∫Èó¥ÔºéËøô‰∏™ÂàÜËß£‰∏é Fisher ÁöÑ ÂàÜËß£Áõ∏ÂêåÔºé</li>
</ul>
<p><img alt="" src="../media/ASL-note1/2021-12-04-14-25-45.png" /></p>
<h3 id="logistic-regression">Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permanent link">&para;</a></h3>
<p>ÂèÇËßÅ <a href="https://jozeelin.github.io/2019/06/14/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/">ÈÄªËæëÊñØËíÇÂõûÂΩíÊ®°Âûã‰∏éÊúÄÂ§ßÁÜµÊ®°Âûã</a></p>
<p>ÂõûÈ°æÁªèÂÖ∏ÁöÑ‰∫åÂàÜÁ±ªÈÄªËæëÂõûÂΩí. ÂÖ∂ logit/ÂØπÊï∞Âá†Áéá‰∏∫</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\log \frac{P(Y=1|x)}{1-P(Y=1|x)} = w'x
</div>
<script type="math/tex; mode=display">
\log \frac{P(Y=1|x)}{1-P(Y=1|x)} = w'x
</script>
</div>
<p>‰πüÂç≥Âú®ÈÄªËæëÊñØËíÇÂõûÂΩíÊ®°Âûã‰∏≠ÔºåËæìÂá∫Y=1ÁöÑÂØπÊï∞Âá†ÁéáÊòØËæìÂÖ•xÁöÑÁ∫øÊÄßÂáΩÊï∞„ÄÇ</p>
<p>Êç¢‰∏™ËßíÂ∫¶Êù•ÁúãÔºålogitÂáΩÊï∞ÁöÑÂÄºÂüüÊòØÂÆûÊï∞ÂüüÔºåÈÄöËøáÈÄªËæëÊñØËíÇÂõûÂΩíÊ®°ÂûãÂèØ‰ª•ÊäälogitÂáΩÊï∞ÁöÑÂÆûÊï∞ÂüüÊò†Â∞Ñ‰∏∫[0,1]ÔºåÁõ∏ÂΩì‰∫éÊ¶ÇÁéá„ÄÇ</p>
<p>Â§öÂàÜÁ±ª: Directly model the posterior probabilities of the <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> classes</p>
<p><span class="arithmatex"><span class="MathJax_Preview">\log \frac{\operatorname{Pr}(G=1 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{10}+\beta_{1}^{\top} x</span><script type="math/tex">\log \frac{\operatorname{Pr}(G=1 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{10}+\beta_{1}^{\top} x</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\log \frac{\operatorname{Pr}(G=2 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{20}+\beta_{2}^{\top} x</span><script type="math/tex">\log \frac{\operatorname{Pr}(G=2 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{20}+\beta_{2}^{\top} x</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\log \frac{\operatorname{Pr}(G=K-1 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{(K-1) 0}+\beta_{K-1}^{\top} x</span><script type="math/tex">\log \frac{\operatorname{Pr}(G=K-1 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{(K-1) 0}+\beta_{K-1}^{\top} x</script></span></p>
<p>Class <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> as reference level, all probabilities sum to 1.
Each probability lie in <span class="arithmatex"><span class="MathJax_Preview">[0,1]</span><script type="math/tex">[0,1]</script></span></p>
<p>ÂàÜÂà´Áî®Á¨¨K‰∏™Á±ªÂà´‰Ωú‰∏∫Âü∫ÂáÜ, ËÆ°ÁÆóÂÖ∂‰ªñÁ±ªÂà´Áõ∏ËæÉ‰∫éÂÆÉÁöÑ log-odd, ÁÑ∂ÂêéËøõË°åÂΩí‰∏ÄÂåñ</p>
<p>For <span class="arithmatex"><span class="MathJax_Preview">k=1, \ldots, K-1</span><script type="math/tex">k=1, \ldots, K-1</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(G=k \mid X=x)=\frac{\exp \left(\beta_{k 0}+\beta_{k}^{\top} x\right)}{1+\sum_{l=1}^{K-1} \exp \left(\beta_{10}+\beta_{1}^{\top} x\right)}</span><script type="math/tex">\operatorname{Pr}(G=k \mid X=x)=\frac{\exp \left(\beta_{k 0}+\beta_{k}^{\top} x\right)}{1+\sum_{l=1}^{K-1} \exp \left(\beta_{10}+\beta_{1}^{\top} x\right)}</script></span>
For <span class="arithmatex"><span class="MathJax_Preview">k=K-1</span><script type="math/tex">k=K-1</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(G=k \mid X=x)=\frac{1}{1+\sum_{l=1}^{K-1} \exp \left(\beta_{10}+\beta_{1}^{\top} x\right)}</span><script type="math/tex">\operatorname{Pr}(G=k \mid X=x)=\frac{1}{1+\sum_{l=1}^{K-1} \exp \left(\beta_{10}+\beta_{1}^{\top} x\right)}</script></span></p>
<p>ÂèØÁü•ÂàÜÁ±ªËæπÁïåÊòØÁ∫øÊÄßÁöÑ. Linear decision boundary between classes</p>
<div class="arithmatex">
<div class="MathJax_Preview">
x: \operatorname{Pr}(G=k \mid X=x)=\operatorname{Pr}(G=1 \mid X=x)
</div>
<script type="math/tex; mode=display">
x: \operatorname{Pr}(G=k \mid X=x)=\operatorname{Pr}(G=1 \mid X=x)
</script>
</div>
<p>which is the same as</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\left\{x:\left(\beta_{k 0}-\beta_{10}\right)+\left(\beta_{k}-\beta_{l}\right)^{\top} x=0\right\}
</div>
<script type="math/tex; mode=display">
\left\{x:\left(\beta_{k 0}-\beta_{10}\right)+\left(\beta_{k}-\beta_{l}\right)^{\top} x=0\right\}
</script>
</div>
<p>for <span class="arithmatex"><span class="MathJax_Preview">k=1, \ldots, K-1</span><script type="math/tex">k=1, \ldots, K-1</script></span> and <span class="arithmatex"><span class="MathJax_Preview">1=1, \ldots, K-1</span><script type="math/tex">1=1, \ldots, K-1</script></span></p>
<h4 id="logistic-regression-model-estimation">Logistic Regression Model Estimation<a class="headerlink" href="#logistic-regression-model-estimation" title="Permanent link">&para;</a></h4>
<p>ÂèÇÊï∞‰º∞ËÆ°, Áúã‰∫åÂàÜÁ±ª, ÂØπÊï∞‰ººÁÑ∂</p>
<p><strong>Two-class case</strong>, log likelihood can be written as</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">l(\beta)=\sum_{i=1}^{N}\left\{y_{i} \log p\left(x_{i} ; \beta\right)+\left(1-y_{i}\right) \log \left(1-p\left(x_{i} ; \beta\right)\right)\right\}</span><script type="math/tex">l(\beta)=\sum_{i=1}^{N}\left\{y_{i} \log p\left(x_{i} ; \beta\right)+\left(1-y_{i}\right) \log \left(1-p\left(x_{i} ; \beta\right)\right)\right\}</script></span></li>
<li>logit <span class="arithmatex"><span class="MathJax_Preview">p\left(x_{i} ; \beta\right)=\log \frac{p\left(x_{i} ; \beta\right)}{1-p\left(x_{i} ; \beta\right)}=\beta^{\top} x_{i}, \beta=\left(\beta_{0}, \beta_{1}\right)</span><script type="math/tex">p\left(x_{i} ; \beta\right)=\log \frac{p\left(x_{i} ; \beta\right)}{1-p\left(x_{i} ; \beta\right)}=\beta^{\top} x_{i}, \beta=\left(\beta_{0}, \beta_{1}\right)</script></span></li>
</ul>
<p>To maximize the log likelihood, obtain the first and second derivative</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\frac{\partial(\beta)}{\partial \beta}=\sum_{i=1}^{N} x_{i}\left(y_{i}- p\left(x_{i} ; \beta\right)\right)</span><script type="math/tex">\frac{\partial(\beta)}{\partial \beta}=\sum_{i=1}^{N} x_{i}\left(y_{i}- p\left(x_{i} ; \beta\right)\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\frac{\partial^{2}(\beta)}{\partial \beta \partial \beta^{\top}}=-\sum_{i=1}^{N} x_{i} x_{i}^{\top} p\left(x_{i} ; \beta\right)\left(1-p\left(x_{i} ; \beta\right)\right)</span><script type="math/tex">\frac{\partial^{2}(\beta)}{\partial \beta \partial \beta^{\top}}=-\sum_{i=1}^{N} x_{i} x_{i}^{\top} p\left(x_{i} ; \beta\right)\left(1-p\left(x_{i} ; \beta\right)\right)</script></span></li>
</ul>
<p>Iteratively update the initial value by <strong>Newton-Raphson algorithm</strong></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\beta^{\text {new }}=\beta^{\text {old }}-\left\{\frac{\partial l(\beta)}{\partial \beta \partial \beta^{\top}}\right\}^{-1} \frac{\partial(\beta)}{\partial \beta}</span><script type="math/tex">\beta^{\text {new }}=\beta^{\text {old }}-\left\{\frac{\partial l(\beta)}{\partial \beta \partial \beta^{\top}}\right\}^{-1} \frac{\partial(\beta)}{\partial \beta}</script></span></li>
<li>Derivatives evaluated at <span class="arithmatex"><span class="MathJax_Preview">\beta^{\text {old }}</span><script type="math/tex">\beta^{\text {old }}</script></span></li>
</ul>
<p>In <strong>matrix notation</strong>, let <span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> be <span class="arithmatex"><span class="MathJax_Preview">N \times N</span><script type="math/tex">N \times N</script></span> weight matrix with <span class="arithmatex"><span class="MathJax_Preview">i_{t h}</span><script type="math/tex">i_{t h}</script></span> diagonal element <span class="arithmatex"><span class="MathJax_Preview">p\left(x_{i} ; \beta^{\text {old }}\right)\left(1-p\left(x_{i} ; \beta^{\text {old }}\right)\right)</span><script type="math/tex">p\left(x_{i} ; \beta^{\text {old }}\right)\left(1-p\left(x_{i} ; \beta^{\text {old }}\right)\right)</script></span>
Then the first and second derivatives can be express as</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\begin{aligned}
&amp;\frac{\partial l(\beta)}{\partial \beta}=X^{\top}(y-p) \\
&amp;\frac{\partial^{2} l(\beta)}{\partial \beta \partial \beta^{\top}}=-X^{\top} W X
\end{aligned}</span><script type="math/tex">\begin{aligned}
&\frac{\partial l(\beta)}{\partial \beta}=X^{\top}(y-p) \\
&\frac{\partial^{2} l(\beta)}{\partial \beta \partial \beta^{\top}}=-X^{\top} W X
\end{aligned}</script></span></li>
</ul>
<p>Newton-Raphson updates</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\begin{aligned}
&amp;\beta^{\text {new }}=\beta^{\text {old }}+\left(X^{\top} W X\right)^{-1} X^{\top}(y-p)= \\
&amp;\left(x^{\top} W X\right)^{-1} X^{\top} W\left(X \beta^{\text {old }}+W^{-1}(y-p)\right)=\left(X^{\top} W X\right)^{-1} X^{\top} W z
\end{aligned}</span><script type="math/tex">\begin{aligned}
&\beta^{\text {new }}=\beta^{\text {old }}+\left(X^{\top} W X\right)^{-1} X^{\top}(y-p)= \\
&\left(x^{\top} W X\right)^{-1} X^{\top} W\left(X \beta^{\text {old }}+W^{-1}(y-p)\right)=\left(X^{\top} W X\right)^{-1} X^{\top} W z
\end{aligned}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">z=X \beta^{\text {old }}+W^{-1}(y-p)</span><script type="math/tex">z=X \beta^{\text {old }}+W^{-1}(y-p)</script></span>, also called <strong>adjusted response</strong> Ë∞ÉÊï¥ÂêéÁöÑÂìçÂ∫îÂèòÈáè</li>
<li><strong>Iteratively reweighted least squares</strong> ‰πüË¢´Âè´ÂÅö„ÄåÂä†ÊùÉËø≠‰ª£ÊúÄÂ∞è‰∫å‰πò„ÄçIRLS, Âõ†‰∏∫ÊØèÊ¨°ÈÉΩÊòØÊ±ÇËß£‰∏ãÈù¢ÁöÑÈóÆÈ¢ò:</li>
<li>At each iteration <span class="arithmatex"><span class="MathJax_Preview">\beta^{\text {new }} \leftarrow \operatorname{argmin}_{\beta}(z-X \beta)^{\top} W(z-X \beta)</span><script type="math/tex">\beta^{\text {new }} \leftarrow \operatorname{argmin}_{\beta}(z-X \beta)^{\top} W(z-X \beta)</script></span></li>
</ul>
<h4 id="south-african-heart-disease">‰æãÂ≠ê: South African Heart Disease<a class="headerlink" href="#south-african-heart-disease" title="Permanent link">&para;</a></h4>
<p>ËøôÈáåÁªôÂá∫‰∫Ü‰∏Ä‰∏™Áº∫Ë°ÄÊÄßÂøÉËÑèÁóÖÁöÑ‰∏Ä‰∏™ÈÄªËæëÂõûÂΩíÁªìÊûú(‰∫åÂàÜÁ±ª). Ë°®‰∏≠ÁöÑ Z scores ÊòØÁ≥ªÊï∞ÊØî‰∏äÊ†áÂáÜÂ∑Æ, ‰∏çÊòæËëóÁöÑËØùÂèØ‰ª•‰ªéÊ®°Âûã‰∏≠ÂâîÈô§. ËøôÈáåÊòØ <strong>Wald test</strong>, nullÂÅáËÆæ‰∏∫ the coeÔ¨Écient in question is zero, while all the others are not. Âú® 5% ÁΩÆ‰ø°Â∫¶‰∏ã Z score greater than approximately 2 ÊòØÊòæËëóÁöÑ.</p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-15-03-45.png" /></p>
<p>ËøôÈáåÂèëÁé∞ Systolic blood pressure (sbp) ‰∏çÊòæËëó, obesity ‰πü‰∏çÊòæËëóÁîöËá≥ÊòØË¥üÁöÑ (‰ΩÜÂÅöËøô‰∏§‰∏™ÁöÑÂõûÂΩí, ÁªìÊûúÈÉΩÊòØÊòæËëó‰∏∫Ê≠£ÁöÑ)</p>
<p>ËøôÊòØÂõ†‰∏∫ÂèòÈáè‰πãÈó¥ÁöÑÁõ∏ÂÖ≥ÊÄß, ÂèØ‰ª•ËøõË°åÊ®°ÂûãÈÄâÊã©Êàñ ÂÅèÂ∑ÆÂàÜÊûê (analysis of deviance).</p>
<p>ÂèÇÊï∞ÁöÑ<strong>Ëß£Èáä</strong>: ‰æãÂ¶ÇÂØπ a coeÔ¨Écient of 0.081 (Std. Error = 0.026) for tobacco, ÂèØ‰ª•Ëß£Èáä‰∏∫ an increase of 1kg in lifetime tobacco usage accounts for an increase in the odds of coronary heart disease of exp(0.081) = 1.084 or 8.4%  ÁªìÂêàÊÆãÂ∑ÆÂèØ‰ª•ÂæóÂà∞ an approximate 95% <strong>conÔ¨Ådence interval</strong> of exp(0.081 ¬± 2 √ó 0.026) = (1.03, 1.14).</p>
<h4 id="l1-regularized-logistic-regression">L1 Regularized Logistic Regression<a class="headerlink" href="#l1-regularized-logistic-regression" title="Permanent link">&para;</a></h4>
<p>Maximize a penalized version of the log likelihood <span class="arithmatex"><span class="MathJax_Preview">\max _{\beta_{0}, \beta}\left\{\sum_{i=1}^{N}\left[y_{i}\left(\beta_{0}+\beta^{\top} x_{i}\right)-\log \left(1+e^{\beta_{0}+\beta^{\top} x_{i}}\right)\right]-\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|\right\}</span><script type="math/tex">\max _{\beta_{0}, \beta}\left\{\sum_{i=1}^{N}\left[y_{i}\left(\beta_{0}+\beta^{\top} x_{i}\right)-\log \left(1+e^{\beta_{0}+\beta^{\top} x_{i}}\right)\right]-\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|\right\}</script></span></p>
<ul>
<li>Standardize predictors first, don&rsquo;t penalize intercept (Âíålasso‰∏ÄÊ†∑)</li>
<li>Penalized likelihood surface is concave, solutions can be found by <strong>nonlinear programming methods</strong> ÈùûÁ∫øÊÄßËßÑÂàíÊñπÊ≥ï</li>
<li>Quadratic approximation using Newton-Raphson by repeated application of a weighted lasso algorithm</li>
</ul>
<p><img alt="" src="../media/ASL-note1/2021-12-04-15-23-15.png" /></p>
<h4 id="logistic-regression-or-lda">Logistic Regression or LDA?<a class="headerlink" href="#logistic-regression-or-lda" title="Permanent link">&para;</a></h4>
<p>LDA decision boundary</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;\log \frac{\operatorname{Pr}(G=k \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}= \\
&amp;\log \frac{\pi_{K}}{\pi_{K}}-\frac{1}{2}\left(\mu_{k}+\mu_{K}\right)^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{K}\right)+x^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{K}\right)=\alpha_{k 0}+\alpha_{k}^{\top} x
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&\log \frac{\operatorname{Pr}(G=k \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}= \\
&\log \frac{\pi_{K}}{\pi_{K}}-\frac{1}{2}\left(\mu_{k}+\mu_{K}\right)^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{K}\right)+x^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{K}\right)=\alpha_{k 0}+\alpha_{k}^{\top} x
\end{aligned}
</script>
</div>
<p>Logistic regression</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\log \frac{\operatorname{Pr}(G=k \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{k 0}+\beta_{k}^{\top} x
</div>
<script type="math/tex; mode=display">
\log \frac{\operatorname{Pr}(G=k \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{k 0}+\beta_{k}^{\top} x
</script>
</div>
<p>Difference? <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(X, G=k)=\operatorname{Pr}(X) \operatorname{Pr}(G=k \mid X)</span><script type="math/tex">\operatorname{Pr}(X, G=k)=\operatorname{Pr}(X) \operatorname{Pr}(G=k \mid X)</script></span></p>
<ul>
<li>LDA is fit by maximizing the full likelihood <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(X, G=k)=\phi\left(X ; \mu_{k}, \Sigma\right) \pi_{k}</span><script type="math/tex">\operatorname{Pr}(X, G=k)=\phi\left(X ; \mu_{k}, \Sigma\right) \pi_{k}</script></span></li>
<li>Logistic regression leaves the marginal density of <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> as an arbitrary, and is fit by maximizing the conditional likelihood <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(G=k \mid X)</span><script type="math/tex">\operatorname{Pr}(G=k \mid X)</script></span></li>
</ul>
<p>The marginal density in LDA: <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(X)=\sum_{k=1}^{k} \pi_{k} \phi\left(X ; \mu_{k}, \Sigma\right)</span><script type="math/tex">\operatorname{Pr}(X)=\sum_{k=1}^{k} \pi_{k} \phi\left(X ; \mu_{k}, \Sigma\right)</script></span></p>
<p>Â∞ΩÁÆ°ÂÆÉ‰ª¨Á°ÆÂÆûÊúâÁõ∏ÂêåÁöÑÂΩ¢ÂºèÔºåÂå∫Âà´Âú®‰∫éÂÆÉ‰ª¨Á≥ªÊï∞‰º∞ËÆ°ÁöÑÊñπÂºè. <strong>ÈÄªËæëÊñØËíÇÂõûÂΩíÊ®°ÂûãÊõ¥Âä†‰∏ÄËà¨ÔºåÂõ†‰∏∫ÂÆÉÂÅö‰∫ÜÊõ¥Â∞ëÁöÑÂÅáËÆæ</strong>.</p>
<p>If <span class="arithmatex"><span class="MathJax_Preview">f_{k}(x) \sim \mathcal{N}</span><script type="math/tex">f_{k}(x) \sim \mathcal{N}</script></span></p>
<ul>
<li>LDA gives more info about the parameters</li>
<li>LDA can estimate parameters more efficiently (lower variance). ÈÄöËøá‰ª•Êù•È¢ùÂ§ñÁöÑÊ®°ÂûãÂÅáËÆæ, ÂèØ‰ª•ÂæóÂà∞Êõ¥Â§öÁöÑÂèÇÊï∞‰ø°ÊÅØ, Âõ†Ê≠§ÊñπÂ∑ÆÊõ¥‰Ωé</li>
</ul>
<p>Bad things about LDA&hellip;</p>
<ul>
<li>Less robust to gross outliers</li>
<li>Observations far from decision boundary influence <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}</span><script type="math/tex">\hat{\Sigma}</script></span></li>
<li>Observations without class labels have information about the parameters</li>
<li><strong>Quanlitative</strong> variables in <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span></li>
</ul>
<h3 id="naive-bayes">Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permanent link">&para;</a></h3>
<p>ÂèÇËßÅ <a href="https://jozeelin.github.io/2019/06/14/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/">Êú¥Á¥†Ë¥ùÂè∂ÊñØÊ≥ï</a></p>
<ul>
<li>Êú¥Á¥†Ë¥ùÂè∂ÊñØÊ≥ïÊòØÂü∫‰∫éË¥ùÂè∂ÊñØÂÆöÁêÜ‰∏éÁâπÂæÅÊù°‰ª∂Áã¨Á´ãÂÅáËÆæÁöÑÂàÜÁ±ªÊñπÊ≥ï„ÄÇ</li>
<li>Êú¥Á¥†Ë¥ùÂè∂ÊñØÊ≥ïÂÆûÈôÖ‰∏äÂ≠¶‰π†Âà∞ÁîüÊàêÊï∞ÊçÆÁöÑÊú∫Âà∂ÔºåÊâÄ‰ª•Â±û‰∫é<strong>ÁîüÊàêÊ®°Âûã</strong>„ÄÇ</li>
<li><strong>Êù°‰ª∂Áã¨Á´ãÂÅáËÆæ</strong>Á≠â‰∫éÊòØËØ¥Áî®‰∫éÂàÜÁ±ªÁöÑÁâπÂæÅÂú®Á±ªÁ°ÆÂÆöÁöÑÊù°‰ª∂‰∏ãÈÉΩÊòØÊù°‰ª∂Áã¨Á´ãÁöÑ„ÄÇËøô‰∏ÄÂÅáËÆæ‰ΩøÂæóÊù°‰ª∂Ê¶ÇÁéáÂàÜÂ∏ÉÁöÑÂ≠¶‰π†ÈöæÂ∫¶Èôç‰Ωé‰∫ÜÔºå‰ΩÜ‰ºöÁâ∫Áâ≤‰∏ÄÂÆöÁöÑÂàÜÁ±ªÂáÜÁ°ÆÁéá„ÄÇ</li>
<li>Êú¥Á¥†Ë¥ùÂè∂ÊñØÊ≥ïÂàÜÁ±ªÊó∂ÔºåÂØπÁªôÂÆöÁöÑËæìÂÖ•xÔºåÈÄöËøáÂ≠¶‰π†Âà∞ÁöÑÊ®°ÂûãËÆ°ÁÆóÂêéÈ™åÊ¶ÇÁéáÂàÜÂ∏É <span class="arithmatex"><span class="MathJax_Preview">P(Y=c_k|X=x)</span><script type="math/tex">P(Y=c_k|X=x)</script></span>ÔºåÂ∞ÜÂêéÈ™åÊ¶ÇÁéáÊúÄÂ§ßÁöÑÁ±ª‰Ωú‰∏∫xÁöÑÁ±ªËæìÂá∫„ÄÇ<ul>
<li>‰∏∫‰ªÄ‰πàÂ∞ÜÂêéÈ™åÊ¶ÇÁéáÊúÄÂ§ßÁöÑÁ±ª‰Ωú‰∏∫xÁöÑËæìÂá∫ÔºüÂõ†‰∏∫<strong>ÂêéÈ™åÊ¶ÇÁéáÊúÄÂ§ßÂåñÁ≠âÊïà‰∫éÊúüÊúõÈ£éÈô©ÊúÄÂ∞èÂåñ</strong>„ÄÇ</li>
</ul>
</li>
</ul>
<h4 id="_4">Ë°•ÂÖÖ: ÂêéÈ™åÊ¶ÇÁéáÊúÄÂ§ßÂåñÁ≠âÊïà‰∫éÊúüÊúõÈ£éÈô©ÊúÄÂ∞èÂåñ<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h4>
<p>ËÆæÈÄâÊã© <span class="arithmatex"><span class="MathJax_Preview">0-1</span><script type="math/tex">0-1</script></span> ÊçüÂ§±ÂáΩÊï∞:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L(Y, f(X))=\left\{\begin{array}{l}
1, Y \neq f(X) \\
0, Y=f(x)
\end{array}\right.
</div>
<script type="math/tex; mode=display">
L(Y, f(X))=\left\{\begin{array}{l}
1, Y \neq f(X) \\
0, Y=f(x)
\end{array}\right.
</script>
</div>
<p>Âºè‰∏≠ <span class="arithmatex"><span class="MathJax_Preview">f(X)</span><script type="math/tex">f(X)</script></span> ÊòØÂàÜÁ±ªÂÜ≥Á≠ñÂáΩÊï∞„ÄÇËøôÊó∂, ÊúüÊúõÈ£éÈô©ÂáΩÊï∞‰∏∫:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
R_{\exp }(f)=E[L(Y, f(X))]
</div>
<script type="math/tex; mode=display">
R_{\exp }(f)=E[L(Y, f(X))]
</script>
</div>
<p>ÊúüÊúõÊòØÂØπËÅîÂêàÂàÜÂ∏É <span class="arithmatex"><span class="MathJax_Preview">P(X, Y)</span><script type="math/tex">P(X, Y)</script></span> ÂèñÁöÑ„ÄÇÁî±Ê≠§ÂèØÂæóÂÖ≥‰∫éÊù°‰ª∂Ê¶ÇÁéá <span class="arithmatex"><span class="MathJax_Preview">P(Y \mid X)</span><script type="math/tex">P(Y \mid X)</script></span> ÁöÑÊúüÊúõ(ÊçüÂ§±ÂáΩÊï∞ÂÖ≥‰∫éÊù°‰ª∂Ê¶ÇÁéáÁöÑÊúüÊúõ):</p>
<div class="arithmatex">
<div class="MathJax_Preview">
R_{\exp }(f)=E_{X} \sum_{k=1}^{K}\left[L\left(c_{k}, f(X)\right)\right] P\left(c_{k} \mid X\right)
</div>
<script type="math/tex; mode=display">
R_{\exp }(f)=E_{X} \sum_{k=1}^{K}\left[L\left(c_{k}, f(X)\right)\right] P\left(c_{k} \mid X\right)
</script>
</div>
<p>‰∏∫‰∫Ü‰ΩøÂæóÊúüÊúõÈ£éÈô©ÊúÄÂ∞èÂåñ, Âè™ÈúÄÂØπ <span class="arithmatex"><span class="MathJax_Preview">X=x</span><script type="math/tex">X=x</script></span> ÈÄê‰∏™ÊûÅÂ∞èÂåñ(ÈÅçÂéÜÊâÄÊúâÁöÑÊ†∑Êú¨), Áî±Ê≠§ÂæóÂà∞:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
f(x) &amp;=\arg \min_{y \in \mathcal{Y}} \sum_{k=1}^{K} L\left(c_{k}, y\right) P\left(c_{k} \mid X=x\right) \\
&amp;=\arg \min_{y \in \mathcal{Y}} \sum_{k=1}^{K} P\left(y \neq c_{k} \mid X=x\right) \\
&amp;=\arg \min _{y \in \mathcal{Y}}\left(1-P\left(y=c_{k} \mid X=x\right)\right) \\
&amp;=\arg \max _{y \in \mathcal{Y}} P\left(y=c_{k} \mid X=x\right)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
f(x) &=\arg \min_{y \in \mathcal{Y}} \sum_{k=1}^{K} L\left(c_{k}, y\right) P\left(c_{k} \mid X=x\right) \\
&=\arg \min_{y \in \mathcal{Y}} \sum_{k=1}^{K} P\left(y \neq c_{k} \mid X=x\right) \\
&=\arg \min _{y \in \mathcal{Y}}\left(1-P\left(y=c_{k} \mid X=x\right)\right) \\
&=\arg \max _{y \in \mathcal{Y}} P\left(y=c_{k} \mid X=x\right)
\end{aligned}
</script>
</div>
<p>ËøôÊ†∑‰∏ÄÊù•, Ê†πÊçÆÊúüÊúõÈ£éÈô©ÊúÄÂ∞èÂáÜÂàôÂ∞±ÂæóÂà∞‰∫ÜÂêéÈ™åÊ¶ÇÂÆ§ÊúÄÂ§ßÂåñÂáÜÂàô:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f(x)=\arg \max _{c_{k}} P\left(y=c_{k} \mid X=x\right)(4.8)
</div>
<script type="math/tex; mode=display">
f(x)=\arg \max _{c_{k}} P\left(y=c_{k} \mid X=x\right)(4.8)
</script>
</div>
<h4 id="nb">NB Êé®ÂØº<a class="headerlink" href="#nb" title="Permanent link">&para;</a></h4>
<p>From Bayes Theorem</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(Y=k \mid x_{1}, \ldots, x_{p}\right) \propto \pi_{k} P\left(x_{1}, \ldots, x_{p} \mid Y=k\right)
</div>
<script type="math/tex; mode=display">
P\left(Y=k \mid x_{1}, \ldots, x_{p}\right) \propto \pi_{k} P\left(x_{1}, \ldots, x_{p} \mid Y=k\right)
</script>
</div>
<p>Naive Bayes assumption: conditional independence of <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span> &lsquo;s given <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(x_{1}, \ldots, x_{p} \mid Y=k\right)=\prod_{j=1}^{p} P\left(x_{j} \mid Y=k\right)
</div>
<script type="math/tex; mode=display">
P\left(x_{1}, \ldots, x_{p} \mid Y=k\right)=\prod_{j=1}^{p} P\left(x_{j} \mid Y=k\right)
</script>
</div>
<p>The relationship is thus simplified to</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(y \mid x_{1}, \ldots, x_{p}\right) \propto \pi_{k} \prod_{j=1}^{p} P\left(x_{j} \mid y\right)
</div>
<script type="math/tex; mode=display">
P\left(y \mid x_{1}, \ldots, x_{p}\right) \propto \pi_{k} \prod_{j=1}^{p} P\left(x_{j} \mid y\right)
</script>
</div>
<p>Therefore, the classification rule is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{y}=\operatorname{argmax}_{k} \pi_{k} \prod_{j=1}^{p} P\left(x_{j} \mid Y=k\right)
</div>
<script type="math/tex; mode=display">
\hat{y}=\operatorname{argmax}_{k} \pi_{k} \prod_{j=1}^{p} P\left(x_{j} \mid Y=k\right)
</script>
</div>
<p>Despite apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations (document classification and spam filtering)</p>
<p>Note: Naive Bayes is known as <strong>a decent classifier, but a bad estimator</strong> (probability outputs are not to be taken too seriously).</p>
<h4 id="gaussian-naive-bayes">Gaussian Naive Bayes<a class="headerlink" href="#gaussian-naive-bayes" title="Permanent link">&para;</a></h4>
<p>ÂØπ‰∫éËøûÁª≠ÂèòÈáè, ÂÅáËÆæÂàÜÂ∏É‰∏∫ Gaussian, MLE‰º∞ËÆ°.
The likelihood of the feature is assumed to be Gaussian</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(X_{j} \mid Y=k\right)=\frac{1}{\sqrt{2 \pi \sigma_{k}^{2}} \exp \left(-\frac{\left(x_{j}-\mu_{k}\right)^{2}}{2 \pi \sigma_{k}^{2}}\right)}
</div>
<script type="math/tex; mode=display">
P\left(X_{j} \mid Y=k\right)=\frac{1}{\sqrt{2 \pi \sigma_{k}^{2}} \exp \left(-\frac{\left(x_{j}-\mu_{k}\right)^{2}}{2 \pi \sigma_{k}^{2}}\right)}
</script>
</div>
<p>Parameters <span class="arithmatex"><span class="MathJax_Preview">\sigma_{k}</span><script type="math/tex">\sigma_{k}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\mu_{k}</span><script type="math/tex">\mu_{k}</script></span> are estimated using maximum likelihood method.</p>
<h4 id="multinomial-naive-bayes">Multinomial Naive Bayes<a class="headerlink" href="#multinomial-naive-bayes" title="Permanent link">&para;</a></h4>
<p>Often used in <strong>text classification</strong></p>
<ul>
<li>Data are typically represented as word vector counts</li>
</ul>
<p>Parameter vector <span class="arithmatex"><span class="MathJax_Preview">\theta_{k}=\left(\theta_{k 1}, \ldots, \theta_{k p}\right)</span><script type="math/tex">\theta_{k}=\left(\theta_{k 1}, \ldots, \theta_{k p}\right)</script></span> for each class <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">p:</span><script type="math/tex">p:</script></span> the number of features (the size of vocabulary)</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\theta_{k j}: P\left(x_{j} \mid Y=k\right)</span><script type="math/tex">\theta_{k j}: P\left(x_{j} \mid Y=k\right)</script></span>, the probability of feature <span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> appearing in a sample belonging to class <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span></li>
</ul>
<p><span class="arithmatex"><span class="MathJax_Preview">\theta_{k}</span><script type="math/tex">\theta_{k}</script></span> can be estimated by <strong>smooth version of maximum likelihood</strong></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta}_{k j}=\frac{N_{k j}+\alpha}{N_{k}+\alpha p}
</div>
<script type="math/tex; mode=display">
\hat{\theta}_{k j}=\frac{N_{k j}+\alpha}{N_{k}+\alpha p}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">N_{k j}:</span><script type="math/tex">N_{k j}:</script></span> number of times feature <span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> appears in class <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> in the training set</li>
<li><span class="arithmatex"><span class="MathJax_Preview">N_{k}</span><script type="math/tex">N_{k}</script></span> : total count of all features for class <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> in the training set</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\alpha&gt;0:</span><script type="math/tex">\alpha>0:</script></span> smoothing priors, prevent zero probabilities for feature not present in the training set</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\alpha=1</span><script type="math/tex">\alpha=1</script></span> : Laplace smoothing</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\alpha&lt;1</span><script type="math/tex">\alpha<1</script></span> : Lidstone smoothing</li>
</ul>
<h4 id="complement-naive-bayes">Complement Naive Bayes<a class="headerlink" href="#complement-naive-bayes" title="Permanent link">&para;</a></h4>
<ul>
<li>Adaptation of the standard Multinomial Naive Bayes algorithm</li>
<li>Suited for <strong>imbalanced data sets</strong> Á±ªÂà´‰∏çÂùáË°°</li>
<li>Uses statistics from the complement of each class to compute model&rsquo;s weights</li>
<li>Parameter estimates of CNB are more stable than those for MNB</li>
<li>CNB regularly outperforms MNB on text classification</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta}_{k j}=\frac{\alpha_{j}+\sum_{i: y_{i} \neq k} d_{i j}}{\alpha+\sum_{i: y_{i} \neq k} \sum_{l} d_{i l}}
</div>
<script type="math/tex; mode=display">
\hat{\theta}_{k j}=\frac{\alpha_{j}+\sum_{i: y_{i} \neq k} d_{i j}}{\alpha+\sum_{i: y_{i} \neq k} \sum_{l} d_{i l}}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">d_{i j}</span><script type="math/tex">d_{i j}</script></span> : count of term <span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> in document <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\alpha=\Sigma_{j} \alpha_{j}</span><script type="math/tex">\alpha=\Sigma_{j} \alpha_{j}</script></span></li>
<li>summation over all documents <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> not in class <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
w_{k j}=\frac{\log \hat{\theta}_{k j}}{\sum_{l} \log \hat{\theta}_{k l}} \\
\hat{k}=\operatorname{argmin}_{k} \sum_{j} t_{j} w_{k j}
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
w_{k j}=\frac{\log \hat{\theta}_{k j}}{\sum_{l} \log \hat{\theta}_{k l}} \\
\hat{k}=\operatorname{argmin}_{k} \sum_{j} t_{j} w_{k j}
\end{gathered}
</script>
</div>
<p>i.e., Classification rule: assign document to the class that is the poorest complement match</p>
<h3 id="seprating-hyperplanes">Seprating Hyperplanes Áï•<a class="headerlink" href="#seprating-hyperplanes" title="Permanent link">&para;</a></h3>
<h4 id="rosenblatts-perceptron-learning-algorithm">Rosenblatt&rsquo;s Perceptron Learning Algorithm ÊÑüÁü•Êú∫<a class="headerlink" href="#rosenblatts-perceptron-learning-algorithm" title="Permanent link">&para;</a></h4>
<p>ÂèÇËßÅ <a href="https://jozeelin.github.io/2019/06/13/%E6%84%9F%E7%9F%A5%E6%9C%BA/">ÊÑüÁü•Êú∫</a></p>
<ul>
<li>ÊÑüÁü•Êú∫ÊòØ‰∫åÂàÜÁ±ªÁöÑÁ∫øÊÄßÂàÜÁ±ªÊ®°Âûã„ÄÇ<ul>
<li>Ê®°ÂûãÔºöÊÑüÁü•Êú∫ÂØπÂ∫î‰∫éÂ∞ÜËæìÂÖ•Á©∫Èó¥(ÁâπÂæÅÁ©∫Èó¥)‰∏≠ÁöÑÂÆû‰æãÂàíÂàÜ‰∏∫Ê≠£Ë¥ü‰∏§Á±ªÁöÑÂàÜÁ¶ªË∂ÖÂπ≥Èù¢ÔºåÂ±û‰∫é<strong>Âà§Âà´Ê®°Âûã</strong>„ÄÇ</li>
<li>Á≠ñÁï•ÔºöÂü∫‰∫éËØØÂàÜÁ±ªÁöÑÊçüÂ§±ÂáΩÊï∞</li>
<li>ÁÆóÊ≥ïÔºöÂà©Áî®Ê¢ØÂ∫¶‰∏ãÈôçÊ≥ïÂØπÊçüÂ§±ÂáΩÊï∞ËøõË°åÊûÅÂ∞èÂåñ„ÄÇ</li>
</ul>
</li>
<li>ÊÑüÁü•Êú∫Â≠¶‰π†ÁÆóÊ≥ïÂàÜ‰∏∫ÂéüÂßãÂΩ¢ÂºèÂíåÂØπÂÅ∂ÂΩ¢Âºè„ÄÇ</li>
<li>ÊÑüÁü•Êú∫‰∫é1957Âπ¥RosenblattÊèêÂá∫ÔºåÊòØÁ•ûÁªèÁΩëÁªú‰∏éÊîØÊåÅÂêëÈáèÊú∫ÁöÑÂü∫Á°Ä„ÄÇ</li>
</ul>
<p>Find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary</p>
<div class="arithmatex">
<div class="MathJax_Preview">
D\left(\beta, \beta_{0}\right)=-\sum_{i \in \mathcal{M}} y_{i}\left(\beta_{0}+X_{i}^{\top} \beta\right)
</div>
<script type="math/tex; mode=display">
D\left(\beta, \beta_{0}\right)=-\sum_{i \in \mathcal{M}} y_{i}\left(\beta_{0}+X_{i}^{\top} \beta\right)
</script>
</div>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">y_{i}=1</span><script type="math/tex">y_{i}=1</script></span> is misclassified, <span class="arithmatex"><span class="MathJax_Preview">\beta_{0}+X_{i}^{\top} \beta&lt;0</span><script type="math/tex">\beta_{0}+X_{i}^{\top} \beta<0</script></span></li>
<li>If <span class="arithmatex"><span class="MathJax_Preview">y_{i}=-1</span><script type="math/tex">y_{i}=-1</script></span> is misclassified, <span class="arithmatex"><span class="MathJax_Preview">\beta_{0}+X_{i}^{\top} \beta&gt;0</span><script type="math/tex">\beta_{0}+X_{i}^{\top} \beta>0</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathcal{M}</span><script type="math/tex">\mathcal{M}</script></span> indexes the set of misclassified points (assumed fixed)</li>
</ul>
<p>The quantity <span class="arithmatex"><span class="MathJax_Preview">D\left(\beta, \beta_{0}\right)</span><script type="math/tex">D\left(\beta, \beta_{0}\right)</script></span> is non-negative and proportional to the distance of the misclassified points to the decision boundary defined by <span class="arithmatex"><span class="MathJax_Preview">\beta_{0}+X_{i}^{\top} \beta=0</span><script type="math/tex">\beta_{0}+X_{i}^{\top} \beta=0</script></span></p>
<h4 id="parameter-estimation-for-perceptron">Parameter Estimation for Perceptron<a class="headerlink" href="#parameter-estimation-for-perceptron" title="Permanent link">&para;</a></h4>
<p>Gradient is given by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;\quad \frac{\partial D\left(\beta, \beta_{0}\right)}{\partial \beta}=-\sum_{i \in \mathcal{M}} y_{i} x_{i} \\
&amp;\quad \frac{\partial D\left(\beta, \beta_{0}\right)}{\partial \beta_{0}}=-\sum_{i \in \mathcal{M}} y_{i}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&\quad \frac{\partial D\left(\beta, \beta_{0}\right)}{\partial \beta}=-\sum_{i \in \mathcal{M}} y_{i} x_{i} \\
&\quad \frac{\partial D\left(\beta, \beta_{0}\right)}{\partial \beta_{0}}=-\sum_{i \in \mathcal{M}} y_{i}
\end{aligned}
</script>
</div>
<p>Stochastic gradient descent</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\left[\begin{array}{c}
\beta \\
\beta_{0}
\end{array}\right] \leftarrow\left[\begin{array}{c}
\beta \\
\beta_{0}
\end{array}\right]+\rho\left[\begin{array}{c}
y_{i} x_{i} \\
y_{i}
\end{array}\right]
</div>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
\beta \\
\beta_{0}
\end{array}\right] \leftarrow\left[\begin{array}{c}
\beta \\
\beta_{0}
\end{array}\right]+\rho\left[\begin{array}{c}
y_{i} x_{i} \\
y_{i}
\end{array}\right]
</script>
</div>
<ul>
<li>A step is taken after <strong>each observation is visited</strong></li>
<li>Along the negative gradient direction</li>
<li>Repeat until no points are misclassified</li>
<li>Step size <span class="arithmatex"><span class="MathJax_Preview">\rho</span><script type="math/tex">\rho</script></span></li>
<li>More efficient than regular Gradient Descent</li>
</ul>
<h4 id="problems">ÊÑüÁü•Êú∫ÁöÑ Problems<a class="headerlink" href="#problems" title="Permanent link">&para;</a></h4>
<p>Details summarized in Ripley (1996)</p>
<ul>
<li>All separating hyperplanes are considered equally valid</li>
<li>When the data are separable, there are many solutions, and which one is found depends on the starting values</li>
<li>The &ldquo;finite&rdquo; number of steps can be very large. The smaller the gap, the longer the time to find it</li>
<li>When the data are not separable, the algorithm will not converge, and cycles develop. The cycles can be long and therefore hard to detect</li>
</ul>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2021-2022 Easonshi
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/Lightblues" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://lightblues.github.io/" target="_blank" rel="noopener" title="lightblues.github.io" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M575.8 255.5c0 18-15 32.1-32 32.1h-32l.7 160.2c0 2.7-.2 5.4-.5 8.1v16.2c0 22.1-17.9 40-40 40h-16c-1.1 0-2.2 0-3.3-.1-1.4.1-2.8.1-4.2.1L416 512h-24c-22.1 0-40-17.9-40-40v-88c0-17.7-14.3-32-32-32h-64c-17.7 0-32 14.3-32 32v88c0 22.1-17.9 40-40 40h-55.9c-1.5 0-3-.1-4.5-.2-1.2.1-2.4.2-3.6.2h-16c-22.1 0-40-17.9-40-40V360c0-.9 0-1.9.1-2.8v-69.7h-32c-18 0-32-14-32-32.1 0-9 3-17 10-24L266.4 8c7-7 15-8 22-8s15 2 21 7l255.4 224.5c8 7 12 15 11 24z"/></svg>
    </a>
  
    
    
    
    
    <a href="mailto:oldcitystal@gmail.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 112c-8.8 0-16 7.2-16 16v22.1l172.5 141.6c20.7 17 50.4 17 71.1 0L464 150.1V128c0-8.8-7.2-16-16-16H64zM48 212.2V384c0 8.8 7.2 16 16 16h384c8.8 0 16-7.2 16-16V212.2L322 328.8c-38.4 31.5-93.7 31.5-132 0L48 212.2zM0 128c0-35.3 28.7-64 64-64h384c35.3 0 64 28.7 64 64v256c0 35.3-28.7 64-64 64H64c-35.3 0-64-28.7-64-64V128z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.top"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
        <script src="../../js/baidu-tongji.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>