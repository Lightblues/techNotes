
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="O'ver curious">
      
      
        <meta name="author" content="Easonshi">
      
      
        <link rel="canonical" href="https://lightblues.github.io/techNotes/stat/ASL-note1/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.0, mkdocs-material-8.5.3">
    
    
      
        <title>ASL1 - techNotes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.7a952b86.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.cbb835fc.min.css">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent="">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#asl-note" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="techNotes" class="md-header__button md-logo" aria-label="techNotes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            techNotes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ASL1
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/lightblues/techNotes" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    lightblues/techNotes
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../.." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../../code/CS/git-note/" class="md-tabs__link">
        Code
      </a>
    </li>
  

  

      
        
  
  
    
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../" class="md-tabs__link md-tabs__link--active">
        Stat
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../Linux/Linux/" class="md-tabs__link">
        Linux
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../os/EFI/" class="md-tabs__link">
        OS
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../NLP/Lexicon-%E8%AF%8D%E6%B1%87%E5%A2%9E%E5%BC%BA/" class="md-tabs__link">
        NLP
      </a>
    </li>
  

      
        
  
  


  <li class="md-tabs__item">
    <a href="/" class="md-tabs__link">
      Blog
    </a>
  </li>

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="techNotes" class="md-nav__button md-logo" aria-label="techNotes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    techNotes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/lightblues/techNotes" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    lightblues/techNotes
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Code
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Code" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Code
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_1" type="checkbox" id="__nav_2_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_1">
          CS
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="CS" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          CS
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/CS/git-note/" class="md-nav__link">
        Git
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_2">
          Python
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Python" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          Python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/Python/Python-note/" class="md-nav__link">
        课程笔记
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_3" type="checkbox" id="__nav_2_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_3">
          JavaScripe
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="JavaScripe" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_3">
          <span class="md-nav__icon md-icon"></span>
          JavaScripe
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/js-mindmap/" class="md-nav__link">
        Mindmap
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/js-note/" class="md-nav__link">
        js note
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/jQuery-note/" class="md-nav__link">
        jQuery
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/node-js-note/" class="md-nav__link">
        Node.js
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Stat
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Stat" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Stat
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_1" type="checkbox" id="__nav_3_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3_1">
          高等统计学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="高等统计学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_1">
          <span class="md-nav__icon md-icon"></span>
          高等统计学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-mindmap/" class="md-nav__link">
        ASL mindmap
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          ASL1
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        ASL1
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
    <nav class="md-nav" aria-label="Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    引入
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regression-classification" class="md-nav__link">
    Regression \&amp; Classification
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    两种简单的预测方法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#statistical-decision-theory" class="md-nav__link">
    Statistical Decision Theory
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#curse-of-dimensionality" class="md-nav__link">
    Curse of Dimensionality
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    统计模型，监督学习和函数逼近
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#restricted-estimators" class="md-nav__link">
    限制性估计 Restricted Estimators
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-selection-and-the-biasvariance-tradeoff" class="md-nav__link">
    Model Selection and the Bias–Variance Tradeoff
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-methods-for-regression" class="md-nav__link">
    Linear Methods for Regression
  </a>
  
    <nav class="md-nav" aria-label="Linear Methods for Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction-of-linear-methods" class="md-nav__link">
    Introduction of Linear Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recap-of-linear-regression" class="md-nav__link">
    Recap of Linear Regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#subset-selection" class="md-nav__link">
    Subset Selection
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shrinkage-methods" class="md-nav__link">
    Shrinkage methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods-using-derived-input-directions" class="md-nav__link">
    Methods Using Derived Input Directions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#some-algebra" class="md-nav__link">
    Some Algebra
  </a>
  
    <nav class="md-nav" aria-label="Some Algebra">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#eigen-decomposition-square-matrix" class="md-nav__link">
    Eigen decomposition (square matrix)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singular-value-decomposition-any-matrix-m-times-nm-times-n" class="md-nav__link">
    Singular value decomposition (any matrix m \times nm \times n )
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pca-principle-component-analysis" class="md-nav__link">
    PCA (Principle Component Analysis)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-methods-for-classification" class="md-nav__link">
    Linear Methods for Classification
  </a>
  
    <nav class="md-nav" aria-label="Linear Methods for Classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#linear-regression-of-an-indicator-matrix" class="md-nav__link">
    Linear Regression of an Indicator Matrix
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linear-discriminant-analysis-lda" class="md-nav__link">
    Linear Discriminant Analysis (LDA)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lda-practicalities" class="md-nav__link">
    LDA Practicalities
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logistic-regression" class="md-nav__link">
    Logistic Regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#naive-bayes" class="md-nav__link">
    Naive Bayes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#seprating-hyperplanes" class="md-nav__link">
    Seprating Hyperplanes 略
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-note2/" class="md-nav__link">
        ASL2
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-note3/" class="md-nav__link">
        ASL3
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-note4/" class="md-nav__link">
        ASL4
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_2" type="checkbox" id="__nav_3_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_3_2">
          算法导论
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="算法导论" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_2">
          <span class="md-nav__icon md-icon"></span>
          算法导论
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-MinCut-2SAT/" class="md-nav__link">
        basic
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-prob-Markov-Chebyshev-Hoeffding/" class="md-nav__link">
        概率基础
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-computation-%E8%AE%A1%E7%AE%97%E7%90%86%E8%AE%BA/" class="md-nav__link">
        计算理论
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-graph-Adjacency-Laplacian/" class="md-nav__link">
        图相关, Laplacian
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-differential-privacy-%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/" class="md-nav__link">
        差分隐私
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../optimization-%E5%87%B8%E4%BC%98%E5%8C%96/" class="md-nav__link">
        凸优化
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Linux
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Linux" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Linux
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/Linux/" class="md-nav__link">
        General
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/tutor-%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83%E7%9B%B8%E5%85%B3/" class="md-nav__link">
        实验环境相关
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/Linux-maintain/" class="md-nav__link">
        系统维护
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/Linux-configure-make-install-%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/" class="md-nav__link">
        编译安装软件
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          OS
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="OS" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          OS
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/EFI/" class="md-nav__link">
        EFI
      </a>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_2" type="checkbox" id="__nav_5_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_2">
          OSs
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="OSs" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          OSs
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/macOS/" class="md-nav__link">
        macOS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/macOS-softwares/" class="md-nav__link">
        macOS 软件列表
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Hackintosh/" class="md-nav__link">
        hackintosh
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Arch/" class="md-nav__link">
        Arch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/CentOS/" class="md-nav__link">
        CentOS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Ubuntu/" class="md-nav__link">
        Ubuntu
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Windows/" class="md-nav__link">
        Windows
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6">
          NLP
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          NLP
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/Lexicon-%E8%AF%8D%E6%B1%87%E5%A2%9E%E5%BC%BA/" class="md-nav__link">
        Lexicon 词汇增强
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/Prompt-%E6%8F%90%E7%A4%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="md-nav__link">
        Prompt 提示语言模型
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/ABSA-Sentiment-Analysis-%E5%9F%BA%E4%BA%8E%E6%96%B9%E9%9D%A2%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/" class="md-nav__link">
        ABSA
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="/" class="md-nav__link">
        Blog
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
    <nav class="md-nav" aria-label="Introduction">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#_1" class="md-nav__link">
    引入
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regression-classification" class="md-nav__link">
    Regression \&amp; Classification
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_2" class="md-nav__link">
    两种简单的预测方法
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#statistical-decision-theory" class="md-nav__link">
    Statistical Decision Theory
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#curse-of-dimensionality" class="md-nav__link">
    Curse of Dimensionality
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#_3" class="md-nav__link">
    统计模型，监督学习和函数逼近
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#restricted-estimators" class="md-nav__link">
    限制性估计 Restricted Estimators
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-selection-and-the-biasvariance-tradeoff" class="md-nav__link">
    Model Selection and the Bias–Variance Tradeoff
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-methods-for-regression" class="md-nav__link">
    Linear Methods for Regression
  </a>
  
    <nav class="md-nav" aria-label="Linear Methods for Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#introduction-of-linear-methods" class="md-nav__link">
    Introduction of Linear Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recap-of-linear-regression" class="md-nav__link">
    Recap of Linear Regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#subset-selection" class="md-nav__link">
    Subset Selection
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#shrinkage-methods" class="md-nav__link">
    Shrinkage methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#methods-using-derived-input-directions" class="md-nav__link">
    Methods Using Derived Input Directions
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#some-algebra" class="md-nav__link">
    Some Algebra
  </a>
  
    <nav class="md-nav" aria-label="Some Algebra">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#eigen-decomposition-square-matrix" class="md-nav__link">
    Eigen decomposition (square matrix)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singular-value-decomposition-any-matrix-m-times-nm-times-n" class="md-nav__link">
    Singular value decomposition (any matrix m \times nm \times n )
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#pca-principle-component-analysis" class="md-nav__link">
    PCA (Principle Component Analysis)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#linear-methods-for-classification" class="md-nav__link">
    Linear Methods for Classification
  </a>
  
    <nav class="md-nav" aria-label="Linear Methods for Classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#linear-regression-of-an-indicator-matrix" class="md-nav__link">
    Linear Regression of an Indicator Matrix
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#linear-discriminant-analysis-lda" class="md-nav__link">
    Linear Discriminant Analysis (LDA)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#lda-practicalities" class="md-nav__link">
    LDA Practicalities
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#logistic-regression" class="md-nav__link">
    Logistic Regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#naive-bayes" class="md-nav__link">
    Naive Bayes
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#seprating-hyperplanes" class="md-nav__link">
    Seprating Hyperplanes 略
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  <a href="https://github.com/lightblues/techNotes/edit/main/docs/stat/ASL-note1.md" title="Edit this page" class="md-content__button md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>


<h1 id="asl-note">ASL note<a class="headerlink" href="#asl-note" title="Permanent link">&para;</a></h1>
<p>参见</p>
<ul>
<li>ESL, <a href="https://esl.hohoweiya.xyz/">中文版</a> ⭐️</li>
<li><a href="https://jozeelin.github.io/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/">Jozee&rsquo;s技术博客|机器学习</a> ⭐️</li>
</ul>
<p>其他</p>
<ul>
<li><a href="https://baidinghub.github.io/2020/04/03/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%EF%BC%88%E5%85%AD%EF%BC%89%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/">机器学习（六）贝叶斯分类器</a>；Blog <a href="https://baidinghub.github.io/">https://baidinghub.github.io/</a> 很赞</li>
<li><a href="https://weirping.github.io/blog/Bayesian-Probabilities-in-ML.html">叶斯线性回归与贝叶斯逻辑回归</a></li>
<li>PRML 笔记 <a href="http://www.datakit.cn/category/#PRML">http://www.datakit.cn/category/#PRML</a></li>
<li><a href="https://keson96.github.io/2017/04/07/2017-04-07-Bayesian-Model-Comparison/">贝叶斯模型比较(Bayesian Model Comparison)</a></li>
</ul>
<h2 id="introduction">Introduction<a class="headerlink" href="#introduction" title="Permanent link">&para;</a></h2>
<p>Outline</p>
<ul>
<li>Variable Types and Terminology</li>
<li>Two Simple Approaches to Prediction<ul>
<li>Least Squares</li>
<li>Nearest-Neighbour Methods</li>
<li>Comparison</li>
</ul>
</li>
<li>Statistical Decision Theory</li>
<li>Curse of Dimensionality</li>
<li>Statistical Models, Supervised Learning \&amp; Function Approximation</li>
<li>Class of Restricted Estimators</li>
<li>Bias Variance Tradeoff</li>
</ul>
<h3 id="_1">引入<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h3>
<p>Cases:</p>
<ul>
<li>Email spam</li>
<li>Prostate Cancer (continuous)</li>
<li>Handwritten Digit Recognition</li>
<li>DNA Expression Microarrays (cluster)</li>
</ul>
<h3 id="regression-classification">Regression \&amp; Classification<a class="headerlink" href="#regression-classification" title="Permanent link">&para;</a></h3>
<p>Inputs - predictors, independent variables, features&hellip;
Outputs - outcome, response, dependent variable, label&hellip;</p>
<ul>
<li>Regression problems - outputs being quantitative measurement<ul>
<li>Measurements close in value are also close in nature</li>
<li>Prostate cancer example</li>
</ul>
</li>
<li>Classification problems - outputs being qualitative<ul>
<li>No explicit ordering in the outputs</li>
<li>Email spam, Handwritten digits recognition</li>
</ul>
</li>
<li>Special case - ordered categorical outputs<ul>
<li>e.g., {small, medium, large}</li>
<li>difference between medium and small need not be the same as that between large and medium</li>
</ul>
</li>
</ul>
<h3 id="_2">两种简单的预测方法<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h3>
<h4 id="least-squares">Least Squares<a class="headerlink" href="#least-squares" title="Permanent link">&para;</a></h4>
<p>Given a vector of inputs <span class="arithmatex"><span class="MathJax_Preview">X^{\top}=\left(X_{1}, X_{2}, \ldots, X_{P}\right)</span><script type="math/tex">X^{\top}=\left(X_{1}, X_{2}, \ldots, X_{P}\right)</script></span>, predict the output <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> (scalar) by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{Y}=X^{\top} \hat{\beta}
</div>
<script type="math/tex; mode=display">
\hat{Y}=X^{\top} \hat{\beta}
</script>
</div>
<p>Note: in this course, we will always assume <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> includes a constant variable <span class="arithmatex"><span class="MathJax_Preview">1, \beta</span><script type="math/tex">1, \beta</script></span> include the intercept <span class="arithmatex"><span class="MathJax_Preview">\beta_{0}</span><script type="math/tex">\beta_{0}</script></span></p>
<p>Question: How do we fit the linear model to a set of training data <span class="arithmatex"><span class="MathJax_Preview">\left\{\left(X_{1}, y_{1}\right),\left(X_{2}, y_{2}\right), \ldots,\left(X_{N}, y_{N}\right)\right\} ?</span><script type="math/tex">\left\{\left(X_{1}, y_{1}\right),\left(X_{2}, y_{2}\right), \ldots,\left(X_{N}, y_{N}\right)\right\} ?</script></span></p>
<p>Answer: Many different methods, most popular - least squares</p>
<ul>
<li>Pick <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> that minimize the residual sum of squares (RSS)</li>
<li><span class="arithmatex"><span class="MathJax_Preview">R S S(\beta)=\sum_{i=1}^{N}\left(y_{i}-x_{i}^{\top} \beta\right)^{2}</span><script type="math/tex">R S S(\beta)=\sum_{i=1}^{N}\left(y_{i}-x_{i}^{\top} \beta\right)^{2}</script></span></li>
<li>In matrix notation <span class="arithmatex"><span class="MathJax_Preview">R S S(\beta)=(y-X \beta)^{\top}(y-X \beta)</span><script type="math/tex">R S S(\beta)=(y-X \beta)^{\top}(y-X \beta)</script></span></li>
<li>Quandratic function of <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span>, so its minimum always exists (but may not be unique!)</li>
<li>Differentiating w.r.t. <span class="arithmatex"><span class="MathJax_Preview">\beta \rightarrow X^{\top}(y-X \beta)=0</span><script type="math/tex">\beta \rightarrow X^{\top}(y-X \beta)=0</script></span></li>
<li>If <span class="arithmatex"><span class="MathJax_Preview">X^{\top} X</span><script type="math/tex">X^{\top} X</script></span> is nonsingular, the unique solution <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}=\left(X^{\top} X\right)^{-1} X^{\top} y</span><script type="math/tex">\hat{\beta}=\left(X^{\top} X\right)^{-1} X^{\top} y</script></span></li>
</ul>
<p>Fitted value at <span class="arithmatex"><span class="MathJax_Preview">x_{i}: \hat{y}_{i}=x_{i}^{\top} \hat{\beta}</span><script type="math/tex">x_{i}: \hat{y}_{i}=x_{i}^{\top} \hat{\beta}</script></span></p>
<p>Predicted value at an arbituary input <span class="arithmatex"><span class="MathJax_Preview">x_{0}: \hat{y}_{0}=x_{0}^{\top} \hat{\beta}</span><script type="math/tex">x_{0}: \hat{y}_{0}=x_{0}^{\top} \hat{\beta}</script></span></p>
<h4 id="nearest-neighbour-methods">Nearest-Neighbour Methods<a class="headerlink" href="#nearest-neighbour-methods" title="Permanent link">&para;</a></h4>
<p>Given a vector of inputs <span class="arithmatex"><span class="MathJax_Preview">X^{\top}=\left(X_{1}, X_{2}, \ldots, X_{p}\right)</span><script type="math/tex">X^{\top}=\left(X_{1}, X_{2}, \ldots, X_{p}\right)</script></span>, predict the output <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> (scalar) by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{Y}(x)=\frac{1}{k} \sum_{x_{i} \in N_{k}(x)} y_{i}
</div>
<script type="math/tex; mode=display">
\hat{Y}(x)=\frac{1}{k} \sum_{x_{i} \in N_{k}(x)} y_{i}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">N_{k}(x)</span><script type="math/tex">N_{k}(x)</script></span> is the neighborhood of <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> defined by the <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> closest points <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> in the training sample</p>
<p>Question: How to define &ldquo;closest&rdquo;?</p>
<ul>
<li>Can be defined on different metrics.</li>
<li>For the moment, assume Euclidean distance</li>
</ul>
<h4 id="least-square-vs-knn">Least Square vs KNN (都可以完成分类和回归任务)<a class="headerlink" href="#least-square-vs-knn" title="Permanent link">&para;</a></h4>
<p>比较线性模型和kNN</p>
<ul>
<li>Linear<ul>
<li>Smooth and stable decision boundry</li>
<li>Linear assumption</li>
<li>High bias and low variance</li>
</ul>
</li>
<li>kNN<ul>
<li>Wiggly and unstable decision boundry</li>
<li>Assumption free</li>
<li>Low bias and high variance (kNN 的有效参数数量为 <span class="arithmatex"><span class="MathJax_Preview">N/k</span><script type="math/tex">N/k</script></span>, 一般大于线性模型的参数量)</li>
</ul>
</li>
</ul>
<p><img alt="" src="../media/ASL-note1/2022-01-06-15-19-40.png" /></p>
<p>这里展示了在一个数据集上(更适合 kNN)的表现, 紫色为 Bayes 错误率, 两个方块是线性模型的表现, 而两条折线则是 kNN 在选择不同参数时的表现.</p>
<h4 id="variants-of-l-sl-s-and-knn">Variants of <span class="arithmatex"><span class="MathJax_Preview">L S</span><script type="math/tex">L S</script></span> and KNN<a class="headerlink" href="#variants-of-l-sl-s-and-knn" title="Permanent link">&para;</a></h4>
<ul>
<li>Kernal methods<ul>
<li>Weights decrease smoothly as distance further from the target point</li>
<li>In high-dimensional space, distance kernels are modified to emphasize on some variables more than others</li>
</ul>
</li>
<li>Local regression<ul>
<li>Fit linear models by locally weighted least squares</li>
</ul>
</li>
<li>Basis expansion<ul>
<li>Allow more complex inputs</li>
</ul>
</li>
</ul>
<h3 id="statistical-decision-theory">Statistical Decision Theory<a class="headerlink" href="#statistical-decision-theory" title="Permanent link">&para;</a></h3>
<h4 id="loss-function">Loss Function<a class="headerlink" href="#loss-function" title="Permanent link">&para;</a></h4>
<ul>
<li>Squared loss<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathcal{L}(Y, f(x))=(Y-f(x))^{2}</span><script type="math/tex">\mathcal{L}(Y, f(x))=(Y-f(x))^{2}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">f(x)=\operatorname{argmin}_{c}\left(E_{Y \mid X}(Y-c)^{2} \mid X=x\right)=E(Y \mid X=x)</span><script type="math/tex">f(x)=\operatorname{argmin}_{c}\left(E_{Y \mid X}(Y-c)^{2} \mid X=x\right)=E(Y \mid X=x)</script></span></li>
</ul>
</li>
<li>L1 loss<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathcal{L}(Y, f(x))=|Y-f(x)|</span><script type="math/tex">\mathcal{L}(Y, f(x))=|Y-f(x)|</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\rightarrow f(x)=\operatorname{median}(Y \mid X=x)</span><script type="math/tex">\rightarrow f(x)=\operatorname{median}(Y \mid X=x)</script></span></li>
</ul>
</li>
<li>0-1 loss<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\rightarrow \mathcal{L}(G, G(x))=\sum_{k} \mathcal{L}\left(G_{k}-\hat{G}(x)\right) \operatorname{Pr}\left(G_{k} \mid X\right)</span><script type="math/tex">\rightarrow \mathcal{L}(G, G(x))=\sum_{k} \mathcal{L}\left(G_{k}-\hat{G}(x)\right) \operatorname{Pr}\left(G_{k} \mid X\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{G}(x)=\operatorname{argmax}_{G \in \mathcal{G}} \operatorname{Pr}(G \mid X=x)</span><script type="math/tex">\hat{G}(x)=\operatorname{argmax}_{G \in \mathcal{G}} \operatorname{Pr}(G \mid X=x)</script></span></li>
</ul>
</li>
</ul>
<h4 id="squared-loss">Squared Loss<a class="headerlink" href="#squared-loss" title="Permanent link">&para;</a></h4>
<ul>
<li><strong>Expected prediction error</strong><ul>
<li><span class="arithmatex"><span class="MathJax_Preview">E P E(f)=E(Y-f(x))^{2}=\int(y-f(x))^{2} \operatorname{Pr}(d x, d y)</span><script type="math/tex">E P E(f)=E(Y-f(x))^{2}=\int(y-f(x))^{2} \operatorname{Pr}(d x, d y)</script></span></li>
<li>注意这里是对于 X 和 Y 都取期望</li>
</ul>
</li>
<li>By conditioning on <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>, we can write EPE as <span class="arithmatex"><span class="MathJax_Preview">E P E(f)=E_{X} E_{Y \mid X}\left[(y-f(x))^{2} \mid x\right]</span><script type="math/tex">E P E(f)=E_{X} E_{Y \mid X}\left[(y-f(x))^{2} \mid x\right]</script></span></li>
<li>It suffices to minimize <span class="arithmatex"><span class="MathJax_Preview">E P E</span><script type="math/tex">E P E</script></span> pointwise<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">f(x)=\operatorname{argmin}_{c} E_{Y \mid X}\left[(Y-c)^{2} \mid X=x\right]</span><script type="math/tex">f(x)=\operatorname{argmin}_{c} E_{Y \mid X}\left[(Y-c)^{2} \mid X=x\right]</script></span></li>
<li>我们只需要预测函数 <strong>逐点最小</strong> 即可</li>
</ul>
</li>
<li>The solution is 条件期望<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">f(x)=E(Y \mid X=x)</span><script type="math/tex">f(x)=E(Y \mid X=x)</script></span></li>
<li>也被称作 回归 (regression) 函数</li>
</ul>
</li>
</ul>
<p>For least squares</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
f(x) &amp;=E(Y \mid X=x)=X \beta \\
f(x) &amp;=X \hat{\beta}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
f(x) &=E(Y \mid X=x)=X \beta \\
f(x) &=X \hat{\beta}
\end{aligned}
</script>
</div>
<p>For k-nearest-neighbour</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}(x)=\operatorname{Ave}\left(y_{i} \mid x_{i} \in N_{k}(x)\right)
</div>
<script type="math/tex; mode=display">
\hat{f}(x)=\operatorname{Ave}\left(y_{i} \mid x_{i} \in N_{k}(x)\right)
</script>
</div>
<ul>
<li>Expectation is approximated by averaging over sample data</li>
<li>Conditioning at a point is relaxed to conditioning on some region &ldquo;close&rdquo; to the target point</li>
<li>For large training set and large <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>, approximation works better (但正如下一节所说, 会有维度灾难的问题)</li>
</ul>
<h4 id="0-1-loss">0-1 LOSS<a class="headerlink" href="#0-1-loss" title="Permanent link">&para;</a></h4>
<ul>
<li>Expected prediction error for categorical output <span class="arithmatex"><span class="MathJax_Preview">G</span><script type="math/tex">G</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">E P E=E[L(G, \hat{G}(x))]</span><script type="math/tex">E P E=E[L(G, \hat{G}(x))]</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">L(k, l)</span><script type="math/tex">L(k, l)</script></span> denotes the price paid for classifying an observation belonging to <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> as <span class="arithmatex"><span class="MathJax_Preview">l</span><script type="math/tex">l</script></span>.</li>
<li>By conditioning on <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>, we can write EPE as</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
E P E=E_{x} \sum_{k=1}^{k} L\left[G_{k}, \hat{G}(x)\right] \operatorname{Pr}\left(G_{k} \mid X=x\right)
</div>
<script type="math/tex; mode=display">
E P E=E_{x} \sum_{k=1}^{k} L\left[G_{k}, \hat{G}(x)\right] \operatorname{Pr}\left(G_{k} \mid X=x\right)
</script>
</div>
<ul>
<li>同样也是逐点最小化, So <span class="arithmatex"><span class="MathJax_Preview">\hat{G}(x)=\operatorname{argmin}_{G \in \mathcal{G}} \sum_{k=1}^{k} L\left(G_{k}, g\right) \operatorname{Pr}\left(G_{k} \mid X=x\right)</span><script type="math/tex">\hat{G}(x)=\operatorname{argmin}_{G \in \mathcal{G}} \sum_{k=1}^{k} L\left(G_{k}, g\right) \operatorname{Pr}\left(G_{k} \mid X=x\right)</script></span></li>
<li>For 0-1 loss, <span class="arithmatex"><span class="MathJax_Preview">\hat{G}(X)=G_{k}</span><script type="math/tex">\hat{G}(X)=G_{k}</script></span> if <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}\left(G_{k} \mid X=x\right)=\max _{g} \operatorname{Pr}(g \mid X=x)</span><script type="math/tex">\operatorname{Pr}\left(G_{k} \mid X=x\right)=\max _{g} \operatorname{Pr}(g \mid X=x)</script></span></li>
<li>In words, classify to the most probable class.</li>
</ul>
<p>当条件分布已知的情况下, 理想情况就是按照后验概率进行估计, 称为 <strong>贝叶斯分类器</strong> (Bayes classifier); 贝叶斯分类的误差率被称作 贝叶斯率 (Bayes rate).</p>
<p>For least squares (indicator variable regression)</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">G(x)</span><script type="math/tex">G(x)</script></span> is approximated by <span class="arithmatex"><span class="MathJax_Preview">X \hat{\beta}</span><script type="math/tex">X \hat{\beta}</script></span></li>
<li>in practice problems can occur</li>
</ul>
<p>For k-nearest-neighbour</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{G}(x)</span><script type="math/tex">\hat{G}(x)</script></span> - majority vote in a neighbourhood</li>
<li>Conditional probability at a point is relaxed to conditional probability within a neighborhood of a point</li>
<li>Probabilities are estimated by training-sample proportions.</li>
</ul>
<h3 id="curse-of-dimensionality">Curse of Dimensionality<a class="headerlink" href="#curse-of-dimensionality" title="Permanent link">&para;</a></h3>
<p>对维数灾难的直观理解</p>
<ul>
<li>数据分布稀疏: 考虑在 p 维宽度为 1 立方体中均匀分布的情况, 如果在一个小的立方体中包括 10% 的数据, 当 p=10 时需要的立方体大小为 0.8 (其实就是, 要覆盖 p 维的数据点要很大)</li>
<li>数据很多会出现在边界附近: 例子是在一个 p 维球体中, 各数据点距离球心的平均距离随着维度的增加而增加, 也即多考虑边界; 这样, 就不能采用内差而需要外推. (也可理解为数据稀疏?)</li>
</ul>
<h3 id="_3">统计模型，监督学习和函数逼近<a class="headerlink" href="#_3" title="Permanent link">&para;</a></h3>
<p>我们的目标是寻找函数 <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> 的一个有用的近似 <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(x)</span><script type="math/tex">\hat{f}(x)</script></span>, 函数 <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> 蕴含着输入与输出之间的预测关系. 在前面统计判别理论的章节的理论准备中, 对于定量的响应, 我们看到平方误差损失引导我们得到了回归函数 <span class="arithmatex"><span class="MathJax_Preview">f(X)=\mathrm{E}(Y \mid X=x)</span><script type="math/tex">f(X)=\mathrm{E}(Y \mid X=x)</script></span>. 最近邻方法可以看成是对条件期望的直接估计, 但是我们可以看到至少在两个方面它们不起作用</p>
<ul>
<li>如果输入空间的维数高, 则最近邻不必离目标点近, 而且可能导致大的误差</li>
<li>如果知道存在特殊的结构, 可以用来降低估计的偏差与方差.</li>
</ul>
<p>我们预先用了关于 <span class="arithmatex"><span class="MathJax_Preview">f(X)</span><script type="math/tex">f(X)</script></span> 的其它类别的模型, 在很多情形下是为了解决维数问题而特别设计的, 现在我们讨论把它们合并进一个预测问题的框架.</p>
<h4 id="statistical-models">Statistical Models<a class="headerlink" href="#statistical-models" title="Permanent link">&para;</a></h4>
<ul>
<li>Goal<ul>
<li>Predict the relationship between inputs and outputs with <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span></li>
<li>Approximate <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> using <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(x)</span><script type="math/tex">\hat{f}(x)</script></span></li>
</ul>
</li>
<li>Quantitative output<ul>
<li>Squared error loss <span class="arithmatex"><span class="MathJax_Preview">\rightarrow f(x)=E(Y \mid X=x)</span><script type="math/tex">\rightarrow f(x)=E(Y \mid X=x)</script></span></li>
<li>Choice for <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(x)</span><script type="math/tex">\hat{f}(x)</script></span> : additive error model, nearest neighbour, &hellip;</li>
</ul>
</li>
<li>Qualitative output<ul>
<li>Additive error models typically not used (additive 误差模型: 在实际的 f 中有一个误差项) 在分类问题中一般不做这个假设, 因此直接对于条件概率建模</li>
<li>Directly model <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(G \mid X)</span><script type="math/tex">\operatorname{Pr}(G \mid X)</script></span> : logistic regression, probit regression, <span class="arithmatex"><span class="MathJax_Preview">\ldots</span><script type="math/tex">\ldots</script></span></li>
</ul>
</li>
</ul>
<h4 id="function-approximation">Function Approximation<a class="headerlink" href="#function-approximation" title="Permanent link">&para;</a></h4>
<p>一些基本的模型</p>
<p>Common approximations ( <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> denotes parameters)</p>
<ul>
<li>Linear model <span class="arithmatex"><span class="MathJax_Preview">f(x)=X^{\top} \beta</span><script type="math/tex">f(x)=X^{\top} \beta</script></span></li>
<li>Basis expansion <span class="arithmatex"><span class="MathJax_Preview">f_{\theta}(x)=\sum_{k=1}^{K} h(x) \theta_{k}</span><script type="math/tex">f_{\theta}(x)=\sum_{k=1}^{K} h(x) \theta_{k}</script></span><ul>
<li><span class="arithmatex"><span class="MathJax_Preview">h(x)</span><script type="math/tex">h(x)</script></span> are transformations of <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span></li>
<li>Polynomial and trigonometric expansions <span class="arithmatex"><span class="MathJax_Preview">x_{1}^{2}, x_{1} x_{2}^{2}, \cos \left(x_{1}\right)</span><script type="math/tex">x_{1}^{2}, x_{1} x_{2}^{2}, \cos \left(x_{1}\right)</script></span></li>
<li>Sigmoid transformation <span class="arithmatex"><span class="MathJax_Preview">\frac{1}{1+\exp \left(-x^{\top} \beta_{k}\right)}</span><script type="math/tex">\frac{1}{1+\exp \left(-x^{\top} \beta_{k}\right)}</script></span></li>
</ul>
</li>
</ul>
<p>Parameter estimate</p>
<ul>
<li>Least squares<ul>
<li>Closed form for linear model</li>
<li>May require iterative methods or numerical optimization</li>
</ul>
</li>
<li>Maximum likelihood estimation - more general<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathcal{L}(\theta)=\sum_{i=1}^{N} \log P r_{\theta}\left(y_{i}\right)</span><script type="math/tex">\mathcal{L}(\theta)=\sum_{i=1}^{N} \log P r_{\theta}\left(y_{i}\right)</script></span> <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}=\operatorname{argmax}_{\theta} \mathcal{L}(\theta)</span><script type="math/tex">\hat{\theta}=\operatorname{argmax}_{\theta} \mathcal{L}(\theta)</script></span></li>
</ul>
</li>
</ul>
<h5 id="mle-normal-distribution">例子: MLE - Normal Distribution<a class="headerlink" href="#mle-normal-distribution" title="Permanent link">&para;</a></h5>
<p>Normal distribution</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f\left(x \mid \mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
</div>
<script type="math/tex; mode=display">
f\left(x \mid \mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}}
</script>
</div>
<p>Normal likelihood</p>
<ul>
<li>Assume <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(Y \mid X, \beta)=\mathcal{N}\left(X \beta, \sigma^{2}\right)</span><script type="math/tex">\operatorname{Pr}(Y \mid X, \beta)=\mathcal{N}\left(X \beta, \sigma^{2}\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">I(\beta)=-\frac{N}{2} \log (2 \pi)-N \log (\sigma)-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{N}\left(y_{i}-X_{i} \beta\right)^{2}</span><script type="math/tex">I(\beta)=-\frac{N}{2} \log (2 \pi)-N \log (\sigma)-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{N}\left(y_{i}-X_{i} \beta\right)^{2}</script></span></li>
<li>Maximizing <span class="arithmatex"><span class="MathJax_Preview">I(\theta)</span><script type="math/tex">I(\theta)</script></span> only involves the last term <span class="arithmatex"><span class="MathJax_Preview">-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{N}\left(y_{i}-f_{\theta}\left(x_{i}\right)\right)^{2}</span><script type="math/tex">-\frac{1}{2 \sigma^{2}} \sum_{i=1}^{N}\left(y_{i}-f_{\theta}\left(x_{i}\right)\right)^{2}</script></span>, which is equivalent to minimizing RSS</li>
</ul>
<p>结论: 在正态分布条件下, MLE 等价于 最小化RSS.</p>
<h5 id="mle-multinomial-distribution">例子: MLE - Multinomial Distribution<a class="headerlink" href="#mle-multinomial-distribution" title="Permanent link">&para;</a></h5>
<p>Multinomial distribution</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(X_{1}=x_{1}, \ldots, X_{m}=x_{m}\right)=\frac{n !}{\prod_{i=1}^{m} x_{i} !} \prod_{i=1}^{m} \theta_{i}^{x_{i}}
</div>
<script type="math/tex; mode=display">
P\left(X_{1}=x_{1}, \ldots, X_{m}=x_{m}\right)=\frac{n !}{\prod_{i=1}^{m} x_{i} !} \prod_{i=1}^{m} \theta_{i}^{x_{i}}
</script>
</div>
<p>Multinomial likelihood</p>
<ul>
<li>Assume <span class="arithmatex"><span class="MathJax_Preview">X_{1}, \ldots, X_{m}</span><script type="math/tex">X_{1}, \ldots, X_{m}</script></span> are counts in cells up to <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>, each cell has a different probability <span class="arithmatex"><span class="MathJax_Preview">\theta_{1}, \ldots, \theta_{m}</span><script type="math/tex">\theta_{1}, \ldots, \theta_{m}</script></span></li>
<li>constraints: <span class="arithmatex"><span class="MathJax_Preview">\sum_{i=1}^{m} X_{i}=n, \sum_{i=1}^{m} \theta_{i}=1</span><script type="math/tex">\sum_{i=1}^{m} X_{i}=n, \sum_{i=1}^{m} \theta_{i}=1</script></span></li>
<li>The joint probability of <span class="arithmatex"><span class="MathJax_Preview">\left(X_{1}, \ldots, X_{m}\right)</span><script type="math/tex">\left(X_{1}, \ldots, X_{m}\right)</script></span> is multinomial</li>
<li><span class="arithmatex"><span class="MathJax_Preview">I\left(\theta_{1}, \ldots, \theta_{m}\right)=\log n !-\sum_{i=1}^{m} \log x_{i}+\sum_{i=1}^{m} x_{i} \log \theta_{i}</span><script type="math/tex">I\left(\theta_{1}, \ldots, \theta_{m}\right)=\log n !-\sum_{i=1}^{m} \log x_{i}+\sum_{i=1}^{m} x_{i} \log \theta_{i}</script></span></li>
<li>Introduce Lagrange multipliers, <span class="arithmatex"><span class="MathJax_Preview">I\left(\theta_{1}, \ldots, \theta_{m}\right)=\log n !-\sum_{i=1}^{m} \log x_{i}+\sum_{i=1}^{m} x_{i} \log \theta_{i}+\lambda\left(1-\sum_{i=1}^{m} \theta_{i}\right)</span><script type="math/tex">I\left(\theta_{1}, \ldots, \theta_{m}\right)=\log n !-\sum_{i=1}^{m} \log x_{i}+\sum_{i=1}^{m} x_{i} \log \theta_{i}+\lambda\left(1-\sum_{i=1}^{m} \theta_{i}\right)</script></span></li>
<li>solution <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}_{i}=\frac{x_{i}}{n}</span><script type="math/tex">\hat{\theta}_{i}=\frac{x_{i}}{n}</script></span>, that is, the sample proportion</li>
</ul>
<h3 id="restricted-estimators">限制性估计 Restricted Estimators<a class="headerlink" href="#restricted-estimators" title="Permanent link">&para;</a></h3>
<p>介绍了三类方法</p>
<h4 id="roughness-penalty-and-bayesian-methods">Roughness Penalty and Bayesian Methods<a class="headerlink" href="#roughness-penalty-and-bayesian-methods" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
P R S S(f ; \lambda)=R S S(f)+\lambda J(f)
</div>
<script type="math/tex; mode=display">
P R S S(f ; \lambda)=R S S(f)+\lambda J(f)
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathrm{J}(\mathrm{f})</span><script type="math/tex">\mathrm{J}(\mathrm{f})</script></span> will be large for functions <span class="arithmatex"><span class="MathJax_Preview">\mathrm{f}</span><script type="math/tex">\mathrm{f}</script></span> that vary too rapidly over small regions of input space.</li>
<li>smoothing parameter <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> controls the amount of penalty</li>
<li>e.g., <span class="arithmatex"><span class="MathJax_Preview">P R S S(f ; \lambda)=\sum_{i=1}^{N}\left(y_{i}-f\left(x_{i}\right)\right)^{2}+\lambda \int\left[f(x)^{\prime \prime}\right]^{2}</span><script type="math/tex">P R S S(f ; \lambda)=\sum_{i=1}^{N}\left(y_{i}-f\left(x_{i}\right)\right)^{2}+\lambda \int\left[f(x)^{\prime \prime}\right]^{2}</script></span><ul>
<li>Solution - cubic smoothing spline 上面惩罚项的解是 <strong>三次光滑样条</strong></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\lambda=0</span><script type="math/tex">\lambda=0</script></span> 时, 没有惩罚, 可以使用任意插值函数; <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> 为无穷时, 仅仅允许 x 的线性函数.</li>
</ul>
</li>
</ul>
<p><span class="arithmatex"><span class="MathJax_Preview">J(f)</span><script type="math/tex">J(f)</script></span> reflects our <strong>prior belief</strong> that <span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> exhibit a certain type of smooth behavior</p>
<ul>
<li>Bayesian analogy 正则项表达了我们的 prior belief, 确实可以套进贝叶斯的模型中</li>
<li>Penalty J - log-prior</li>
<li>PRSS <span class="arithmatex"><span class="MathJax_Preview">(f ; \lambda)</span><script type="math/tex">(f ; \lambda)</script></span> - log-posterior distribution</li>
<li>Minimizing <span class="arithmatex"><span class="MathJax_Preview">P R S S(f ; \lambda)</span><script type="math/tex">P R S S(f ; \lambda)</script></span> - finding the posterior mode.</li>
<li>我们将在 <code>第 5 章</code> 中讨论粗糙惩罚方法并在 <code>第 8 章</code> 中讨论贝叶斯范式</li>
</ul>
<h4 id="kernel-methods-and-local-regression">Kernel Methods and Local Regression<a class="headerlink" href="#kernel-methods-and-local-regression" title="Permanent link">&para;</a></h4>
<p>这些方法可以认为是通过确定局部邻域的本质来显式给出回归函数的估计或条件期望，并且属于局部拟合得很好的规则函数类．</p>
<p>The local neighborhood is specified by a kernel function <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, x\right)</span><script type="math/tex">K_{\lambda}\left(x_{0}, x\right)</script></span></p>
<ul>
<li>Assign weight <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, x\right)</span><script type="math/tex">K_{\lambda}\left(x_{0}, x\right)</script></span> to points <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> in a region around <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span></li>
<li>e.g., <strong>Gaussian kernal</strong> <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, x\right)=\frac{1}{\lambda} \exp \left[-\frac{\left\|x-x_{0}\right\|^{2}}{2 \lambda}\right]</span><script type="math/tex">K_{\lambda}\left(x_{0}, x\right)=\frac{1}{\lambda} \exp \left[-\frac{\left\|x-x_{0}\right\|^{2}}{2 \lambda}\right]</script></span> 高斯核</li>
<li>Weights die exponentialy with squared Euclidean distance from <span class="arithmatex"><span class="MathJax_Preview">x_{0}</span><script type="math/tex">x_{0}</script></span>.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
R S S\left(f_{\theta}, x_{0}\right)=\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left(y_{i}-f_{\theta}\left(x_{i}\right)\right)^{2}
</div>
<script type="math/tex; mode=display">
R S S\left(f_{\theta}, x_{0}\right)=\sum_{i=1}^{N} K_{\lambda}\left(x_{0}, x_{i}\right)\left(y_{i}-f_{\theta}\left(x_{i}\right)\right)^{2}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">f_{\hat{\theta}}\left(x_{0}\right)</span><script type="math/tex">f_{\hat{\theta}}\left(x_{0}\right)</script></span> is the local regression estimate of <span class="arithmatex"><span class="MathJax_Preview">f\left(x_{0}\right)</span><script type="math/tex">f\left(x_{0}\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}</span><script type="math/tex">\hat{\theta}</script></span> minimizes <span class="arithmatex"><span class="MathJax_Preview">R S S\left(f_{\theta}, x_{0}\right)</span><script type="math/tex">R S S\left(f_{\theta}, x_{0}\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">f_{\theta}(x)=\theta_{0} \rightarrow</span><script type="math/tex">f_{\theta}(x)=\theta_{0} \rightarrow</script></span> <strong>Nadaraya-Watson estimate</strong></li>
<li><span class="arithmatex"><span class="MathJax_Preview">f_{\theta}(x)=\theta_{0}+\theta_{1} x \rightarrow</span><script type="math/tex">f_{\theta}(x)=\theta_{0}+\theta_{1} x \rightarrow</script></span> local linear regression model. 局部线性回归</li>
</ul>
<p>最近邻方法可以看成是某个更加依赖数据的度量的核方法. Nearest Neighbour 所对应的核为 <span class="arithmatex"><span class="MathJax_Preview">K_{\lambda}\left(x_{0}, x\right)=I\left(\left\|x-x_{0}\right\| \leqslant\left\|x_{k}-x_{0}\right\|\right)</span><script type="math/tex">K_{\lambda}\left(x_{0}, x\right)=I\left(\left\|x-x_{0}\right\| \leqslant\left\|x_{k}-x_{0}\right\|\right)</script></span></p>
<p>为了避免维数灾难，这些方法在高维情形下要做修正．将在 <code>第 6 章</code> 讨论不同的改编．</p>
<h4 id="basis-functions-and-dictionary-methods">Basis Functions and Dictionary Methods<a class="headerlink" href="#basis-functions-and-dictionary-methods" title="Permanent link">&para;</a></h4>
<p>Linear basis expansion <span class="arithmatex"><span class="MathJax_Preview">f_{\theta}(x)=\sum_{m=1}^{M}\left(\theta_{m} h_{m}(x)\right)</span><script type="math/tex">f_{\theta}(x)=\sum_{m=1}^{M}\left(\theta_{m} h_{m}(x)\right)</script></span></p>
<ul>
<li>Piecewise polynomial of degree <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> at <span class="arithmatex"><span class="MathJax_Preview">M-K</span><script type="math/tex">M-K</script></span> knots (见 5.2 节)</li>
<li>径向基函数 <strong>Radial basis functions</strong> <span class="arithmatex"><span class="MathJax_Preview">f_{\theta}(x)=\sum_{m=1}^{M} K_{\lambda_{m}}\left(\mu_{m}, x\right) \theta_{m}</span><script type="math/tex">f_{\theta}(x)=\sum_{m=1}^{M} K_{\lambda_{m}}\left(\mu_{m}, x\right) \theta_{m}</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">\mu_{m}</span><script type="math/tex">\mu_{m}</script></span> are centroids (见 6.7 节)</li>
</ul>
<p>Neural Network</p>
<ul>
<li>单层的向前反馈的带有线性输出权重的神经网络模型可以认为是一种自适应的基函数方法</li>
<li><span class="arithmatex"><span class="MathJax_Preview">f_{\theta}(x)=\sum_{i=1}^{M} \beta_{m} \sigma\left(\alpha_{m}^{\top} x+b_{m}\right)</span><script type="math/tex">f_{\theta}(x)=\sum_{i=1}^{M} \beta_{m} \sigma\left(\alpha_{m}^{\top} x+b_{m}\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\sigma(x)=\frac{1}{1+e^{(-x)}}</span><script type="math/tex">\sigma(x)=\frac{1}{1+e^{(-x)}}</script></span> activation function</li>
<li>Direction <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> and bias <span class="arithmatex"><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> need to be computed</li>
</ul>
<p>Dictionary methods</p>
<ul>
<li>Functions are adaptively chosen 这些自适应选择基函数的方法也被称作 字典 (dictionary) 方法</li>
<li>A possibly infinite set or dictionary <span class="arithmatex"><span class="MathJax_Preview">\mathcal{D}</span><script type="math/tex">\mathcal{D}</script></span> of candidate basis functions models employ some kind of search mechanism.</li>
</ul>
<h3 id="model-selection-and-the-biasvariance-tradeoff">Model Selection and the Bias–Variance Tradeoff<a class="headerlink" href="#model-selection-and-the-biasvariance-tradeoff" title="Permanent link">&para;</a></h3>
<h4 id="model-complexity">Model complexity<a class="headerlink" href="#model-complexity" title="Permanent link">&para;</a></h4>
<p>上面讨论的所有模型以及其他将要在后面章节讨论的模型都有一个 光滑 (smoothing) 或 复杂性 (complexity) 参数需要确定</p>
<ul>
<li>The multiplier of the penalty <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span></li>
<li>Width of the kernel</li>
<li>Number of basis functions</li>
</ul>
<p>Decomposition of EPE (expected prediction error) 这里假设了 x 是固定的而非随机的 (也即 pointwise 最小)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
E P E\left(x_{0}\right)&amp;=E\left[\left(y_{0}-\hat{f}\left(x_{0}\right)\right)^{2} \mid X=x_{0}\right] \\
&amp;=E\left[\left(y_{0}-f\left(x_{0}\right)+f\left(x_{0}\right)-E \hat{f}\left(x_{0}\right)+E \hat{f}\left(x_{0}\right)-\hat{f}\left(x_{0}\right)\right)^{2} \mid X=x_{0}\right] \\
&amp;=E\left[y_{0}-f\left(x_{0}\right)\right]^{2}+E\left[f\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right) \right]^{2}+E\left[\hat{f}\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right)\right]^{2}.
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
E P E\left(x_{0}\right)&=E\left[\left(y_{0}-\hat{f}\left(x_{0}\right)\right)^{2} \mid X=x_{0}\right] \\
&=E\left[\left(y_{0}-f\left(x_{0}\right)+f\left(x_{0}\right)-E \hat{f}\left(x_{0}\right)+E \hat{f}\left(x_{0}\right)-\hat{f}\left(x_{0}\right)\right)^{2} \mid X=x_{0}\right] \\
&=E\left[y_{0}-f\left(x_{0}\right)\right]^{2}+E\left[f\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right) \right]^{2}+E\left[\hat{f}\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right)\right]^{2}.
\end{aligned}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">E\left[y_{0}-f\left(x_{0}\right)\right]^{2}</span><script type="math/tex">E\left[y_{0}-f\left(x_{0}\right)\right]^{2}</script></span> irreducible error, beyond our control 不可约减的</li>
<li><span class="arithmatex"><span class="MathJax_Preview">E\left[f\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right)\right]^{2}</span><script type="math/tex">E\left[f\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right)\right]^{2}</script></span> Bias squared, decrease with complexity 偏差部分</li>
<li><span class="arithmatex"><span class="MathJax_Preview">E\left[\hat{f}\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right)\right]^{2}</span><script type="math/tex">E\left[\hat{f}\left(x_{0}\right)-E\left(\hat{f}\left(x_{0}\right)\right)\right]^{2}</script></span> Variance, increase with complexity 方差部分</li>
</ul>
<p>后面的两项构成了 <strong>均方误差</strong> (mean squared error)</p>
<p><img alt="" src="../media/ASL-note1/2022-01-06-17-25-01.png" /></p>
<p>一般地, 我们选择模型复杂度使偏差与方差达到均衡从而使测试误差最小。测试误差的一个明显的 估计是 训练误差 (training error) <span class="arithmatex"><span class="MathJax_Preview">\frac{1}{N} \sum_{i}\left(y_{i}-\hat{y}_{i}\right)^{2}</span><script type="math/tex">\frac{1}{N} \sum_{i}\left(y_{i}-\hat{y}_{i}\right)^{2}</script></span>. 不幸的是, 训练误差不是测试误差的良好估计, 因为这不能解释模型复杂度.</p>
<p>在 <code>第 7 章</code> (7.2, 7.3) 中，我们考虑估计预测方法的测试误差的各种方式，并因此在给定的预测方法和训练集下，估计出最优的模型复杂度．</p>
<h2 id="linear-methods-for-regression">Linear Methods for Regression<a class="headerlink" href="#linear-methods-for-regression" title="Permanent link">&para;</a></h2>
<p>Outline</p>
<ul>
<li>Intro</li>
<li>Linear Regression \&amp; Least Squares</li>
<li>Subset Selection</li>
<li>Shrinkage methods</li>
<li>Methods Using Derived Input Directions</li>
</ul>
<h3 id="introduction-of-linear-methods">Introduction of Linear Methods<a class="headerlink" href="#introduction-of-linear-methods" title="Permanent link">&para;</a></h3>
<p>Advantages of Linear Models</p>
<ul>
<li>Simple and often provide an adequate and interpretable description of how the inputs affect the output.</li>
<li>Sometimes outperform fancier nonlinear models, especially in situations with small numbers of training cases, low signal-to-noise ratio or sparse data.</li>
<li>Can be applied to transformations of the inputs and this considerably expands their scope.</li>
</ul>
<h3 id="recap-of-linear-regression">Recap of Linear Regression<a class="headerlink" href="#recap-of-linear-regression" title="Permanent link">&para;</a></h3>
<div class="arithmatex">
<div class="MathJax_Preview">
f(X)=\beta_{0}+\sum_{j=1}^{p} X_{j} \beta_{j}
</div>
<script type="math/tex; mode=display">
f(X)=\beta_{0}+\sum_{j=1}^{p} X_{j} \beta_{j}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\beta_{j}</span><script type="math/tex">\beta_{j}</script></span> unknown parameter</li>
<li>Sources of <span class="arithmatex"><span class="MathJax_Preview">X_{j}</span><script type="math/tex">X_{j}</script></span><ul>
<li>quantitative inputs</li>
<li>transformations</li>
<li>basis expansions</li>
<li>dummy coding of qualitative variables</li>
<li>interactions between variables</li>
</ul>
</li>
<li>Model is linear in <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span></li>
</ul>
<h4 id="estimation">Estimation<a class="headerlink" href="#estimation" title="Permanent link">&para;</a></h4>
<p>Training data:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\left(x_{1}, y_{1}\right), \ldots,\left(x_{N}, y_{N}\right)
</div>
<script type="math/tex; mode=display">
\left(x_{1}, y_{1}\right), \ldots,\left(x_{N}, y_{N}\right)
</script>
</div>
<p>Feature measurements for the <span class="arithmatex"><span class="MathJax_Preview">i_{t h}</span><script type="math/tex">i_{t h}</script></span> case: <span class="arithmatex"><span class="MathJax_Preview">x_{i}=\left(x_{i 1}, x_{i 2}, \ldots, x_{i p}\right)^{\top}:</span><script type="math/tex">x_{i}=\left(x_{i 1}, x_{i 2}, \ldots, x_{i p}\right)^{\top}:</script></span>
Least squares:
choose <span class="arithmatex"><span class="MathJax_Preview">\beta=\left(\beta_{0}, \beta_{1}, \ldots, \beta_{p}\right)^{\top}</span><script type="math/tex">\beta=\left(\beta_{0}, \beta_{1}, \ldots, \beta_{p}\right)^{\top}</script></span> to minimize the Residual Sum of Squares.
<span class="arithmatex"><span class="MathJax_Preview">R S S=\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}</span><script type="math/tex">R S S=\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}</script></span></p>
<h4 id="matrix-notation">Matrix notation<a class="headerlink" href="#matrix-notation" title="Permanent link">&para;</a></h4>
<p>记为矩阵形式, 可以得到解析解. (在 X 列满秩的情况下)</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">X: N \times(p+1)</span><script type="math/tex">X: N \times(p+1)</script></span> matrix</li>
<li><span class="arithmatex"><span class="MathJax_Preview">Y: N</span><script type="math/tex">Y: N</script></span> vector</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
R S S(\beta)=(y-X \beta)^{\top}(y-X \beta)
</div>
<script type="math/tex; mode=display">
R S S(\beta)=(y-X \beta)^{\top}(y-X \beta)
</script>
</div>
<ul>
<li>Quadratic function of <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span></li>
<li>Differentiating w.r.t. <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span></li>
</ul>
<p><span class="arithmatex"><span class="MathJax_Preview">\frac{\partial R S S}{\partial \beta}=-2 X^{\top}(y-X \beta)</span><script type="math/tex">\frac{\partial R S S}{\partial \beta}=-2 X^{\top}(y-X \beta)</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\frac{\partial^{2} R S S}{\partial \beta \partial \beta^{\top}}=2 X^{\top} X</span><script type="math/tex">\frac{\partial^{2} R S S}{\partial \beta \partial \beta^{\top}}=2 X^{\top} X</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\beta}=\left(X^{\top} X\right)^{-1} X^{\top} y
</div>
<script type="math/tex; mode=display">
\hat{\beta}=\left(X^{\top} X\right)^{-1} X^{\top} y
</script>
</div>
<ul>
<li>Assume <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> has full column rank</li>
<li>Fitted value <span class="arithmatex"><span class="MathJax_Preview">\hat{y}=X \hat{\beta}=X\left(X^{\top} X\right)^{-1} X^{\top} y</span><script type="math/tex">\hat{y}=X \hat{\beta}=X\left(X^{\top} X\right)^{-1} X^{\top} y</script></span></li>
<li>Hat matrix <span class="arithmatex"><span class="MathJax_Preview">H=X\left(X^{\top} X\right)^{-1} X^{\top}</span><script type="math/tex">H=X\left(X^{\top} X\right)^{-1} X^{\top}</script></span></li>
</ul>
<p><img alt="" src="../media/ASL-note1/2021-11-29-16-20-17.png" /></p>
<h4 id="sampling-properties-of-hatbetahatbeta">Sampling Properties of <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}</span><script type="math/tex">\hat{\beta}</script></span><a class="headerlink" href="#sampling-properties-of-hatbetahatbeta" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\beta}=\left(X^{\top} X\right)^{-1} X^{\top} y
</div>
<script type="math/tex; mode=display">
\hat{\beta}=\left(X^{\top} X\right)^{-1} X^{\top} y
</script>
</div>
<ul>
<li>参数估计的协方差矩阵(covariance matrix, variance-covariance matrix, dispersion matrix 这些名字都是) <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Var}(\hat{\beta})=\left(X^{\top} X\right)^{-1} \sigma^{2}</span><script type="math/tex">\operatorname{Var}(\hat{\beta})=\left(X^{\top} X\right)^{-1} \sigma^{2}</script></span></li>
<li>估计方差 <span class="arithmatex"><span class="MathJax_Preview">\hat{\sigma}^{2}=\frac{1}{N-p-1} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}</span><script type="math/tex">\hat{\sigma}^{2}=\frac{1}{N-p-1} \sum_{i=1}^{N}\left(y_{i}-\hat{y}_{i}\right)^{2}</script></span></li>
</ul>
<p>Additional assumptions 加上正态性假设</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">y=\beta_{0}+\sum_{j=1}^{p} X_{j} \beta_{j}+\epsilon</span><script type="math/tex">y=\beta_{0}+\sum_{j=1}^{p} X_{j} \beta_{j}+\epsilon</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)</span><script type="math/tex">\epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)</script></span></li>
</ul>
<p>在这一假设下, 我们可以得到参数估计的分布: Sampling distribution of <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}, \hat{\sigma}^{2}</span><script type="math/tex">\hat{\beta}, \hat{\sigma}^{2}</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\beta} \sim \mathcal{N}\left(\beta,\left(X^{\top} X\right)^{-1} \sigma^{2}\right)</span><script type="math/tex">\hat{\beta} \sim \mathcal{N}\left(\beta,\left(X^{\top} X\right)^{-1} \sigma^{2}\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">(N-p-1) \hat{\sigma}^{2} \sim \sigma^{2} \chi_{N-p-1}^{2}</span><script type="math/tex">(N-p-1) \hat{\sigma}^{2} \sim \sigma^{2} \chi_{N-p-1}^{2}</script></span></li>
</ul>
<p>从而之后可以进行假设检验和置信区间.</p>
<h4 id="hypothesis-testing-single-parameter">Hypothesis Testing - single parameter<a class="headerlink" href="#hypothesis-testing-single-parameter" title="Permanent link">&para;</a></h4>
<p>When <span class="arithmatex"><span class="MathJax_Preview">\sigma^{2}</span><script type="math/tex">\sigma^{2}</script></span> is known (rare)</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">v_{j}</span><script type="math/tex">v_{j}</script></span> is the <span class="arithmatex"><span class="MathJax_Preview">j_{t h}</span><script type="math/tex">j_{t h}</script></span> diagonal element of <span class="arithmatex"><span class="MathJax_Preview">\left(X^{\top} X\right)^{-1}</span><script type="math/tex">\left(X^{\top} X\right)^{-1}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">z_{j}=\frac{\hat{\beta}_{j}}{\sigma \sqrt{v_{j}}}</span><script type="math/tex">z_{j}=\frac{\hat{\beta}_{j}}{\sigma \sqrt{v_{j}}}</script></span></li>
<li>Under <span class="arithmatex"><span class="MathJax_Preview">H_{0}, z_{j} \sim \mathcal{N}(0,1)</span><script type="math/tex">H_{0}, z_{j} \sim \mathcal{N}(0,1)</script></span></li>
</ul>
<p>When <span class="arithmatex"><span class="MathJax_Preview">\sigma^{2}</span><script type="math/tex">\sigma^{2}</script></span> is unknown (most common)</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">z_{j}=\frac{\hat{\beta}_{j}}{\hat{\sigma} \sqrt{v_{j}}}</span><script type="math/tex">z_{j}=\frac{\hat{\beta}_{j}}{\hat{\sigma} \sqrt{v_{j}}}</script></span></li>
<li>Under <span class="arithmatex"><span class="MathJax_Preview">H_{0}, z_{j} \sim t(N-p-1)</span><script type="math/tex">H_{0}, z_{j} \sim t(N-p-1)</script></span></li>
</ul>
<p>The differences between <span class="arithmatex"><span class="MathJax_Preview">t(N-p-1)</span><script type="math/tex">t(N-p-1)</script></span> and standard normal become negligible as the sample size increases</p>
<h4 id="hypothesis-testing-group-of-coefficients">Hypothesis Testing - group of coefficients<a class="headerlink" href="#hypothesis-testing-group-of-coefficients" title="Permanent link">&para;</a></h4>
<p>Bigger model</p>
<ul>
<li>residual sum-of-squares for the least squares fit <span class="arithmatex"><span class="MathJax_Preview">R S S_{1}</span><script type="math/tex">R S S_{1}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">p_{1}+1</span><script type="math/tex">p_{1}+1</script></span> parameters</li>
</ul>
<p>Nested smaller model</p>
<ul>
<li>residual sum-of-squares for the least squares fit <span class="arithmatex"><span class="MathJax_Preview">R S S_{0}</span><script type="math/tex">R S S_{0}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">p_{0}+1</span><script type="math/tex">p_{0}+1</script></span> parameters</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
F=\frac{\left(R S S_{0}-R S S_{1}\right) /\left(P_{1}-P_{0}\right)}{R S S_{1} /\left(N-P_{1}-1\right)}
</div>
<script type="math/tex; mode=display">
F=\frac{\left(R S S_{0}-R S S_{1}\right) /\left(P_{1}-P_{0}\right)}{R S S_{1} /\left(N-P_{1}-1\right)}
</script>
</div>
<ul>
<li><strong>Measures the change in RSS per additional parameter</strong> in the bigger model 衡量了大模型中每个增加的系数对于RSS的改变, 并且用方差的估计值进行标准化</li>
<li>Under <span class="arithmatex"><span class="MathJax_Preview">H_{0}, F \sim \mathcal{F}_{p_{1}-p_{0}, N-p_{1}-1}</span><script type="math/tex">H_{0}, F \sim \mathcal{F}_{p_{1}-p_{0}, N-p_{1}-1}</script></span></li>
</ul>
<h4 id="confidence-interval-region">Confidence Interval / Region<a class="headerlink" href="#confidence-interval-region" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex"><span class="MathJax_Preview">\beta_{j}: 1-2 \alpha</span><script type="math/tex">\beta_{j}: 1-2 \alpha</script></span> confidence interval 其中 <span class="arithmatex"><span class="MathJax_Preview">z^{1-\alpha}</span><script type="math/tex">z^{1-\alpha}</script></span> 为正态分布的 <span class="arithmatex"><span class="MathJax_Preview">{1-\alpha}</span><script type="math/tex">{1-\alpha}</script></span> 分位数</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\left(\hat{\beta}_{j}-z^{1-\alpha} v_{j}^{1 / 2} \hat{\sigma}, \hat{\beta}_{j}+z^{1-\alpha} v_{j}^{1 / 2} \hat{\sigma}\right)
</div>
<script type="math/tex; mode=display">
\left(\hat{\beta}_{j}-z^{1-\alpha} v_{j}^{1 / 2} \hat{\sigma}, \hat{\beta}_{j}+z^{1-\alpha} v_{j}^{1 / 2} \hat{\sigma}\right)
</script>
</div>
<p>Entire parameter vector <span class="arithmatex"><span class="MathJax_Preview">\beta: 1-2 \alpha</span><script type="math/tex">\beta: 1-2 \alpha</script></span> confidence region</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathcal{C}_{\beta}=\left\{\beta \mid(\hat{\beta}-\beta)^{\top} X^{\top} X(\hat{\beta}-\beta) \leqslant \hat{\sigma}^{2} {\chi_{p+1}^{2}}^{1-2 \alpha}\right\}
</div>
<script type="math/tex; mode=display">
\mathcal{C}_{\beta}=\left\{\beta \mid(\hat{\beta}-\beta)^{\top} X^{\top} X(\hat{\beta}-\beta) \leqslant \hat{\sigma}^{2} {\chi_{p+1}^{2}}^{1-2 \alpha}\right\}
</script>
</div>
<h4 id="gauss-markov-theorem">Gauss Markov Theorem<a class="headerlink" href="#gauss-markov-theorem" title="Permanent link">&para;</a></h4>
<p>Why least squares?</p>
<p><strong>Gauss Markov Theorem</strong>: least squares estimates of the parameters <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> have the smallest variance among all linear unbiased estimates</p>
<p><strong>Gauss-Markov 定理</strong>: 在参数 <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> 的所有无偏估计中, 最小二乘估计是其中方差最小的.</p>
<p>In mathematical term: suppose <span class="arithmatex"><span class="MathJax_Preview">\theta=\alpha^{\top} \beta</span><script type="math/tex">\theta=\alpha^{\top} \beta</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}=\alpha^{\top} \hat{\beta}</span><script type="math/tex">\hat{\theta}=\alpha^{\top} \hat{\beta}</script></span> is unbiased for <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span></li>
<li>Consider any other linear estimator <span class="arithmatex"><span class="MathJax_Preview">\tilde{\theta}=c^{\top} y</span><script type="math/tex">\tilde{\theta}=c^{\top} y</script></span> such that <span class="arithmatex"><span class="MathJax_Preview">E\left(c^{\top} y\right)=\alpha^{\top} \beta</span><script type="math/tex">E\left(c^{\top} y\right)=\alpha^{\top} \beta</script></span></li>
<li>According to Gauss Markov Theorem <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Var}\left(\alpha^{\top} \hat{\beta}\right)&lt;\operatorname{Var}\left(c^{\top} y\right)</span><script type="math/tex">\operatorname{Var}\left(\alpha^{\top} \hat{\beta}\right)<\operatorname{Var}\left(c^{\top} y\right)</script></span></li>
</ul>
<p>虽然 Gauss-Markov 定理 说明了参数估计在无偏估计中是最好的, 但根据下面的估计值的均方误差分解, 可知其由估计的方差和估计偏差共同决定 —— 因此引入了下面的 shrink 方法.</p>
<p><strong>Mean squared error</strong> of an estimator <span class="arithmatex"><span class="MathJax_Preview">\tilde{\theta}</span><script type="math/tex">\tilde{\theta}</script></span> in estimating <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> :</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{MSE}(\tilde{\theta})=E(\tilde{\theta}-\theta)^{2}=E(\tilde{\theta}-E(\tilde{\theta})+E(\tilde{\theta})-\theta)^{2}=\operatorname{Var}(\tilde{\theta})+\operatorname{Bias}(\tilde{\theta})^{2}
</div>
<script type="math/tex; mode=display">
\operatorname{MSE}(\tilde{\theta})=E(\tilde{\theta}-\theta)^{2}=E(\tilde{\theta}-E(\tilde{\theta})+E(\tilde{\theta})-\theta)^{2}=\operatorname{Var}(\tilde{\theta})+\operatorname{Bias}(\tilde{\theta})^{2}
</script>
</div>
<p>How about biased estimator (best subset, shrinkage)?</p>
<ul>
<li>Might have smaller MSE by trading a little bias for a larger reduction in variance.</li>
</ul>
<p>&ldquo;All models are wrong, some are useful&rdquo;</p>
<ul>
<li>Picking the right model amounts to creating the right balance between bias and variance.</li>
</ul>
<h4 id="multiple-linear-regression">Multiple Linear Regression<a class="headerlink" href="#multiple-linear-regression" title="Permanent link">&para;</a></h4>
<p>多元线性回归和一元的关系: 当所有自变量都是正交的情况下, 得到的系数都是一样的. 而对于不正交的情况, 可以采用 Gram-Schmidt 正交化, 当然在两个变量相关性较高的情况下, 估计会不稳定.</p>
<p><span class="arithmatex"><span class="MathJax_Preview">y_{i}=\beta_{0}+\sum_{j=1}^{p} x_{i j} \beta_{j}</span><script type="math/tex">y_{i}=\beta_{0}+\sum_{j=1}^{p} x_{i j} \beta_{j}</script></span> where <span class="arithmatex"><span class="MathJax_Preview">p&gt;1</span><script type="math/tex">p>1</script></span></p>
<p>When <span class="arithmatex"><span class="MathJax_Preview">x_{1}, x_{2}, \ldots, x_{p}</span><script type="math/tex">x_{1}, x_{2}, \ldots, x_{p}</script></span> are orthogonal <span class="arithmatex"><span class="MathJax_Preview">\left(\left\langle x_{j}, x_{k}\right\rangle=0\right.</span><script type="math/tex">\left(\left\langle x_{j}, x_{k}\right\rangle=0\right.</script></span> for all <span class="arithmatex"><span class="MathJax_Preview">\left.j \neq k\right)</span><script type="math/tex">\left.j \neq k\right)</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}_{j}=\frac{\left\langle x_{j}, y\right\rangle}{\left\langle x_{j}, x_{j}\right\rangle}</span><script type="math/tex">\hat{\beta}_{j}=\frac{\left\langle x_{j}, y\right\rangle}{\left\langle x_{j}, x_{j}\right\rangle}</script></span>, same as univariate solution</li>
</ul>
<p>When <span class="arithmatex"><span class="MathJax_Preview">x_{1}, x_{2}, \ldots, x_{p}</span><script type="math/tex">x_{1}, x_{2}, \ldots, x_{p}</script></span> are not orthogonal</p>
<ul>
<li>Orthogonalize <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span> to get residual vector <span class="arithmatex"><span class="MathJax_Preview">z_{j}, j=1, \ldots, p</span><script type="math/tex">z_{j}, j=1, \ldots, p</script></span></li>
<li>Regress <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> on the residual <span class="arithmatex"><span class="MathJax_Preview">z_{p}</span><script type="math/tex">z_{p}</script></span> to give the estimate <span class="arithmatex"><span class="MathJax_Preview">\beta_{p}</span><script type="math/tex">\beta_{p}</script></span></li>
<li>Also known as <strong>Gram-Schmidt</strong> procedure</li>
<li>Note: if <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span> is highly correlated with <span class="arithmatex"><span class="MathJax_Preview">x_{k}, z_{j}</span><script type="math/tex">x_{k}, z_{j}</script></span> will be close to zero, and <span class="arithmatex"><span class="MathJax_Preview">\beta_{j}</span><script type="math/tex">\beta_{j}</script></span> will be very unstable.</li>
</ul>
<h4 id="multiple-outputs-y_1-y_2-ldots-y_ky_1-y_2-ldots-y_k">Multiple Outputs <span class="arithmatex"><span class="MathJax_Preview">Y_{1}, Y_{2}, \ldots, Y_{K}</span><script type="math/tex">Y_{1}, Y_{2}, \ldots, Y_{K}</script></span><a class="headerlink" href="#multiple-outputs-y_1-y_2-ldots-y_ky_1-y_2-ldots-y_k" title="Permanent link">&para;</a></h4>
<p>对于多元的输出而言, 若随机项相互独立, 则和一元Y一致.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Y_{k}=\beta_{0 k}+\sum_{j=1}^{p} X_{j} \beta_{j k}+\epsilon_{k}=f_{k}(X)+\epsilon_{k}
</div>
<script type="math/tex; mode=display">
Y_{k}=\beta_{0 k}+\sum_{j=1}^{p} X_{j} \beta_{j k}+\epsilon_{k}=f_{k}(X)+\epsilon_{k}
</script>
</div>
<p>or in matrix notation <span class="arithmatex"><span class="MathJax_Preview">Y=X B+E</span><script type="math/tex">Y=X B+E</script></span></p>
<p>If errors <span class="arithmatex"><span class="MathJax_Preview">\epsilon=\left(\epsilon_{1}, \epsilon_{2}, \ldots, \epsilon_{k}\right)</span><script type="math/tex">\epsilon=\left(\epsilon_{1}, \epsilon_{2}, \ldots, \epsilon_{k}\right)</script></span> are i.i.d.
不同的 <span class="arithmatex"><span class="MathJax_Preview">y_i</span><script type="math/tex">y_i</script></span> 之间误差项独立, 也即可分别看成独立的 LR.</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">R S S(B)=\sum_{k=1}^{K} \sum_{i=1}^{N}\left(y_{i k}-f_{k}\left(x_{i}\right)\right)^{2}=\operatorname{tr}\left[(Y-X B)^{\top}(Y-X B)\right]</span><script type="math/tex">R S S(B)=\sum_{k=1}^{K} \sum_{i=1}^{N}\left(y_{i k}-f_{k}\left(x_{i}\right)\right)^{2}=\operatorname{tr}\left[(Y-X B)^{\top}(Y-X B)\right]</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{B}=\operatorname{argmin}_{B} R S S(B)=\left(X^{\top} X\right)^{-1} X^{\top} Y</span><script type="math/tex">\hat{B}=\operatorname{argmin}_{B} R S S(B)=\left(X^{\top} X\right)^{-1} X^{\top} Y</script></span></li>
<li>Same form as univariate output</li>
</ul>
<p>If errors <span class="arithmatex"><span class="MathJax_Preview">\epsilon=\left(\epsilon_{1}, \epsilon_{2}, \ldots, \epsilon_{k}\right)</span><script type="math/tex">\epsilon=\left(\epsilon_{1}, \epsilon_{2}, \ldots, \epsilon_{k}\right)</script></span> are correlated</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">R S S(B, \Sigma)=\sum_{i=1}^{N}\left(y_{i}-f\left(x_{i}\right)\right)^{\top} \Sigma^{-1}\left(y_{i}-f\left(x_{i}\right)\right)</span><script type="math/tex">R S S(B, \Sigma)=\sum_{i=1}^{N}\left(y_{i}-f\left(x_{i}\right)\right)^{\top} \Sigma^{-1}\left(y_{i}-f\left(x_{i}\right)\right)</script></span> 注意这里用了误差的协方差矩阵进行设置不同估计误差之间的系数, 叫做 「多重变量加权准则」, 可由「多变量高斯定理」推出.</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{B}=\operatorname{argmin}_{B} R S S(B, \Sigma)=\left(X^{\top} X\right)^{-1} X^{\top} Y</span><script type="math/tex">\hat{B}=\operatorname{argmin}_{B} R S S(B, \Sigma)=\left(X^{\top} X\right)^{-1} X^{\top} Y</script></span> 注意这里的形式和上面误差项独立时一致</li>
<li>Note: If <span class="arithmatex"><span class="MathJax_Preview">\Sigma_{i}</span><script type="math/tex">\Sigma_{i}</script></span> vary among observations, results no longer hold</li>
</ul>
<h3 id="subset-selection">Subset Selection<a class="headerlink" href="#subset-selection" title="Permanent link">&para;</a></h3>
<p>两点理由: 预测精确性, 模型的可解释性</p>
<ul>
<li>Prediction accuracy<ul>
<li>least squares estimates often have low bias but large variance.</li>
<li>Prediction accuracy can sometimes be improved by shrinking or setting some coeﬃcients to zero.</li>
</ul>
</li>
<li>Interpretability<ul>
<li>Determine a smaller subset that exhibit the strongest effects.</li>
<li>Sacriﬁce some of the small details in order to get the ”big picture”</li>
</ul>
</li>
</ul>
<h4 id="forward-backward-stepwise-selection">Forward, Backward, Stepwise Selection<a class="headerlink" href="#forward-backward-stepwise-selection" title="Permanent link">&para;</a></h4>
<p>相较于采用F统计量来选择「显著性」项然后删除非显著项, 它的问题在于没有考虑「多重检验」问题, 因此 Forward/Backward-stepwise selection 更好.</p>
<p>有理想的情况, 自然是从所有可能的子集中选择最优的, 但是计算量过大.</p>
<ul>
<li>Forward<ul>
<li>Starts with the intercept</li>
<li>Sequentially adds the predictor that most improves the ﬁt</li>
<li>Must determine model size k</li>
<li>前向选择可以理解为一种贪心算法. 1. 在 computational上代价较小; 2. 前向逐步是一种有「更多约束」的搜索, 会有更低的方差, 但可能会有更高的误差.</li>
</ul>
</li>
<li>backward<ul>
<li>Starts with the full model</li>
<li>Sequentially deletes the predictor that has the least impact on ﬁt</li>
<li>Candidate for dropping is the variable with the smallest Z-score</li>
</ul>
</li>
<li>Stepwise<ul>
<li>Consider both forward and backward moves at each step</li>
<li>Select the “best” of the two</li>
<li>R <code>step</code> function: at each step add or drop based on AIC</li>
</ul>
</li>
</ul>
<p>优缺点</p>
<ul>
<li>Pros<ul>
<li>Interpretable</li>
<li>Possibly lower prediction error than the full model.</li>
</ul>
</li>
<li>Cons<ul>
<li>Discrete process - variables either retained or discarded 相较于下面的 shrink 方法</li>
<li>Often exhibits high variance 由于变量少了, 并且是离散约束</li>
</ul>
</li>
</ul>
<h3 id="shrinkage-methods">Shrinkage methods<a class="headerlink" href="#shrinkage-methods" title="Permanent link">&para;</a></h3>
<p>上面的自己选择可以得到一个解释性更强的模型, (预测误差可能比全模型好); 然而由于这是一个离散的过程(变量选入或者丢弃), 因此可能表现为高方差; 而这里的shrinkage 方法, 更加连续, 因此不会受到 high variability 的影响.</p>
<h4 id="ridge-regression">Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permanent link">&para;</a></h4>
<p>Shrinks the regression coefficients by imposing a penalty on their size.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\beta}^{\text {ridge }}=\operatorname{argmin}_{\beta}\left\{\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}\right\}
</div>
<script type="math/tex; mode=display">
\hat{\beta}^{\text {ridge }}=\operatorname{argmin}_{\beta}\left\{\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}+\lambda \sum_{j=1}^{p} \beta_{j}^{2}\right\}
</script>
</div>
<p>which is equivalent to (利用 Lagrange 乘子法)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\beta}^{\text {ridge }}=\operatorname{argmin}_{\beta} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}
 \text{subject to} \sum_{j=1}^{p} \beta_{j}^{2} \leqslant t
</div>
<script type="math/tex; mode=display">
\hat{\beta}^{\text {ridge }}=\operatorname{argmin}_{\beta} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}
 \text{subject to} \sum_{j=1}^{p} \beta_{j}^{2} \leqslant t
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\beta_{0}</span><script type="math/tex">\beta_{0}</script></span> is <strong>left out</strong> of the penalty term</li>
<li><strong>Normalize</strong> inputs before solving for solutions. 注意, 对于岭回归, 对输入按照比例进行缩放, 得到的结果是不同的, 因此在上面的求解前需要见进行标准化. 可以证明, 经过对于变量中心化之后, 可以分解为求截距和其他系数两部分, 因此可以写成下面的形式(这里的参数维度为 p 而非 p+1 了).</li>
<li>In matrix notation <span class="arithmatex"><span class="MathJax_Preview">R S S(\lambda)=(y-X \beta)^{\top}(y-X \beta)+\lambda \beta^{\top} \beta</span><script type="math/tex">R S S(\lambda)=(y-X \beta)^{\top}(y-X \beta)+\lambda \beta^{\top} \beta</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {ridge }}=\left(X^{\top} X+\lambda I\right)^{-1} X^{\top} y</span><script type="math/tex">\hat{\beta}^{\text {ridge }}=\left(X^{\top} X+\lambda I\right)^{-1} X^{\top} y</script></span></li>
</ul>
<h5 id="estimation-gaussian">Estimation - Gaussian<a class="headerlink" href="#estimation-gaussian" title="Permanent link">&para;</a></h5>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\beta}^{\text {ridge }}=\left(X^{\top} X+\lambda I\right)^{-1} X^{\top} y
</div>
<script type="math/tex; mode=display">
\hat{\beta}^{\text {ridge }}=\left(X^{\top} X+\lambda I\right)^{-1} X^{\top} y
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {ridge }}</span><script type="math/tex">\hat{\beta}^{\text {ridge }}</script></span> is again a linear function of <span class="arithmatex"><span class="MathJax_Preview">\mathrm{y}</span><script type="math/tex">\mathrm{y}</script></span></li>
<li>Makes it nonsingular even if <span class="arithmatex"><span class="MathJax_Preview">X^{\top} X</span><script type="math/tex">X^{\top} X</script></span> is not of full rank</li>
<li>When <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> are orthogonal, <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {ridge }}</span><script type="math/tex">\hat{\beta}^{\text {ridge }}</script></span> are just a scaled version of <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{O L S}</span><script type="math/tex">\hat{\beta}^{O L S}</script></span> 在正交输入的情形下，岭回归估计仅仅是最小二乘估计的缩小版本, 即 <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {ridge}}=\frac{1}{1+\lambda}\hat{\beta}^{O L S}</span><script type="math/tex">\hat{\beta}^{\text {ridge}}=\frac{1}{1+\lambda}\hat{\beta}^{O L S}</script></span></li>
<li>Equivalent Bayesian context - <strong>Gaussian prior</strong> for <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> 可以证明岭回归等价于参数在高斯先验下的后验估计, 且满足关系 <span class="arithmatex"><span class="MathJax_Preview">\lambda = \sigma^2 / \tau^2</span><script type="math/tex">\lambda = \sigma^2 / \tau^2</script></span>, 其中 <span class="arithmatex"><span class="MathJax_Preview">\sigma^2</span><script type="math/tex">\sigma^2</script></span> 为噪声项方差, <span class="arithmatex"><span class="MathJax_Preview">\tau^2</span><script type="math/tex">\tau^2</script></span> 为prior高斯方差.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
X \hat{\beta}^{\text {ridge }}=\sum_{j=1}^{p} \mu_{j} \frac{d_{j}^{2}}{d_{j}^{2}+\lambda} \mu_{j}^{\top} y
</div>
<script type="math/tex; mode=display">
X \hat{\beta}^{\text {ridge }}=\sum_{j=1}^{p} \mu_{j} \frac{d_{j}^{2}}{d_{j}^{2}+\lambda} \mu_{j}^{\top} y
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">u_{j}</span><script type="math/tex">u_{j}</script></span> are the columns of <span class="arithmatex"><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> where <span class="arithmatex"><span class="MathJax_Preview">X=U D V^{\top}</span><script type="math/tex">X=U D V^{\top}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">d_{j}</span><script type="math/tex">d_{j}</script></span> are the singular values of <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span></li>
<li>Greater amount of shrinkage is applied to the coordinates of basis vectors with smaller <span class="arithmatex"><span class="MathJax_Preview">d_{j}</span><script type="math/tex">d_{j}</script></span> 对于方差越小的主成分, ridge 的收缩越厉害</li>
<li>effective degrees of freedom <span class="arithmatex"><span class="MathJax_Preview">d f(\lambda)=\sum_{j=1}^{p} \frac{d_{j}^{2}}{d_{j}^{2}+\lambda}</span><script type="math/tex">d f(\lambda)=\sum_{j=1}^{p} \frac{d_{j}^{2}}{d_{j}^{2}+\lambda}</script></span></li>
</ul>
<p>观察上面的式子, <span class="arithmatex"><span class="MathJax_Preview">\mu_{j}^{\top} y</span><script type="math/tex">\mu_{j}^{\top} y</script></span> 将y投影到 X 的各个奇异向量(主成分), 然后根据对应的奇异值(<span class="arithmatex"><span class="MathJax_Preview">X^TX</span><script type="math/tex">X^TX</script></span>的特征值) 进行收缩, 根据分式可知奇异值越小的收缩越厉害.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned} \mathrm{df}(\lambda) &amp;=\operatorname{tr}\left[\mathbf{X}\left(\mathbf{X}^{\mathbf{T}} \mathbf{X}+\lambda \mathbf{I}\right)^{-1} \mathbf{X}^{T}\right] \\ &amp;=\operatorname{tr}\left(\mathbf{H}_{\lambda}\right) \\ &amp;=\sum_{j=1}^{p} \frac{d_{j}^{2}}{d_{j}^{2}+\lambda} \end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned} \mathrm{df}(\lambda) &=\operatorname{tr}\left[\mathbf{X}\left(\mathbf{X}^{\mathbf{T}} \mathbf{X}+\lambda \mathbf{I}\right)^{-1} \mathbf{X}^{T}\right] \\ &=\operatorname{tr}\left(\mathbf{H}_{\lambda}\right) \\ &=\sum_{j=1}^{p} \frac{d_{j}^{2}}{d_{j}^{2}+\lambda} \end{aligned}
</script>
</div>
<p>是岭回归的有效自由度, <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span> 的递减函数.</p>
<p><img alt="" src="../media/ASL-note1/2021-11-29-17-38-39.png" /></p>
<h4 id="the-lasso">The LASSO<a class="headerlink" href="#the-lasso" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex"><span class="MathJax_Preview">L_{2}</span><script type="math/tex">L_{2}</script></span> penalty <span class="arithmatex"><span class="MathJax_Preview">\sum_{j=1}^{p} \beta_{j}^{2}</span><script type="math/tex">\sum_{j=1}^{p} \beta_{j}^{2}</script></span> is replaced by <span class="arithmatex"><span class="MathJax_Preview">L_{1}</span><script type="math/tex">L_{1}</script></span> penalty <span class="arithmatex"><span class="MathJax_Preview">\sum_{j=1}^{p}\left|\beta_{j}\right|</span><script type="math/tex">\sum_{j=1}^{p}\left|\beta_{j}\right|</script></span>.
<span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {lasso }}=\operatorname{argmin}_{\beta}\left\{\frac{1}{2} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|\right\}</span><script type="math/tex">\hat{\beta}^{\text {lasso }}=\operatorname{argmin}_{\beta}\left\{\frac{1}{2} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|\right\}</script></span>
Which is equivalent to
<span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {lasso }}=\operatorname{argmin}_{\beta} \frac{1}{2} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}</span><script type="math/tex">\hat{\beta}^{\text {lasso }}=\operatorname{argmin}_{\beta} \frac{1}{2} \sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}</script></span> subject to <span class="arithmatex"><span class="MathJax_Preview">\sum_{j=1}^{p}\left|\beta_{j}\right| \leqslant t</span><script type="math/tex">\sum_{j=1}^{p}\left|\beta_{j}\right| \leqslant t</script></span></p>
<ul>
<li>Non-linear in y</li>
<li>No closed form expression</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {lasso }}</span><script type="math/tex">\hat{\beta}^{\text {lasso }}</script></span> becomes <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {OLS }}</span><script type="math/tex">\hat{\beta}^{\text {OLS }}</script></span> when <span class="arithmatex"><span class="MathJax_Preview">t&gt;\sum_{j=1}^{p}\left|\beta_{j}^{\text {OLS }}\right|</span><script type="math/tex">t>\sum_{j=1}^{p}\left|\beta_{j}^{\text {OLS }}\right|</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}^{\text {lasso }}=0</span><script type="math/tex">\hat{\beta}^{\text {lasso }}=0</script></span> when <span class="arithmatex"><span class="MathJax_Preview">t \rightarrow 0</span><script type="math/tex">t \rightarrow 0</script></span></li>
<li>Continuous subset selection.</li>
</ul>
<h4 id="comparisons-orthonormal-inputs">Comparisons - Orthonormal Inputs<a class="headerlink" href="#comparisons-orthonormal-inputs" title="Permanent link">&para;</a></h4>
<p>在 X 正交的情况, 三者都有显式解.</p>
<p><img alt="" src="../media/ASL-note1/2021-12-09-15-42-21.png" /></p>
<ul>
<li>Best Subset<ul>
<li>Drop all variables with coefficients smaller than the M-th</li>
<li>&ldquo;hard-thresholding.</li>
</ul>
</li>
<li>Ridge<ul>
<li>Proportional shrinkage</li>
</ul>
</li>
<li>LASSO<ul>
<li>translates by a constant factor <span class="arithmatex"><span class="MathJax_Preview">\lambda</span><script type="math/tex">\lambda</script></span>, truncating at zero.</li>
<li>&ldquo;soft thresholding&rdquo;</li>
</ul>
</li>
</ul>
<h4 id="comparisons-nonorthogonal-inputs">Comparisons - Nonorthogonal Inputs<a class="headerlink" href="#comparisons-nonorthogonal-inputs" title="Permanent link">&para;</a></h4>
<p><img alt="" src="../media/ASL-note1/2021-12-09-15-45-11.png" /></p>
<h4 id="generalization-of-ridge-and-lasso">Generalization of Ridge and LASSO<a class="headerlink" href="#generalization-of-ridge-and-lasso" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
\tilde{\beta}=\operatorname{argmin}_{\beta}\left\{\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|^{q}\right\}, q \geqslant 0
</div>
<script type="math/tex; mode=display">
\tilde{\beta}=\operatorname{argmin}_{\beta}\left\{\sum_{i=1}^{N}\left(y_{i}-\beta_{0}-\sum_{j=1}^{p} x_{i j} \beta_{j}\right)^{2}+\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|^{q}\right\}, q \geqslant 0
</script>
</div>
<p><img alt="" src="../media/ASL-note1/2021-12-09-15-48-58.png" /></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">q=0</span><script type="math/tex">q=0</script></span>, subset selection</li>
<li><span class="arithmatex"><span class="MathJax_Preview">0 \leqslant q&lt;1</span><script type="math/tex">0 \leqslant q<1</script></span>, non-convex constraint regions, difficult optimization <span class="arithmatex"><span class="MathJax_Preview">q=1</span><script type="math/tex">q=1</script></span>, LASSO</li>
<li><span class="arithmatex"><span class="MathJax_Preview">1&lt;q&lt;2</span><script type="math/tex">1<q<2</script></span> : &ldquo;compromise&rdquo; between Ridge and LASSO, but not desirable</li>
<li><span class="arithmatex"><span class="MathJax_Preview">q=2</span><script type="math/tex">q=2</script></span>, ridge</li>
</ul>
<p>Elastic net: <span class="arithmatex"><span class="MathJax_Preview">\lambda \sum_{j=1}^{p}\left(\alpha \beta_{j}^{2}+(1-\alpha)\left|\beta_{j}\right|\right), 0 \leqslant \alpha \leqslant 1</span><script type="math/tex">\lambda \sum_{j=1}^{p}\left(\alpha \beta_{j}^{2}+(1-\alpha)\left|\beta_{j}\right|\right), 0 \leqslant \alpha \leqslant 1</script></span></p>
<h4 id="least-angle-regression">Least Angle Regression<a class="headerlink" href="#least-angle-regression" title="Permanent link">&para;</a></h4>
<p>参见 <a href="https://blog.csdn.net/guofei_fly/article/details/103845342">最小角回归算法（LARS）</a> 图示很清楚</p>
<p>LAR（Least angle regression，最小角回归），由Efron等（2004）提出。这是一种非常有效的求解LASSO的算法，可以得到LASSO的解的路径。</p>
<p>Similar to forward stepwise, but only enters &ldquo;as much&rdquo; of a predictor as it deserves.</p>
<ul>
<li>Identifies the variable most correlated with the response</li>
<li>Moves the coef continuously toward its least-squares value</li>
<li>As soon as another variable &ldquo;catches up&rdquo;, stop</li>
<li>The second variable joins the active set</li>
<li>Repeat&hellip;</li>
</ul>
<p>Least Angle Regression - Algorithm</p>
<ul>
<li><strong>Standardize</strong> the predictors to have mean zero and unit norm. Start with the residual <span class="arithmatex"><span class="MathJax_Preview">r=y-\bar{y}, \beta_{1}, \beta_{2}, \ldots, \beta_{p}=0</span><script type="math/tex">r=y-\bar{y}, \beta_{1}, \beta_{2}, \ldots, \beta_{p}=0</script></span>.</li>
<li>Find the predictor <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span> most <strong>correlated</strong> with <span class="arithmatex"><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span></li>
<li>Move <span class="arithmatex"><span class="MathJax_Preview">\beta_{j}</span><script type="math/tex">\beta_{j}</script></span> from 0 towards its least-squares coefficient <span class="arithmatex"><span class="MathJax_Preview">\left\langle x_{j}, r\right\rangle</span><script type="math/tex">\left\langle x_{j}, r\right\rangle</script></span> , until some other competitor <span class="arithmatex"><span class="MathJax_Preview">x_{k}</span><script type="math/tex">x_{k}</script></span> has as much correlation with the current residual as does <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span></li>
<li>Move <span class="arithmatex"><span class="MathJax_Preview">\beta_{j}</span><script type="math/tex">\beta_{j}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\beta_{k}</span><script type="math/tex">\beta_{k}</script></span> in the direction defined by their <strong>joint least squares coefficient</strong> of the current residual on <span class="arithmatex"><span class="MathJax_Preview">\left(x_{j}, x_{k}\right)</span><script type="math/tex">\left(x_{j}, x_{k}\right)</script></span> until some other competitor <span class="arithmatex"><span class="MathJax_Preview">x_{l}</span><script type="math/tex">x_{l}</script></span> has as much correlation with the current residual</li>
<li>Continue in this way until all <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> predictors have been entered. After <span class="arithmatex"><span class="MathJax_Preview">\min (N-1, p)</span><script type="math/tex">\min (N-1, p)</script></span> steps, we arrive at the full least-squares solution.</li>
</ul>
<p><img alt="" src="../media/ASL-note1/2021-12-09-15-58-56.png" /></p>
<p><img alt="" src="../media/ASL-note1/2021-12-09-15-59-21.png" /></p>
<h3 id="methods-using-derived-input-directions">Methods Using Derived Input Directions<a class="headerlink" href="#methods-using-derived-input-directions" title="Permanent link">&para;</a></h3>
<p>图 3.18 显示了不同方法下当它们惩罚参数改变时的系数曲线．上图 ρ = 0.5，下图 ρ = −0.5．岭回归和 lasso 的惩罚参数在一个连续的区域内变化，而最优子集，PLS 和 PCR 只要两个离散的步骤便达到了最小二乘解．在上面的图中，从零点开始，岭回归整体收缩参数直到最后收缩到最小二乘．尽管 PLS 和 PCR 是离散 的且更加极端，但它们显示了类似岭回归的行为．最优子集超出解然后回溯． <strong>lasso 的行为是其他方法的过渡</strong>．当相关系数为负数时（下图），PLS 和 PCR 再一次大致地跟随岭回归的路径，而所有的方法都更加相似．</p>
<p>比较不同方法的收缩行为是很有趣的．岭回归对所有方向都有收缩但在低方差方向收缩程度更厉害．主成分回归将 M 个高方差的方向单独取出来，然后丢掉剩下的．有趣的是，可以证明<strong>偏最小二乘也趋向于收缩低方差的方向</strong>，但是实际上会使得某些高方差方向膨胀．这使得 PLS 稍微不太稳定，因此相比于岭回归会有较大的预测误差．整个研究由 Frank and Friedman (1993) 给出．他们总结到<strong>对于最小化预测误差，岭回归一般比变量子集选择、主成分回归和偏最小二乘更好</strong>．然而，相对于后两种方法的提高只是很小的．</p>
<p>总结一下，PLS，PCR 以及岭回归趋向于表现一致．岭回归可能会更好，因为它收缩得很光滑，不像离散步骤中一样．<strong>Lasso 介于岭回归和最优子集回归中间，并且有两者的部分性质</strong>．</p>
<p><img alt="" src="../media/ASL-note1/2021-12-09-16-36-40.png" /></p>
<h4 id="principle-components-regression-pcr">Principle Components Regression 主成分回归 PCR<a class="headerlink" href="#principle-components-regression-pcr" title="Permanent link">&para;</a></h4>
<p>Eigen decomposition of <span class="arithmatex"><span class="MathJax_Preview">X^{\top} X: X^{\top} X=V D^{2} V^{\top}</span><script type="math/tex">X^{\top} X: X^{\top} X=V D^{2} V^{\top}</script></span></p>
<ul>
<li>Eigen vector <span class="arithmatex"><span class="MathJax_Preview">v_{j}</span><script type="math/tex">v_{j}</script></span> (column of <span class="arithmatex"><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> ): principal components directions</li>
<li><span class="arithmatex"><span class="MathJax_Preview">z_{j}=X v_{j}:</span><script type="math/tex">z_{j}=X v_{j}:</script></span> principle components</li>
<li><span class="arithmatex"><span class="MathJax_Preview">z_{1}</span><script type="math/tex">z_{1}</script></span> has the largest sample variance amongst all normalized linear combinations of the columns of <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span>.</li>
<li>Derived input columns <span class="arithmatex"><span class="MathJax_Preview">\left(z_{1}, z_{2}, \ldots, z_{M}\right)</span><script type="math/tex">\left(z_{1}, z_{2}, \ldots, z_{M}\right)</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">M \leqslant p</span><script type="math/tex">M \leqslant p</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{y}_{(M)}^{p c r}=\bar{y} \mathbb{I}+\sum_{m=1}^{M} \hat{\theta}_{m} z_{m}</span><script type="math/tex">\hat{y}_{(M)}^{p c r}=\bar{y} \mathbb{I}+\sum_{m=1}^{M} \hat{\theta}_{m} z_{m}</script></span></li>
</ul>
<p><img alt="" src="../media/ASL-note1/2021-12-09-16-09-16.png" /></p>
<ul>
<li>当 <span class="arithmatex"><span class="MathJax_Preview">M=p</span><script type="math/tex">M=p</script></span> 时就是一般的最小二乘估计, 而对于 <span class="arithmatex"><span class="MathJax_Preview">M&lt;p</span><script type="math/tex">M<p</script></span> 我们得到一个降维的回归问题.</li>
<li>类似上面的 Ridge, 都是对于 PC 进行操作. Ridge 中对于主成分系数进行了收缩, 而PCR丢掉了 p-M 个最小的特征值分量.</li>
</ul>
<h4 id="partial-least-squares">Partial Least Squares 偏最小二乘<a class="headerlink" href="#partial-least-squares" title="Permanent link">&para;</a></h4>
<p>基本的思路就是, 计算每一个 <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> 方向上的回归系数, 然后将这些方向汇总起来得到一个虚拟的变量进行回归; 然后将所有的 <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> 去除该方向上的分量(正交化), 继续, 直到找到 M 个.</p>
<p>Use the output <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> (in addition to <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> ) in construction of the derived input</p>
<ul>
<li>Compute <span class="arithmatex"><span class="MathJax_Preview">\hat{\phi}_{1 j}=\left\langle x_{j}, y\right\rangle</span><script type="math/tex">\hat{\phi}_{1 j}=\left\langle x_{j}, y\right\rangle</script></span> for each <span class="arithmatex"><span class="MathJax_Preview">j=1, \ldots, p</span><script type="math/tex">j=1, \ldots, p</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">z_{1}=\sum_{j} \hat{\phi}_{1 j} x_{j}</span><script type="math/tex">z_{1}=\sum_{j} \hat{\phi}_{1 j} x_{j}</script></span> : <strong>first partial least squares direction</strong></li>
<li>Output <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> is regressed on <span class="arithmatex"><span class="MathJax_Preview">z_{1}</span><script type="math/tex">z_{1}</script></span> giving coefficient <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}_{1}</span><script type="math/tex">\hat{\theta}_{1}</script></span> 所谓「y is regressed on x」就是用x来预测y</li>
<li>Orthogonalize <span class="arithmatex"><span class="MathJax_Preview">x_{1}, \ldots, x_{p}</span><script type="math/tex">x_{1}, \ldots, x_{p}</script></span> w.r.t. <span class="arithmatex"><span class="MathJax_Preview">z_{1}</span><script type="math/tex">z_{1}</script></span></li>
<li>Continue until <span class="arithmatex"><span class="MathJax_Preview">M \leqslant p</span><script type="math/tex">M \leqslant p</script></span> directions have been obtained</li>
</ul>
<p>Note: Each <span class="arithmatex"><span class="MathJax_Preview">z_{m}</span><script type="math/tex">z_{m}</script></span> is weighted by the strength of their univariate effect on <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span></p>
<p>Partial Least Squares - Algorithm</p>
<ul>
<li>Standardize each <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span> to have mean zero and variance one. Set <span class="arithmatex"><span class="MathJax_Preview">\hat{y}^{(0)}=\bar{y}, x_{j}^{(0)}=x_{j}, j=1, \ldots, p .</span><script type="math/tex">\hat{y}^{(0)}=\bar{y}, x_{j}^{(0)}=x_{j}, j=1, \ldots, p .</script></span></li>
<li>For <span class="arithmatex"><span class="MathJax_Preview">m=1,2, \ldots, p</span><script type="math/tex">m=1,2, \ldots, p</script></span>
  1. <span class="arithmatex"><span class="MathJax_Preview">z_{m}=\sum_{j=1}^{p} \hat{\phi}_{m j} x_{j}^{(m-1)}</span><script type="math/tex">z_{m}=\sum_{j=1}^{p} \hat{\phi}_{m j} x_{j}^{(m-1)}</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">\hat{\phi}_{m j}=\left\langle x_{j}^{(m-1)}, y\right\rangle</span><script type="math/tex">\hat{\phi}_{m j}=\left\langle x_{j}^{(m-1)}, y\right\rangle</script></span>
  2. <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}_{m}=\frac{z_{m, y}}{z_{m}, z_{m}}</span><script type="math/tex">\hat{\theta}_{m}=\frac{z_{m, y}}{z_{m}, z_{m}}</script></span>
  3. <span class="arithmatex"><span class="MathJax_Preview">\hat{y}^{(m)}=\hat{y}^{(m-1)}+\hat{\theta}_{m} z_{m}</span><script type="math/tex">\hat{y}^{(m)}=\hat{y}^{(m-1)}+\hat{\theta}_{m} z_{m}</script></span>
  4. Orthogonalize each <span class="arithmatex"><span class="MathJax_Preview">x_{j}^{(m-1)}</span><script type="math/tex">x_{j}^{(m-1)}</script></span> w.r.t. <span class="arithmatex"><span class="MathJax_Preview">z_{m}: x_{j}^{(m)}=x_{j}^{(m-1)} - \frac{z_{m}, x_{j}^{(m-1)}}{z_{m}, z_{m}} z_{m}</span><script type="math/tex">z_{m}: x_{j}^{(m)}=x_{j}^{(m-1)} - \frac{z_{m}, x_{j}^{(m-1)}}{z_{m}, z_{m}} z_{m}</script></span>, <span class="arithmatex"><span class="MathJax_Preview">j=1, \ldots, p</span><script type="math/tex">j=1, \ldots, p</script></span></li>
<li>Output the sequence of fitted vectors <span class="arithmatex"><span class="MathJax_Preview">\left\{\hat{y}^{(m)}\right\}</span><script type="math/tex">\left\{\hat{y}^{(m)}\right\}</script></span>, recover coef <span class="arithmatex"><span class="MathJax_Preview">\hat{y}^{(m)}=X \hat{\beta}^{P L S}(m)</span><script type="math/tex">\hat{y}^{(m)}=X \hat{\beta}^{P L S}(m)</script></span></li>
</ul>
<h4 id="comparison-pcr-pls">Comparison - PCR \&amp; PLS<a class="headerlink" href="#comparison-pcr-pls" title="Permanent link">&para;</a></h4>
<p>PCR: seeks directions that have high variance</p>
<ul>
<li>The <span class="arithmatex"><span class="MathJax_Preview">m_{\text {th }}</span><script type="math/tex">m_{\text {th }}</script></span> principal component direction <span class="arithmatex"><span class="MathJax_Preview">\nu_{m}</span><script type="math/tex">\nu_{m}</script></span> solves</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\max _{\alpha} \operatorname{Var}(X \alpha)</span><script type="math/tex">\max _{\alpha} \operatorname{Var}(X \alpha)</script></span></li>
<li>subject to <span class="arithmatex"><span class="MathJax_Preview">\|\alpha\|=1, \alpha^{\top} S \nu_{l}=0, I=1, \ldots, m-1</span><script type="math/tex">\|\alpha\|=1, \alpha^{\top} S \nu_{l}=0, I=1, \ldots, m-1</script></span></li>
</ul>
<p>PLS: seeks directions that have high variance \&amp; high correlation with the output</p>
<ul>
<li>The <span class="arithmatex"><span class="MathJax_Preview">m_{t h}</span><script type="math/tex">m_{t h}</script></span> PLS direction <span class="arithmatex"><span class="MathJax_Preview">\hat{\phi}_{m}</span><script type="math/tex">\hat{\phi}_{m}</script></span> solves</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\max _{\alpha} \operatorname{Corr}^{2}(y, X \alpha) \operatorname{Var}(X \alpha)</span><script type="math/tex">\max _{\alpha} \operatorname{Corr}^{2}(y, X \alpha) \operatorname{Var}(X \alpha)</script></span></li>
<li>subject to <span class="arithmatex"><span class="MathJax_Preview">\|\alpha\|=1, \alpha^{\top} S \hat{\phi}_{1}=0, I=1, \ldots, m-1</span><script type="math/tex">\|\alpha\|=1, \alpha^{\top} S \hat{\phi}_{1}=0, I=1, \ldots, m-1</script></span></li>
</ul>
<p>Note: the variance aspect tends to dominate, so PLS behaves much like ridge and <span class="arithmatex"><span class="MathJax_Preview">P C R</span><script type="math/tex">P C R</script></span></p>
<h2 id="some-algebra">Some Algebra<a class="headerlink" href="#some-algebra" title="Permanent link">&para;</a></h2>
<p>关于奇异值分解, 参见 <a href="https://zhuanlan.zhihu.com/p/26306568">奇异值分解的揭秘（一）：矩阵的奇异值分解过程</a></p>
<ul>
<li>方阵的对角化分解/特征分解 <span class="arithmatex"><span class="MathJax_Preview">A=U\Lambda U^{-1}</span><script type="math/tex">A=U\Lambda U^{-1}</script></span>, 其中U的每一列是特征向量<ul>
<li>实对称阵的特征值都是实数, 且有N个线性无关的特征向量, 并且这些特征向量可以<strong>正交单位化</strong>为一组正交单位向量, 因此可分解为 <span class="arithmatex"><span class="MathJax_Preview">A=Q\Lambda Q^{-1}= Q\Lambda Q</span><script type="math/tex">A=Q\Lambda Q^{-1}= Q\Lambda Q</script></span> 其中Q为正交阵 (矩阵A的 <strong>对称对角化分解</strong>)</li>
</ul>
</li>
<li>奇异值分解 <span class="arithmatex"><span class="MathJax_Preview">A=P\Sigma Q</span><script type="math/tex">A=P\Sigma Q</script></span><ul>
<li>这里的P和Q分别是左奇异向量, 分别是 <span class="arithmatex"><span class="MathJax_Preview">AA^T=P\Lambda_1 P^T, A^TA=Q\Lambda_2 Q^T</span><script type="math/tex">AA^T=P\Lambda_1 P^T, A^TA=Q\Lambda_2 Q^T</script></span>, 这里两个特征值对角阵中, 非零特征值相同 (注意到 AB 和 BA 的特征值相同)</li>
<li>奇异值和特征值关系: <span class="arithmatex"><span class="MathJax_Preview">AA^T=P\Sigma^2 P^T</span><script type="math/tex">AA^T=P\Sigma^2 P^T</script></span>, 因此有 <span class="arithmatex"><span class="MathJax_Preview">\Sigma^2=\Lambda_1</span><script type="math/tex">\Sigma^2=\Lambda_1</script></span>, 也即奇异值是 <span class="arithmatex"><span class="MathJax_Preview">AA^T</span><script type="math/tex">AA^T</script></span> 的特征值的平方根 (注意 <span class="arithmatex"><span class="MathJax_Preview">AA^T</span><script type="math/tex">AA^T</script></span> 一定是<strong>正定</strong>的)</li>
</ul>
</li>
<li>附: 「正定阵」是在实对称阵的基础上定义的</li>
</ul>
<h3 id="eigen-decomposition-square-matrix">Eigen decomposition (square matrix)<a class="headerlink" href="#eigen-decomposition-square-matrix" title="Permanent link">&para;</a></h3>
<ul>
<li>A: eigenvalues <span class="arithmatex"><span class="MathJax_Preview">\lambda_{1}, \ldots, \lambda_{k}</span><script type="math/tex">\lambda_{1}, \ldots, \lambda_{k}</script></span>, eigenvectors <span class="arithmatex"><span class="MathJax_Preview">\nu_{1}, \ldots, \nu_{k}</span><script type="math/tex">\nu_{1}, \ldots, \nu_{k}</script></span></li>
<li>Let <span class="arithmatex"><span class="MathJax_Preview">D \equiv\left[\begin{array}{c}\lambda_{1}, 0, \ldots, 0 \\ 0, \lambda_{2}, \ldots, 0 \\ \ldots, \ldots, \ldots, \ldots \\ 0,0, \ldots, \lambda_{k}\end{array}\right], P \equiv\left[\nu_{1}, \nu_{2}, \ldots, \nu_{k}\right]</span><script type="math/tex">D \equiv\left[\begin{array}{c}\lambda_{1}, 0, \ldots, 0 \\ 0, \lambda_{2}, \ldots, 0 \\ \ldots, \ldots, \ldots, \ldots \\ 0,0, \ldots, \lambda_{k}\end{array}\right], P \equiv\left[\nu_{1}, \nu_{2}, \ldots, \nu_{k}\right]</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">A=P D P^{-1}, A^{2}=P D^{2} P^{-1}, \ldots, A^{n}=P D^{n} P^{-1}</span><script type="math/tex">A=P D P^{-1}, A^{2}=P D^{2} P^{-1}, \ldots, A^{n}=P D^{n} P^{-1}</script></span></li>
</ul>
<h3 id="singular-value-decomposition-any-matrix-m-times-nm-times-n">Singular value decomposition (any matrix <span class="arithmatex"><span class="MathJax_Preview">m \times n</span><script type="math/tex">m \times n</script></span> )<a class="headerlink" href="#singular-value-decomposition-any-matrix-m-times-nm-times-n" title="Permanent link">&para;</a></h3>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">A=U \Sigma V^{\top}</span><script type="math/tex">A=U \Sigma V^{\top}</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">U_{m \times n}, \Sigma_{n \times n}</span><script type="math/tex">U_{m \times n}, \Sigma_{n \times n}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">V_{n \times n}</span><script type="math/tex">V_{n \times n}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> is orthonormal and span the column space of <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span>,</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\Sigma</span><script type="math/tex">\Sigma</script></span> is diagonal with singular values of <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> has eigen vectors of <span class="arithmatex"><span class="MathJax_Preview">A^{\top} A</span><script type="math/tex">A^{\top} A</script></span> in the column</li>
</ul>
<p>也可以写成下面的形式:</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">U \in \mathbb{R}^{m \times m}</span><script type="math/tex">U \in \mathbb{R}^{m \times m}</script></span> orthonormal, <span class="arithmatex"><span class="MathJax_Preview">U U^{\top}=I</span><script type="math/tex">U U^{\top}=I</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">V \in \mathbb{R}^{n \times n}</span><script type="math/tex">V \in \mathbb{R}^{n \times n}</script></span> orthonormal, <span class="arithmatex"><span class="MathJax_Preview">V V^{\top}=I</span><script type="math/tex">V V^{\top}=I</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\Sigma \in \mathbb{R}^{m \times n}</span><script type="math/tex">\Sigma \in \mathbb{R}^{m \times n}</script></span> rectangular diagonal, <span class="arithmatex"><span class="MathJax_Preview">\Sigma=\operatorname{diag}\left(\sigma_{1}, \sigma_{2}, \ldots, \sigma_{p}\right)</span><script type="math/tex">\Sigma=\operatorname{diag}\left(\sigma_{1}, \sigma_{2}, \ldots, \sigma_{p}\right)</script></span> with <span class="arithmatex"><span class="MathJax_Preview">\sigma_{1} \geq \sigma_{2} \geq \ldots \geq \sigma_{p} \geq 0, p=\min (m, n)</span><script type="math/tex">\sigma_{1} \geq \sigma_{2} \geq \ldots \geq \sigma_{p} \geq 0, p=\min (m, n)</script></span></li>
<li>( <span class="arithmatex"><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> : left singular vector, <span class="arithmatex"><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> : right singular vector, <span class="arithmatex"><span class="MathJax_Preview">\sigma</span><script type="math/tex">\sigma</script></span> &lsquo;s: singular values)</li>
</ul>
<p><strong>Theorem</strong>: <strong>给定一个实数矩阵, 总能找到其 SVD 分解</strong>, 满足 <span class="arithmatex"><span class="MathJax_Preview">\Sigma</span><script type="math/tex">\Sigma</script></span> 为对角阵并且对角线元素非负. (Proof is not required)
If <span class="arithmatex"><span class="MathJax_Preview">A \in \mathbb{R}^{m \times n}</span><script type="math/tex">A \in \mathbb{R}^{m \times n}</script></span> is a real matrix, <span class="arithmatex"><span class="MathJax_Preview">\exists</span><script type="math/tex">\exists</script></span> orthonormal matrix <span class="arithmatex"><span class="MathJax_Preview">U \in \mathbb{R}^{m \times m}</span><script type="math/tex">U \in \mathbb{R}^{m \times m}</script></span>, orthornormal matrix <span class="arithmatex"><span class="MathJax_Preview">V \in \mathbb{R}^{n \times n}</span><script type="math/tex">V \in \mathbb{R}^{n \times n}</script></span> and rectangular diagonal matrix <span class="arithmatex"><span class="MathJax_Preview">\Sigma \in \mathbb{R}^{m \times n}</span><script type="math/tex">\Sigma \in \mathbb{R}^{m \times n}</script></span> with descending and non-negative diagonal elements, such that</p>
<div class="arithmatex">
<div class="MathJax_Preview">
A=U \Sigma V^{\top}
</div>
<script type="math/tex; mode=display">
A=U \Sigma V^{\top}
</script>
</div>
<p><img alt="" src="../media/ASL-note1/2021-11-29-17-51-22.png" /></p>
<h4 id="compact-svd">Compact SVD<a class="headerlink" href="#compact-svd" title="Permanent link">&para;</a></h4>
<p>If <span class="arithmatex"><span class="MathJax_Preview">A \in \mathbb{R}^{m \times n}</span><script type="math/tex">A \in \mathbb{R}^{m \times n}</script></span> is a real matrix with <span class="arithmatex"><span class="MathJax_Preview">\operatorname{rank}(A)=r \leq \min (m, n)</span><script type="math/tex">\operatorname{rank}(A)=r \leq \min (m, n)</script></span>, compact SVD:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
A=U_{r} \Sigma_{r} V_{r}^{\top}
</div>
<script type="math/tex; mode=display">
A=U_{r} \Sigma_{r} V_{r}^{\top}
</script>
</div>
<p><span class="arithmatex"><span class="MathJax_Preview">U_{r} \in \mathbb{R}^{m \times r}:</span><script type="math/tex">U_{r} \in \mathbb{R}^{m \times r}:</script></span> first <span class="arithmatex"><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span> columns of <span class="arithmatex"><span class="MathJax_Preview">U \in \mathbb{R}^{m \times m}</span><script type="math/tex">U \in \mathbb{R}^{m \times m}</script></span>
<span class="arithmatex"><span class="MathJax_Preview">V_{r} \in \mathbb{R}^{n \times r}:</span><script type="math/tex">V_{r} \in \mathbb{R}^{n \times r}:</script></span> first <span class="arithmatex"><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span> columns of <span class="arithmatex"><span class="MathJax_Preview">V \in \mathbb{R}^{n \times n}</span><script type="math/tex">V \in \mathbb{R}^{n \times n}</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\Sigma_{r} \in \mathbb{R}^{r \times r}:</span><script type="math/tex">\Sigma_{r} \in \mathbb{R}^{r \times r}:</script></span> first <span class="arithmatex"><span class="MathJax_Preview">r</span><script type="math/tex">r</script></span> diagonal elements of <span class="arithmatex"><span class="MathJax_Preview">\Sigma \in \mathbb{R}^{m \times n}</span><script type="math/tex">\Sigma \in \mathbb{R}^{m \times n}</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\operatorname{rank}\left(\Sigma_{r}\right)=\operatorname{rank}(A)</span><script type="math/tex">\operatorname{rank}\left(\Sigma_{r}\right)=\operatorname{rank}(A)</script></span></p>
<h4 id="truncated-svd">Truncated SVD<a class="headerlink" href="#truncated-svd" title="Permanent link">&para;</a></h4>
<p>If <span class="arithmatex"><span class="MathJax_Preview">A \in \mathbb{R}^{m \times n}</span><script type="math/tex">A \in \mathbb{R}^{m \times n}</script></span> is a real matrix with <span class="arithmatex"><span class="MathJax_Preview">\operatorname{rank}(A)=r \leq \min (m, n), 0&lt;k&lt;r</span><script type="math/tex">\operatorname{rank}(A)=r \leq \min (m, n), 0<k<r</script></span>, truncated SVD:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
A \approx U_{k} \Sigma_{k} V_{k}^{\top}
</div>
<script type="math/tex; mode=display">
A \approx U_{k} \Sigma_{k} V_{k}^{\top}
</script>
</div>
<p><span class="arithmatex"><span class="MathJax_Preview">U_{k} \in \mathbb{R}^{m \times k}</span><script type="math/tex">U_{k} \in \mathbb{R}^{m \times k}</script></span> : first <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> columns of <span class="arithmatex"><span class="MathJax_Preview">U \in \mathbb{R}^{m \times m}</span><script type="math/tex">U \in \mathbb{R}^{m \times m}</script></span>
<span class="arithmatex"><span class="MathJax_Preview">V_{k} \in \mathbb{R}^{n \times k}</span><script type="math/tex">V_{k} \in \mathbb{R}^{n \times k}</script></span> : first <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> columns of <span class="arithmatex"><span class="MathJax_Preview">V \in \mathbb{R}^{n \times n}</span><script type="math/tex">V \in \mathbb{R}^{n \times n}</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\Sigma_{k} \in \mathbb{R}^{k \times k}:</span><script type="math/tex">\Sigma_{k} \in \mathbb{R}^{k \times k}:</script></span> first <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> diagonal elements of <span class="arithmatex"><span class="MathJax_Preview">\Sigma \in \mathbb{R}^{m \times n}</span><script type="math/tex">\Sigma \in \mathbb{R}^{m \times n}</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\operatorname{rank}\left(\Sigma_{k}\right)&lt;\operatorname{rank}(A)</span><script type="math/tex">\operatorname{rank}\left(\Sigma_{k}\right)<\operatorname{rank}(A)</script></span></p>
<h4 id="geometry-of-svd">Geometry of SVD<a class="headerlink" href="#geometry-of-svd" title="Permanent link">&para;</a></h4>
<p><span class="arithmatex"><span class="MathJax_Preview">A \in \mathbb{R}^{m \times n}:</span><script type="math/tex">A \in \mathbb{R}^{m \times n}:</script></span> a linear transformation from <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{n}</span><script type="math/tex">\mathbb{R}^{n}</script></span> to <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{m}</span><script type="math/tex">\mathbb{R}^{m}</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
T: \quad x \rightarrow A x \quad\left(U \Sigma V^{\top} x\right)
</div>
<script type="math/tex; mode=display">
T: \quad x \rightarrow A x \quad\left(U \Sigma V^{\top} x\right)
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">x \in \mathbb{R}^{n}, A x \in \mathbb{R}^{m}</span><script type="math/tex">x \in \mathbb{R}^{n}, A x \in \mathbb{R}^{m}</script></span></li>
<li>A linear transformation can be decomposed into 3 steps: rotation, stretch/scale, rotation again<ul>
<li>coordinates rotated by <span class="arithmatex"><span class="MathJax_Preview">V^{\top}</span><script type="math/tex">V^{\top}</script></span> (orthonormal basis in <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{n}</span><script type="math/tex">\mathbb{R}^{n}</script></span> )</li>
<li>coordinates stretched or scaled by <span class="arithmatex"><span class="MathJax_Preview">\Sigma\left(\sigma_{1}, \sigma_{2}, \ldots, \sigma_{n}\right)</span><script type="math/tex">\Sigma\left(\sigma_{1}, \sigma_{2}, \ldots, \sigma_{n}\right)</script></span></li>
<li>coordinates rotated by <span class="arithmatex"><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> (orthonormal basis in <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{m}</span><script type="math/tex">\mathbb{R}^{m}</script></span> )</li>
</ul>
</li>
</ul>
<p><img alt="" src="../media/ASL-note1/2021-12-04-10-01-01.png" /></p>
<h4 id="characteristics-of-svd">Characteristics of SVD<a class="headerlink" href="#characteristics-of-svd" title="Permanent link">&para;</a></h4>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\begin{aligned} A^{\top} A &amp;=\left(U \Sigma V^{\top}\right)^{\top}\left(U \Sigma V^{\top}\right)=V\left(\Sigma^{\top} \Sigma\right) V^{\top}, \\ A A^{\top} &amp;=\left(U \Sigma V^{\top}\right)\left(U \Sigma V^{\top}\right)^{\top}=U\left(\Sigma \Sigma^{\top}\right) U^{\top} \end{aligned}</span><script type="math/tex">\begin{aligned} A^{\top} A &=\left(U \Sigma V^{\top}\right)^{\top}\left(U \Sigma V^{\top}\right)=V\left(\Sigma^{\top} \Sigma\right) V^{\top}, \\ A A^{\top} &=\left(U \Sigma V^{\top}\right)\left(U \Sigma V^{\top}\right)^{\top}=U\left(\Sigma \Sigma^{\top}\right) U^{\top} \end{aligned}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">A V=U \Sigma U^{\top} V=U \Sigma</span><script type="math/tex">A V=U \Sigma U^{\top} V=U \Sigma</script></span>, i.e., <span class="arithmatex"><span class="MathJax_Preview">A \nu_{j}=\sigma_{j} \mu_{j}, j=1,2, \ldots, n</span><script type="math/tex">A \nu_{j}=\sigma_{j} \mu_{j}, j=1,2, \ldots, n</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">A^{\top} U=V \Sigma^{\top}</span><script type="math/tex">A^{\top} U=V \Sigma^{\top}</script></span>, i.e., <span class="arithmatex"><span class="MathJax_Preview">A^{\top} \mu_{j}=\sigma_{j} \nu_{j}, j=1,2, \ldots, n</span><script type="math/tex">A^{\top} \mu_{j}=\sigma_{j} \nu_{j}, j=1,2, \ldots, n</script></span> and <span class="arithmatex"><span class="MathJax_Preview">A^{\top} \mu_{j}=0</span><script type="math/tex">A^{\top} \mu_{j}=0</script></span>, <span class="arithmatex"><span class="MathJax_Preview">j=n+1, n+2, \ldots, m</span><script type="math/tex">j=n+1, n+2, \ldots, m</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\sigma_{1}, \sigma_{2}, \ldots, \sigma_{n}</span><script type="math/tex">\sigma_{1}, \sigma_{2}, \ldots, \sigma_{n}</script></span> is unique, but <span class="arithmatex"><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> and <span class="arithmatex"><span class="MathJax_Preview">V^{\top}</span><script type="math/tex">V^{\top}</script></span> is not 注意到, 奇异向量是不唯一的</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{rank}(A)=\operatorname{rank}(\Sigma)=\mathbb{I}\left(\sigma_{i}&gt;0\right)</span><script type="math/tex">\operatorname{rank}(A)=\operatorname{rank}(\Sigma)=\mathbb{I}\left(\sigma_{i}>0\right)</script></span></li>
</ul>
<h4 id="matrix-approximation-and-svd">Matrix Approximation and SVD<a class="headerlink" href="#matrix-approximation-and-svd" title="Permanent link">&para;</a></h4>
<p>SVD和矩阵近似: 对于一个矩阵A, 要求其在F范数意义下的 rank-k 的近似, 这种近似的下界为 <span class="arithmatex"><span class="MathJax_Preview">\left(\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}</span><script type="math/tex">\left(\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}</script></span>, 并且可以通过SVD的前 k 个奇异值算出来.</p>
<p><span class="arithmatex"><span class="MathJax_Preview">A=\left[a_{i j}\right]_{m \times n}</span><script type="math/tex">A=\left[a_{i j}\right]_{m \times n}</script></span> with <span class="arithmatex"><span class="MathJax_Preview">\|A\|_{F}=\left(\sum_{i=1}^{m} \sum_{j=1}^{n}\left(a_{i j}\right)^{2}\right)^{\frac{1}{2}}=\left(\sigma_{1}^{2}+\sigma_{2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}</span><script type="math/tex">\|A\|_{F}=\left(\sum_{i=1}^{m} \sum_{j=1}^{n}\left(a_{i j}\right)^{2}\right)^{\frac{1}{2}}=\left(\sigma_{1}^{2}+\sigma_{2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}</script></span>, if <span class="arithmatex"><span class="MathJax_Preview">\operatorname{rank}(A)=r</span><script type="math/tex">\operatorname{rank}(A)=r</script></span> and <span class="arithmatex"><span class="MathJax_Preview">A=U \Sigma V^{\top}</span><script type="math/tex">A=U \Sigma V^{\top}</script></span>, denote <span class="arithmatex"><span class="MathJax_Preview">\mathcal{M}</span><script type="math/tex">\mathcal{M}</script></span> the set of all matrix in <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{m \times n}</span><script type="math/tex">\mathbb{R}^{m \times n}</script></span> with rank at most <span class="arithmatex"><span class="MathJax_Preview">k, 0&lt;k&lt;r</span><script type="math/tex">k, 0<k<r</script></span>, if <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> with <span class="arithmatex"><span class="MathJax_Preview">\operatorname{ran} k(X)=k</span><script type="math/tex">\operatorname{ran} k(X)=k</script></span> satisfies</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\|A-X\|_{F}=\min_{S \in \mathcal{M}}\|A-S\|_{F}
</div>
<script type="math/tex; mode=display">
\|A-X\|_{F}=\min_{S \in \mathcal{M}}\|A-S\|_{F}
</script>
</div>
<p>then</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\|A-X\|_{F}=\left(\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}
</div>
<script type="math/tex; mode=display">
\|A-X\|_{F}=\left(\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}
</script>
</div>
<p>Specifically, if <span class="arithmatex"><span class="MathJax_Preview">A^{\prime}=U \Sigma^{\prime} V^{\top}</span><script type="math/tex">A^{\prime}=U \Sigma^{\prime} V^{\top}</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">\Sigma^{\prime}=\left\{\begin{array}{cc}\Sigma_{k} &amp; 0 \\ 0 &amp; 0\end{array}\right\}</span><script type="math/tex">\Sigma^{\prime}=\left\{\begin{array}{cc}\Sigma_{k} & 0 \\ 0 & 0\end{array}\right\}</script></span>, then</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\left\|A-A^{\prime}\right\|_{F}=\left(\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}=\min _{S \in \mathcal{M}}\|A-S\|_{F}
</div>
<script type="math/tex; mode=display">
\left\|A-A^{\prime}\right\|_{F}=\left(\sigma_{k+1}^{2}+\sigma_{k+2}^{2}+\ldots+\sigma_{n}^{2}\right)^{\frac{1}{2}}=\min _{S \in \mathcal{M}}\|A-S\|_{F}
</script>
</div>
<h3 id="pca-principle-component-analysis">PCA (Principle Component Analysis)<a class="headerlink" href="#pca-principle-component-analysis" title="Permanent link">&para;</a></h3>
<p>参见 <a href="https://jozeelin.github.io/2019/08/28/pca/">PCA主成分分析</a></p>
<ul>
<li>首先对给定数据进行规范化，使得数据每一变量的平均值为0，方差为1</li>
<li>对数据进行正交变换。原来由线性相关变量表示的数据，通过正交变换变成由若干个线性无关的新变量表示的数据。新变量是可能的正交变换中变量的方差的和(信息保存)最大的，方差表示在新变量上的信息的大小。</li>
</ul>
<p>数据集合中的样本由实数空间(正交坐标系)中的点表示，空间的一个坐标轴表示一个变量，规范化处理后得到的数据分布在原点附近。对原坐标系中的数据进行主成分分析等价于进行坐标系旋转变换，将数据投影到新坐标系的坐标轴上；新坐标系的第一坐标轴、第二坐标轴等分别表示第一主成分、第二主成分等。数据在每一轴上的坐标值的平方表示相应变量的方差；并且，这个坐标系是在所有可能的新的坐标系中，坐标轴上的方差的和最大的。</p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-10-47-10.png" /></p>
<ul>
<li>在数据总体上进行主成分分析称为总体主成分分析。</li>
<li>在有限样本上进行主成分分析称为样本主成分分析。</li>
</ul>
<h4 id="population-pca">Population PCA<a class="headerlink" href="#population-pca" title="Permanent link">&para;</a></h4>
<p>Suppose <span class="arithmatex"><span class="MathJax_Preview">x=\left(x_{1}, x_{2}, \ldots, x_{m}\right)^{\top}</span><script type="math/tex">x=\left(x_{1}, x_{2}, \ldots, x_{m}\right)^{\top}</script></span> is a <span class="arithmatex"><span class="MathJax_Preview">\mathrm{m}</span><script type="math/tex">\mathrm{m}</script></span>-dim random vector, with</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;\mu=E(x)=\left(\mu_{1}, \mu_{2}, \ldots, \mu_{m}\right)^{\top} \\
&amp;\Sigma=\operatorname{cov}(x, x)=E\left[(x-\mu)(x-\mu)^{\top}\right]
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&\mu=E(x)=\left(\mu_{1}, \mu_{2}, \ldots, \mu_{m}\right)^{\top} \\
&\Sigma=\operatorname{cov}(x, x)=E\left[(x-\mu)(x-\mu)^{\top}\right]
\end{aligned}
</script>
</div>
<p>Consider a linear transformation from <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> to <span class="arithmatex"><span class="MathJax_Preview">y=\left(y_{1}, y_{2}, \ldots, y_{m}\right)^{\top}</span><script type="math/tex">y=\left(y_{1}, y_{2}, \ldots, y_{m}\right)^{\top}</script></span>, with <span class="arithmatex"><span class="MathJax_Preview">\alpha_{i}^{\top}=\left(\alpha_{1 i}, \alpha_{2 i}, \ldots, \alpha_{m i}\right), i=1,2, \ldots, m</span><script type="math/tex">\alpha_{i}^{\top}=\left(\alpha_{1 i}, \alpha_{2 i}, \ldots, \alpha_{m i}\right), i=1,2, \ldots, m</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
y_{i}=\alpha_{i}^{\top} x=\alpha_{1 i} x_{1}+\alpha_{2 i} x_{2}+\ldots+\alpha_{m i} x_{m}
</div>
<script type="math/tex; mode=display">
y_{i}=\alpha_{i}^{\top} x=\alpha_{1 i} x_{1}+\alpha_{2 i} x_{2}+\ldots+\alpha_{m i} x_{m}
</script>
</div>
<p>Then we know that</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">E\left(y_{i}\right)=\alpha_{i}^{\top} \mu</span><script type="math/tex">E\left(y_{i}\right)=\alpha_{i}^{\top} \mu</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{var}\left(y_{i}\right)=\alpha_{i}^{\top} \Sigma \alpha_{i}</span><script type="math/tex">\operatorname{var}\left(y_{i}\right)=\alpha_{i}^{\top} \Sigma \alpha_{i}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{cov}\left(y_{i}, y_{j}\right)=\alpha_{i}^{\top} \Sigma \alpha_{j}</span><script type="math/tex">\operatorname{cov}\left(y_{i}, y_{j}\right)=\alpha_{i}^{\top} \Sigma \alpha_{j}</script></span></li>
</ul>
<p>问题定义:</p>
<p>Given a linear transformation</p>
<div class="arithmatex">
<div class="MathJax_Preview">
y_{i}=\alpha_{i}^{\top} x=\alpha_{1 i} x_{1}+\alpha_{2 i} x_{2}+\ldots+\alpha_{m i} x_{m}
</div>
<script type="math/tex; mode=display">
y_{i}=\alpha_{i}^{\top} x=\alpha_{1 i} x_{1}+\alpha_{2 i} x_{2}+\ldots+\alpha_{m i} x_{m}
</script>
</div>
<p>if</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\alpha_{i}^{\top} \alpha_{i}=1</span><script type="math/tex">\alpha_{i}^{\top} \alpha_{i}=1</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{cov}\left(y_{i}, y_{j}\right)=0</span><script type="math/tex">\operatorname{cov}\left(y_{i}, y_{j}\right)=0</script></span> for <span class="arithmatex"><span class="MathJax_Preview">i \neq j</span><script type="math/tex">i \neq j</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">y_{1}</span><script type="math/tex">y_{1}</script></span> has the largest variance among all linear transformations of <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>; <span class="arithmatex"><span class="MathJax_Preview">y_{2}</span><script type="math/tex">y_{2}</script></span> has the largest variance among all linear transformations that are uncorrelated with <span class="arithmatex"><span class="MathJax_Preview">y_{1}, \ldots</span><script type="math/tex">y_{1}, \ldots</script></span></li>
</ul>
<p>Then, we call <span class="arithmatex"><span class="MathJax_Preview">y_{1}, y_{2}, \ldots, y_{m}</span><script type="math/tex">y_{1}, y_{2}, \ldots, y_{m}</script></span> the first, second, &hellip;, <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>-th principle component of <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>.</p>
<h4 id="theorem-of-pca">Theorem of PCA<a class="headerlink" href="#theorem-of-pca" title="Permanent link">&para;</a></h4>
<p><strong>定理</strong>: 对于随机向量x来说, 其第 k 个PC正是由其协方差矩阵的第 k 个奇异向量 (对称半正定阵的特征和奇异值分解应该是一样的), 并且这个 PC 的方差正是第 k 个奇异值(特征值).</p>
<p>Suppose <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>-dim random variable with <strong>covariance matrix</strong> <span class="arithmatex"><span class="MathJax_Preview">\Sigma</span><script type="math/tex">\Sigma</script></span>, <span class="arithmatex"><span class="MathJax_Preview">\lambda_{1} \geq \lambda_{2} \geq \ldots \geq \lambda_{m} \geq 0</span><script type="math/tex">\lambda_{1} \geq \lambda_{2} \geq \ldots \geq \lambda_{m} \geq 0</script></span> are the eigenvalues of <span class="arithmatex"><span class="MathJax_Preview">\Sigma</span><script type="math/tex">\Sigma</script></span>, and <span class="arithmatex"><span class="MathJax_Preview">\alpha_{1}, \alpha_{2}, \ldots, \alpha_{m}</span><script type="math/tex">\alpha_{1}, \alpha_{2}, \ldots, \alpha_{m}</script></span> are the corresponding eigenvectors, then the <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>-th principle component of <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is given by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
y_{k}=\alpha_{k}^{\top} x=\alpha_{1 k} x_{1}+\alpha_{2 k} x_{2}+\ldots+\alpha_{m k} x_{m}
</div>
<script type="math/tex; mode=display">
y_{k}=\alpha_{k}^{\top} x=\alpha_{1 k} x_{1}+\alpha_{2 k} x_{2}+\ldots+\alpha_{m k} x_{m}
</script>
</div>
<p>for <span class="arithmatex"><span class="MathJax_Preview">k=1,2, \ldots, m</span><script type="math/tex">k=1,2, \ldots, m</script></span>, and the variance of <span class="arithmatex"><span class="MathJax_Preview">y_{k}</span><script type="math/tex">y_{k}</script></span> is given by (the <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>-th eigen value of <span class="arithmatex"><span class="MathJax_Preview">\Sigma</span><script type="math/tex">\Sigma</script></span> )</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{var}\left(y_{k}\right)=\alpha_{k}^{\top} \Sigma \alpha_{k}=\lambda_{k}
</div>
<script type="math/tex; mode=display">
\operatorname{var}\left(y_{k}\right)=\alpha_{k}^{\top} \Sigma \alpha_{k}=\lambda_{k}
</script>
</div>
<p>若特征值有重根，对应的特征向量组成m维空间 <span class="arithmatex"><span class="MathJax_Preview">\mathcal{R}^m</span><script type="math/tex">\mathcal{R}^m</script></span> 的一个子空间，子空间的维数等于重根数，在子空间任取一个正交坐标系，这个坐标系的单位向量就可作为特征向量。这是坐标系取法不唯一。</p>
<p>[m维随机变量 <span class="arithmatex"><span class="MathJax_Preview">y=(y_1,y_2,…,y_m)^T</span><script type="math/tex">y=(y_1,y_2,…,y_m)^T</script></span> 的分量依次是x 的第一主成分到第m主成分的充要条件是]</p>
<p>The components of <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>-dim random variable <span class="arithmatex"><span class="MathJax_Preview">y=\left(y_{1}, y_{2}, \ldots, y_{m}\right)^{\top}</span><script type="math/tex">y=\left(y_{1}, y_{2}, \ldots, y_{m}\right)^{\top}</script></span> are the first, second, &hellip;, <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>-th principle component of <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> iff</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">y=A^{\top} x</span><script type="math/tex">y=A^{\top} x</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">A</span><script type="math/tex">A</script></span> is orthogonal with <span class="arithmatex"><span class="MathJax_Preview">A=\left\{\begin{array}{cccc}a_{11} &amp; a_{12} &amp; \ldots &amp; a_{1 m} \\ a_{21} &amp; a_{22} &amp; \ldots &amp; a_{2 m} \\ \ldots &amp; \ldots &amp; \ldots &amp; \ldots \\ a_{m 1} &amp; a_{m 2} &amp; \ldots &amp; a_{m m}\end{array}\right\}</span><script type="math/tex">A=\left\{\begin{array}{cccc}a_{11} & a_{12} & \ldots & a_{1 m} \\ a_{21} & a_{22} & \ldots & a_{2 m} \\ \ldots & \ldots & \ldots & \ldots \\ a_{m 1} & a_{m 2} & \ldots & a_{m m}\end{array}\right\}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{cov}(y)=\Lambda=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}\right)</span><script type="math/tex">\operatorname{cov}(y)=\Lambda=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}\right)</script></span>, with <span class="arithmatex"><span class="MathJax_Preview">\lambda_{1} \geq \lambda_{2} \geq \ldots \geq \lambda_{m}</span><script type="math/tex">\lambda_{1} \geq \lambda_{2} \geq \ldots \geq \lambda_{m}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\lambda_{k}</span><script type="math/tex">\lambda_{k}</script></span> is the <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span>-th eigenvalue of <span class="arithmatex"><span class="MathJax_Preview">\Sigma_{1}</span><script type="math/tex">\Sigma_{1}</script></span>, <span class="arithmatex"><span class="MathJax_Preview">\alpha_{k}</span><script type="math/tex">\alpha_{k}</script></span> is the corresponding eigenvector, for <span class="arithmatex"><span class="MathJax_Preview">k=1,2, \ldots, m</span><script type="math/tex">k=1,2, \ldots, m</script></span></li>
</ul>
<h4 id="characteristics-of-pca">Characteristics of PCA<a class="headerlink" href="#characteristics-of-pca" title="Permanent link">&para;</a></h4>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\operatorname{cov}(y)=\Lambda=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}\right)</span><script type="math/tex">\operatorname{cov}(y)=\Lambda=\operatorname{diag}\left(\lambda_{1}, \lambda_{2}, \ldots, \lambda_{m}\right)</script></span> 各个PC的方差就是协方差阵的各个奇异值/特征值</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\sum_{i=1}^{m} \lambda_{i}=\sum_{i=1}^{m} \sigma_{i i}</span><script type="math/tex">\sum_{i=1}^{m} \lambda_{i}=\sum_{i=1}^{m} \sigma_{i i}</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">\sigma_{i i}</span><script type="math/tex">\sigma_{i i}</script></span> is the variance of <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span>  总体主成分y的方差之和等于随机变量x的方差之和<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\sum_{i=1}^{m} \operatorname{var}\left(x_{i}\right) = \operatorname{tr}\left(\Sigma^{\top}\right)=\operatorname{tr}\left(A \Lambda A^{\top}\right)=\operatorname{tr}\left(A^{\top} \Lambda A\right) =\operatorname{tr}(\Lambda)=\sum_{i=1}^{m} \lambda_{i}=\sum_{i=1}^{m} \operatorname{var}\left(y_{i}\right)</span><script type="math/tex">\sum_{i=1}^{m} \operatorname{var}\left(x_{i}\right) = \operatorname{tr}\left(\Sigma^{\top}\right)=\operatorname{tr}\left(A \Lambda A^{\top}\right)=\operatorname{tr}\left(A^{\top} \Lambda A\right) =\operatorname{tr}(\Lambda)=\sum_{i=1}^{m} \lambda_{i}=\sum_{i=1}^{m} \operatorname{var}\left(y_{i}\right)</script></span></li>
</ul>
</li>
<li>factor loading: correlation coefficient between <span class="arithmatex"><span class="MathJax_Preview">y_{k}</span><script type="math/tex">y_{k}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span>, <span class="arithmatex"><span class="MathJax_Preview">\rho\left(y_{k}, x_{i}\right)=\frac{\sqrt{\lambda_{k}} \alpha_{i k}}{\sqrt{\sigma_{i i}}}</span><script type="math/tex">\rho\left(y_{k}, x_{i}\right)=\frac{\sqrt{\lambda_{k}} \alpha_{i k}}{\sqrt{\sigma_{i i}}}</script></span> 第k个主成分 <span class="arithmatex"><span class="MathJax_Preview">y_k</span><script type="math/tex">y_k</script></span> 与变量xi的相关系数 <span class="arithmatex"><span class="MathJax_Preview">ρ(y_k,x_i)</span><script type="math/tex">ρ(y_k,x_i)</script></span> 称为<strong>因子负荷量</strong>(factor loading)，它表示第k个主成分 <span class="arithmatex"><span class="MathJax_Preview">y_k</span><script type="math/tex">y_k</script></span> 与变量 <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> 的相关性<ul>
<li>注意到 <span class="arithmatex"><span class="MathJax_Preview">\operatorname{cov}(y_k, x_i) = \operatorname{cov}\left(\alpha_{k}^{\top} x, e_{i}^{\top} x\right)=\alpha_{k}^{\top} \Sigma e_{i}=e_{i}^{\top} \Sigma \alpha_{k}=\lambda_{k} e_{i}^{\top} \alpha_{k}=\lambda_{k} \alpha_{i k}</span><script type="math/tex">\operatorname{cov}(y_k, x_i) = \operatorname{cov}\left(\alpha_{k}^{\top} x, e_{i}^{\top} x\right)=\alpha_{k}^{\top} \Sigma e_{i}=e_{i}^{\top} \Sigma \alpha_{k}=\lambda_{k} e_{i}^{\top} \alpha_{k}=\lambda_{k} \alpha_{i k}</script></span></li>
</ul>
</li>
<li>sum of factor loading for <span class="arithmatex"><span class="MathJax_Preview">y_{k}</span><script type="math/tex">y_{k}</script></span> over <span class="arithmatex"><span class="MathJax_Preview">x: \sum_{i=1}^{m} \sigma_{i i} \rho^{2}\left(y_{k}, x_{i}\right)=\lambda_{k}</span><script type="math/tex">x: \sum_{i=1}^{m} \sigma_{i i} \rho^{2}\left(y_{k}, x_{i}\right)=\lambda_{k}</script></span> 第k个主成分 <span class="arithmatex"><span class="MathJax_Preview">y_k</span><script type="math/tex">y_k</script></span> 与m个变量的因子负荷量<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\sum_{i=1}^{m} \sigma_{i i} \rho^{2}\left(y_{k}, x_{i}\right)=\sum_{i=1}^{m} \lambda_{k} \alpha_{i j}^{2}=\lambda_{k} \alpha_{k}^{\top} \alpha_{k}=\lambda_{k}</span><script type="math/tex">\sum_{i=1}^{m} \sigma_{i i} \rho^{2}\left(y_{k}, x_{i}\right)=\sum_{i=1}^{m} \lambda_{k} \alpha_{i j}^{2}=\lambda_{k} \alpha_{k}^{\top} \alpha_{k}=\lambda_{k}</script></span></li>
<li>也即, 以x的各个分量的方差作为系数, 可以得到<strong>第 k 个PC的方差和因子负荷量</strong>的关系</li>
</ul>
</li>
<li>sum of factor loading for <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> over <span class="arithmatex"><span class="MathJax_Preview">y: \sum_{k=1}^{m} \rho^{2}\left(y_{k}, x_{i}\right)=1</span><script type="math/tex">y: \sum_{k=1}^{m} \rho^{2}\left(y_{k}, x_{i}\right)=1</script></span><ul>
<li>m个主成分与第i个变量 <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> 的因子负荷量, 也即所有的主成分可以解释 <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> 的全部方差</li>
</ul>
</li>
</ul>
<p>How many principle components to choose?</p>
<ul>
<li>variance contribution of <span class="arithmatex"><span class="MathJax_Preview">y_{k}: \eta_{k}=\frac{\lambda_{k}}{\sum_{i=1}^{m} \lambda_{i}}</span><script type="math/tex">y_{k}: \eta_{k}=\frac{\lambda_{k}}{\sum_{i=1}^{m} \lambda_{i}}</script></span></li>
<li>cumulative variance contribution of <span class="arithmatex"><span class="MathJax_Preview">y_{1}, y_{2}, \ldots, y_{k}: \frac{\sum_{i=1}^{k} \lambda_{i}}{\sum_{i=1}^{m} \lambda_{i}}</span><script type="math/tex">y_{1}, y_{2}, \ldots, y_{k}: \frac{\sum_{i=1}^{k} \lambda_{i}}{\sum_{i=1}^{m} \lambda_{i}}</script></span><ul>
<li>通常取k使得累计<strong>方差贡献率</strong>达到某个阈值，比如70%～80%以上。累计方差贡献率反映了主成分保留信息的比例。</li>
<li>usually choose <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> such that <span class="arithmatex"><span class="MathJax_Preview">\frac{\sum_{i=1}^{k} \lambda_{i}}{\sum_{i=1}^{m} \lambda_{i}}&gt;70 \%</span><script type="math/tex">\frac{\sum_{i=1}^{k} \lambda_{i}}{\sum_{i=1}^{m} \lambda_{i}}>70 \%</script></span> or <span class="arithmatex"><span class="MathJax_Preview">80 \%</span><script type="math/tex">80 \%</script></span></li>
<li>cumulative variance contribution reflects the contribution of <span class="arithmatex"><span class="MathJax_Preview">y_{1}, \ldots, y_{k}</span><script type="math/tex">y_{1}, \ldots, y_{k}</script></span> to <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> but not specifically to <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span></li>
</ul>
</li>
<li>contribution to <span class="arithmatex"><span class="MathJax_Preview">x_{i}: \nu_{i}=\rho^{2}\left(x_{i},\left(y_{1}, \ldots, y_{k}\right)\right)=\sum_{j=1}^{k} \frac{\lambda_{j} \alpha_{i j}^{2}}{\sigma_{i i}}</span><script type="math/tex">x_{i}: \nu_{i}=\rho^{2}\left(x_{i},\left(y_{1}, \ldots, y_{k}\right)\right)=\sum_{j=1}^{k} \frac{\lambda_{j} \alpha_{i j}^{2}}{\sigma_{i i}}</script></span></li>
</ul>
<p>Note: <strong>normalize variables before PCA</strong> 在进行PCA之前需要先进行归一化</p>
<ul>
<li>在实际问题中，不同变量可能有不同的量纲，直接求主成分有时会产生不合理的结果。为了消除这个影响，需要对各个随机变量实施规范化，使其均值为0，方差为1.<ul>
<li>显然，<strong>规范化随机变量的协方差矩阵就是相关矩阵R</strong>。</li>
<li>对照总体主成分的性质, 可以得到规范化随机变量的总体主成分分析的类似性质</li>
<li>注意到此时的 <span class="arithmatex"><span class="MathJax_Preview">\sigma_{ii}=1</span><script type="math/tex">\sigma_{ii}=1</script></span></li>
</ul>
</li>
</ul>
<h4 id="pca">PCA 例子<a class="headerlink" href="#pca" title="Permanent link">&para;</a></h4>
<p><img alt="" src="../media/ASL-note1/2021-12-04-11-18-33.png" /></p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-11-18-47.png" /></p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-11-19-08.png" /></p>
<ul>
<li>对于 correlation matrix 进行特征分解</li>
<li>根据因子负荷量选取合适的 k, 对应的特征向量就是 PC 的权重</li>
<li>然后还可以计算对于x的各分量的 Factor loading</li>
</ul>
<h2 id="linear-methods-for-classification">Linear Methods for Classification<a class="headerlink" href="#linear-methods-for-classification" title="Permanent link">&para;</a></h2>
<h3 id="linear-regression-of-an-indicator-matrix">Linear Regression of an Indicator Matrix<a class="headerlink" href="#linear-regression-of-an-indicator-matrix" title="Permanent link">&para;</a></h3>
<p>就是用线性回归来解决分类问题. 通过对于 0-1 指标进行回归, 得到各个类别下的激活值, 然后取其中最大的(或者归一化为分布).</p>
<p>Training data <span class="arithmatex"><span class="MathJax_Preview">\left\{\left(x_{i}, g_{i}\right)\right\}_{i=1}^{n}</span><script type="math/tex">\left\{\left(x_{i}, g_{i}\right)\right\}_{i=1}^{n}</script></span>, where each <span class="arithmatex"><span class="MathJax_Preview">x_{i} \in \mathbb{R}^{p}</span><script type="math/tex">x_{i} \in \mathbb{R}^{p}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">g_{i} \in\{1, \ldots, K\}</span><script type="math/tex">g_{i} \in\{1, \ldots, K\}</script></span>
For each <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> construct a linear discriminant <span class="arithmatex"><span class="MathJax_Preview">\delta_{k}(x)</span><script type="math/tex">\delta_{k}(x)</script></span> via:</p>
<ol>
<li>For <span class="arithmatex"><span class="MathJax_Preview">i=1, \ldots, n</span><script type="math/tex">i=1, \ldots, n</script></span>, set</li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
y_{i k}= \begin{cases}0, &amp; \text { if } g_{i} \neq k \\ 1, &amp; \text { if } g_{i}=k\end{cases}
</div>
<script type="math/tex; mode=display">
y_{i k}= \begin{cases}0, & \text { if } g_{i} \neq k \\ 1, & \text { if } g_{i}=k\end{cases}
</script>
</div>
<ol start="2">
<li>Compute <span class="arithmatex"><span class="MathJax_Preview">\left(\hat{\beta}_{0 k}, \hat{\beta}_{k}\right)=\operatorname{argmin}_{\beta_{0 k}, \beta_{k}} \sum_{i=1}^{n}\left(y_{i k}-\beta_{0 k}-\beta_{k}^{\top} x_{i}\right)^{2}</span><script type="math/tex">\left(\hat{\beta}_{0 k}, \hat{\beta}_{k}\right)=\operatorname{argmin}_{\beta_{0 k}, \beta_{k}} \sum_{i=1}^{n}\left(y_{i k}-\beta_{0 k}-\beta_{k}^{\top} x_{i}\right)^{2}</script></span></li>
<li>Define <span class="arithmatex"><span class="MathJax_Preview">\delta_{k}(x)=\hat{\beta}_{0 k}+\hat{\beta}_{k}^{\top} x</span><script type="math/tex">\delta_{k}(x)=\hat{\beta}_{0 k}+\hat{\beta}_{k}^{\top} x</script></span>
Classify a new point <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> with
<span class="arithmatex"><span class="MathJax_Preview">G(x)=\operatorname{argmax}_{k} \delta_{k}(x)</span><script type="math/tex">G(x)=\operatorname{argmax}_{k} \delta_{k}(x)</script></span></li>
</ol>
<p>反思: 这里是用来估计条件概率 <span class="arithmatex"><span class="MathJax_Preview">E(Y_k∣ X = x) = Pr(G = k ∣ X = x)</span><script type="math/tex">E(Y_k∣ X = x) = Pr(G = k ∣ X = x)</script></span></p>
<ul>
<li>这里的近似好吗?</li>
<li>比如线性回归的输出范围不在 [0, 1] 中</li>
</ul>
<p>下面给了一个反例, 注意到由于线性回归得到的结果不能很好得近似后验分布, 会屏蔽掉一些类别.</p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-12-51-17.png" /></p>
<p>下面的图是各个线性回归的输出值(坐标进行了变换), 可以看到第二没有显著的概率. 而右图加入了二次项, 效果得到了提升. 然而这只是类别数 3 的情况, 对于 k分类而言会出现交叉项指数增长的情况.</p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-12-51-43.png" /></p>
<h3 id="linear-discriminant-analysis-lda">Linear Discriminant Analysis (LDA)<a class="headerlink" href="#linear-discriminant-analysis-lda" title="Permanent link">&para;</a></h3>
<p>Model each <span class="arithmatex"><span class="MathJax_Preview">f_{k}(x)</span><script type="math/tex">f_{k}(x)</script></span> as a multivariate Gaussian</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f_{k}(x)=\frac{1}{(2 \pi)^{p / 2} \sqrt{\left|\Sigma_{k}\right|}} \exp \left\{\frac{1}{2}\left(x-\mu_{k}\right)^{\top} \Sigma_{k}^{-1}\left(x-\mu_{k}\right)\right\}
</div>
<script type="math/tex; mode=display">
f_{k}(x)=\frac{1}{(2 \pi)^{p / 2} \sqrt{\left|\Sigma_{k}\right|}} \exp \left\{\frac{1}{2}\left(x-\mu_{k}\right)^{\top} \Sigma_{k}^{-1}\left(x-\mu_{k}\right)\right\}
</script>
</div>
<p>Linear Discriminant Analysis (LDA) arises in the special case when</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\Sigma_{k}=\Sigma \text { for all } k
</div>
<script type="math/tex; mode=display">
\Sigma_{k}=\Sigma \text { for all } k
</script>
</div>
<p>考虑两个类别 k和l</p>
<p><span class="arithmatex"><span class="MathJax_Preview">\log \frac{\operatorname{Pr}(G=k \mid X=x)}{\operatorname{Pr}(G=l \mid X=x)}=\log \frac{f_{k}(x)}{f_{l}(x)}+\log \frac{\pi_{k}}{\pi_{l}}=</span><script type="math/tex">\log \frac{\operatorname{Pr}(G=k \mid X=x)}{\operatorname{Pr}(G=l \mid X=x)}=\log \frac{f_{k}(x)}{f_{l}(x)}+\log \frac{\pi_{k}}{\pi_{l}}=</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\log \frac{\pi_{k}}{\pi_{l}}-\frac{1}{2}\left(\mu_{k}+\mu_{l}\right)^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{l}\right)+x^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{l}\right)</span><script type="math/tex">\log \frac{\pi_{k}}{\pi_{l}}-\frac{1}{2}\left(\mu_{k}+\mu_{l}\right)^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{l}\right)+x^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{l}\right)</script></span>
The equal covariance matrices allow <span class="arithmatex"><span class="MathJax_Preview">x^{\top} \Sigma_{k}^{-1} x</span><script type="math/tex">x^{\top} \Sigma_{k}^{-1} x</script></span> and <span class="arithmatex"><span class="MathJax_Preview">x^{\top} \Sigma_{l}^{-1} x</span><script type="math/tex">x^{\top} \Sigma_{l}^{-1} x</script></span> cancel out</p>
<p>上式定义了两个类别的分类边界. 实际上, 对于一个类别 k, 可以算出一个「分数」, 分类器的输出就是分数最高的那一个.</p>
<p>From the log odds, we see that the linear discriminant functions</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\delta_{k}(x)=x^{\top} \Sigma^{-1} \mu_{k}-\frac{1}{2} \mu_{k}^{\top} \Sigma^{-1} \mu_{k}+\log \pi_{k}
</div>
<script type="math/tex; mode=display">
\delta_{k}(x)=x^{\top} \Sigma^{-1} \mu_{k}-\frac{1}{2} \mu_{k}^{\top} \Sigma^{-1} \mu_{k}+\log \pi_{k}
</script>
</div>
<p>are an equivalent description of the decision rule with</p>
<div class="arithmatex">
<div class="MathJax_Preview">
G(x)=\operatorname{argmax}_{k} \delta_{k}(x)
</div>
<script type="math/tex; mode=display">
G(x)=\operatorname{argmax}_{k} \delta_{k}(x)
</script>
</div>
<h3 id="lda-practicalities">LDA Practicalities<a class="headerlink" href="#lda-practicalities" title="Permanent link">&para;</a></h3>
<p>In practice we do not know the parameters of the Gaussian distributions, and will need to estimate them using our training data</p>
<p>实际上, 我们需要通过数据来估计上面的样本均值和协方差矩阵. (注意这里是「多分类」的样本方差, 所以分母为 <span class="arithmatex"><span class="MathJax_Preview">n-K</span><script type="math/tex">n-K</script></span>?)</p>
<p>Let <span class="arithmatex"><span class="MathJax_Preview">n_{k}</span><script type="math/tex">n_{k}</script></span> be the number of class <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> observations then</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
\hat{\pi}_{k} &amp;=\frac{n_{k}}{n} \\
\hat{\mu}_{k} &amp;=\sum_{g_{i}=k} \frac{x_{i}}{n_{k}} \\
\hat{\Sigma} &amp;=\sum_{k=1}^{K} \sum_{g_{i}=k} \frac{\left(x_{i}-\hat{\mu}_{k}\right)\left(x_{i}-\hat{\mu}_{k}\right)^{\top}}{n-K}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\hat{\pi}_{k} &=\frac{n_{k}}{n} \\
\hat{\mu}_{k} &=\sum_{g_{i}=k} \frac{x_{i}}{n_{k}} \\
\hat{\Sigma} &=\sum_{k=1}^{K} \sum_{g_{i}=k} \frac{\left(x_{i}-\hat{\mu}_{k}\right)\left(x_{i}-\hat{\mu}_{k}\right)^{\top}}{n-K}
\end{aligned}
</script>
</div>
<p>Note: for <span class="arithmatex"><span class="MathJax_Preview">k=2</span><script type="math/tex">k=2</script></span> there is a simple correspondence between LDA and linear regression of indicator matrix, which is not true for <span class="arithmatex"><span class="MathJax_Preview">k&gt;2</span><script type="math/tex">k>2</script></span>.</p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-13-19-42.png" /></p>
<h4 id="when-sigma_ksigma_k-not-equal-qda-quadratic-discriminant-analysis">When <span class="arithmatex"><span class="MathJax_Preview">\Sigma_{k}</span><script type="math/tex">\Sigma_{k}</script></span> Not Equal: QDA (Quadratic Discriminant Analysis)<a class="headerlink" href="#when-sigma_ksigma_k-not-equal-qda-quadratic-discriminant-analysis" title="Permanent link">&para;</a></h4>
<p>LDA 是假设了各类别有相同的方差</p>
<p>If the <span class="arithmatex"><span class="MathJax_Preview">\Sigma_{k}</span><script type="math/tex">\Sigma_{k}</script></span> are not assumed to be equal, then the quadratic terms remain and we get quandratic discriminant functions <span class="arithmatex"><span class="MathJax_Preview">Q D A</span><script type="math/tex">Q D A</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\delta_{k}(x)=-0.5 \log \left|\Sigma_{k}\right|-0.5\left(x-\mu_{k}\right)^{\top} \Sigma_{k}^{-1}\left(x-\mu_{k}\right)+\log \pi_{k}
</div>
<script type="math/tex; mode=display">
\delta_{k}(x)=-0.5 \log \left|\Sigma_{k}\right|-0.5\left(x-\mu_{k}\right)^{\top} \Sigma_{k}^{-1}\left(x-\mu_{k}\right)+\log \pi_{k}
</script>
</div>
<p>In this case, the decision boundary between classes are described by a <strong>quadratic equation</strong> <span class="arithmatex"><span class="MathJax_Preview">\left\{x: \delta_{k}(x)=\delta_{l}(x)\right\}</span><script type="math/tex">\left\{x: \delta_{k}(x)=\delta_{l}(x)\right\}</script></span></p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-13-26-31.png" /></p>
<h4 id="rda-regularized-discriminant-analysis">RDA: Regularized Discriminant Analysis<a class="headerlink" href="#rda-regularized-discriminant-analysis" title="Permanent link">&para;</a></h4>
<p>平衡LDA和QDA</p>
<p><span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}_{k}(\alpha)=\alpha \hat{\Sigma}_{k}+(1-\alpha) \hat{\Sigma}, \alpha \in[0,1]</span><script type="math/tex">\hat{\Sigma}_{k}(\alpha)=\alpha \hat{\Sigma}_{k}+(1-\alpha) \hat{\Sigma}, \alpha \in[0,1]</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}_{k}(\alpha)</span><script type="math/tex">\hat{\Sigma}_{k}(\alpha)</script></span> Regularized covariance matrices</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}</span><script type="math/tex">\hat{\Sigma}</script></span>: pooled covariance matrix</li>
</ul>
<p>Can also replace <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}</span><script type="math/tex">\hat{\Sigma}</script></span> by <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}(\gamma)=\gamma \hat{\Sigma}+(1-\gamma) \hat{\sigma}^{2} I, \gamma \in[0,1]</span><script type="math/tex">\hat{\Sigma}(\gamma)=\gamma \hat{\Sigma}+(1-\gamma) \hat{\sigma}^{2} I, \gamma \in[0,1]</script></span></p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-13-33-12.png" /></p>
<h4 id="computations-for-lda-qda">Computations for LDA &amp; QDA<a class="headerlink" href="#computations-for-lda-qda" title="Permanent link">&para;</a></h4>
<p>Diagonalizing <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}</span><script type="math/tex">\hat{\Sigma}</script></span> or <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}_{k}</span><script type="math/tex">\hat{\Sigma}_{k}</script></span> by eigen decomposition <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}_{k}=U_{k} D_{k} U_{k}^{\top}</span><script type="math/tex">\hat{\Sigma}_{k}=U_{k} D_{k} U_{k}^{\top}</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">U_{k}</span><script type="math/tex">U_{k}</script></span> is <span class="arithmatex"><span class="MathJax_Preview">p \times p</span><script type="math/tex">p \times p</script></span> orthornormal, and <span class="arithmatex"><span class="MathJax_Preview">D_{k}</span><script type="math/tex">D_{k}</script></span> is a diagonal matrix of positive eigenvalues <span class="arithmatex"><span class="MathJax_Preview">d_{k l}</span><script type="math/tex">d_{k l}</script></span>. The ingredients for <span class="arithmatex"><span class="MathJax_Preview">\delta_{k}(x)</span><script type="math/tex">\delta_{k}(x)</script></span> are</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;(x-\hat{\mu}_{k})^{\top} \hat{\Sigma}_{k}^{-1}\left(x-\hat{\mu}_{k}\right)=\left[U_{k}^{\top}\left(x-\hat{\mu}_{k}\right)\right]^{\top} D_{k}^{-1}\left[U_{k}^{\top}\left(x-\hat{\mu}_{k}\right)\right] \\
&amp;\log \left|\hat{\Sigma}_{k}\right|=\sum_{l} \log d_{k l}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&(x-\hat{\mu}_{k})^{\top} \hat{\Sigma}_{k}^{-1}\left(x-\hat{\mu}_{k}\right)=\left[U_{k}^{\top}\left(x-\hat{\mu}_{k}\right)\right]^{\top} D_{k}^{-1}\left[U_{k}^{\top}\left(x-\hat{\mu}_{k}\right)\right] \\
&\log \left|\hat{\Sigma}_{k}\right|=\sum_{l} \log d_{k l}
\end{aligned}
</script>
</div>
<p>Therefore, LDA can be implemented by:</p>
<ul>
<li><strong>Sphere</strong> the data with respect to the common covariance estimate <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}: X^{\star} \leftarrow D^{-\frac{1}{2}} U^{\top} X</span><script type="math/tex">\hat{\Sigma}: X^{\star} \leftarrow D^{-\frac{1}{2}} U^{\top} X</script></span>, where <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}=U D U^{\top}</span><script type="math/tex">\hat{\Sigma}=U D U^{\top}</script></span>. The common covariance estimate of <span class="arithmatex"><span class="MathJax_Preview">X^{\star}</span><script type="math/tex">X^{\star}</script></span> will now be the identity 球面化? 从而使得变换后的 <span class="arithmatex"><span class="MathJax_Preview">X^*</span><script type="math/tex">X^*</script></span> 协方差为单位阵</li>
<li>Classify to the closet class <strong>centroid</strong> in the transformed space, modulo the effect of the class prior probabilities <span class="arithmatex"><span class="MathJax_Preview">\pi_{k}</span><script type="math/tex">\pi_{k}</script></span></li>
</ul>
<h4 id="reduced-rank-lda">Reduced Rank LDA<a class="headerlink" href="#reduced-rank-lda" title="Permanent link">&para;</a></h4>
<p>对于在p维空间的数据, K个类别的中心至多张成 K-1 维子空间; 而我们在进行判定的时候, 其实看的是到这些类别中心的距离, 因此其实可以 <strong>降维</strong> 到这个子空间上.</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathrm{K}</span><script type="math/tex">\mathrm{K}</script></span> <strong>centroids</strong> in <span class="arithmatex"><span class="MathJax_Preview">\mathrm{p}</span><script type="math/tex">\mathrm{p}</script></span>-dimensional input space lie in an affine subspace of dimension <span class="arithmatex"><span class="MathJax_Preview">\leqslant K-1</span><script type="math/tex">\leqslant K-1</script></span></li>
<li>if <span class="arithmatex"><span class="MathJax_Preview">p \gg K</span><script type="math/tex">p \gg K</script></span>, there is a big drop in dimension</li>
<li>To locate the closest centroid can ignore the directions orthogonal to this subspace.</li>
<li>Therefore, can project <span class="arithmatex"><span class="MathJax_Preview">X^{*}</span><script type="math/tex">X^{*}</script></span> onto this centroid-spanning subspace <span class="arithmatex"><span class="MathJax_Preview">H_{K-1}</span><script type="math/tex">H_{K-1}</script></span> and make comparisons there</li>
<li>LDA thus performs dimensionality reduction and one need only consider the data in a subspace of dimension at most K-1.</li>
</ul>
<p>进一步, What About a Subspace of Dim <span class="arithmatex"><span class="MathJax_Preview">L&lt;K-1</span><script type="math/tex">L<K-1</script></span></p>
<p>Which subspace of <span class="arithmatex"><span class="MathJax_Preview">\operatorname{dim} L&lt;K-1</span><script type="math/tex">\operatorname{dim} L<K-1</script></span> should we project onto for optimal LDA?</p>
<p>Finding the sequences of optimal subspaces for LDA involves:</p>
<ul>
<li>Compute the <span class="arithmatex"><span class="MathJax_Preview">K \times p</span><script type="math/tex">K \times p</script></span> matrix of class centroids <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> and the common covariance matrix <span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> (for <strong>within-class covariance</strong>)</li>
<li>Compute <span class="arithmatex"><span class="MathJax_Preview">M^{*}=M W^{-\frac{1}{2}}</span><script type="math/tex">M^{*}=M W^{-\frac{1}{2}}</script></span></li>
<li>Compute <span class="arithmatex"><span class="MathJax_Preview">B^{\star}</span><script type="math/tex">B^{\star}</script></span>, the covariance matrix of <span class="arithmatex"><span class="MathJax_Preview">M^{\star}</span><script type="math/tex">M^{\star}</script></span> (B for <strong>between-class covariance</strong>), and its eigen-decomposition <span class="arithmatex"><span class="MathJax_Preview">B^{\star}=V^{\star} D_{B} V^{* \top}</span><script type="math/tex">B^{\star}=V^{\star} D_{B} V^{* \top}</script></span></li>
<li>The columns <span class="arithmatex"><span class="MathJax_Preview">v_{1}^{\star}</span><script type="math/tex">v_{1}^{\star}</script></span> of <span class="arithmatex"><span class="MathJax_Preview">V^{*}</span><script type="math/tex">V^{*}</script></span> in sequence from first to last define the coordinates of the optimal subspaces.</li>
<li><span class="arithmatex"><span class="MathJax_Preview">D_{\text {th }}</span><script type="math/tex">D_{\text {th }}</script></span> discriminant variable <span class="arithmatex"><span class="MathJax_Preview">Z_{l}=v_{l}^{\top} X, v_{l}=W^{-\frac{1}{2}} v_{l}^{\star}</span><script type="math/tex">Z_{l}=v_{l}^{\top} X, v_{l}=W^{-\frac{1}{2}} v_{l}^{\star}</script></span></li>
</ul>
<p>(看上去好复杂, 「主要思想是先求 <span class="arithmatex"><span class="MathJax_Preview">W^{-\frac{1}{2}}BW^{-\frac{1}{2}}</span><script type="math/tex">W^{-\frac{1}{2}}BW^{-\frac{1}{2}}</script></span> 的特征向量 <span class="arithmatex"><span class="MathJax_Preview">v_l^*</span><script type="math/tex">v_l^*</script></span>, 则 <span class="arithmatex"><span class="MathJax_Preview">W^{-1}B</span><script type="math/tex">W^{-1}B</script></span> 的特征向量为 <span class="arithmatex"><span class="MathJax_Preview">W^{-\frac{1}{2}}v_l^*</span><script type="math/tex">W^{-\frac{1}{2}}v_l^*</script></span>」, 结合 Ex4.1 的证明过程 <a href="https://github.com/szcf-weiya/ESL-CN/issues/142">https://github.com/szcf-weiya/ESL-CN/issues/142</a>)</p>
<h4 id="fisher-criterion">Fisher Criterion 另一个角度<a class="headerlink" href="#fisher-criterion" title="Permanent link">&para;</a></h4>
<p>Fisher 的准则: 寻找线性组合 <span class="arithmatex"><span class="MathJax_Preview">Z = a^T X</span><script type="math/tex">Z = a^T X</script></span> 使得组间方差相对于组内方差最大化．</p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-14-14-19.png" /></p>
<ul>
<li>W: pooled covariance about the means</li>
<li>B: covariance of the class means</li>
<li>Then for the projected data <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> 组内和组间方差<ul>
<li>The between-class variance of <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> is <span class="arithmatex"><span class="MathJax_Preview">a^{\top} B a</span><script type="math/tex">a^{\top} B a</script></span></li>
<li>The within-class variance of <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> is <span class="arithmatex"><span class="MathJax_Preview">a^{\top} W a</span><script type="math/tex">a^{\top} W a</script></span></li>
</ul>
</li>
</ul>
<p>补充: 下面是 <strong>组内和组间方差</strong> 的定义. B, W, T 的自由度分别为 K-1, N-K, N-1. 易证 <span class="arithmatex"><span class="MathJax_Preview">T=B+W</span><script type="math/tex">T=B+W</script></span></p>
<p><span class="arithmatex"><span class="MathJax_Preview">\begin{aligned} \mathbf{B} &amp;=\sum_{i=1}^{K} n_{i}\left(\bar{x}_{i}-\bar{x}\right)\left(\bar{x}_{i}-\bar{x}\right)^{\prime} \\ \mathbf{W} &amp;=\sum_{i=1}^{K} \sum_{j=1}^{n_{i}}\left(x_{i j}-\bar{x}_{i}\right)\left(x_{i j}-\bar{x}_{i}\right)^{\prime} \\ \mathbf{T} &amp;=\sum_{i=1}^{K} \sum_{j=1}^{n_{i}}\left(x_{i j}-\bar{x}\right)\left(x_{i j}-\bar{x}\right)^{\prime} \end{aligned}</span><script type="math/tex">\begin{aligned} \mathbf{B} &=\sum_{i=1}^{K} n_{i}\left(\bar{x}_{i}-\bar{x}\right)\left(\bar{x}_{i}-\bar{x}\right)^{\prime} \\ \mathbf{W} &=\sum_{i=1}^{K} \sum_{j=1}^{n_{i}}\left(x_{i j}-\bar{x}_{i}\right)\left(x_{i j}-\bar{x}_{i}\right)^{\prime} \\ \mathbf{T} &=\sum_{i=1}^{K} \sum_{j=1}^{n_{i}}\left(x_{i j}-\bar{x}\right)\left(x_{i j}-\bar{x}\right)^{\prime} \end{aligned}</script></span></p>
<p><strong>Fisher&rsquo;s criterion</strong>:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\max _{a} \frac{a^{\top} B a}{a^{\top} W a}
</div>
<script type="math/tex; mode=display">
\max _{a} \frac{a^{\top} B a}{a^{\top} W a}
</script>
</div>
<p>or equivalently</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\max _{a} a^{\top} B a</span><script type="math/tex">\max _{a} a^{\top} B a</script></span> subject to <span class="arithmatex"><span class="MathJax_Preview">a^{\top} W a=1</span><script type="math/tex">a^{\top} W a=1</script></span></li>
</ul>
<p><span class="arithmatex"><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> is given by the largest eigenvalue of <span class="arithmatex"><span class="MathJax_Preview">W^{-1} B</span><script type="math/tex">W^{-1} B</script></span>, which is identical to <span class="arithmatex"><span class="MathJax_Preview">v=W^{-1 / 2} v_{1}^{*}</span><script type="math/tex">v=W^{-1 / 2} v_{1}^{*}</script></span> defined before. (参考上面的 Ex4.1)</p>
<h4 id="lda">LDA总结<a class="headerlink" href="#lda" title="Permanent link">&para;</a></h4>
<p>To summarize the developments so far:</p>
<ul>
<li>Gaussian classification with common covariances leads to linear decision boundaries. Classification can be achieved by sphering the data with respect to <span class="arithmatex"><span class="MathJax_Preview">\mathbf{W}</span><script type="math/tex">\mathbf{W}</script></span>, and classifying to the <strong>closest centroid</strong> (modulo <span class="arithmatex"><span class="MathJax_Preview">\left.\log \pi_{k}\right)</span><script type="math/tex">\left.\log \pi_{k}\right)</script></span> in the sphered space.</li>
<li>Since only the relative distances to the centroids count, one can confine the data to <strong>the subspace spanned by the centroids</strong> in the sphered space.</li>
<li>This subspace can be further <strong>decomposed into successively optimal subspaces</strong> in term of centroid separation. This decomposition is identical to the decomposition due to <strong>Fisher</strong>. 子空间可以进一步分解为关于形心分离的最优子空间．这个分解与 Fisher 的 分解相同．</li>
</ul>
<p><img alt="" src="../media/ASL-note1/2021-12-04-14-25-45.png" /></p>
<h3 id="logistic-regression">Logistic Regression<a class="headerlink" href="#logistic-regression" title="Permanent link">&para;</a></h3>
<p>参见 <a href="https://jozeelin.github.io/2019/06/14/%E9%80%BB%E8%BE%91%E6%96%AF%E8%92%82%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B%E4%B8%8E%E6%9C%80%E5%A4%A7%E7%86%B5%E6%A8%A1%E5%9E%8B/">逻辑斯蒂回归模型与最大熵模型</a></p>
<p>回顾经典的二分类逻辑回归. 其 logit/对数几率为</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\log \frac{P(Y=1|x)}{1-P(Y=1|x)} = w'x
</div>
<script type="math/tex; mode=display">
\log \frac{P(Y=1|x)}{1-P(Y=1|x)} = w'x
</script>
</div>
<p>也即在逻辑斯蒂回归模型中，输出Y=1的对数几率是输入x的线性函数。</p>
<p>换个角度来看，logit函数的值域是实数域，通过逻辑斯蒂回归模型可以把logit函数的实数域映射为[0,1]，相当于概率。</p>
<p>多分类: Directly model the posterior probabilities of the <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> classes</p>
<p><span class="arithmatex"><span class="MathJax_Preview">\log \frac{\operatorname{Pr}(G=1 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{10}+\beta_{1}^{\top} x</span><script type="math/tex">\log \frac{\operatorname{Pr}(G=1 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{10}+\beta_{1}^{\top} x</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\log \frac{\operatorname{Pr}(G=2 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{20}+\beta_{2}^{\top} x</span><script type="math/tex">\log \frac{\operatorname{Pr}(G=2 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{20}+\beta_{2}^{\top} x</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\log \frac{\operatorname{Pr}(G=K-1 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{(K-1) 0}+\beta_{K-1}^{\top} x</span><script type="math/tex">\log \frac{\operatorname{Pr}(G=K-1 \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{(K-1) 0}+\beta_{K-1}^{\top} x</script></span></p>
<p>Class <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> as reference level, all probabilities sum to 1.
Each probability lie in <span class="arithmatex"><span class="MathJax_Preview">[0,1]</span><script type="math/tex">[0,1]</script></span></p>
<p>分别用第K个类别作为基准, 计算其他类别相较于它的 log-odd, 然后进行归一化</p>
<p>For <span class="arithmatex"><span class="MathJax_Preview">k=1, \ldots, K-1</span><script type="math/tex">k=1, \ldots, K-1</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(G=k \mid X=x)=\frac{\exp \left(\beta_{k 0}+\beta_{k}^{\top} x\right)}{1+\sum_{l=1}^{K-1} \exp \left(\beta_{10}+\beta_{1}^{\top} x\right)}</span><script type="math/tex">\operatorname{Pr}(G=k \mid X=x)=\frac{\exp \left(\beta_{k 0}+\beta_{k}^{\top} x\right)}{1+\sum_{l=1}^{K-1} \exp \left(\beta_{10}+\beta_{1}^{\top} x\right)}</script></span>
For <span class="arithmatex"><span class="MathJax_Preview">k=K-1</span><script type="math/tex">k=K-1</script></span>
<span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(G=k \mid X=x)=\frac{1}{1+\sum_{l=1}^{K-1} \exp \left(\beta_{10}+\beta_{1}^{\top} x\right)}</span><script type="math/tex">\operatorname{Pr}(G=k \mid X=x)=\frac{1}{1+\sum_{l=1}^{K-1} \exp \left(\beta_{10}+\beta_{1}^{\top} x\right)}</script></span></p>
<p>可知分类边界是线性的. Linear decision boundary between classes</p>
<div class="arithmatex">
<div class="MathJax_Preview">
x: \operatorname{Pr}(G=k \mid X=x)=\operatorname{Pr}(G=1 \mid X=x)
</div>
<script type="math/tex; mode=display">
x: \operatorname{Pr}(G=k \mid X=x)=\operatorname{Pr}(G=1 \mid X=x)
</script>
</div>
<p>which is the same as</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\left\{x:\left(\beta_{k 0}-\beta_{10}\right)+\left(\beta_{k}-\beta_{l}\right)^{\top} x=0\right\}
</div>
<script type="math/tex; mode=display">
\left\{x:\left(\beta_{k 0}-\beta_{10}\right)+\left(\beta_{k}-\beta_{l}\right)^{\top} x=0\right\}
</script>
</div>
<p>for <span class="arithmatex"><span class="MathJax_Preview">k=1, \ldots, K-1</span><script type="math/tex">k=1, \ldots, K-1</script></span> and <span class="arithmatex"><span class="MathJax_Preview">1=1, \ldots, K-1</span><script type="math/tex">1=1, \ldots, K-1</script></span></p>
<h4 id="logistic-regression-model-estimation">Logistic Regression Model Estimation<a class="headerlink" href="#logistic-regression-model-estimation" title="Permanent link">&para;</a></h4>
<p>参数估计, 看二分类, 对数似然</p>
<p><strong>Two-class case</strong>, log likelihood can be written as</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">l(\beta)=\sum_{i=1}^{N}\left\{y_{i} \log p\left(x_{i} ; \beta\right)+\left(1-y_{i}\right) \log \left(1-p\left(x_{i} ; \beta\right)\right)\right\}</span><script type="math/tex">l(\beta)=\sum_{i=1}^{N}\left\{y_{i} \log p\left(x_{i} ; \beta\right)+\left(1-y_{i}\right) \log \left(1-p\left(x_{i} ; \beta\right)\right)\right\}</script></span></li>
<li>logit <span class="arithmatex"><span class="MathJax_Preview">p\left(x_{i} ; \beta\right)=\log \frac{p\left(x_{i} ; \beta\right)}{1-p\left(x_{i} ; \beta\right)}=\beta^{\top} x_{i}, \beta=\left(\beta_{0}, \beta_{1}\right)</span><script type="math/tex">p\left(x_{i} ; \beta\right)=\log \frac{p\left(x_{i} ; \beta\right)}{1-p\left(x_{i} ; \beta\right)}=\beta^{\top} x_{i}, \beta=\left(\beta_{0}, \beta_{1}\right)</script></span></li>
</ul>
<p>To maximize the log likelihood, obtain the first and second derivative</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\frac{\partial(\beta)}{\partial \beta}=\sum_{i=1}^{N} x_{i}\left(y_{i}- p\left(x_{i} ; \beta\right)\right)</span><script type="math/tex">\frac{\partial(\beta)}{\partial \beta}=\sum_{i=1}^{N} x_{i}\left(y_{i}- p\left(x_{i} ; \beta\right)\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\frac{\partial^{2}(\beta)}{\partial \beta \partial \beta^{\top}}=-\sum_{i=1}^{N} x_{i} x_{i}^{\top} p\left(x_{i} ; \beta\right)\left(1-p\left(x_{i} ; \beta\right)\right)</span><script type="math/tex">\frac{\partial^{2}(\beta)}{\partial \beta \partial \beta^{\top}}=-\sum_{i=1}^{N} x_{i} x_{i}^{\top} p\left(x_{i} ; \beta\right)\left(1-p\left(x_{i} ; \beta\right)\right)</script></span></li>
</ul>
<p>Iteratively update the initial value by <strong>Newton-Raphson algorithm</strong></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\beta^{\text {new }}=\beta^{\text {old }}-\left\{\frac{\partial l(\beta)}{\partial \beta \partial \beta^{\top}}\right\}^{-1} \frac{\partial(\beta)}{\partial \beta}</span><script type="math/tex">\beta^{\text {new }}=\beta^{\text {old }}-\left\{\frac{\partial l(\beta)}{\partial \beta \partial \beta^{\top}}\right\}^{-1} \frac{\partial(\beta)}{\partial \beta}</script></span></li>
<li>Derivatives evaluated at <span class="arithmatex"><span class="MathJax_Preview">\beta^{\text {old }}</span><script type="math/tex">\beta^{\text {old }}</script></span></li>
</ul>
<p>In <strong>matrix notation</strong>, let <span class="arithmatex"><span class="MathJax_Preview">W</span><script type="math/tex">W</script></span> be <span class="arithmatex"><span class="MathJax_Preview">N \times N</span><script type="math/tex">N \times N</script></span> weight matrix with <span class="arithmatex"><span class="MathJax_Preview">i_{t h}</span><script type="math/tex">i_{t h}</script></span> diagonal element <span class="arithmatex"><span class="MathJax_Preview">p\left(x_{i} ; \beta^{\text {old }}\right)\left(1-p\left(x_{i} ; \beta^{\text {old }}\right)\right)</span><script type="math/tex">p\left(x_{i} ; \beta^{\text {old }}\right)\left(1-p\left(x_{i} ; \beta^{\text {old }}\right)\right)</script></span>
Then the first and second derivatives can be express as</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\begin{aligned}
&amp;\frac{\partial l(\beta)}{\partial \beta}=X^{\top}(y-p) \\
&amp;\frac{\partial^{2} l(\beta)}{\partial \beta \partial \beta^{\top}}=-X^{\top} W X
\end{aligned}</span><script type="math/tex">\begin{aligned}
&\frac{\partial l(\beta)}{\partial \beta}=X^{\top}(y-p) \\
&\frac{\partial^{2} l(\beta)}{\partial \beta \partial \beta^{\top}}=-X^{\top} W X
\end{aligned}</script></span></li>
</ul>
<p>Newton-Raphson updates</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\begin{aligned}
&amp;\beta^{\text {new }}=\beta^{\text {old }}+\left(X^{\top} W X\right)^{-1} X^{\top}(y-p)= \\
&amp;\left(x^{\top} W X\right)^{-1} X^{\top} W\left(X \beta^{\text {old }}+W^{-1}(y-p)\right)=\left(X^{\top} W X\right)^{-1} X^{\top} W z
\end{aligned}</span><script type="math/tex">\begin{aligned}
&\beta^{\text {new }}=\beta^{\text {old }}+\left(X^{\top} W X\right)^{-1} X^{\top}(y-p)= \\
&\left(x^{\top} W X\right)^{-1} X^{\top} W\left(X \beta^{\text {old }}+W^{-1}(y-p)\right)=\left(X^{\top} W X\right)^{-1} X^{\top} W z
\end{aligned}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">z=X \beta^{\text {old }}+W^{-1}(y-p)</span><script type="math/tex">z=X \beta^{\text {old }}+W^{-1}(y-p)</script></span>, also called <strong>adjusted response</strong> 调整后的响应变量</li>
<li><strong>Iteratively reweighted least squares</strong> 也被叫做「加权迭代最小二乘」IRLS, 因为每次都是求解下面的问题:</li>
<li>At each iteration <span class="arithmatex"><span class="MathJax_Preview">\beta^{\text {new }} \leftarrow \operatorname{argmin}_{\beta}(z-X \beta)^{\top} W(z-X \beta)</span><script type="math/tex">\beta^{\text {new }} \leftarrow \operatorname{argmin}_{\beta}(z-X \beta)^{\top} W(z-X \beta)</script></span></li>
</ul>
<h4 id="south-african-heart-disease">例子: South African Heart Disease<a class="headerlink" href="#south-african-heart-disease" title="Permanent link">&para;</a></h4>
<p>这里给出了一个缺血性心脏病的一个逻辑回归结果(二分类). 表中的 Z scores 是系数比上标准差, 不显著的话可以从模型中剔除. 这里是 <strong>Wald test</strong>, null假设为 the coeﬃcient in question is zero, while all the others are not. 在 5% 置信度下 Z score greater than approximately 2 是显著的.</p>
<p><img alt="" src="../media/ASL-note1/2021-12-04-15-03-45.png" /></p>
<p>这里发现 Systolic blood pressure (sbp) 不显著, obesity 也不显著甚至是负的 (但做这两个的回归, 结果都是显著为正的)</p>
<p>这是因为变量之间的相关性, 可以进行模型选择或 偏差分析 (analysis of deviance).</p>
<p>参数的<strong>解释</strong>: 例如对 a coeﬃcient of 0.081 (Std. Error = 0.026) for tobacco, 可以解释为 an increase of 1kg in lifetime tobacco usage accounts for an increase in the odds of coronary heart disease of exp(0.081) = 1.084 or 8.4%  结合残差可以得到 an approximate 95% <strong>conﬁdence interval</strong> of exp(0.081 ± 2 × 0.026) = (1.03, 1.14).</p>
<h4 id="l1-regularized-logistic-regression">L1 Regularized Logistic Regression<a class="headerlink" href="#l1-regularized-logistic-regression" title="Permanent link">&para;</a></h4>
<p>Maximize a penalized version of the log likelihood <span class="arithmatex"><span class="MathJax_Preview">\max _{\beta_{0}, \beta}\left\{\sum_{i=1}^{N}\left[y_{i}\left(\beta_{0}+\beta^{\top} x_{i}\right)-\log \left(1+e^{\beta_{0}+\beta^{\top} x_{i}}\right)\right]-\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|\right\}</span><script type="math/tex">\max _{\beta_{0}, \beta}\left\{\sum_{i=1}^{N}\left[y_{i}\left(\beta_{0}+\beta^{\top} x_{i}\right)-\log \left(1+e^{\beta_{0}+\beta^{\top} x_{i}}\right)\right]-\lambda \sum_{j=1}^{p}\left|\beta_{j}\right|\right\}</script></span></p>
<ul>
<li>Standardize predictors first, don&rsquo;t penalize intercept (和lasso一样)</li>
<li>Penalized likelihood surface is concave, solutions can be found by <strong>nonlinear programming methods</strong> 非线性规划方法</li>
<li>Quadratic approximation using Newton-Raphson by repeated application of a weighted lasso algorithm</li>
</ul>
<p><img alt="" src="../media/ASL-note1/2021-12-04-15-23-15.png" /></p>
<h4 id="logistic-regression-or-lda">Logistic Regression or LDA?<a class="headerlink" href="#logistic-regression-or-lda" title="Permanent link">&para;</a></h4>
<p>LDA decision boundary</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;\log \frac{\operatorname{Pr}(G=k \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}= \\
&amp;\log \frac{\pi_{K}}{\pi_{K}}-\frac{1}{2}\left(\mu_{k}+\mu_{K}\right)^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{K}\right)+x^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{K}\right)=\alpha_{k 0}+\alpha_{k}^{\top} x
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&\log \frac{\operatorname{Pr}(G=k \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}= \\
&\log \frac{\pi_{K}}{\pi_{K}}-\frac{1}{2}\left(\mu_{k}+\mu_{K}\right)^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{K}\right)+x^{\top} \Sigma^{-1}\left(\mu_{k}-\mu_{K}\right)=\alpha_{k 0}+\alpha_{k}^{\top} x
\end{aligned}
</script>
</div>
<p>Logistic regression</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\log \frac{\operatorname{Pr}(G=k \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{k 0}+\beta_{k}^{\top} x
</div>
<script type="math/tex; mode=display">
\log \frac{\operatorname{Pr}(G=k \mid X=x)}{\operatorname{Pr}(G=K \mid X=x)}=\beta_{k 0}+\beta_{k}^{\top} x
</script>
</div>
<p>Difference? <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(X, G=k)=\operatorname{Pr}(X) \operatorname{Pr}(G=k \mid X)</span><script type="math/tex">\operatorname{Pr}(X, G=k)=\operatorname{Pr}(X) \operatorname{Pr}(G=k \mid X)</script></span></p>
<ul>
<li>LDA is fit by maximizing the full likelihood <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(X, G=k)=\phi\left(X ; \mu_{k}, \Sigma\right) \pi_{k}</span><script type="math/tex">\operatorname{Pr}(X, G=k)=\phi\left(X ; \mu_{k}, \Sigma\right) \pi_{k}</script></span></li>
<li>Logistic regression leaves the marginal density of <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> as an arbitrary, and is fit by maximizing the conditional likelihood <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(G=k \mid X)</span><script type="math/tex">\operatorname{Pr}(G=k \mid X)</script></span></li>
</ul>
<p>The marginal density in LDA: <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(X)=\sum_{k=1}^{k} \pi_{k} \phi\left(X ; \mu_{k}, \Sigma\right)</span><script type="math/tex">\operatorname{Pr}(X)=\sum_{k=1}^{k} \pi_{k} \phi\left(X ; \mu_{k}, \Sigma\right)</script></span></p>
<p>尽管它们确实有相同的形式，区别在于它们系数估计的方式. <strong>逻辑斯蒂回归模型更加一般，因为它做了更少的假设</strong>.</p>
<p>If <span class="arithmatex"><span class="MathJax_Preview">f_{k}(x) \sim \mathcal{N}</span><script type="math/tex">f_{k}(x) \sim \mathcal{N}</script></span></p>
<ul>
<li>LDA gives more info about the parameters</li>
<li>LDA can estimate parameters more efficiently (lower variance). 通过以来额外的模型假设, 可以得到更多的参数信息, 因此方差更低</li>
</ul>
<p>Bad things about LDA&hellip;</p>
<ul>
<li>Less robust to gross outliers</li>
<li>Observations far from decision boundary influence <span class="arithmatex"><span class="MathJax_Preview">\hat{\Sigma}</span><script type="math/tex">\hat{\Sigma}</script></span></li>
<li>Observations without class labels have information about the parameters</li>
<li><strong>Quanlitative</strong> variables in <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span></li>
</ul>
<h3 id="naive-bayes">Naive Bayes<a class="headerlink" href="#naive-bayes" title="Permanent link">&para;</a></h3>
<p>参见 <a href="https://jozeelin.github.io/2019/06/14/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E6%B3%95/">朴素贝叶斯法</a></p>
<ul>
<li>朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。</li>
<li>朴素贝叶斯法实际上学习到生成数据的机制，所以属于<strong>生成模型</strong>。</li>
<li><strong>条件独立假设</strong>等于是说用于分类的特征在类确定的条件下都是条件独立的。这一假设使得条件概率分布的学习难度降低了，但会牺牲一定的分类准确率。</li>
<li>朴素贝叶斯法分类时，对给定的输入x，通过学习到的模型计算后验概率分布 <span class="arithmatex"><span class="MathJax_Preview">P(Y=c_k|X=x)</span><script type="math/tex">P(Y=c_k|X=x)</script></span>，将后验概率最大的类作为x的类输出。<ul>
<li>为什么将后验概率最大的类作为x的输出？因为<strong>后验概率最大化等效于期望风险最小化</strong>。</li>
</ul>
</li>
</ul>
<h4 id="_4">补充: 后验概率最大化等效于期望风险最小化<a class="headerlink" href="#_4" title="Permanent link">&para;</a></h4>
<p>设选择 <span class="arithmatex"><span class="MathJax_Preview">0-1</span><script type="math/tex">0-1</script></span> 损失函数:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L(Y, f(X))=\left\{\begin{array}{l}
1, Y \neq f(X) \\
0, Y=f(x)
\end{array}\right.
</div>
<script type="math/tex; mode=display">
L(Y, f(X))=\left\{\begin{array}{l}
1, Y \neq f(X) \\
0, Y=f(x)
\end{array}\right.
</script>
</div>
<p>式中 <span class="arithmatex"><span class="MathJax_Preview">f(X)</span><script type="math/tex">f(X)</script></span> 是分类决策函数。这时, 期望风险函数为:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
R_{\exp }(f)=E[L(Y, f(X))]
</div>
<script type="math/tex; mode=display">
R_{\exp }(f)=E[L(Y, f(X))]
</script>
</div>
<p>期望是对联合分布 <span class="arithmatex"><span class="MathJax_Preview">P(X, Y)</span><script type="math/tex">P(X, Y)</script></span> 取的。由此可得关于条件概率 <span class="arithmatex"><span class="MathJax_Preview">P(Y \mid X)</span><script type="math/tex">P(Y \mid X)</script></span> 的期望(损失函数关于条件概率的期望):</p>
<div class="arithmatex">
<div class="MathJax_Preview">
R_{\exp }(f)=E_{X} \sum_{k=1}^{K}\left[L\left(c_{k}, f(X)\right)\right] P\left(c_{k} \mid X\right)
</div>
<script type="math/tex; mode=display">
R_{\exp }(f)=E_{X} \sum_{k=1}^{K}\left[L\left(c_{k}, f(X)\right)\right] P\left(c_{k} \mid X\right)
</script>
</div>
<p>为了使得期望风险最小化, 只需对 <span class="arithmatex"><span class="MathJax_Preview">X=x</span><script type="math/tex">X=x</script></span> 逐个极小化(遍历所有的样本), 由此得到:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
f(x) &amp;=\arg \min_{y \in \mathcal{Y}} \sum_{k=1}^{K} L\left(c_{k}, y\right) P\left(c_{k} \mid X=x\right) \\
&amp;=\arg \min_{y \in \mathcal{Y}} \sum_{k=1}^{K} P\left(y \neq c_{k} \mid X=x\right) \\
&amp;=\arg \min _{y \in \mathcal{Y}}\left(1-P\left(y=c_{k} \mid X=x\right)\right) \\
&amp;=\arg \max _{y \in \mathcal{Y}} P\left(y=c_{k} \mid X=x\right)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
f(x) &=\arg \min_{y \in \mathcal{Y}} \sum_{k=1}^{K} L\left(c_{k}, y\right) P\left(c_{k} \mid X=x\right) \\
&=\arg \min_{y \in \mathcal{Y}} \sum_{k=1}^{K} P\left(y \neq c_{k} \mid X=x\right) \\
&=\arg \min _{y \in \mathcal{Y}}\left(1-P\left(y=c_{k} \mid X=x\right)\right) \\
&=\arg \max _{y \in \mathcal{Y}} P\left(y=c_{k} \mid X=x\right)
\end{aligned}
</script>
</div>
<p>这样一来, 根据期望风险最小准则就得到了后验概室最大化准则:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f(x)=\arg \max _{c_{k}} P\left(y=c_{k} \mid X=x\right)(4.8)
</div>
<script type="math/tex; mode=display">
f(x)=\arg \max _{c_{k}} P\left(y=c_{k} \mid X=x\right)(4.8)
</script>
</div>
<h4 id="nb">NB 推导<a class="headerlink" href="#nb" title="Permanent link">&para;</a></h4>
<p>From Bayes Theorem</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(Y=k \mid x_{1}, \ldots, x_{p}\right) \propto \pi_{k} P\left(x_{1}, \ldots, x_{p} \mid Y=k\right)
</div>
<script type="math/tex; mode=display">
P\left(Y=k \mid x_{1}, \ldots, x_{p}\right) \propto \pi_{k} P\left(x_{1}, \ldots, x_{p} \mid Y=k\right)
</script>
</div>
<p>Naive Bayes assumption: conditional independence of <span class="arithmatex"><span class="MathJax_Preview">x_{j}</span><script type="math/tex">x_{j}</script></span> &lsquo;s given <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(x_{1}, \ldots, x_{p} \mid Y=k\right)=\prod_{j=1}^{p} P\left(x_{j} \mid Y=k\right)
</div>
<script type="math/tex; mode=display">
P\left(x_{1}, \ldots, x_{p} \mid Y=k\right)=\prod_{j=1}^{p} P\left(x_{j} \mid Y=k\right)
</script>
</div>
<p>The relationship is thus simplified to</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(y \mid x_{1}, \ldots, x_{p}\right) \propto \pi_{k} \prod_{j=1}^{p} P\left(x_{j} \mid y\right)
</div>
<script type="math/tex; mode=display">
P\left(y \mid x_{1}, \ldots, x_{p}\right) \propto \pi_{k} \prod_{j=1}^{p} P\left(x_{j} \mid y\right)
</script>
</div>
<p>Therefore, the classification rule is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{y}=\operatorname{argmax}_{k} \pi_{k} \prod_{j=1}^{p} P\left(x_{j} \mid Y=k\right)
</div>
<script type="math/tex; mode=display">
\hat{y}=\operatorname{argmax}_{k} \pi_{k} \prod_{j=1}^{p} P\left(x_{j} \mid Y=k\right)
</script>
</div>
<p>Despite apparently over-simplified assumptions, naive Bayes classifiers have worked quite well in many real-world situations (document classification and spam filtering)</p>
<p>Note: Naive Bayes is known as <strong>a decent classifier, but a bad estimator</strong> (probability outputs are not to be taken too seriously).</p>
<h4 id="gaussian-naive-bayes">Gaussian Naive Bayes<a class="headerlink" href="#gaussian-naive-bayes" title="Permanent link">&para;</a></h4>
<p>对于连续变量, 假设分布为 Gaussian, MLE估计.
The likelihood of the feature is assumed to be Gaussian</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(X_{j} \mid Y=k\right)=\frac{1}{\sqrt{2 \pi \sigma_{k}^{2}} \exp \left(-\frac{\left(x_{j}-\mu_{k}\right)^{2}}{2 \pi \sigma_{k}^{2}}\right)}
</div>
<script type="math/tex; mode=display">
P\left(X_{j} \mid Y=k\right)=\frac{1}{\sqrt{2 \pi \sigma_{k}^{2}} \exp \left(-\frac{\left(x_{j}-\mu_{k}\right)^{2}}{2 \pi \sigma_{k}^{2}}\right)}
</script>
</div>
<p>Parameters <span class="arithmatex"><span class="MathJax_Preview">\sigma_{k}</span><script type="math/tex">\sigma_{k}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\mu_{k}</span><script type="math/tex">\mu_{k}</script></span> are estimated using maximum likelihood method.</p>
<h4 id="multinomial-naive-bayes">Multinomial Naive Bayes<a class="headerlink" href="#multinomial-naive-bayes" title="Permanent link">&para;</a></h4>
<p>Often used in <strong>text classification</strong></p>
<ul>
<li>Data are typically represented as word vector counts</li>
</ul>
<p>Parameter vector <span class="arithmatex"><span class="MathJax_Preview">\theta_{k}=\left(\theta_{k 1}, \ldots, \theta_{k p}\right)</span><script type="math/tex">\theta_{k}=\left(\theta_{k 1}, \ldots, \theta_{k p}\right)</script></span> for each class <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span></p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">p:</span><script type="math/tex">p:</script></span> the number of features (the size of vocabulary)</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\theta_{k j}: P\left(x_{j} \mid Y=k\right)</span><script type="math/tex">\theta_{k j}: P\left(x_{j} \mid Y=k\right)</script></span>, the probability of feature <span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> appearing in a sample belonging to class <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span></li>
</ul>
<p><span class="arithmatex"><span class="MathJax_Preview">\theta_{k}</span><script type="math/tex">\theta_{k}</script></span> can be estimated by <strong>smooth version of maximum likelihood</strong></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta}_{k j}=\frac{N_{k j}+\alpha}{N_{k}+\alpha p}
</div>
<script type="math/tex; mode=display">
\hat{\theta}_{k j}=\frac{N_{k j}+\alpha}{N_{k}+\alpha p}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">N_{k j}:</span><script type="math/tex">N_{k j}:</script></span> number of times feature <span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> appears in class <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> in the training set</li>
<li><span class="arithmatex"><span class="MathJax_Preview">N_{k}</span><script type="math/tex">N_{k}</script></span> : total count of all features for class <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> in the training set</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\alpha&gt;0:</span><script type="math/tex">\alpha>0:</script></span> smoothing priors, prevent zero probabilities for feature not present in the training set</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\alpha=1</span><script type="math/tex">\alpha=1</script></span> : Laplace smoothing</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\alpha&lt;1</span><script type="math/tex">\alpha<1</script></span> : Lidstone smoothing</li>
</ul>
<h4 id="complement-naive-bayes">Complement Naive Bayes<a class="headerlink" href="#complement-naive-bayes" title="Permanent link">&para;</a></h4>
<ul>
<li>Adaptation of the standard Multinomial Naive Bayes algorithm</li>
<li>Suited for <strong>imbalanced data sets</strong> 类别不均衡</li>
<li>Uses statistics from the complement of each class to compute model&rsquo;s weights</li>
<li>Parameter estimates of CNB are more stable than those for MNB</li>
<li>CNB regularly outperforms MNB on text classification</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta}_{k j}=\frac{\alpha_{j}+\sum_{i: y_{i} \neq k} d_{i j}}{\alpha+\sum_{i: y_{i} \neq k} \sum_{l} d_{i l}}
</div>
<script type="math/tex; mode=display">
\hat{\theta}_{k j}=\frac{\alpha_{j}+\sum_{i: y_{i} \neq k} d_{i j}}{\alpha+\sum_{i: y_{i} \neq k} \sum_{l} d_{i l}}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">d_{i j}</span><script type="math/tex">d_{i j}</script></span> : count of term <span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> in document <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\alpha=\Sigma_{j} \alpha_{j}</span><script type="math/tex">\alpha=\Sigma_{j} \alpha_{j}</script></span></li>
<li>summation over all documents <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> not in class <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
w_{k j}=\frac{\log \hat{\theta}_{k j}}{\sum_{l} \log \hat{\theta}_{k l}} \\
\hat{k}=\operatorname{argmin}_{k} \sum_{j} t_{j} w_{k j}
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
w_{k j}=\frac{\log \hat{\theta}_{k j}}{\sum_{l} \log \hat{\theta}_{k l}} \\
\hat{k}=\operatorname{argmin}_{k} \sum_{j} t_{j} w_{k j}
\end{gathered}
</script>
</div>
<p>i.e., Classification rule: assign document to the class that is the poorest complement match</p>
<h3 id="seprating-hyperplanes">Seprating Hyperplanes 略<a class="headerlink" href="#seprating-hyperplanes" title="Permanent link">&para;</a></h3>
<h4 id="rosenblatts-perceptron-learning-algorithm">Rosenblatt&rsquo;s Perceptron Learning Algorithm 感知机<a class="headerlink" href="#rosenblatts-perceptron-learning-algorithm" title="Permanent link">&para;</a></h4>
<p>参见 <a href="https://jozeelin.github.io/2019/06/13/%E6%84%9F%E7%9F%A5%E6%9C%BA/">感知机</a></p>
<ul>
<li>感知机是二分类的线性分类模型。<ul>
<li>模型：感知机对应于将输入空间(特征空间)中的实例划分为正负两类的分离超平面，属于<strong>判别模型</strong>。</li>
<li>策略：基于误分类的损失函数</li>
<li>算法：利用梯度下降法对损失函数进行极小化。</li>
</ul>
</li>
<li>感知机学习算法分为原始形式和对偶形式。</li>
<li>感知机于1957年Rosenblatt提出，是神经网络与支持向量机的基础。</li>
</ul>
<p>Find a separating hyperplane by minimizing the distance of misclassified points to the decision boundary</p>
<div class="arithmatex">
<div class="MathJax_Preview">
D\left(\beta, \beta_{0}\right)=-\sum_{i \in \mathcal{M}} y_{i}\left(\beta_{0}+X_{i}^{\top} \beta\right)
</div>
<script type="math/tex; mode=display">
D\left(\beta, \beta_{0}\right)=-\sum_{i \in \mathcal{M}} y_{i}\left(\beta_{0}+X_{i}^{\top} \beta\right)
</script>
</div>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">y_{i}=1</span><script type="math/tex">y_{i}=1</script></span> is misclassified, <span class="arithmatex"><span class="MathJax_Preview">\beta_{0}+X_{i}^{\top} \beta&lt;0</span><script type="math/tex">\beta_{0}+X_{i}^{\top} \beta<0</script></span></li>
<li>If <span class="arithmatex"><span class="MathJax_Preview">y_{i}=-1</span><script type="math/tex">y_{i}=-1</script></span> is misclassified, <span class="arithmatex"><span class="MathJax_Preview">\beta_{0}+X_{i}^{\top} \beta&gt;0</span><script type="math/tex">\beta_{0}+X_{i}^{\top} \beta>0</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\mathcal{M}</span><script type="math/tex">\mathcal{M}</script></span> indexes the set of misclassified points (assumed fixed)</li>
</ul>
<p>The quantity <span class="arithmatex"><span class="MathJax_Preview">D\left(\beta, \beta_{0}\right)</span><script type="math/tex">D\left(\beta, \beta_{0}\right)</script></span> is non-negative and proportional to the distance of the misclassified points to the decision boundary defined by <span class="arithmatex"><span class="MathJax_Preview">\beta_{0}+X_{i}^{\top} \beta=0</span><script type="math/tex">\beta_{0}+X_{i}^{\top} \beta=0</script></span></p>
<h4 id="parameter-estimation-for-perceptron">Parameter Estimation for Perceptron<a class="headerlink" href="#parameter-estimation-for-perceptron" title="Permanent link">&para;</a></h4>
<p>Gradient is given by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;\quad \frac{\partial D\left(\beta, \beta_{0}\right)}{\partial \beta}=-\sum_{i \in \mathcal{M}} y_{i} x_{i} \\
&amp;\quad \frac{\partial D\left(\beta, \beta_{0}\right)}{\partial \beta_{0}}=-\sum_{i \in \mathcal{M}} y_{i}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&\quad \frac{\partial D\left(\beta, \beta_{0}\right)}{\partial \beta}=-\sum_{i \in \mathcal{M}} y_{i} x_{i} \\
&\quad \frac{\partial D\left(\beta, \beta_{0}\right)}{\partial \beta_{0}}=-\sum_{i \in \mathcal{M}} y_{i}
\end{aligned}
</script>
</div>
<p>Stochastic gradient descent</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\left[\begin{array}{c}
\beta \\
\beta_{0}
\end{array}\right] \leftarrow\left[\begin{array}{c}
\beta \\
\beta_{0}
\end{array}\right]+\rho\left[\begin{array}{c}
y_{i} x_{i} \\
y_{i}
\end{array}\right]
</div>
<script type="math/tex; mode=display">
\left[\begin{array}{c}
\beta \\
\beta_{0}
\end{array}\right] \leftarrow\left[\begin{array}{c}
\beta \\
\beta_{0}
\end{array}\right]+\rho\left[\begin{array}{c}
y_{i} x_{i} \\
y_{i}
\end{array}\right]
</script>
</div>
<ul>
<li>A step is taken after <strong>each observation is visited</strong></li>
<li>Along the negative gradient direction</li>
<li>Repeat until no points are misclassified</li>
<li>Step size <span class="arithmatex"><span class="MathJax_Preview">\rho</span><script type="math/tex">\rho</script></span></li>
<li>More efficient than regular Gradient Descent</li>
</ul>
<h4 id="problems">感知机的 Problems<a class="headerlink" href="#problems" title="Permanent link">&para;</a></h4>
<p>Details summarized in Ripley (1996)</p>
<ul>
<li>All separating hyperplanes are considered equally valid</li>
<li>When the data are separable, there are many solutions, and which one is found depends on the starting values</li>
<li>The &ldquo;finite&rdquo; number of steps can be very large. The smaller the gap, the longer the time to find it</li>
<li>When the data are not separable, the algorithm will not converge, and cycles develop. The cycles can be long and therefore hard to detect</li>
</ul>





                
              </article>
            </div>
          
          
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" hidden>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../ASL-mindmap/" class="md-footer__link md-footer__link--prev" aria-label="Previous: ASL mindmap" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              ASL mindmap
            </div>
          </div>
        </a>
      
      
        
        <a href="../ASL-note2/" class="md-footer__link md-footer__link--next" aria-label="Next: ASL2" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              ASL2
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2021-2022 Easonshi
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://github.com/Lightblues" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://lightblues.github.io/" target="_blank" rel="noopener" title="lightblues.github.io" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M575.8 255.5c0 18-15 32.1-32 32.1h-32l.7 160.2c0 2.7-.2 5.4-.5 8.1V472c0 22.1-17.9 40-40 40h-16c-1.1 0-2.2 0-3.3-.1-1.4.1-2.8.1-4.2.1H392c-22.1 0-40-17.9-40-40v-88c0-17.7-14.3-32-32-32h-64c-17.7 0-32 14.3-32 32v88c0 22.1-17.9 40-40 40h-55.9c-1.5 0-3-.1-4.5-.2-1.2.1-2.4.2-3.6.2h-16c-22.1 0-40-17.9-40-40V360c0-.9 0-1.9.1-2.8v-69.6H32c-18 0-32-14-32-32.1 0-9 3-17 10-24L266.4 8c7-7 15-8 22-8s15 2 21 7l255.4 224.5c8 7 12 15 11 24z"/></svg>
    </a>
  
    
    
    <a href="mailto:oldcitystal@gmail.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.2.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M0 128c0-35.35 28.65-64 64-64h384c35.3 0 64 28.65 64 64v256c0 35.3-28.7 64-64 64H64c-35.35 0-64-28.7-64-64V128zm48 0v22.1l172.5 141.6c20.6 17 50.4 17 71 0L464 150.1v-23c0-7.9-7.2-16-16-16H64c-8.84 0-16 8.1-16 16v.9zm0 84.2V384c0 8.8 7.16 16 16 16h384c8.8 0 16-7.2 16-16V212.2L322 328.8c-38.4 31.5-93.6 31.5-132.9 0L48 212.2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.top"], "search": "../../assets/javascripts/workers/search.5bf1dace.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.37e9125f.min.js"></script>
      
        <script src="../../js/baidu-tongji.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>