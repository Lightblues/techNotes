
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="O'ver curious">
      
      
        <meta name="author" content="Easonshi">
      
      
        <link rel="canonical" href="https://lightblues.github.io/techNotes/stat/ASL-note3/">
      
      
        <link rel="prev" href="../ASL-note2/">
      
      
        <link rel="next" href="../ASL-note4/">
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.0, mkdocs-material-9.5.30">
    
    
      
        <title>ASL3 - techNotes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.3cba04c6.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#model-inference-averaging" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="techNotes" class="md-header__button md-logo" aria-label="techNotes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            techNotes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ASL3
            
          </span>
        </div>
      </div>
    </div>
    
      
        <form class="md-header__option" data-md-component="palette">
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_0">
    
      <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_1" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
      </label>
    
  
    
    
    
    <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="indigo" data-md-color-accent="indigo"  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_1">
    
      <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_0" hidden>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
      </label>
    
  
</form>
      
    
    
      <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/lightblues/techNotes" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    lightblues/techNotes
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    <li class="md-tabs__item">
      <a href="../.." class="md-tabs__link">
        
  
    
  
  Home

      </a>
    </li>
  

      
        
  
  
  
    
    
      
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../code/CS/git-note/" class="md-tabs__link">
          
  
  Code

        </a>
      </li>
    
  

    
  

      
        
  
  
    
  
  
    
    
      
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../" class="md-tabs__link">
          
  
  Stat

        </a>
      </li>
    
  

    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../Linux/Linux/" class="md-tabs__link">
          
  
  Linux

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../os/EFI/" class="md-tabs__link">
          
  
  OS

        </a>
      </li>
    
  

      
        
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../NLP/Lexicon-%E8%AF%8D%E6%B1%87%E5%A2%9E%E5%BC%BA/" class="md-tabs__link">
          
  
  NLP

        </a>
      </li>
    
  

      
        
  
  
  
    <li class="md-tabs__item">
      <a href="https://blog.easonsi.site" class="md-tabs__link">
        
  
    
  
  ğŸ”—Blog

      </a>
    </li>
  

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="techNotes" class="md-nav__button md-logo" aria-label="techNotes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    techNotes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/lightblues/techNotes" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    lightblues/techNotes
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Home
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2" >
        
          
          <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Code
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2">
            <span class="md-nav__icon md-icon"></span>
            Code
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_1" >
        
          
          <label class="md-nav__link" for="__nav_2_1" id="__nav_2_1_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    CS
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_1_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_1">
            <span class="md-nav__icon md-icon"></span>
            CS
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/CS/git-note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Git
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_2" >
        
          
          <label class="md-nav__link" for="__nav_2_2" id="__nav_2_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Python
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_2">
            <span class="md-nav__icon md-icon"></span>
            Python
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/Python/Python-note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    è¯¾ç¨‹ç¬”è®°
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_2_3" >
        
          
          <label class="md-nav__link" for="__nav_2_3" id="__nav_2_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    JavaScripe
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_2_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_2_3">
            <span class="md-nav__icon md-icon"></span>
            JavaScripe
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/js-mindmap/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Mindmap
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/js-note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    js note
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/jQuery-note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    jQuery
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/node-js-note/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Node.js
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
    
  
  
  
    
    
    
      
        
        
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3" checked>
        
          
          <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="">
            
  
  <span class="md-ellipsis">
    Stat
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_3_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3">
            <span class="md-nav__icon md-icon"></span>
            Stat
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_1" checked>
        
          
          <label class="md-nav__link" for="__nav_3_1" id="__nav_3_1_label" tabindex="">
            
  
  <span class="md-ellipsis">
    é«˜ç­‰ç»Ÿè®¡å­¦ä¹ 
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_3_1">
            <span class="md-nav__icon md-icon"></span>
            é«˜ç­‰ç»Ÿè®¡å­¦ä¹ 
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    index
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-mindmap/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ASL mindmap
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-note1/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ASL1
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-note2/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ASL2
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  <span class="md-ellipsis">
    ASL3
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  <span class="md-ellipsis">
    ASL3
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#model-inference-averaging" class="md-nav__link">
    <span class="md-ellipsis">
      Model Inference &amp; Averaging
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model Inference & Averaging">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#maximum-likelihood-methods-bootstrap" class="md-nav__link">
    <span class="md-ellipsis">
      Maximum Likelihood Methods &amp; Bootstrap
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayesian-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Bayesian Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-em-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      The EM Algorithm
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mcmc-sampling-from-posterior" class="md-nav__link">
    <span class="md-ellipsis">
      MCMC - Sampling from Posterior
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bagging" class="md-nav__link">
    <span class="md-ellipsis">
      Bagging
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-averaging-stacking" class="md-nav__link">
    <span class="md-ellipsis">
      Model Averaging &amp; Stacking
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stochastic-search-bumping" class="md-nav__link">
    <span class="md-ellipsis">
      Stochastic Search: Bumping
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additive-models-trees-related-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Additive Models, Trees, &amp; Related Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Additive Models, Trees, & Related Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generalized-additive-models" class="md-nav__link">
    <span class="md-ellipsis">
      Generalized Additive Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tree-based-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Tree Based Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#patient-rule-induction-method-prim" class="md-nav__link">
    <span class="md-ellipsis">
      Patient Rule Induction Method (PRIM)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multivariate-adaptive-regression-splines-mars" class="md-nav__link">
    <span class="md-ellipsis">
      Multivariate Adaptive Regression Splines (MARS)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hierarchical-mixture-of-experts-hme" class="md-nav__link">
    <span class="md-ellipsis">
      Hierarchical Mixture of Experts (HME)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#boosting-additive-trees" class="md-nav__link">
    <span class="md-ellipsis">
      Boosting &amp; Additive Trees
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Boosting & Additive Trees">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#boosting-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Boosting Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#boosting-fits-an-additive-model" class="md-nav__link">
    <span class="md-ellipsis">
      Boosting Fits an Additive Model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exponential-loss-adaboost" class="md-nav__link">
    <span class="md-ellipsis">
      Exponential Loss &amp; AdaBoost
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss-functions-robustness" class="md-nav__link">
    <span class="md-ellipsis">
      Loss Functions &amp; Robustness
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#off-the-shelf-procedure-for-data-mining" class="md-nav__link">
    <span class="md-ellipsis">
      â€Off-the-Shelfâ€ Procedure for Data Mining
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#boosting-trees" class="md-nav__link">
    <span class="md-ellipsis">
      Boosting Trees
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-boosting" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Boosting
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#boosting-tree-size" class="md-nav__link">
    <span class="md-ellipsis">
      Boosting Tree Size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-note4/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ASL4
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
          
          
        
      
    
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_3_2" >
        
          
          <label class="md-nav__link" for="__nav_3_2" id="__nav_3_2_label" tabindex="">
            
  
  <span class="md-ellipsis">
    ç®—æ³•å¯¼è®º
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_3_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_3_2">
            <span class="md-nav__icon md-icon"></span>
            ç®—æ³•å¯¼è®º
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-MinCut-2SAT/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    basic
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-prob-Markov-Chebyshev-Hoeffding/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    æ¦‚ç‡åŸºç¡€
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-computation-%E8%AE%A1%E7%AE%97%E7%90%86%E8%AE%BA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    è®¡ç®—ç†è®º
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-graph-Adjacency-Laplacian/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    å›¾ç›¸å…³, Laplacian
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../Algo-differential-privacy-%E5%B7%AE%E5%88%86%E9%9A%90%E7%A7%81/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    å·®åˆ†éšç§
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../optimization-%E5%87%B8%E4%BC%98%E5%8C%96/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    å‡¸ä¼˜åŒ–
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" >
        
          
          <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    Linux
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_4">
            <span class="md-nav__icon md-icon"></span>
            Linux
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/Linux/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    General
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/tutor-%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83%E7%9B%B8%E5%85%B3/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    å®éªŒç¯å¢ƒç›¸å…³
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/Linux-maintain/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ç³»ç»Ÿç»´æŠ¤
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/Linux-configure-make-install-%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ç¼–è¯‘å®‰è£…è½¯ä»¶
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5" >
        
          
          <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    OS
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5">
            <span class="md-nav__icon md-icon"></span>
            OS
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/EFI/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    EFI
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_2" >
        
          
          <label class="md-nav__link" for="__nav_5_2" id="__nav_5_2_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    OSs
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_2">
            <span class="md-nav__icon md-icon"></span>
            OSs
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/macOS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    macOS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/macOS-intro/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    macOS-intro
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/macOS-softwares/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    macOS è½¯ä»¶åˆ—è¡¨
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Hackintosh/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    hackintosh
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Arch/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Arch
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/CentOS/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    CentOS
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Ubuntu/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Ubuntu
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Windows/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Windows
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_5_3" >
        
          
          <label class="md-nav__link" for="__nav_5_3" id="__nav_5_3_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    softwares
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_5_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_5_3">
            <span class="md-nav__icon md-icon"></span>
            softwares
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/vscode/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VSCode
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../os/vscode-markdown/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    VSCode Markdown
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
    
      
      
        
      
    
    
    <li class="md-nav__item md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_6" >
        
          
          <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0">
            
  
  <span class="md-ellipsis">
    NLP
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_6">
            <span class="md-nav__icon md-icon"></span>
            NLP
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/Lexicon-%E8%AF%8D%E6%B1%87%E5%A2%9E%E5%BC%BA/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Lexicon è¯æ±‡å¢å¼º
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/Prompt-%E6%8F%90%E7%A4%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    Prompt æç¤ºè¯­è¨€æ¨¡å‹
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/ABSA-Sentiment-Analysis-%E5%9F%BA%E4%BA%8E%E6%96%B9%E9%9D%A2%E7%9A%84%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90/" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ABSA
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="https://blog.easonsi.site" class="md-nav__link">
        
  
  <span class="md-ellipsis">
    ğŸ”—Blog
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#model-inference-averaging" class="md-nav__link">
    <span class="md-ellipsis">
      Model Inference &amp; Averaging
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Model Inference & Averaging">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#maximum-likelihood-methods-bootstrap" class="md-nav__link">
    <span class="md-ellipsis">
      Maximum Likelihood Methods &amp; Bootstrap
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayesian-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Bayesian Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-em-algorithm" class="md-nav__link">
    <span class="md-ellipsis">
      The EM Algorithm
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mcmc-sampling-from-posterior" class="md-nav__link">
    <span class="md-ellipsis">
      MCMC - Sampling from Posterior
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bagging" class="md-nav__link">
    <span class="md-ellipsis">
      Bagging
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-averaging-stacking" class="md-nav__link">
    <span class="md-ellipsis">
      Model Averaging &amp; Stacking
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stochastic-search-bumping" class="md-nav__link">
    <span class="md-ellipsis">
      Stochastic Search: Bumping
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additive-models-trees-related-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Additive Models, Trees, &amp; Related Methods
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Additive Models, Trees, & Related Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generalized-additive-models" class="md-nav__link">
    <span class="md-ellipsis">
      Generalized Additive Models
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tree-based-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Tree Based Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#patient-rule-induction-method-prim" class="md-nav__link">
    <span class="md-ellipsis">
      Patient Rule Induction Method (PRIM)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multivariate-adaptive-regression-splines-mars" class="md-nav__link">
    <span class="md-ellipsis">
      Multivariate Adaptive Regression Splines (MARS)
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hierarchical-mixture-of-experts-hme" class="md-nav__link">
    <span class="md-ellipsis">
      Hierarchical Mixture of Experts (HME)
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#boosting-additive-trees" class="md-nav__link">
    <span class="md-ellipsis">
      Boosting &amp; Additive Trees
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Boosting & Additive Trees">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#boosting-methods" class="md-nav__link">
    <span class="md-ellipsis">
      Boosting Methods
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#boosting-fits-an-additive-model" class="md-nav__link">
    <span class="md-ellipsis">
      Boosting Fits an Additive Model
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exponential-loss-adaboost" class="md-nav__link">
    <span class="md-ellipsis">
      Exponential Loss &amp; AdaBoost
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss-functions-robustness" class="md-nav__link">
    <span class="md-ellipsis">
      Loss Functions &amp; Robustness
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#off-the-shelf-procedure-for-data-mining" class="md-nav__link">
    <span class="md-ellipsis">
      â€Off-the-Shelfâ€ Procedure for Data Mining
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#boosting-trees" class="md-nav__link">
    <span class="md-ellipsis">
      Boosting Trees
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-boosting" class="md-nav__link">
    <span class="md-ellipsis">
      Gradient Boosting
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#boosting-tree-size" class="md-nav__link">
    <span class="md-ellipsis">
      Boosting Tree Size
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regularization" class="md-nav__link">
    <span class="md-ellipsis">
      Regularization
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  

  
  


  <h1>ASL3</h1>

<h2 id="model-inference-averaging">Model Inference &amp; Averaging<a class="headerlink" href="#model-inference-averaging" title="Permanent link">&para;</a></h2>
<p>æœ¬ä¹¦çš„å¤§éƒ¨åˆ†ç« èŠ‚ä¸­ï¼Œå¯¹äºå›å½’è€Œè¨€ï¼Œæ¨¡å‹çš„æ‹Ÿåˆï¼ˆå­¦ä¹ ï¼‰é€šè¿‡æœ€å°åŒ–å¹³æ–¹å’Œå®ç°ï¼›æˆ–å¯¹äºåˆ†ç±»è€Œè¨€ï¼Œé€šè¿‡æœ€å°åŒ–äº¤å‰ç†µå®ç°ï¼äº‹å®ä¸Šï¼Œè¿™ä¸¤ç§æœ€å°åŒ–éƒ½æ˜¯ç”¨æå¤§ä¼¼ç„¶æ¥æ‹Ÿåˆçš„å®ä¾‹ï¼</p>
<p>è¿™ç« ä¸­ï¼Œæˆ‘ä»¬ç»™å‡ºæå¤§ä¼¼ç„¶æ³•çš„ä¸€ä¸ªä¸€èˆ¬æ€§çš„æè¿°ï¼Œä»¥åŠç”¨äºæ¨æ–­çš„è´å¶æ–¯æ–¹æ³•ï¼åœ¨ç¬¬7ç« ä¸­è®¨è®ºçš„è‡ªåŠ©æ³•åœ¨æœ¬ç« ä¸­ä¹Ÿç»§ç»­è®¨è®ºï¼Œè€Œä¸”æè¿°äº†å®ƒä¸æå¤§ä¼¼ç„¶å’Œè´å¶æ–¯ä¹‹é—´çš„è”ç³»ï¼æœ€åï¼Œæˆ‘ä»¬æå‡ºæ¨¡å‹å¹³å‡å’Œæ”¹å–„çš„ç›¸å…³æŠ€å·§ï¼ŒåŒ…æ‹¬ committee æ–¹æ³•ã€baggingã€stacking å’Œ bumpingï¼</p>
<h3 id="maximum-likelihood-methods-bootstrap">Maximum Likelihood Methods &amp; Bootstrap<a class="headerlink" href="#maximum-likelihood-methods-bootstrap" title="Permanent link">&para;</a></h3>
<h4 id="maximum-likelihood-inference">Maximum Likelihood Inference<a class="headerlink" href="#maximum-likelihood-inference" title="Permanent link">&para;</a></h4>
<p>Specify a probability density / mass function for observations</p>
<div class="arithmatex">
<div class="MathJax_Preview">
z_{i} \sim g_{\theta}(z)
</div>
<script type="math/tex; mode=display">
z_{i} \sim g_{\theta}(z)
</script>
</div>
<p>Likelihood function (function of <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> given <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> )</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L(\theta ; Z)=\prod_{i=1}^{N} g_{\theta}\left(z_{i}\right)
</div>
<script type="math/tex; mode=display">
L(\theta ; Z)=\prod_{i=1}^{N} g_{\theta}\left(z_{i}\right)
</script>
</div>
<p>Easy to deal with the logarithm of <span class="arithmatex"><span class="MathJax_Preview">L(\theta ; Z)</span><script type="math/tex">L(\theta ; Z)</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
l(\theta ; Z)=\sum_{i=1}^{N} /\left(\theta ; z_{i}\right)=\sum_{i=1}^{N} \log g_{\theta}\left(z_{i}\right)
</div>
<script type="math/tex; mode=display">
l(\theta ; Z)=\sum_{i=1}^{N} /\left(\theta ; z_{i}\right)=\sum_{i=1}^{N} \log g_{\theta}\left(z_{i}\right)
</script>
</div>
<p>Maximum Likelihood estimate</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta}=\operatorname{argmax}_{\theta} /(\theta ; Z)
</div>
<script type="math/tex; mode=display">
\hat{\theta}=\operatorname{argmax}_{\theta} /(\theta ; Z)
</script>
</div>
<p>å®šä¹‰å¾—åˆ†å‡½æ•° <strong>Score function</strong></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\dot{l}(\theta ; Z)=\sum_{i=1}^{N} \dot{l}\left(\theta ; z_{i}\right), \text { where } \dot{I}\left(\theta ; z_{i}\right)=\frac{\partial l\left(\theta ; z_{i}\right)}{\partial \theta}
</div>
<script type="math/tex; mode=display">
\dot{l}(\theta ; Z)=\sum_{i=1}^{N} \dot{l}\left(\theta ; z_{i}\right), \text { where } \dot{I}\left(\theta ; z_{i}\right)=\frac{\partial l\left(\theta ; z_{i}\right)}{\partial \theta}
</script>
</div>
<p>Assuming likelihood takes its maximum in the interior of the parameter space, <span class="arithmatex"><span class="MathJax_Preview">l(\hat{\theta} ; Z)=0</span><script type="math/tex">l(\hat{\theta} ; Z)=0</script></span>
ä¿¡æ¯çŸ©é˜µ <strong>Information matrix</strong></p>
<div class="arithmatex">
<div class="MathJax_Preview">
I(\theta)=-\sum_{i=1}^{N} \frac{\partial^{2} l\left(\theta ; z_{i}\right)}{\partial \theta \partial \theta^{\top}}
</div>
<script type="math/tex; mode=display">
I(\theta)=-\sum_{i=1}^{N} \frac{\partial^{2} l\left(\theta ; z_{i}\right)}{\partial \theta \partial \theta^{\top}}
</script>
</div>
<ul>
<li>Observed information <span class="arithmatex"><span class="MathJax_Preview">\left.I(\theta)\right|_{\theta=\hat{\theta}}</span><script type="math/tex">\left.I(\theta)\right|_{\theta=\hat{\theta}}</script></span></li>
<li><strong>Fisher information</strong> <span class="arithmatex"><span class="MathJax_Preview">i(\theta)=E_{\theta}[I(\theta)]</span><script type="math/tex">i(\theta)=E_{\theta}[I(\theta)]</script></span> Fisher ä¿¡æ¯é‡ï¼ˆæˆ–è€…æœŸæœ›ä¿¡æ¯é‡ï¼‰</li>
</ul>
<p>å¯ä»¥è®¤ä¸º, Fisher ä¿¡æ¯é‡æ˜¯å‚æ•°ä¼°è®¡çš„æ–¹å·®.</p>
<p>ç›¸å…³æ¨å¯¼å‚è§ <a href="https://esl.hohoweiya.xyz/08-Model-Inference-and-Averaging/8.2-The-Bootstrap-and-Maximum-Likelihood-Methods/index.html">here</a> çš„æ³¨é‡Š.</p>
<p>Sampling distribution of MLE <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}</span><script type="math/tex">\hat{\theta}</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta} \rightarrow \mathcal{N}\left(\theta_{0}, i\left(\theta_{0}\right)^{-1}\right) \text { as } N \rightarrow \infty
</div>
<script type="math/tex; mode=display">
\hat{\theta} \rightarrow \mathcal{N}\left(\theta_{0}, i\left(\theta_{0}\right)^{-1}\right) \text { as } N \rightarrow \infty
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\theta_{0}</span><script type="math/tex">\theta_{0}</script></span> denotes the true value of <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>
Can be approximated by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathcal{N}\left(\hat{\theta}, i(\hat{\theta})^{-1}\right) \text { or } \mathcal{N}\left(\hat{\theta}, I(\hat{\theta})^{-1}\right)
</div>
<script type="math/tex; mode=display">
\mathcal{N}\left(\hat{\theta}, i(\hat{\theta})^{-1}\right) \text { or } \mathcal{N}\left(\hat{\theta}, I(\hat{\theta})^{-1}\right)
</script>
</div>
<p>Estimates for standard error of <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}_{j}</span><script type="math/tex">\hat{\theta}_{j}</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\sqrt{i(\hat{\theta})_{j j}^{-1}} \text { or } \sqrt{l(\hat{\theta})_{j j}^{-1}}
</div>
<script type="math/tex; mode=display">
\sqrt{i(\hat{\theta})_{j j}^{-1}} \text { or } \sqrt{l(\hat{\theta})_{j j}^{-1}}
</script>
</div>
<p>Confidence interval</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta}_{j}-z^{1-\alpha} \cdot \sqrt{i(\hat{\theta})_{j j}^{-1}} \text { or } \hat{\theta}_{j}-z^{1-\alpha} \cdot \sqrt{I(\hat{\theta})_{j j}^{-1}}
</div>
<script type="math/tex; mode=display">
\hat{\theta}_{j}-z^{1-\alpha} \cdot \sqrt{i(\hat{\theta})_{j j}^{-1}} \text { or } \hat{\theta}_{j}-z^{1-\alpha} \cdot \sqrt{I(\hat{\theta})_{j j}^{-1}}
</script>
</div>
<p>Joint confidence region</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\left\{\theta ; 2(l(\hat{\theta})-l(\theta)) \leqslant \chi_{p}^{2,(1-2 \alpha)}\right\}
</div>
<script type="math/tex; mode=display">
\left\{\theta ; 2(l(\hat{\theta})-l(\theta)) \leqslant \chi_{p}^{2,(1-2 \alpha)}\right\}
</script>
</div>
<p>Since <span class="arithmatex"><span class="MathJax_Preview">2\left[l(\hat{\theta})-l\left(\theta_{0}\right)\right] \sim \chi_{p}^{2}</span><script type="math/tex">2\left[l(\hat{\theta})-l\left(\theta_{0}\right)\right] \sim \chi_{p}^{2}</script></span></p>
<h4 id="_1">è‡ªåŠ©æ³•å’Œæœ€å¤§ä¼¼ç„¶æ³•<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h4>
<p>æœ¬è´¨ä¸Šè‡ªåŠ©æ³•æ˜¯éå‚æœ€å¤§ä¼¼ç„¶æˆ–è€…å‚æ•°æœ€å¤§ä¼¼ç„¶æ³•çš„è®¡ç®—æœºå®ç°ï¼The bootstrap can be thought of as a â€œnonparametricâ€ MLE.</p>
<p>ä¸æœ€å¤§ä¼¼ç„¶æ³•ç›¸æ¯”è‡ªåŠ©æ³•çš„å¥½å¤„æ˜¯å…è®¸æˆ‘ä»¬åœ¨æ²¡æœ‰å…¬å¼çš„æƒ…å†µä¸‹è®¡ç®—æ ‡å‡†è¯¯å·®å’Œå…¶ä»–ä¸€äº›é‡çš„æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼</p>
<h3 id="bayesian-methods">Bayesian Methods<a class="headerlink" href="#bayesian-methods" title="Permanent link">&para;</a></h3>
<p>Posterior distribution of <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> given prior and data
$$
p(\theta \mid Z)=\frac{p(Z \mid \theta) p(\theta)}{\int p\left(Z \mid \theta^{\prime}\right) p\left(\theta^{\prime}\right) d \theta^{\prime}}
$$
<strong>Predictive distribution</strong> for new data
$$
p\left(z^{\text {new }} \mid Z\right)=\int p\left(z^{\text {new }} \mid \theta\right) p(\theta \mid Z) d \theta
$$
However, by Maximum Likelihood methods, the predictive distribution for new data <span class="arithmatex"><span class="MathJax_Preview">z^{\text {new }}</span><script type="math/tex">z^{\text {new }}</script></span> is <span class="arithmatex"><span class="MathJax_Preview">p\left(z^{\text {new }} \mid \hat{\theta}_{M L E}\right)</span><script type="math/tex">p\left(z^{\text {new }} \mid \hat{\theta}_{M L E}\right)</script></span>, which does not account for the uncertainty in estimating <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span></p>
<h4 id="back-to-example">Back to Example<a class="headerlink" href="#back-to-example" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
Y=\sum_{j=1}^{J} \beta_{j} h_{j}(X)+\epsilon, \text { with } \epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)
</div>
<script type="math/tex; mode=display">
Y=\sum_{j=1}^{J} \beta_{j} h_{j}(X)+\epsilon, \text { with } \epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)
</script>
</div>
<ul>
<li>Let <span class="arithmatex"><span class="MathJax_Preview">u(x)=\sum_{j=1}^{J} \beta_{j} h_{j}(X), B</span><script type="math/tex">u(x)=\sum_{j=1}^{J} \beta_{j} h_{j}(X), B</script></span> be the design matrix with <span class="arithmatex"><span class="MathJax_Preview">i j_{t h}</span><script type="math/tex">i j_{t h}</script></span> element <span class="arithmatex"><span class="MathJax_Preview">h_{j}\left(x_{i}\right)</span><script type="math/tex">h_{j}\left(x_{i}\right)</script></span></li>
<li>Prior on <span class="arithmatex"><span class="MathJax_Preview">\beta=\left(\beta_{1}, \ldots, \beta_{p}\right)^{\top}: \beta \sim \mathcal{N}(0, \tau \Sigma)</span><script type="math/tex">\beta=\left(\beta_{1}, \ldots, \beta_{p}\right)^{\top}: \beta \sim \mathcal{N}(0, \tau \Sigma)</script></span></li>
<li>Data likelihood: <span class="arithmatex"><span class="MathJax_Preview">p(y \mid X, \beta)=\mathcal{N}\left(y ; B \beta, \sigma^{2} I_{n}\right)</span><script type="math/tex">p(y \mid X, \beta)=\mathcal{N}\left(y ; B \beta, \sigma^{2} I_{n}\right)</script></span></li>
<li>Posterior distribution for <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> :
$$
p(\beta \mid Z)=p(\beta \mid X, y)=\frac{p(y \mid X, \beta) p(\beta)}{p(y \mid X)}=\mathcal{N}\left(\beta ; A^{-1} B^{\top} y, A^{-1} \sigma^{2}\right)
$$
with <span class="arithmatex"><span class="MathJax_Preview">A=B^{\top} B+\frac{\sigma^{2}}{\tau} \Sigma^{-1}</span><script type="math/tex">A=B^{\top} B+\frac{\sigma^{2}}{\tau} \Sigma^{-1}</script></span></li>
</ul>
<p><img alt="" src="../media/ASL-note3/2021-12-22-17-29-21.png" /></p>
<p>å¯ä»¥çœ‹åˆ°, å¯¹äº <span class="arithmatex"><span class="MathJax_Preview">\tau=1</span><script type="math/tex">\tau=1</script></span> æ‹Ÿåˆçš„æ›²çº¿æ›´ä¸ºå…‰æ»‘ (å› ä¸ºæˆ‘ä»¬åœ¨å…‰æ»‘æ€§ä¸Šé¢å¼ºåŠ äº†æ›´å¤šå…ˆéªŒæƒé‡). è€Œ <span class="arithmatex"><span class="MathJax_Preview">\tao=100</span><script type="math/tex">\tao=100</script></span> æ—¶çš„æ›²çº¿æ›´æ¥è¿‘ MLE çš„ç»“æœ.</p>
<p>å®é™…ä¸Š, å½“ <span class="arithmatex"><span class="MathJax_Preview">\tau \rightarrow \infty</span><script type="math/tex">\tau \rightarrow \infty</script></span> æ—¶ç§°ä¹‹ä¸º <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> çš„ <strong>noninformative prior</strong> æ— ä¿¡æ¯å…ˆéªŒ. åœ¨é«˜æ–¯æ¨¡å‹ä¸­ï¼Œæå¤§ä¼¼ç„¶å’Œå‚æ•°è‡ªåŠ©æ³•è¶‹å‘äºä¸å¯¹è‡ªç”±å‚æ•°ä½¿ç”¨äº†æ— ä¿¡æ¯å…ˆéªŒçš„è´å¶æ–¯ä¿æŒä¸€è‡´ï¼å› ä¸ºåœ¨å¸¸å€¼å…ˆéªŒæƒ…å†µä¸‹ï¼ŒåéªŒåˆ†å¸ƒä¸ä¼¼ç„¶å‡½æ•°æˆæ¯”ä¾‹ï¼Œæ‰€ä»¥è¿™äº›æ–¹æ³•è¶‹å‘äºä¸€è‡´ï¼è¿™ç§å¯¹åº”ä¹Ÿå¯ä»¥æ¨å¹¿åˆ°éå‚çš„æƒ…å½¢ï¼å…¶ä¸­éå‚è‡ªåŠ©æ³•è¿‘ä¼¼äºä¸€ä¸ªæ— ä¿¡æ¯å…ˆéªŒçš„è´å¶æ–¯åˆ†æï¼›8.4 èŠ‚å°†è¯¦ç»†ä»‹ç»ï¼</p>
<h3 id="the-em-algorithm">The EM Algorithm<a class="headerlink" href="#the-em-algorithm" title="Permanent link">&para;</a></h3>
<p>å¯ä»¥å‚è€ƒä¹‹å‰ç»Ÿè®¡è®¡ç®—çš„ <a href="https://lightblues.github.io/posts/346b3c08/">note</a>, æ€è·¯å’Œ <a href="https://jozeelin.github.io/2019/06/14/EM%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%8E%A8%E5%B9%BF/">EMç®—æ³•åŠå…¶æ¨å¹¿</a> æ¯”è¾ƒåƒ. ESL çš„ç‰ˆæœ¬ <a href="https://esl.hohoweiya.xyz/08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm/index.html">here</a>.</p>
<p>EM ç®—æ³•æ˜¯ç®€åŒ–å¤æ‚æå¤§ä¼¼ç„¶é—®é¢˜çš„ä¸€ç§å¾ˆå—æ¬¢è¿çš„å·¥å…·ï¼æˆ‘ä»¬é¦–å…ˆåœ¨ä¸€ä¸ªç®€å•çš„æ··åˆæ¨¡å‹ä¸­è®¨è®ºå®ƒï¼</p>
<h4 id="gaussian-mixture-models-gmm">Gaussian Mixture Models (GMM)<a class="headerlink" href="#gaussian-mixture-models-gmm" title="Permanent link">&para;</a></h4>
<p>Mathematical definition</p>
<div class="arithmatex">
<div class="MathJax_Preview">
p(x \mid \theta)=\sum_{k=1}^{k} \pi_{k} \cdot N\left(x_{k} ; \mu_{k}, \Sigma_{k}\right)
</div>
<script type="math/tex; mode=display">
p(x \mid \theta)=\sum_{k=1}^{k} \pi_{k} \cdot N\left(x_{k} ; \mu_{k}, \Sigma_{k}\right)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\sum_{k=1}^{K} \pi_{k}=1</span><script type="math/tex">\sum_{k=1}^{K} \pi_{k}=1</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\pi_{k} \geqslant 0</span><script type="math/tex">\pi_{k} \geqslant 0</script></span> for <span class="arithmatex"><span class="MathJax_Preview">k=1, \ldots, K</span><script type="math/tex">k=1, \ldots, K</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\theta=\left(\mu_{1}, \ldots, \mu_{K}, \Sigma_{1}, \ldots, \Sigma_{K}, \pi_{1}, \ldots, \pi_{K}\right)</span><script type="math/tex">\theta=\left(\mu_{1}, \ldots, \mu_{K}, \Sigma_{1}, \ldots, \Sigma_{K}, \pi_{1}, \ldots, \pi_{K}\right)</script></span></p>
<h4 id="parameter-estimation-for-gmm">Parameter Estimation for GMM<a class="headerlink" href="#parameter-estimation-for-gmm" title="Permanent link">&para;</a></h4>
<p>Given <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> independent training samples <span class="arithmatex"><span class="MathJax_Preview">x_{1}, x_{2}, \ldots, x_{n}</span><script type="math/tex">x_{1}, x_{2}, \ldots, x_{n}</script></span> from a GMM, can still use MLE to estimate <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>, but&hellip;
Assume <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> is fixed and known, log likelihood of the data is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
I(\theta ; x)=\sum_{i=1}^{n} \log \left(\sum_{k=1}^{k} \pi_{k} N\left(x_{i} ; \mu_{k}, \Sigma_{k}\right)\right)
</div>
<script type="math/tex; mode=display">
I(\theta ; x)=\sum_{i=1}^{n} \log \left(\sum_{k=1}^{k} \pi_{k} N\left(x_{i} ; \mu_{k}, \Sigma_{k}\right)\right)
</script>
</div>
<p>Let&rsquo;s try to maximize it</p>
<ul>
<li>By eyeballing</li>
<li>Analytically</li>
<li>Numerically</li>
</ul>
<h4 id="the-mmalgorithm-minimization">The MMAlgorithm - Minimization<a class="headerlink" href="#the-mmalgorithm-minimization" title="Permanent link">&para;</a></h4>
<p>Majorize-Minimization/Maximization. MMç®—æ³•æ˜¯ä¸€ç§è¿­ä»£ä¼˜åŒ–æ–¹æ³•ï¼Œå®ƒåˆ©ç”¨å‡½æ•°çš„å‡¸æ€§æ¥æ‰¾åˆ°å®ƒä»¬çš„æœ€å¤§å€¼æˆ–æœ€å°å€¼ã€‚å½“ç›®æ ‡å‡½æ•°éš¾ä»¥ä¼˜åŒ–çš„æ—¶å€™, è½¬è€Œå»æ‰¾ä¸€ä¸ªå®¹æ˜“ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°æ›¿ä»£, è¿­ä»£.</p>
<p>ä¾‹å¦‚, å¯¹äºæœ€å°åŒ–é—®é¢˜, æ¯æ¬¡è¿­ä»£æ‰¾åˆ°ä¸€ä¸ªç›®æ ‡å‡½æ•°çš„ä¸Šç•Œå‡½æ•°ï¼Œæ±‚ä¸Šç•Œå‡½æ•°çš„æœ€å°å€¼ã€‚</p>
<ul>
<li>To minimize an objective function <span class="arithmatex"><span class="MathJax_Preview">f(\theta)</span><script type="math/tex">f(\theta)</script></span></li>
<li>The MM algorithm is a prescription for constructing optimization algorithms</li>
<li>An MM algorithm creates a surrogate æ›¿ä»£ function that majories the objective function.</li>
<li>When the surrogate function is minimized, the objective function is decreased</li>
<li>For minimization, <span class="arithmatex"><span class="MathJax_Preview">M M \equiv</span><script type="math/tex">M M \equiv</script></span> majorize</li>
</ul>
<h5 id="majorization">Majorization<a class="headerlink" href="#majorization" title="Permanent link">&para;</a></h5>
<p>A function <span class="arithmatex"><span class="MathJax_Preview">g\left(\theta ; \theta^{(t)}\right)</span><script type="math/tex">g\left(\theta ; \theta^{(t)}\right)</script></span> <strong>majorizes</strong> the function <span class="arithmatex"><span class="MathJax_Preview">f(\theta)</span><script type="math/tex">f(\theta)</script></span> at <span class="arithmatex"><span class="MathJax_Preview">\theta^{(t)}</span><script type="math/tex">\theta^{(t)}</script></span> if</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">f\left(\theta^{(t)}\right)=g\left(\theta^{(t)} ; \theta^{(t)}\right)</span><script type="math/tex">f\left(\theta^{(t)}\right)=g\left(\theta^{(t)} ; \theta^{(t)}\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">f(\theta) \leqslant g\left(\theta ; \theta^{(t)}\right)</span><script type="math/tex">f(\theta) \leqslant g\left(\theta ; \theta^{(t)}\right)</script></span> for all <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> å³æ›¿ä»£å‡½æ•°æ˜¯åŸå‡½æ•°çš„ä¸Šå±Š</li>
</ul>
<p>If <span class="arithmatex"><span class="MathJax_Preview">f(\theta)</span><script type="math/tex">f(\theta)</script></span> is difficult to minimize, find <span class="arithmatex"><span class="MathJax_Preview">g\left(\theta ; \theta^{(t)}\right)</span><script type="math/tex">g\left(\theta ; \theta^{(t)}\right)</script></span>, which is easy to minimize. Let
$$
\theta^{(t+1)}=\operatorname{argmin}_{\theta} g\left(\theta ; \theta^{(t)}\right)
$$</p>
<p>MM minimization algorithm satisfied the <strong>descent property</strong> as
$$
\begin{aligned}
f\left(\theta^{(t+1)}\right) &amp; \leqslant g\left(\theta^{(t+1)} ; \theta^{(t)}\right) \
&amp; \leqslant g\left(\theta^{(t)} ; \theta^{(t)}\right) \
&amp;=f\left(\theta^{(t)}\right)
\end{aligned}
$$
In summary
$$
f\left(\theta^{(t+1)}\right) \leqslant f\left(\theta^{(t)}\right)
$$
The descent property makes the MM algorithm very stable algorithms converges to local minima or saddle point</p>
<h5 id="big-question">Big Question<a class="headerlink" href="#big-question" title="Permanent link">&para;</a></h5>
<p>How do we majorize / minorize a function?
Some generic tricks and tools</p>
<ul>
<li>Jensen&rsquo;s inequality</li>
<li>Chord above the graph property of a convex function</li>
<li>Supporting hyperplane property of a convex function</li>
<li>Quadratic upper bound principle</li>
<li>Arithmetic-geometric mean inequality</li>
<li>Cauchy-Schwartz inequality</li>
</ul>
<p>We&rsquo;ll only focus on Jensen&rsquo;s inequality here</p>
<h5 id="jensens-inequality">Jensen&rsquo;s Inequality<a class="headerlink" href="#jensens-inequality" title="Permanent link">&para;</a></h5>
<p>You&rsquo;ve probably minorized via Jensen&rsquo;s Inequality without noticing it</p>
<p>Remember Jensen&rsquo;s Inequality [ç”»å›¾å¾ˆå®¹æ˜“ç†è§£]</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">h(\cdot)</span><script type="math/tex">h(\cdot)</script></span> be a concave function</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\pi_{1}, \ldots, \pi_{K}</span><script type="math/tex">\pi_{1}, \ldots, \pi_{K}</script></span> be <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> non-negative numbers, with <span class="arithmatex"><span class="MathJax_Preview">\sum_{k=1}^{K} \pi_{k}=1</span><script type="math/tex">\sum_{k=1}^{K} \pi_{k}=1</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> arbituary numbers <span class="arithmatex"><span class="MathJax_Preview">a_{1}, \ldots, a_{K}</span><script type="math/tex">a_{1}, \ldots, a_{K}</script></span></li>
</ul>
<p>Then</p>
<div class="arithmatex">
<div class="MathJax_Preview">
h\left(\sum_{k=1}^{K} \pi_{k} a_{k}\right) \geqslant \sum_{k=1}^{K} \pi_{k} h\left(a_{k}\right)
</div>
<script type="math/tex; mode=display">
h\left(\sum_{k=1}^{K} \pi_{k} a_{k}\right) \geqslant \sum_{k=1}^{K} \pi_{k} h\left(a_{k}\right)
</script>
</div>
<h4 id="expectation-maximization">Expectation - Maximization<a class="headerlink" href="#expectation-maximization" title="Permanent link">&para;</a></h4>
<p>EM ç®—æ³•å‚è§ <a href="https://jozeelin.github.io/2019/06/14/EM%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%8E%A8%E5%B9%BF/">here</a>. æ³¨æ„åˆ°, é“¾æ¥ä¸­ç†è§£ Z æ˜¯éšå˜é‡(ä¸€éƒ¨åˆ†çš„å› å˜é‡, ä½†æ— æ³•è§‚æµ‹.) ã€Œç”¨Yè¡¨ç¤ºè§‚æµ‹éšæœºå˜é‡çš„æ•°æ®ï¼ŒZè¡¨ç¤ºéšéšæœºå˜é‡çš„æ•°æ®ã€‚Yå’ŒZè¿åœ¨ä¸€èµ·ç§°ä¸ºå®Œå…¨æ•°æ®ï¼Œè§‚æµ‹æ•°æ®Yåˆç§°ä¸ºä¸å®Œå…¨æ•°æ®ã€‚ã€</p>
<p>é™¤äº†ä¸Šé¢é“¾æ¥ä¸­çš„ç†è§£, è¿™é‡Œç›´æ¥åŸºäº Jenson ä¸ç­‰å¼</p>
<ul>
<li>The EM algorithm is a MM algorithm EM ç®—æ³•æ˜¯ MM ç®—æ³•çš„ä¸€ç§</li>
<li>EM uses Jensen&rsquo;s inequality to minorize the log likelihood</li>
</ul>
<p>Step 1</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
l(\theta ; X) &amp;=\log \left(\sum_{j=1}^{n_{z}} p\left(X, Z=z_{j} \mid \theta\right)\right) \\
&amp;=\log \left(\sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \frac{p\left(X, Z=z_{j} \mid \theta\right)}{f^{(t)}\left(Z=z_{j}\right)}\right) \\
&amp; \geqslant \sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \log \left(\frac{P\left(X, Z=z_{j} \mid \theta\right)}{f^{(t)}\left(Z=z_{j}\right)}\right)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
l(\theta ; X) &=\log \left(\sum_{j=1}^{n_{z}} p\left(X, Z=z_{j} \mid \theta\right)\right) \\
&=\log \left(\sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \frac{p\left(X, Z=z_{j} \mid \theta\right)}{f^{(t)}\left(Z=z_{j}\right)}\right) \\
& \geqslant \sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \log \left(\frac{P\left(X, Z=z_{j} \mid \theta\right)}{f^{(t)}\left(Z=z_{j}\right)}\right)
\end{aligned}
</script>
</div>
<p>Summary of step 1 , we&rsquo;ve got</p>
<div class="arithmatex">
<div class="MathJax_Preview">
l(\theta ; X) \geqslant \sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \log \left(\frac{p\left(X, Z=z_{j} \mid \theta\right)}{f^{(t)}\left(Z=z_{j}\right)}\right)
</div>
<script type="math/tex; mode=display">
l(\theta ; X) \geqslant \sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \log \left(\frac{p\left(X, Z=z_{j} \mid \theta\right)}{f^{(t)}\left(Z=z_{j}\right)}\right)
</script>
</div>
<p>Step 2: find <span class="arithmatex"><span class="MathJax_Preview">f^{(t)}(Z)</span><script type="math/tex">f^{(t)}(Z)</script></span></p>
<p>The lower bound must touch the <span class="arithmatex"><span class="MathJax_Preview">\log</span><script type="math/tex">\log</script></span> likelihood at <span class="arithmatex"><span class="MathJax_Preview">\theta^{(t)}</span><script type="math/tex">\theta^{(t)}</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
l\left(\theta^{(t)} ; X\right)=\sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \log \left(\frac{p\left(X, Z=z_{j} \mid \theta^{(t)}\right)}{f^{(t)}\left(Z=z_{j}\right)}\right)
</div>
<script type="math/tex; mode=display">
l\left(\theta^{(t)} ; X\right)=\sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \log \left(\frac{p\left(X, Z=z_{j} \mid \theta^{(t)}\right)}{f^{(t)}\left(Z=z_{j}\right)}\right)
</script>
</div>
<p>From this we can calculate <span class="arithmatex"><span class="MathJax_Preview">f^{(t)}(Z)</span><script type="math/tex">f^{(t)}(Z)</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
f^{(t)}(Z)=p\left(Z \mid X, \theta^{(t)}\right)
</div>
<script type="math/tex; mode=display">
f^{(t)}(Z)=p\left(Z \mid X, \theta^{(t)}\right)
</script>
</div>
<p>The log likelihood is minorized by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
g\left(\theta ; \theta^{(t)}\right)=\sum_{j=1}^{n_{z}} p\left(Z=z_{j} \mid X, \theta^{(t)}\right) \log \left(\frac{p\left(X, Z=z_{j} \mid \theta\right)}{p\left(Z=z_{j} \mid X, \theta^{(t)}\right)}\right)
</div>
<script type="math/tex; mode=display">
g\left(\theta ; \theta^{(t)}\right)=\sum_{j=1}^{n_{z}} p\left(Z=z_{j} \mid X, \theta^{(t)}\right) \log \left(\frac{p\left(X, Z=z_{j} \mid \theta\right)}{p\left(Z=z_{j} \mid X, \theta^{(t)}\right)}\right)
</script>
</div>
<p>Step 3: find \&amp; max <span class="arithmatex"><span class="MathJax_Preview">g\left(\theta ; \theta^{(t)}\right)</span><script type="math/tex">g\left(\theta ; \theta^{(t)}\right)</script></span>
What are the <span class="arithmatex"><span class="MathJax_Preview">Z^{\prime} s</span><script type="math/tex">Z^{\prime} s</script></span> and where did they come from?</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> is a <strong>random variable</strong> whose pdf conditioned on <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> is completely determined by <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span></li>
<li>Choice of <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> should make the maximization step easy</li>
</ul>
<h4 id="two-component-gmm">Two Component GMM<a class="headerlink" href="#two-component-gmm" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;Y_{1} \sim N\left(\mu_{1}, \sigma_{1}^{2}\right) \\
&amp;Y_{2} \sim N\left(\mu_{2}, \sigma_{2}^{2}\right) \\
&amp;Y=(1-\Delta) \cdot Y_{1}+\Delta \cdot Y_{2}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&Y_{1} \sim N\left(\mu_{1}, \sigma_{1}^{2}\right) \\
&Y_{2} \sim N\left(\mu_{2}, \sigma_{2}^{2}\right) \\
&Y=(1-\Delta) \cdot Y_{1}+\Delta \cdot Y_{2}
\end{aligned}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\Delta=\{0,1\}</span><script type="math/tex">\Delta=\{0,1\}</script></span> with <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(\Delta=1)=\pi</span><script type="math/tex">\operatorname{Pr}(\Delta=1)=\pi</script></span>
Equivalent two-step representation</p>
<ul>
<li>Generate a <span class="arithmatex"><span class="MathJax_Preview">\Delta</span><script type="math/tex">\Delta</script></span> with probability <span class="arithmatex"><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span></li>
<li>Depending on the outcome of <span class="arithmatex"><span class="MathJax_Preview">\Delta</span><script type="math/tex">\Delta</script></span>, deliver either <span class="arithmatex"><span class="MathJax_Preview">Y_{1}</span><script type="math/tex">Y_{1}</script></span> or <span class="arithmatex"><span class="MathJax_Preview">Y_{2}</span><script type="math/tex">Y_{2}</script></span></li>
</ul>
<p>Let <span class="arithmatex"><span class="MathJax_Preview">\phi_{\theta}(x)</span><script type="math/tex">\phi_{\theta}(x)</script></span> denote the normal density with parameter <span class="arithmatex"><span class="MathJax_Preview">\theta=\left(\mu, \sigma^{2}\right)</span><script type="math/tex">\theta=\left(\mu, \sigma^{2}\right)</script></span>, then the density of <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P_{Y}(y)=(1-\pi) \phi_{\theta_{1}}(y)+\pi \phi_{\theta_{2}}(y)
</div>
<script type="math/tex; mode=display">
P_{Y}(y)=(1-\pi) \phi_{\theta_{1}}(y)+\pi \phi_{\theta_{2}}(y)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\theta=\left(\pi, \theta_{1}, \theta_{2}\right)=\left(\pi, \mu_{1}, \sigma_{1}^{2}, \mu_{2}, \sigma_{2}^{2}\right)</span><script type="math/tex">\theta=\left(\pi, \theta_{1}, \theta_{2}\right)=\left(\pi, \mu_{1}, \sigma_{1}^{2}, \mu_{2}, \sigma_{2}^{2}\right)</script></span></p>
<p>The log likelihood of the data</p>
<div class="arithmatex">
<div class="MathJax_Preview">
I(\theta ; Y)=\sum_{i=1}^{N} \log \left[(1-\pi) \phi_{\theta_{1}}\left(y_{i}\right)+\pi \phi_{\theta_{2}}\left(y_{i}\right)\right]
</div>
<script type="math/tex; mode=display">
I(\theta ; Y)=\sum_{i=1}^{N} \log \left[(1-\pi) \phi_{\theta_{1}}\left(y_{i}\right)+\pi \phi_{\theta_{2}}\left(y_{i}\right)\right]
</script>
</div>
<p><strong>Introduce latent variables</strong> <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i}</span><script type="math/tex">\Delta_{i}</script></span> taking values 0 or 1</p>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i}=1, Y_{i}</span><script type="math/tex">\Delta_{i}=1, Y_{i}</script></span> comes from model 2</li>
<li>Otherwise it comes from model 1</li>
</ul>
<p>Suppose we know the values of <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i}</span><script type="math/tex">\Delta_{i}</script></span>, the log likelihood can be written</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
I(\theta ; Y, \Delta)=&amp; \sum_{i=1}^{N}\left[\left(1-\Delta_{i}\right) \log \phi_{\theta_{1}}\left(y_{i}\right)+\Delta_{i} \log \phi_{\theta_{2}}\left(y_{i}\right)\right]+\\
&amp; \sum_{i=1}^{N}\left[\left(1-\Delta_{i}\right) \log (1-\pi)+\Delta_{i} \log \pi\right]
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
I(\theta ; Y, \Delta)=& \sum_{i=1}^{N}\left[\left(1-\Delta_{i}\right) \log \phi_{\theta_{1}}\left(y_{i}\right)+\Delta_{i} \log \phi_{\theta_{2}}\left(y_{i}\right)\right]+\\
& \sum_{i=1}^{N}\left[\left(1-\Delta_{i}\right) \log (1-\pi)+\Delta_{i} \log \pi\right]
\end{aligned}
</script>
</div>
<ul>
<li>The MLE of <span class="arithmatex"><span class="MathJax_Preview">\mu_{1}</span><script type="math/tex">\mu_{1}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\sigma_{1}^{2}</span><script type="math/tex">\sigma_{1}^{2}</script></span> would be the sample mean and variance for those data with <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i}=0</span><script type="math/tex">\Delta_{i}=0</script></span></li>
<li>The MLE of <span class="arithmatex"><span class="MathJax_Preview">\mu_{2}</span><script type="math/tex">\mu_{2}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\sigma_{2}^{2}</span><script type="math/tex">\sigma_{2}^{2}</script></span> would be the sample mean and variance for those data with <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i}=1</span><script type="math/tex">\Delta_{i}=1</script></span></li>
<li>The MLE of <span class="arithmatex"><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span> would be the proportion of <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i}=1</span><script type="math/tex">\Delta_{i}=1</script></span></li>
</ul>
<p>However, <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i}</span><script type="math/tex">\Delta_{i}</script></span> are unknown, and we substibute with its expected value</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\gamma_{i}(\theta)=E\left(\Delta_{i} \mid \theta, Y\right)=\operatorname{Pr}\left(\Delta_{i}=1 \mid \theta, Y\right)
</div>
<script type="math/tex; mode=display">
\gamma_{i}(\theta)=E\left(\Delta_{i} \mid \theta, Y\right)=\operatorname{Pr}\left(\Delta_{i}=1 \mid \theta, Y\right)
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\gamma_{i}</span><script type="math/tex">\gamma_{i}</script></span> is called <strong>responsibility</strong> of model 2 for observation <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span></li>
<li>posteior of the probability that observation <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> belongs to model 2 given the data and current estimate</li>
</ul>
<p>For each iteration....</p>
<p>Expectation step</p>
<ul>
<li>soft assignment of each observation to each model</li>
<li>the current parameter estimates are used to assign responsibilities according to the relative density of the training points under each model</li>
</ul>
<p>Maximization step</p>
<ul>
<li>these responsibilities are used in weighted maximum-likelihood fits to update the parameter estimates</li>
</ul>
<h5 id="em-algorithm-for-two-component-gmm">EM Algorithm for Two-component GMM<a class="headerlink" href="#em-algorithm-for-two-component-gmm" title="Permanent link">&para;</a></h5>
<ol>
<li>Take initial guesses for the parameters <span class="arithmatex"><span class="MathJax_Preview">\left(\mu_{1}, \sigma_{1}^{2}, \mu_{2}, \sigma_{2}^{2}, \pi\right)</span><script type="math/tex">\left(\mu_{1}, \sigma_{1}^{2}, \mu_{2}, \sigma_{2}^{2}, \pi\right)</script></span></li>
<li>Expectation Step: compute the responsibilities</li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\gamma}_{i}=\frac{\hat{\pi} \phi_{\hat{\theta}_{2}}\left(y_{i}\right)}{(1-\hat{\pi}) \phi_{\hat{\theta}_{1}}\left(y_{i}\right)+\hat{\pi} \phi_{\hat{\theta}_{2}}\left(y_{i}\right)}
</div>
<script type="math/tex; mode=display">
\hat{\gamma}_{i}=\frac{\hat{\pi} \phi_{\hat{\theta}_{2}}\left(y_{i}\right)}{(1-\hat{\pi}) \phi_{\hat{\theta}_{1}}\left(y_{i}\right)+\hat{\pi} \phi_{\hat{\theta}_{2}}\left(y_{i}\right)}
</script>
</div>
<ol start="3">
<li>Maximization Step: compute the weighted means and variances and the mixing probability</li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
\hat{\mu}_{1}=\frac{\sum_{i}\left(1-\hat{\gamma}_{i}\right) y_{i}}{\sum_{i}\left(1-\hat{\gamma}_{i}\right)}, \quad \hat{\sigma}_{1}^{2}=\frac{\sum_{i}\left(1-\hat{\gamma}_{i}\right)\left(y_{i}-\hat{\mu}_{1}\right)^{2}}{\sum_{i}\left(1-\hat{\gamma}_{i}\right)} \\
\hat{\mu}_{2}=\frac{\sum_{i} \hat{\gamma}_{i} y_{i}}{\sum_{i} \hat{\gamma}_{i}}, \quad \hat{\sigma}_{1}^{2}=\frac{\sum_{i} \hat{\gamma}_{i}\left(y_{i}-\hat{\mu}_{1}\right)^{2}}{\sum_{i} \hat{\gamma}_{i}} \\
\hat{\pi}=\sum_{i} \frac{\hat{\gamma}_{i}}{N}
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
\hat{\mu}_{1}=\frac{\sum_{i}\left(1-\hat{\gamma}_{i}\right) y_{i}}{\sum_{i}\left(1-\hat{\gamma}_{i}\right)}, \quad \hat{\sigma}_{1}^{2}=\frac{\sum_{i}\left(1-\hat{\gamma}_{i}\right)\left(y_{i}-\hat{\mu}_{1}\right)^{2}}{\sum_{i}\left(1-\hat{\gamma}_{i}\right)} \\
\hat{\mu}_{2}=\frac{\sum_{i} \hat{\gamma}_{i} y_{i}}{\sum_{i} \hat{\gamma}_{i}}, \quad \hat{\sigma}_{1}^{2}=\frac{\sum_{i} \hat{\gamma}_{i}\left(y_{i}-\hat{\mu}_{1}\right)^{2}}{\sum_{i} \hat{\gamma}_{i}} \\
\hat{\pi}=\sum_{i} \frac{\hat{\gamma}_{i}}{N}
\end{gathered}
</script>
</div>
<ol start="4">
<li>Iterate steps 2 and 3 until convergence</li>
</ol>
<h3 id="mcmc-sampling-from-posterior">MCMC - Sampling from Posterior<a class="headerlink" href="#mcmc-sampling-from-posterior" title="Permanent link">&para;</a></h3>
<p>åœ¨å®šä¹‰äº†ä¸€ä¸ªè´å¶æ–¯æ¨¡å‹åï¼Œæœ‰äººä¾¿æƒ³è¦ä»å¾—åˆ°çš„åéªŒåˆ†å¸ƒä¸­é‡‡æ ·æ¥å¯¹å‚æ•°è¿›è¡Œæ¨æ–­ï¼é™¤äº†ç®€å•çš„æ¨¡å‹ï¼Œè¿™é€šå¸¸æ˜¯ä¸€ä¸ªå¾ˆå›°éš¾çš„è®¡ç®—é—®é¢˜ï¼å‚è§ <a href="https://esl.hohoweiya.xyz/08-Model-Inference-and-Averaging/8.6-MCMC-for-Sampling-from-the-Posterior/index.html">here</a> çš„æ³¨é‡Š.</p>
<p>è¿™èŠ‚æˆ‘ä»¬è®¨è®ºç”¨äºåéªŒé‡‡æ ·çš„ é©¬å°”ç§‘å¤«è’™ç‰¹å¡æ´›æ³• (Markov chain Monte Carlo)ï¼æˆ‘ä»¬å°†è¦çœ‹åˆ°å‰å¸ƒæ–¯é‡‡æ ·ï¼ˆä¸€ä¸ª MCMC è¿‡ç¨‹ï¼‰ä¸ EM ç®—æ³•éå¸¸ç›¸å…³ï¼šä¸»è¦çš„åŒºåˆ«åœ¨äºå‰è€…ä»æ¡ä»¶åˆ†å¸ƒä¸­é‡‡æ ·è€Œéå¯¹å®ƒä»¬æœ€å¤§åŒ–ï¼</p>
<h4 id="markov-chain-monte-carlo-method">Markov Chain Monte Carlo Method<a class="headerlink" href="#markov-chain-monte-carlo-method" title="Permanent link">&para;</a></h4>
<p>Aim</p>
<ul>
<li>Generate independent samples <span class="arithmatex"><span class="MathJax_Preview">\left\{x^{(r)}\right\}_{r=1}^{R}</span><script type="math/tex">\left\{x^{(r)}\right\}_{r=1}^{R}</script></span> from a pdf <span class="arithmatex"><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span></li>
<li>Can then use <span class="arithmatex"><span class="MathJax_Preview">x^{(r)}</span><script type="math/tex">x^{(r)}</script></span> to estimate expectations of functions under this distribution</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
E[\phi(x)]=\int_{x} \phi(x) p(x) d x \approx \frac{1}{R} \sum_{r=1} \phi\left(x^{(r)}\right)
</div>
<script type="math/tex; mode=display">
E[\phi(x)]=\int_{x} \phi(x) p(x) d x \approx \frac{1}{R} \sum_{r=1} \phi\left(x^{(r)}\right)
</script>
</div>
<p>Not an easy task</p>
<ul>
<li>Sampling from <span class="arithmatex"><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> is hard in general</li>
<li>Especially when <span class="arithmatex"><span class="MathJax_Preview">x \in \mathbb{R}^{p}</span><script type="math/tex">x \in \mathbb{R}^{p}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> is large</li>
</ul>
<p>Common approach</p>
<ul>
<li>Monte Carlo Markov Chain methods such as Metropolis-Hastings and Gibbs sampling</li>
</ul>
<p>MCMC Assumptions åŸºæœ¬æ€è·¯å°±æ˜¯, æ‰¾åˆ°ä¸€ä¸ªå’Œ p æ¯”è¾ƒåƒçš„åˆ†å¸ƒæ¥è¿‘ä¼¼.</p>
<ul>
<li>Want to draw samples from <span class="arithmatex"><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span></li>
<li>Can evaluate <span class="arithmatex"><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> within a normalization factor</li>
<li>That is, can evaluate a function <span class="arithmatex"><span class="MathJax_Preview">p^{*}(x)</span><script type="math/tex">p^{*}(x)</script></span> such that</li>
</ul>
<p>$$
p(x)=\frac{p^{*}(x)}{Z}
$$
where <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> is a constant</p>
<h4 id="the-metropolis-hastings-method">The Metropolis-Hastings Method<a class="headerlink" href="#the-metropolis-hastings-method" title="Permanent link">&para;</a></h4>
<p>Initially</p>
<ul>
<li>Have an initial state <span class="arithmatex"><span class="MathJax_Preview">x^{(1)}</span><script type="math/tex">x^{(1)}</script></span></li>
<li>Define a proposal density <span class="arithmatex"><span class="MathJax_Preview">Q\left(x^{\prime} ; x^{(t)}\right)</span><script type="math/tex">Q\left(x^{\prime} ; x^{(t)}\right)</script></span> depending on the current state <span class="arithmatex"><span class="MathJax_Preview">x^{(t)}</span><script type="math/tex">x^{(t)}</script></span></li>
<li>Must be able to draw samples from <span class="arithmatex"><span class="MathJax_Preview">Q\left(x^{\prime} ; x^{(t)}\right)</span><script type="math/tex">Q\left(x^{\prime} ; x^{(t)}\right)</script></span></li>
</ul>
<p>At each iteration</p>
<ul>
<li>A <strong>tentative</strong> new state <span class="arithmatex"><span class="MathJax_Preview">x^{\prime}</span><script type="math/tex">x^{\prime}</script></span> is generated from the proposal density <span class="arithmatex"><span class="MathJax_Preview">Q\left(x^{\prime} ; x^{(t)}\right)</span><script type="math/tex">Q\left(x^{\prime} ; x^{(t)}\right)</script></span></li>
<li>Compute</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
a=\min \left(1, \frac{p^{*}\left(x^{\prime}\right) Q\left(x^{(t)} ; x^{\prime}\right)}{p^{*}\left(x^{(t)}\right) Q\left(x^{\prime} ; x^{(t)}\right)}\right)
</div>
<script type="math/tex; mode=display">
a=\min \left(1, \frac{p^{*}\left(x^{\prime}\right) Q\left(x^{(t)} ; x^{\prime}\right)}{p^{*}\left(x^{(t)}\right) Q\left(x^{\prime} ; x^{(t)}\right)}\right)
</script>
</div>
<ul>
<li>Accept new state <span class="arithmatex"><span class="MathJax_Preview">x^{\prime}</span><script type="math/tex">x^{\prime}</script></span> with probability a</li>
<li>Set</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
x^{(t+1)}= \begin{cases}x^{\prime} &amp; \text { if state is accepted } \\ x^{(t)} &amp; \text { if state is not accepted }\end{cases}
</div>
<script type="math/tex; mode=display">
x^{(t+1)}= \begin{cases}x^{\prime} & \text { if state is accepted } \\ x^{(t)} & \text { if state is not accepted }\end{cases}
</script>
</div>
<p><img alt="" src="../media/ASL-note3/2021-12-24-11-19-16.png" /></p>
<p>Convergence</p>
<ul>
<li>For any <span class="arithmatex"><span class="MathJax_Preview">Q</span><script type="math/tex">Q</script></span> s.t. <span class="arithmatex"><span class="MathJax_Preview">Q\left(x^{\prime} ; x\right)&gt;0 \forall x, x^{\prime}</span><script type="math/tex">Q\left(x^{\prime} ; x\right)>0 \forall x, x^{\prime}</script></span> as <span class="arithmatex"><span class="MathJax_Preview">t \rightarrow \infty</span><script type="math/tex">t \rightarrow \infty</script></span></li>
<li>The probability distribution of <span class="arithmatex"><span class="MathJax_Preview">x^{(t)}</span><script type="math/tex">x^{(t)}</script></span> tends to <span class="arithmatex"><span class="MathJax_Preview">p(x)=p^{*}(x) / Z</span><script type="math/tex">p(x)=p^{*}(x) / Z</script></span></li>
</ul>
<h4 id="gibbs-sampling">Gibbs Sampling<a class="headerlink" href="#gibbs-sampling" title="Permanent link">&para;</a></h4>
<p>Given a state <span class="arithmatex"><span class="MathJax_Preview">x^{(t)} \in \mathbb{R}^{p}</span><script type="math/tex">x^{(t)} \in \mathbb{R}^{p}</script></span>, generate a <strong>new state</strong> with</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
x_{1}^{(t+1)} \sim p\left(x_{1} \mid x_{2}^{(t)}, x_{3}^{(t)}, \ldots, x_{p}^{(t)}\right) \\
x_{2}^{(t+1)} \sim p\left(x_{2} \mid x_{1}^{(t+1)}, x_{3}^{(t)}, \ldots, x_{p}^{(t)}\right) \\
x_{3}^{(t+1)} \sim p\left(x_{3} \mid x_{1}^{(t+1)}, x_{2}^{(t+1)}, \ldots, x_{p}^{(t)}\right)
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
x_{1}^{(t+1)} \sim p\left(x_{1} \mid x_{2}^{(t)}, x_{3}^{(t)}, \ldots, x_{p}^{(t)}\right) \\
x_{2}^{(t+1)} \sim p\left(x_{2} \mid x_{1}^{(t+1)}, x_{3}^{(t)}, \ldots, x_{p}^{(t)}\right) \\
x_{3}^{(t+1)} \sim p\left(x_{3} \mid x_{1}^{(t+1)}, x_{2}^{(t+1)}, \ldots, x_{p}^{(t)}\right)
\end{gathered}
</script>
</div>
<p>where it is assumed that we can generate samples from <span class="arithmatex"><span class="MathJax_Preview">p\left(x_{i} \mid\left\{x_{j}\right\}_{j \neq i}\right)</span><script type="math/tex">p\left(x_{i} \mid\left\{x_{j}\right\}_{j \neq i}\right)</script></span>
Convergence</p>
<ul>
<li>Gibbs sampling is a Metropolis method</li>
<li>The probability distribution of <span class="arithmatex"><span class="MathJax_Preview">x^{(t)}</span><script type="math/tex">x^{(t)}</script></span> tends to <span class="arithmatex"><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> as <span class="arithmatex"><span class="MathJax_Preview">t \rightarrow \infty</span><script type="math/tex">t \rightarrow \infty</script></span>, as long as <span class="arithmatex"><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> does not have pathological properties</li>
</ul>
<p><img alt="" src="../media/ASL-note3/2021-12-24-10-57-52.png" /></p>
<h4 id="evolution-of-markov-chain">Evolution of Markov Chain<a class="headerlink" href="#evolution-of-markov-chain" title="Permanent link">&para;</a></h4>
<p>Markov chain defined by an initial <span class="arithmatex"><span class="MathJax_Preview">p^{(0)}(x)</span><script type="math/tex">p^{(0)}(x)</script></span> and a transition probability <span class="arithmatex"><span class="MathJax_Preview">T\left(x^{\prime} ; x\right)</span><script type="math/tex">T\left(x^{\prime} ; x\right)</script></span></p>
<p>Let <span class="arithmatex"><span class="MathJax_Preview">p^{(t)}(x)</span><script type="math/tex">p^{(t)}(x)</script></span> be the pdf of the state after <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> applications of the Markov chain</p>
<p>The pdf of the state at the <span class="arithmatex"><span class="MathJax_Preview">(t+1)_{t h}</span><script type="math/tex">(t+1)_{t h}</script></span> iteration of the Markov Chain is given by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
p^{(t+1)}\left(x^{\prime}\right)=\int_{x} T\left(x^{\prime} ; x\right) p^{(t)}(x) d x
</div>
<script type="math/tex; mode=display">
p^{(t+1)}\left(x^{\prime}\right)=\int_{x} T\left(x^{\prime} ; x\right) p^{(t)}(x) d x
</script>
</div>
<p>Want to find a chain s.t. as <span class="arithmatex"><span class="MathJax_Preview">t \rightarrow \infty</span><script type="math/tex">t \rightarrow \infty</script></span> then <span class="arithmatex"><span class="MathJax_Preview">p^{(t)}(x) \rightarrow p(x)</span><script type="math/tex">p^{(t)}(x) \rightarrow p(x)</script></span></p>
<p>When designing a MCMC method, construct a chain with the following properties</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> is an invariant distribution of the chain</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
p\left(x^{\prime}\right)=\int_{x} T\left(x^{\prime} ; x\right) p(x) d x
</div>
<script type="math/tex; mode=display">
p\left(x^{\prime}\right)=\int_{x} T\left(x^{\prime} ; x\right) p(x) d x
</script>
</div>
<ul>
<li>The chain is ergodic</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
p^{(t)}(x) \rightarrow p(x) \text { as } t \rightarrow \infty \text { for any } p^{(0)}(x)
</div>
<script type="math/tex; mode=display">
p^{(t)}(x) \rightarrow p(x) \text { as } t \rightarrow \infty \text { for any } p^{(0)}(x)
</script>
</div>
<h4 id="gibbs-sampling-for-gmm">Gibbs Sampling for GMM<a class="headerlink" href="#gibbs-sampling-for-gmm" title="Permanent link">&para;</a></h4>
<p>Close connection between Gibbs sampling and the EM algorithm in exponential family models</p>
<p>Let</p>
<ul>
<li>the parameter <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> of the distribution</li>
<li>latent / missing data <span class="arithmatex"><span class="MathJax_Preview">Z^{m}</span><script type="math/tex">Z^{m}</script></span></li>
</ul>
<p>be parameter for Gibbs sampler</p>
<p>To estimate the parameter of a GMM at each iteration</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;\Delta_{i}^{(t+1)} \sim p\left(\Delta_{i} \mid \theta^{(t)}, Z\right) \text { for } i=1, \ldots, n \\
&amp;\theta^{(t+1)} \sim p\left(\theta \mid \Delta^{(t+1)}, Z\right)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&\Delta_{i}^{(t+1)} \sim p\left(\Delta_{i} \mid \theta^{(t)}, Z\right) \text { for } i=1, \ldots, n \\
&\theta^{(t+1)} \sim p\left(\theta \mid \Delta^{(t+1)}, Z\right)
\end{aligned}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i} \in\{1, \ldots, K\}</span><script type="math/tex">\Delta_{i} \in\{1, \ldots, K\}</script></span> represents which component training example <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> is assigned to</p>
<h3 id="bagging">Bagging<a class="headerlink" href="#bagging" title="Permanent link">&para;</a></h3>
<p>æœ¬èŠ‚ä¸­æåˆ°äº†éšæœºæ£®æ—å’Œ boosting æ–¹å¼ï¼ISLR åœ¨è®²åŸºäºæ ‘çš„æ–¹æ³•æ—¶æ˜¯å°†è¿™ä¸¤ç§æ–¹å¼ä¸ bagging ä¸€èµ·ä»‹ç»çš„. ç®€å•æ¥è¯´ï¼Œbagging å’Œéšæœºæ£®æ—éƒ½æ˜¯é’ˆå¯¹ bootstrap æ ·æœ¬ï¼Œä¸”å‰è€…å¯ä»¥çœ‹æˆåè€…çš„ç‰¹æ®Šå½¢å¼ï¼›è€Œ boosting æ˜¯é’ˆå¯¹æ®‹å·®æ ·æœ¬ï¼</p>
<p>Starting point</p>
<ul>
<li>Training set <span class="arithmatex"><span class="MathJax_Preview">Z=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</span><script type="math/tex">Z=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{f}(x)</span><script type="math/tex">\hat{f}(x)</script></span> be the prediction at input <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> learned from <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span></li>
</ul>
<p>Goal</p>
<ul>
<li>Obtain a prediction at input <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> with lower variance than <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(x)</span><script type="math/tex">\hat{f}(x)</script></span></li>
</ul>
<p>How - Bootstrap aggregation a.k.a. <strong>Bagging</strong></p>
<ul>
<li>Obtain bootstrap samples <span class="arithmatex"><span class="MathJax_Preview">Z^{*1}, \ldots, Z^{* B}</span><script type="math/tex">Z^{*1}, \ldots, Z^{* B}</script></span></li>
<li>For each <span class="arithmatex"><span class="MathJax_Preview">Z^{*b}</span><script type="math/tex">Z^{*b}</script></span>, fit the model and get prediction <span class="arithmatex"><span class="MathJax_Preview">\hat{f}^{* b}(x)</span><script type="math/tex">\hat{f}^{* b}(x)</script></span></li>
<li>The bagged estimate is then</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}_{\text {bag }}(x)=\frac{1}{B} \sum_{b=1}^{B} \hat{f}^{* b}(x)
</div>
<script type="math/tex; mode=display">
\hat{f}_{\text {bag }}(x)=\frac{1}{B} \sum_{b=1}^{B} \hat{f}^{* b}(x)
</script>
</div>
<p><img alt="" src="../media/ASL-note3/2021-12-24-11-44-55.png" /></p>
<p>ä¸Šé¢çš„æ©™è‰²æ˜¯å¯¹äºåˆ†ç±»ç»“æœè¿›è¡Œ bagging, ç»¿è‰²æ˜¯å¯¹äºåˆ†ç±»æ¦‚ç‡è¿›è¡Œ bagging.</p>
<h4 id="bagging-for-classification-and-0-1-loss">Bagging for Classification and 0-1 Loss<a class="headerlink" href="#bagging-for-classification-and-0-1-loss" title="Permanent link">&para;</a></h4>
<p>Squared error loss</p>
<ul>
<li>Bagging can dramatically reduce the variance of unstable procedures, leading to improved prediction</li>
</ul>
<p>Classification with 0-1 loss [å› ä¸ºåå·®å’Œæ–¹å·®çš„ä¸å¯åŠ æ€§]</p>
<ul>
<li>Bagging a good classifier can make it better</li>
<li>Bagging a bad classifier can make things worse</li>
<li>Can understand the bagging effect in terms of a consensus of independent weak learners or the wisdom of crowds</li>
</ul>
<h3 id="model-averaging-stacking">Model Averaging &amp; Stacking<a class="headerlink" href="#model-averaging-stacking" title="Permanent link">&para;</a></h3>
<h4 id="bayesian-model-averaging">Bayesian Model Averaging<a class="headerlink" href="#bayesian-model-averaging" title="Permanent link">&para;</a></h4>
<p>Starting point</p>
<ul>
<li>Training data <span class="arithmatex"><span class="MathJax_Preview">X=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</span><script type="math/tex">X=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</script></span></li>
<li>A set of candidate models <span class="arithmatex"><span class="MathJax_Preview">M_{1}, \ldots, M_{M}</span><script type="math/tex">M_{1}, \ldots, M_{M}</script></span></li>
</ul>
<p>Goal</p>
<ul>
<li>Estimate quantity <span class="arithmatex"><span class="MathJax_Preview">\zeta</span><script type="math/tex">\zeta</script></span> - usually a prediction of <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> at <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span></li>
</ul>
<p>A Bayesian solution</p>
<ul>
<li>The posterior distribution of <span class="arithmatex"><span class="MathJax_Preview">\zeta</span><script type="math/tex">\zeta</script></span></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
p(\zeta \mid Z)=\sum_{m=1}^{M} p\left(\zeta \mid M_{m}, Z\right) P\left(M_{m} \mid Z\right)
</div>
<script type="math/tex; mode=display">
p(\zeta \mid Z)=\sum_{m=1}^{M} p\left(\zeta \mid M_{m}, Z\right) P\left(M_{m} \mid Z\right)
</script>
</div>
<ul>
<li>Posterior mean</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
E[\zeta \mid Z]=\sum_{m=1}^{M} E\left[\zeta \mid M_{m}, Z\right] P\left(M_{m} \mid Z\right)
</div>
<script type="math/tex; mode=display">
E[\zeta \mid Z]=\sum_{m=1}^{M} E\left[\zeta \mid M_{m}, Z\right] P\left(M_{m} \mid Z\right)
</script>
</div>
<p>å…³é”®æ˜¯ä¼°è®¡å•ä¸ªæ¨¡å‹çš„æ¦‚ç‡</p>
<p>Committee method å§”å‘˜ä¼šæ–¹æ³•å¯¹æ¯ä¸ªæ¨¡å‹èµ‹äºˆç›¸åŒçš„æƒé‡</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(M_{m} \mid Z\right) \approx \frac{1}{M}
</div>
<script type="math/tex; mode=display">
P\left(M_{m} \mid Z\right) \approx \frac{1}{M}
</script>
</div>
<p>BIC approach (7.7èŠ‚)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(M_{m} \mid Z\right) \approx-2 \log \operatorname{lik}+d_{m} \log (n)
</div>
<script type="math/tex; mode=display">
P\left(M_{m} \mid Z\right) \approx-2 \log \operatorname{lik}+d_{m} \log (n)
</script>
</div>
<p>Hardcore Bayesian å…¨è´å¶æ–¯æ–¹æ³•. [ç„¶è€Œï¼Œç›¸æ¯”æ›´ç®€å•çš„ BIC è¿‘ä¼¼ï¼Œæˆ‘ä»¬æ²¡æœ‰çœ‹åˆ°ä»»ä½•å®é™…çš„è¯æ®æ¥è¡¨æ˜å€¼å¾—è¿™æ ·åš]</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
P\left(M_{m} \mid Z\right) &amp; \propto P\left(M_{m}\right) p\left(Z \mid M_{m}\right) \\
&amp; \propto P\left(M_{m}\right) \int p\left(Z \mid \theta, M_{m}\right) p\left(\theta \mid M_{m}\right) d \theta_{m}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
P\left(M_{m} \mid Z\right) & \propto P\left(M_{m}\right) p\left(Z \mid M_{m}\right) \\
& \propto P\left(M_{m}\right) \int p\left(Z \mid \theta, M_{m}\right) p\left(\theta \mid M_{m}\right) d \theta_{m}
\end{aligned}
</script>
</div>
<h4 id="frequentist-model-averaging">Frequentist Model Averaging<a class="headerlink" href="#frequentist-model-averaging" title="Permanent link">&para;</a></h4>
<p>Starting point</p>
<ul>
<li>Predictions <span class="arithmatex"><span class="MathJax_Preview">\hat{f}_{1}(x), \hat{f}_{2}(x), \ldots, \hat{f}_{M}(x)</span><script type="math/tex">\hat{f}_{1}(x), \hat{f}_{2}(x), \ldots, \hat{f}_{M}(x)</script></span></li>
</ul>
<p>Goal</p>
<ul>
<li>For squared error loss, find weights <span class="arithmatex"><span class="MathJax_Preview">\omega=\left(\omega_{1}, \ldots, \omega_{M}\right)</span><script type="math/tex">\omega=\left(\omega_{1}, \ldots, \omega_{M}\right)</script></span> s.t.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\omega}=\operatorname{argmin}_{\omega} E_{P_{Y \mid X=X}}\left(Y-\sum_{m=1}^{M} \omega_{m} \hat{f}_{m}(x)\right)^{2}
</div>
<script type="math/tex; mode=display">
\hat{\omega}=\operatorname{argmin}_{\omega} E_{P_{Y \mid X=X}}\left(Y-\sum_{m=1}^{M} \omega_{m} \hat{f}_{m}(x)\right)^{2}
</script>
</div>
<p>Solution: <strong>population linear regression</strong> of <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> on <span class="arithmatex"><span class="MathJax_Preview">\hat{F}(x)</span><script type="math/tex">\hat{F}(x)</script></span> æ€»ä½“çº¿æ€§å›å½’ (population linear regression)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\omega}=E_{P}\left[\hat{F}(x) \hat{F}(x)^{\top}\right]^{-1} E_{p}[\hat{F}(x) Y]
</div>
<script type="math/tex; mode=display">
\hat{\omega}=E_{P}\left[\hat{F}(x) \hat{F}(x)^{\top}\right]^{-1} E_{p}[\hat{F}(x) Y]
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\hat{F}(x) \equiv\left[\hat{f}_{1}(x), \ldots, \hat{f}_{M}(x)\right]^{\top}</span><script type="math/tex">\hat{F}(x) \equiv\left[\hat{f}_{1}(x), \ldots, \hat{f}_{M}(x)\right]^{\top}</script></span></p>
<p>For this <span class="arithmatex"><span class="MathJax_Preview">\hat{\omega}</span><script type="math/tex">\hat{\omega}</script></span>, the full regression model has smaller error than any single model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
E_{P}\left(Y-\sum_{m=1}^{M} \omega_{m} \hat{f}_{m}(x)\right)^{2} \leqslant E_{P}\left(Y-\hat{f}_{m}(x)\right)^{2} \forall m
</div>
<script type="math/tex; mode=display">
E_{P}\left(Y-\sum_{m=1}^{M} \omega_{m} \hat{f}_{m}(x)\right)^{2} \leqslant E_{P}\left(Y-\hat{f}_{m}(x)\right)^{2} \forall m
</script>
</div>
<p>Combining models never makes things worse (at population level)</p>
<p>But cannot estimate the population <span class="arithmatex"><span class="MathJax_Preview">\hat{\omega}</span><script type="math/tex">\hat{\omega}</script></span>. Any thoughts?</p>
<p>æ˜¾ç„¶æ— æ³•ç›´æ¥è®¡ç®— population çº§åˆ«çš„çº¿æ€§å›å½’, è€Œç›´æ¥ç”¨è®­ç»ƒé›†ä¼šæœ‰é—®é¢˜, ä¾‹å¦‚çº¿æ€§å›å½’çš„å‡ ä¸ªæ¨¡å‹ä¸­, æ‰€é€‰å˜é‡æœ€å¤šçš„æ¨¡å‹è¡¨ç°æœ€å¥½, åˆ™ bagging å­¦ä¹ åˆ°çš„æƒé‡, å¤æ‚æ¨¡å‹çš„æƒé‡ä¼šè¿‡åˆ†é«˜(=1). å› æ­¤, ä¸‹é¢çš„ å †æ ˆæ³›åŒ– (Stacked generalization) æˆ– stacking æ–¹æ³•, é‡‡ç”¨ç±»ä¼¼ç•™ä¸€çš„æ€è·¯, ç•™å‡ºä¸€ä¸ª sample è¿›è¡Œè®­ç»ƒ.</p>
<h4 id="stacked-generalization-stacking">Stacked Generalization (stacking)<a class="headerlink" href="#stacked-generalization-stacking" title="Permanent link">&para;</a></h4>
<p>Denote <span class="arithmatex"><span class="MathJax_Preview">\hat{f}_{m}^{-i}(x)</span><script type="math/tex">\hat{f}_{m}^{-i}(x)</script></span> as the prediction at <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> using</p>
<ul>
<li>the <span class="arithmatex"><span class="MathJax_Preview">m_{\text {th }}</span><script type="math/tex">m_{\text {th }}</script></span> model</li>
<li>learnt from the dataset with <span class="arithmatex"><span class="MathJax_Preview">i_{t h}</span><script type="math/tex">i_{t h}</script></span> training example removed</li>
</ul>
<p>Then the stacking weights are given by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\omega}^{s t}=\operatorname{argmin}_{\omega} \sum_{i=1}^{n}\left(y_{i}-\sum_{m=1}^{M} \omega_{m} \hat{f}_{m}^{-i}\left(x_{i}\right)\right)^{2}
</div>
<script type="math/tex; mode=display">
\hat{\omega}^{s t}=\operatorname{argmin}_{\omega} \sum_{i=1}^{n}\left(y_{i}-\sum_{m=1}^{M} \omega_{m} \hat{f}_{m}^{-i}\left(x_{i}\right)\right)^{2}
</script>
</div>
<p>The final prediction at point <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{m} \hat{\omega}_{m}^{s t} \hat{f}_{m}(x)
</div>
<script type="math/tex; mode=display">
\sum_{m} \hat{\omega}_{m}^{s t} \hat{f}_{m}(x)
</script>
</div>
<ul>
<li>Better results by forcing <span class="arithmatex"><span class="MathJax_Preview">\hat{\omega}_{m}^{s t}</span><script type="math/tex">\hat{\omega}_{m}^{s t}</script></span> to be <span class="arithmatex"><span class="MathJax_Preview">\geqslant 0</span><script type="math/tex">\geqslant 0</script></span> and sum to 1</li>
<li>Stacking and model selection via leave-one-out cross-validation are closely related</li>
<li>Can apply stacking to other non-linear methods to combine predictions from different models</li>
</ul>
<h3 id="stochastic-search-bumping">Stochastic Search: Bumping<a class="headerlink" href="#stochastic-search-bumping" title="Permanent link">&para;</a></h3>
<p>è¿™ç« æè¿°çš„æœ€åä¸€ä¸ªæ–¹æ³•ä¸æ¶‰åŠå¹³å‡æˆ–è€…ç»“åˆæ¨¡å‹ï¼Œä½†æ˜¯æ˜¯å¯»æ‰¾ä¸€ä¸ªæ›´å¥½å•æ¨¡å‹çš„æ–¹æ³•ï¼Bumping é‡‡ç”¨ bootstrap é‡‡æ ·åœ¨æ¨¡å‹ç©ºé—´ä¸­éšæœºç§»åŠ¨ï¼å¯¹äºæ‹Ÿåˆæ–¹æ³•ç»å¸¸ä¼šæ‰¾åˆ°è®¸å¤šå±€éƒ¨æœ€å°å€¼çš„é—®é¢˜ï¼Œbumping å¯ä»¥å¸®åŠ©è¿™äº›æ–¹æ³•é¿å…é™·å…¥åçš„è§£ï¼</p>
<p>General Approach</p>
<ul>
<li>Draw bootstrap samples <span class="arithmatex"><span class="MathJax_Preview">Z^{*1}, \ldots, Z^{* B}</span><script type="math/tex">Z^{*1}, \ldots, Z^{* B}</script></span></li>
<li>For <span class="arithmatex"><span class="MathJax_Preview">b=1, \ldots,, B</span><script type="math/tex">b=1, \ldots,, B</script></span>
Fit the model to <span class="arithmatex"><span class="MathJax_Preview">Z^{*b}</span><script type="math/tex">Z^{*b}</script></span> giving <span class="arithmatex"><span class="MathJax_Preview">\hat{f}^{* b}(x)</span><script type="math/tex">\hat{f}^{* b}(x)</script></span></li>
<li>Choose the model obtained from bootstrap sample <span class="arithmatex"><span class="MathJax_Preview">\hat{b}</span><script type="math/tex">\hat{b}</script></span> which minimizes the training error</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{b}=\operatorname{argmin}_{b} \frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{f}^{* b}\left(x_{i}\right)\right)^{2}
</div>
<script type="math/tex; mode=display">
\hat{b}=\operatorname{argmin}_{b} \frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{f}^{* b}\left(x_{i}\right)\right)^{2}
</script>
</div>
<p>The model predictions are then <span class="arithmatex"><span class="MathJax_Preview">\hat{f}^{* \hat{b}}(x)</span><script type="math/tex">\hat{f}^{* \hat{b}}(x)</script></span></p>
<p><img alt="" src="../media/ASL-note3/2021-12-24-13-37-59.png" /></p>
<p>ä¾‹å¦‚, å¯¹äºå¼‚æˆ–(exclusive or)(XOR) é—®é¢˜è€Œè¨€, ç”±äºæ•°æ®å¹³è¡¡çš„ç‰¹æ€§, è´ªå¿ƒçš„ CART ç®—æ³•åœ¨ä¸¤ä¸ªç‰¹å¾ä¸Šéƒ½æ— æ³•æ‰¾åˆ°åˆé€‚çš„åˆ†å‰² (åˆ†å‰²çš„ç»“æœæ˜¯å¯èƒ½éšæœºçš„), å› æ­¤å¦‚å·¦å›¾æ— æ³•å¾—åˆ°ä¸€ä¸ªå¥½çš„ç»“æœ. è€Œåœ¨ bumping ç®—æ³•ä¸­, é€šè¿‡æ¯æ¬¡éšæœºé‡‡æ ·ä¸€ç³»åˆ—çš„ç‚¹, <strong>æ‰“ç ´äº†æ•°æ®çš„å¹³è¡¡</strong>, (ç„¶åæ ¹æ®æµ‹è¯•ç»“æœæ‰¾æœ€å¥½çš„æ¨¡å‹), ä»å³å›¾å¯ä»¥çœ‹åˆ°, ä¸€ä¸ª bumping 20æ¬¡çš„æ¨¡å‹å³å¯å¾—åˆ°ä¸€ä¸ªå¾ˆå¥½çš„ç»“æœ.</p>
<ul>
<li>Bumping pertubs the training data</li>
<li>Therefore explore different areas of the model space</li>
<li>Must ensure the complexity of each model ï¬t is comparable</li>
</ul>
<h2 id="additive-models-trees-related-methods">Additive Models, Trees, &amp; Related Methods<a class="headerlink" href="#additive-models-trees-related-methods" title="Permanent link">&para;</a></h2>
<p>è¿™ç« ä¸­æˆ‘ä»¬å¼€å§‹å¯¹ç›‘ç£å­¦ä¹ ä¸­ä¸€äº›ç‰¹å®šçš„æ–¹æ³•è¿›è¡Œè®¨è®ºï¼è¿™é‡Œæ¯ä¸ªæŠ€å·§éƒ½å‡è®¾äº† æœªçŸ¥å›å½’å‡½æ•°ï¼ˆä¸åŒçš„ï¼‰ç»“æ„å½¢å¼ï¼Œè€Œä¸”é€šè¿‡è¿™æ ·å¤„ç†å·§å¦™åœ°è§£å†³äº†ç»´æ•°ç¾éš¾ï¼ å½“ç„¶ï¼Œå®ƒä»¬è¦ä¸ºé”™è¯¯åœ°ç¡®å®šæ¨¡å‹ç±»å‹ä»˜å‡ºå¯èƒ½çš„ä»£ä»·ï¼Œæ‰€ä»¥åœ¨æ¯ç§æƒ…å½¢ä¸‹éƒ½éœ€è¦ åšå‡ºä¸€ä¸ªæƒè¡¡ï¼ç¬¬ 3-6 ç« ç•™ä¸‹çš„é—®é¢˜éƒ½å°†ç»§ç»­è®¨è®ºï¼æˆ‘ä»¬æè¿° 5 ä¸ªç›¸å…³çš„æŠ€å·§ï¼š å¹¿ä¹‰å¯åŠ æ¨¡å‹ (generalized addiÆŸve models)ï¼Œæ ‘ (trees)ï¼Œå¤šå…ƒè‡ªé€‚åº”å›å½’æ ·æ¡ (MARS)ï¼Œè€å¿ƒè§„åˆ™å½’çº³æ³• (PRIM)ï¼Œä»¥åŠ æ··åˆå±‚æ¬¡ä¸“å®¶ (HME)ï¼</p>
<ul>
<li>Generalized Additive Models</li>
<li>Tree Based Methods</li>
<li>Patient Rule Induction Method (PRIM)</li>
<li>Multivariate Adaptive Regression Splines - MARS</li>
<li>Hierarchical Mixture of Experts</li>
</ul>
<h3 id="generalized-additive-models">Generalized Additive Models<a class="headerlink" href="#generalized-additive-models" title="Permanent link">&para;</a></h3>
<p>å›å½’æ¨¡å‹åœ¨è®¸å¤šæ•°æ®åˆ†æä¸­èµ·ç€é‡è¦çš„ä½œç”¨ï¼Œæä¾›äº†é¢„æµ‹å’Œåˆ†ç±»çš„è§„åˆ™ã€ä»¥åŠç† è§£ä¸åŒè¾“å…¥å˜é‡é‡è¦æ€§çš„æ•°æ®åˆ†æå·¥å…·ï¼</p>
<p>å°½ç®¡éå¸¸ç®€å•ï¼Œä½†æ˜¯ä¼ ç»Ÿçš„çº¿æ€§æ¨¡å‹ç»å¸¸åœ¨è¿™äº›æƒ…å½¢ä¸‹å¤±æ•ˆï¼šå®é™…ç”Ÿæ´»ä¸­ï¼Œå˜é‡çš„å½±å“å¾€å¾€ä¸æ˜¯çº¿æ€§çš„ï¼åœ¨å‰é¢çš„ç« èŠ‚ä¸­æˆ‘ä»¬è®¨è®ºä½¿ç”¨é¢„å®šä¹‰çš„åŸºå‡½æ•°æ¥å®ç°é çº¿æ€§çš„æŠ€å·§ï¼è¿™éƒ¨åˆ†æè¿°æ›´å¤š è‡ªåŠ¨çµæ´» (automatic flexible) çš„ç»Ÿè®¡æ–¹æ³•æ¥è¯†åˆ«å’Œ è¡¨å¾éçº¿æ€§å›å½’çš„å½±å“ï¼è¿™äº›æ–¹æ³•è¢«ç§°ä¸ºâ€œå¹¿ä¹‰å¯åŠ æ¨¡å‹â€ï¼</p>
<p>A <strong>generalized additive model</strong> has the form</p>
<div class="arithmatex">
<div class="MathJax_Preview">
E\left(Y \mid X_{1}, \ldots, X_{p}\right)=\alpha+f_{1}\left(X_{1}\right)+f_{2}\left(X_{2}\right)+\ldots+f_{p}\left(X_{p}\right)
</div>
<script type="math/tex; mode=display">
E\left(Y \mid X_{1}, \ldots, X_{p}\right)=\alpha+f_{1}\left(X_{1}\right)+f_{2}\left(X_{2}\right)+\ldots+f_{p}\left(X_{p}\right)
</script>
</div>
<p>where each <span class="arithmatex"><span class="MathJax_Preview">f_{j}{ }^{\prime} s</span><script type="math/tex">f_{j}{ }^{\prime} s</script></span> are smooth, potentially non-parametric functions</p>
<p>Here we consider each <span class="arithmatex"><span class="MathJax_Preview">f_{j}</span><script type="math/tex">f_{j}</script></span> is fit using a <strong>scatter plot smoother</strong></p>
<ul>
<li>cubic smoothing spline</li>
<li>kernel smoother</li>
</ul>
<p>è¿›ä¸€æ­¥å®šä¹‰å¯¹äºäºŒåˆ†ç±»ä»»åŠ¡ Definition for Binary Classification</p>
<p>A generalized additive model has the form</p>
<div class="arithmatex">
<div class="MathJax_Preview">
g(P(Y=1 \mid X))=\alpha+f_{1}\left(X_{1}\right)+f_{2}\left(X_{2}\right)+\ldots+f_{p}\left(X_{p}\right)
</div>
<script type="math/tex; mode=display">
g(P(Y=1 \mid X))=\alpha+f_{1}\left(X_{1}\right)+f_{2}\left(X_{2}\right)+\ldots+f_{p}\left(X_{p}\right)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">g(\cdot)</span><script type="math/tex">g(\cdot)</script></span> is a <strong>link function</strong>.</p>
<p>Common link functions are</p>
<ul>
<li>Identity: <span class="arithmatex"><span class="MathJax_Preview">g(z)=z</span><script type="math/tex">g(z)=z</script></span><ul>
<li>used for linear and additive models for Gaussian response data</li>
<li>é«˜æ–¯å“åº”æ•°æ®</li>
</ul>
</li>
<li>Logit: <span class="arithmatex"><span class="MathJax_Preview">g(z)=\log \frac{z}{1-z}</span><script type="math/tex">g(z)=\log \frac{z}{1-z}</script></span><ul>
<li>æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹å’Œ logit é“¾æ¥å‡½æ•°å°†é¢„æµ‹å˜é‡ä¸äºŒè¿›åˆ¶å“åº”å˜é‡å…³è”èµ·æ¥</li>
<li>used for model binomial probabilities</li>
</ul>
</li>
<li>Log: <span class="arithmatex"><span class="MathJax_Preview">g(z)=\log (z)</span><script type="math/tex">g(z)=\log (z)</script></span><ul>
<li>used for log-linear or log-additive models for possion count data</li>
<li>æ³Šæ¾è®¡æ•°æ•°æ®</li>
</ul>
</li>
</ul>
<p>ä¸Šé¢çš„ä¸‰ç§æƒ…å½¢éƒ½æ˜¯æ¥è‡ªæŒ‡æ•°æ—é‡‡æ ·æ¨¡å‹ï¼Œå¦å¤–ä¹ŸåŒ…æ‹¬ Gamma åˆ†å¸ƒå’Œè´ŸäºŒé¡¹åˆ†å¸ƒï¼è¿™ä¸ªåˆ†å¸ƒæ—äº§ç”Ÿäº†è‘—åçš„å¹¿ä¹‰çº¿æ€§æ¨¡å‹ç±»ï¼Œå®ƒä»¬éƒ½æ˜¯ä»¥åŒæ ·çš„æ–¹å¼æ‰©å±•ä¸º<strong>å¹¿ä¹‰å¯åŠ æ¨¡å‹</strong>ï¼</p>
<p>Advantages of GAM</p>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">f_{j}^{\prime}</span><script type="math/tex">f_{j}^{\prime}</script></span> s are estimated in a flexible way, can reveal <strong>non-linear relationship</strong> between input <span class="arithmatex"><span class="MathJax_Preview">X_{j}</span><script type="math/tex">X_{j}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span></li>
<li>Efficient algorithms to fit them if <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> is not too large</li>
</ul>
<h4 id="fitting-gam">Fitting GAM<a class="headerlink" href="#fitting-gam" title="Permanent link">&para;</a></h4>
<p>Additive model [ç±»ä¼¼ 5.4 èŠ‚ä¸­è®¨è®ºçš„æƒ©ç½šå¹³æ–¹å’Œçš„ (5.9) å‡†åˆ™]</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Y=\alpha+f_{1}\left(X_{1}\right)+f_{2}\left(X_{2}\right)+\ldots+f_{p}\left(X_{p}\right)+\epsilon
</div>
<script type="math/tex; mode=display">
Y=\alpha+f_{1}\left(X_{1}\right)+f_{2}\left(X_{2}\right)+\ldots+f_{p}\left(X_{p}\right)+\epsilon
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">E(\epsilon)=0</span><script type="math/tex">E(\epsilon)=0</script></span></p>
<p>How to estimate the parameters?</p>
<ul>
<li>Training data <span class="arithmatex"><span class="MathJax_Preview">\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{n}</span><script type="math/tex">\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{n}</script></span></li>
<li>Minimize a penalized sum-of-squares</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
\operatorname{PRSS}\left(\alpha, f_{1}, f_{2}, \ldots, f_{n}\right)=
\sum_{i=1}^{n}\left(y_{i}-\alpha-\sum_{j=1}^{p} f_{j}\left(x_{i j}\right)\right)^{2}+\sum_{j=1}^{p} \lambda_{j} \int_{t} f_{j}^{\prime \prime}(t)^{2} d t
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
\operatorname{PRSS}\left(\alpha, f_{1}, f_{2}, \ldots, f_{n}\right)=
\sum_{i=1}^{n}\left(y_{i}-\alpha-\sum_{j=1}^{p} f_{j}\left(x_{i j}\right)\right)^{2}+\sum_{j=1}^{p} \lambda_{j} \int_{t} f_{j}^{\prime \prime}(t)^{2} d t
\end{gathered}
</script>
</div>
<p>where each <span class="arithmatex"><span class="MathJax_Preview">\lambda_{j} \geqslant 0</span><script type="math/tex">\lambda_{j} \geqslant 0</script></span></p>
<p>One solution</p>
<ul>
<li>Let each <span class="arithmatex"><span class="MathJax_Preview">f_{j}\left(X_{j}\right)</span><script type="math/tex">f_{j}\left(X_{j}\right)</script></span> be a <strong>cubic smoothing spline</strong> with knots at <span class="arithmatex"><span class="MathJax_Preview">x_{i j}</span><script type="math/tex">x_{i j}</script></span> and response <span class="arithmatex"><span class="MathJax_Preview">y_{i}</span><script type="math/tex">y_{i}</script></span> for <span class="arithmatex"><span class="MathJax_Preview">i=1, \ldots, n</span><script type="math/tex">i=1, \ldots, n</script></span> [å¯ä»¥è¯æ˜ä¸Šå¼çš„æœ€å°å€¼æ˜¯ã€Œå¯åŠ ä¸‰æ¬¡æ ·æ¡æ¨¡å‹ã€, å³åˆ†åˆ«å¯¹äºæ¯ä¸ªç»„åˆ† <span class="arithmatex"><span class="MathJax_Preview">X_j</span><script type="math/tex">X_j</script></span> è¿›è¡Œæ‹Ÿåˆ <span class="arithmatex"><span class="MathJax_Preview">f_j</span><script type="math/tex">f_j</script></span> ä¸‰æ¬¡æ ·æ¡]</li>
<li>This solution minimizes <span class="arithmatex"><span class="MathJax_Preview">P R S S\left(\alpha, f_{1}, f_{2}, \ldots, f_{n}\right)</span><script type="math/tex">P R S S\left(\alpha, f_{1}, f_{2}, \ldots, f_{n}\right)</script></span></li>
<li>However, it is not the only minimizer as <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> is not identifiable</li>
</ul>
<p>ä½†åœ¨æ²¡æœ‰çº¦æŸçš„æƒ…å†µä¸‹, è§£ä¸å”¯ä¸€, å› æ­¤è§„å®šæ¯ä¸ªç»„åˆ† <span class="arithmatex"><span class="MathJax_Preview">f_j</span><script type="math/tex">f_j</script></span> æ‹Ÿåˆçš„å‡å€¼ä¸º 0. æ­¤æ—¶, å½“è¾“å…¥çŸ©é˜µåˆ—æ»¡ç§©, ä¸Šå¼æœ‰å”¯ä¸€è§£.</p>
<p>To combat this, assume</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{i=1}^{n} f_{j}\left(x_{i j}\right)=0 \text { for } j=1, \ldots, p
</div>
<script type="math/tex; mode=display">
\sum_{i=1}^{n} f_{j}\left(x_{i j}\right)=0 \text { for } j=1, \ldots, p
</script>
</div>
<p>This assumption yields <span class="arithmatex"><span class="MathJax_Preview">\hat{\alpha}=\operatorname{ave}\left(y_{i}\right)</span><script type="math/tex">\hat{\alpha}=\operatorname{ave}\left(y_{i}\right)</script></span></p>
<p>If the data matrix</p>
<div class="arithmatex">
<div class="MathJax_Preview">
X=\left[\begin{array}{cccc}
x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1 n} \\
x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2 n} \\
\ldots &amp; \ldots &amp; \ldots &amp; \ldots \\
x_{p 1} &amp; x_{p 2} &amp; \ldots &amp; x_{p n}
\end{array}\right]
</div>
<script type="math/tex; mode=display">
X=\left[\begin{array}{cccc}
x_{11} & x_{12} & \ldots & x_{1 n} \\
x_{21} & x_{22} & \ldots & x_{2 n} \\
\ldots & \ldots & \ldots & \ldots \\
x_{p 1} & x_{p 2} & \ldots & x_{p n}
\end{array}\right]
</script>
</div>
<p>has <strong>full column rank</strong>, <span class="arithmatex"><span class="MathJax_Preview">\operatorname{PRSS}\left(\alpha, f_{1}, f_{2}, \ldots, f_{n}\right)</span><script type="math/tex">\operatorname{PRSS}\left(\alpha, f_{1}, f_{2}, \ldots, f_{n}\right)</script></span> is convex and the minimizer is unique.</p>
<p><span class="arithmatex"><span class="MathJax_Preview">\exists</span><script type="math/tex">\exists</script></span> a simple iterative procedure for finding this solution</p>
<h5 id="backfitting-algorithm">Backfitting Algorithm<a class="headerlink" href="#backfitting-algorithm" title="Permanent link">&para;</a></h5>
<p>æ ¹æ®ä¸Šé¢çš„å®šç†å¯ä»¥å¾—åˆ°å¦‚ä¸‹æ‹Ÿåˆç®—æ³•</p>
<p>Step 1: Initialize</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\alpha}=\frac{1}{n} \sum_{i} y_{i}, \quad f_{j} \equiv 0 \forall j
</div>
<script type="math/tex; mode=display">
\hat{\alpha}=\frac{1}{n} \sum_{i} y_{i}, \quad f_{j} \equiv 0 \forall j
</script>
</div>
<p>Step 2: Cycle until convergence <span class="arithmatex"><span class="MathJax_Preview">j=1,2, \ldots, p, 1,2, \ldots, p, 1,2, \ldots</span><script type="math/tex">j=1,2, \ldots, p, 1,2, \ldots, p, 1,2, \ldots</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
\hat{f}_{j} \leftarrow S_{j}\left[\left\{y_{i}-\hat{\alpha}-\sum_{k \neq j} \hat{f}_{k}\left(x_{i k}\right)\right\}_{i=1}^{n}\right] \\
\hat{f}_{j} \leftarrow \hat{f}_{j}-\frac{1}{n} \sum_{i=1}^{n} \hat{f}_{j}\left(x_{i j}\right)
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
\hat{f}_{j} \leftarrow S_{j}\left[\left\{y_{i}-\hat{\alpha}-\sum_{k \neq j} \hat{f}_{k}\left(x_{i k}\right)\right\}_{i=1}^{n}\right] \\
\hat{f}_{j} \leftarrow \hat{f}_{j}-\frac{1}{n} \sum_{i=1}^{n} \hat{f}_{j}\left(x_{i j}\right)
\end{gathered}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">S_{j}\left[\left\{y_{i}-\hat{\alpha}-\sum_{k \neq j} \hat{f}_{k}\left(x_{i k}\right)\right\}_{i=1}^{n}\right]</span><script type="math/tex">S_{j}\left[\left\{y_{i}-\hat{\alpha}-\sum_{k \neq j} \hat{f}_{k}\left(x_{i k}\right)\right\}_{i=1}^{n}\right]</script></span> denotes the <strong>cubic smoothing spine</strong> with knots at <span class="arithmatex"><span class="MathJax_Preview">x_{i j}</span><script type="math/tex">x_{i j}</script></span> and responses <span class="arithmatex"><span class="MathJax_Preview">y_{i}-\hat{\alpha}-\sum_{k \neq j} \hat{f}_{k}\left(x_{i k}\right)</span><script type="math/tex">y_{i}-\hat{\alpha}-\sum_{k \neq j} \hat{f}_{k}\left(x_{i k}\right)</script></span> for <span class="arithmatex"><span class="MathJax_Preview">i=1, \ldots, n</span><script type="math/tex">i=1, \ldots, n</script></span></p>
<p>Could use other smoothing operators for <span class="arithmatex"><span class="MathJax_Preview">S_{j}</span><script type="math/tex">S_{j}</script></span></p>
<p>[å®é™…ä¸Š, ç¬¬ 2(2) æ­¥ä¸æ˜¯å¿…éœ€çš„ï¼Œå› ä¸ºå…‰æ»‘æ ·æ¡å¯¹ 0 å‡å€¼å“åº”å˜é‡æ‹Ÿåˆå‡å€¼ä¸º 0. ä½†æœºå™¨èˆå…¥è¯¯å·®ä¼šå¯¼è‡´ä¸‹é™ï¼Œæ‰€ä»¥å»ºè®®è¿›è¡Œè°ƒæ•´.]</p>
<h4 id="example-additive-logistic-regression">Example: Additive Logistic Regression<a class="headerlink" href="#example-additive-logistic-regression" title="Permanent link">&para;</a></h4>
<p>Generalized Additive Logistic Model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\log \frac{P(Y=1 \mid X)}{P(Y=0 \mid X)}=\alpha+f_{1}\left(X_{1}\right)+\ldots+f_{p}\left(X_{p}\right)
</div>
<script type="math/tex; mode=display">
\log \frac{P(Y=1 \mid X)}{P(Y=0 \mid X)}=\alpha+f_{1}\left(X_{1}\right)+\ldots+f_{p}\left(X_{p}\right)
</script>
</div>
<p>Functions <span class="arithmatex"><span class="MathJax_Preview">f_{1}, \ldots, f_{p}</span><script type="math/tex">f_{1}, \ldots, f_{p}</script></span> estimated by a backfitting algorithm within a Newton-Rapson procedure</p>
<p>Goal: maximize the log-likelihood</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L=\sum_{i} L_{i}=\sum_{i}\left[y_{i} \log P\left(Y=1 \mid x_{i}\right)+\left(1-y_{i}\right) \log \left(Y=0 \mid x_{i}\right)\right]
</div>
<script type="math/tex; mode=display">
L=\sum_{i} L_{i}=\sum_{i}\left[y_{i} \log P\left(Y=1 \mid x_{i}\right)+\left(1-y_{i}\right) \log \left(Y=0 \mid x_{i}\right)\right]
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">P\left(Y=1 \mid x_{i}\right)=\frac{e^{\eta_{i}}}{1+e^{\eta_{i}}}</span><script type="math/tex">P\left(Y=1 \mid x_{i}\right)=\frac{e^{\eta_{i}}}{1+e^{\eta_{i}}}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\eta_{i}=\alpha+f_{1}\left(x_{i 1}\right)+\ldots+f_{p}\left(x_{i p}\right)</span><script type="math/tex">\eta_{i}=\alpha+f_{1}\left(x_{i 1}\right)+\ldots+f_{p}\left(x_{i p}\right)</script></span></p>
<p>How: iteratively perform unitl convergence</p>
<ul>
<li>Let each <span class="arithmatex"><span class="MathJax_Preview">\hat{\eta}_{i}=\hat{\alpha}+\sum \hat{f}_{j}\left(x_{i j}\right)</span><script type="math/tex">\hat{\eta}_{i}=\hat{\alpha}+\sum \hat{f}_{j}\left(x_{i j}\right)</script></span> be the estimate of <span class="arithmatex"><span class="MathJax_Preview">\eta_{i}</span><script type="math/tex">\eta_{i}</script></span> given the current estimates of the parameters <span class="arithmatex"><span class="MathJax_Preview">\alpha, f_{1}, \ldots, f_{p}</span><script type="math/tex">\alpha, f_{1}, \ldots, f_{p}</script></span></li>
<li>Use a <strong>Newton-Raphson update</strong> step to produce a new estimate, <span class="arithmatex"><span class="MathJax_Preview">\hat{\eta}_{i}^{n e w}</span><script type="math/tex">\hat{\eta}_{i}^{n e w}</script></span>, of <span class="arithmatex"><span class="MathJax_Preview">\eta_{i}</span><script type="math/tex">\eta_{i}</script></span> s.t. <span class="arithmatex"><span class="MathJax_Preview">L_{i}\left(\hat{\eta}_{i}^{n e w}\right) \geqslant L_{i}\left(\hat{\eta}_{i}\right)</span><script type="math/tex">L_{i}\left(\hat{\eta}_{i}^{n e w}\right) \geqslant L_{i}\left(\hat{\eta}_{i}\right)</script></span> for each <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> [å› ä¸ºè¿™é‡Œæ˜¯äºŒå€¼å“åº”å˜é‡æ²¡æœ‰å…·ä½“çš„æ•°å€¼å“åº”, æ‰€ä»¥ç”¨ Newton æ–¹æ³•æ¥ç”Ÿæˆä¸€ä¸ªè¿‘ä¼¼çš„å€¼?]</li>
<li>Use Backfitting to fit an additive model to the targets <span class="arithmatex"><span class="MathJax_Preview">\hat{\eta}_{i}^{n e w} \forall i</span><script type="math/tex">\hat{\eta}_{i}^{n e w} \forall i</script></span></li>
<li>This produces new estimates of <span class="arithmatex"><span class="MathJax_Preview">\hat{\alpha}, \hat{f}_{j} \forall j</span><script type="math/tex">\hat{\alpha}, \hat{f}_{j} \forall j</script></span></li>
</ul>
<h4 id="gam-summary">GAM - Summary<a class="headerlink" href="#gam-summary" title="Permanent link">&para;</a></h4>
<p>Pros</p>
<ul>
<li>Extension of linear models - more flexible but still interpretable</li>
<li>Parameter estimate via Backfitting method is simple</li>
<li><strong>Backfitting</strong> allows the appropriate fitting method for each input variable å…è®¸å¯¹äºæ¯ä¸ªè¾“å…¥å˜é‡å»é€‰æ‹©åˆé€‚çš„æ‹Ÿåˆæ–¹æ³•</li>
</ul>
<p>Cons</p>
<ul>
<li>No feature selection is performed</li>
<li>Backfitting is not feasible for large <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span></li>
</ul>
<p>For large <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> <strong>forward stagewise fitting</strong> (such as boosting ç¬¬10ç« ) can be a solution&hellip;</p>
<h3 id="tree-based-methods">Tree Based Methods<a class="headerlink" href="#tree-based-methods" title="Permanent link">&para;</a></h3>
<p><img alt="" src="../media/ASL-note3/2021-12-26-10-12-06.png" /></p>
<h4 id="regression-tree">Regression Tree<a class="headerlink" href="#regression-tree" title="Permanent link">&para;</a></h4>
<p>Aim: approximate a regression function <span class="arithmatex"><span class="MathJax_Preview">f: \mathbb{R}^{p} \rightarrow \mathbb{R}</span><script type="math/tex">f: \mathbb{R}^{p} \rightarrow \mathbb{R}</script></span> with</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}(x)=\sum_{i=1}^{M} f_{m}(x) l\left(x \in R_{m}\right)
</div>
<script type="math/tex; mode=display">
\hat{f}(x)=\sum_{i=1}^{M} f_{m}(x) l\left(x \in R_{m}\right)
</script>
</div>
<p>with regions <span class="arithmatex"><span class="MathJax_Preview">R_{1}, \ldots, R_{M}</span><script type="math/tex">R_{1}, \ldots, R_{M}</script></span> partition <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{p}</span><script type="math/tex">\mathbb{R}^{p}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">f_{m}: \mathbb{R}^{p} \rightarrow \mathbb{R}</span><script type="math/tex">f_{m}: \mathbb{R}^{p} \rightarrow \mathbb{R}</script></span></p>
<p>Challenge: (assuming a specific form for <span class="arithmatex"><span class="MathJax_Preview">f_{m}{ }^{\prime} \mathrm{s}</span><script type="math/tex">f_{m}{ }^{\prime} \mathrm{s}</script></span> )</p>
<ul>
<li>Find <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> and the regions <span class="arithmatex"><span class="MathJax_Preview">R_{1}, \ldots, R_{M}</span><script type="math/tex">R_{1}, \ldots, R_{M}</script></span> s.t. <span class="arithmatex"><span class="MathJax_Preview">\hat{f} \approx f</span><script type="math/tex">\hat{f} \approx f</script></span></li>
<li>For training data <span class="arithmatex"><span class="MathJax_Preview">\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)</span><script type="math/tex">\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)</script></span> with each <span class="arithmatex"><span class="MathJax_Preview">x_{i} \in \mathbb{R}^{p}, y_{i} \in \mathbb{R}</span><script type="math/tex">x_{i} \in \mathbb{R}^{p}, y_{i} \in \mathbb{R}</script></span></li>
</ul>
<h5 id="piecewise-constant">Piecewise Constant<a class="headerlink" href="#piecewise-constant" title="Permanent link">&para;</a></h5>
<p>Let <span class="arithmatex"><span class="MathJax_Preview">f_{m}(x)=c_{m}</span><script type="math/tex">f_{m}(x)=c_{m}</script></span> such that the regression function becomes</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f(x)=\sum_{i=1}^{M} c_{m} l\left(x \in R_{m}\right)
</div>
<script type="math/tex; mode=display">
f(x)=\sum_{i=1}^{M} c_{m} l\left(x \in R_{m}\right)
</script>
</div>
<p>If we know the regions <span class="arithmatex"><span class="MathJax_Preview">R_{1}, \ldots, R_{M}</span><script type="math/tex">R_{1}, \ldots, R_{M}</script></span> then to minimize</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{argmin}_{c_{1}, \ldots, c_{n}} \sum_{i=1}^{n}\left(y_{i}-\sum_{m=1}^{M} c_{m} l\left(x_{i} \in \mathbb{R}_{m}\right)\right)^{2}
</div>
<script type="math/tex; mode=display">
\operatorname{argmin}_{c_{1}, \ldots, c_{n}} \sum_{i=1}^{n}\left(y_{i}-\sum_{m=1}^{M} c_{m} l\left(x_{i} \in \mathbb{R}_{m}\right)\right)^{2}
</script>
</div>
<p>one would set</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{c}_{m}=\frac{\sum_{i} \sum_{m} y_{i} l\left(x_{i} \in R_{m}\right)}{\sum_{i} \sum_{m} I\left(x_{i} \in R_{m}\right)}
</div>
<script type="math/tex; mode=display">
\hat{c}_{m}=\frac{\sum_{i} \sum_{m} y_{i} l\left(x_{i} \in R_{m}\right)}{\sum_{i} \sum_{m} I\left(x_{i} \in R_{m}\right)}
</script>
</div>
<p>ä¹Ÿå³, é¢„æµ‹ä¸ºæ¯ä¸ª region ä¸Šçš„å“åº”å˜é‡å‡å€¼. ä½†æ‰¾å‡ºä½¿å¾—å¹³æ–¹å’Œæœ€å°çš„æœ€ä¼˜äºŒåˆ†åœ¨è®¡ç®—ä¸Šä¸€èˆ¬æ˜¯ä¸å¯è¡Œçš„ï¼å› æ­¤æˆ‘ä»¬é‡‡ç”¨ä¸€ç§è´ªå©ªç®—æ³•.</p>
<h5 id="finding-the-optimal-partition">Finding the Optimal Partition<a class="headerlink" href="#finding-the-optimal-partition" title="Permanent link">&para;</a></h5>
<p>First step of the greedy approach</p>
<ul>
<li>Let: <span class="arithmatex"><span class="MathJax_Preview">R_{1}(j, s)=\left\{X \mid X_{j} \leqslant s\right\}</span><script type="math/tex">R_{1}(j, s)=\left\{X \mid X_{j} \leqslant s\right\}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">R_{2}(j, s)=\left\{X \mid X_{j}&gt;s\right\}</span><script type="math/tex">R_{2}(j, s)=\left\{X \mid X_{j}>s\right\}</script></span></li>
<li>Choose <span class="arithmatex"><span class="MathJax_Preview">(j, s)</span><script type="math/tex">(j, s)</script></span> to minimize</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min_{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}
</div>
<script type="math/tex; mode=display">
\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min_{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}
</script>
</div>
<ul>
<li>For a fixed <span class="arithmatex"><span class="MathJax_Preview">(j, s)</span><script type="math/tex">(j, s)</script></span> the minimum occurs when</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{c}_{k}=A v e\left(y_{i} \mid\left(x_{i}, y_{i}\right) \in X\right) \text { and } x_{i} \in R_{k}(j, s) \text { for } k=1,2
</div>
<script type="math/tex; mode=display">
\hat{c}_{k}=A v e\left(y_{i} \mid\left(x_{i}, y_{i}\right) \in X\right) \text { and } x_{i} \in R_{k}(j, s) \text { for } k=1,2
</script>
</div>
<ul>
<li>Determination of best pair <span class="arithmatex"><span class="MathJax_Preview">(j, s)</span><script type="math/tex">(j, s)</script></span> feasible as for each <span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> only have to check <span class="arithmatex"><span class="MathJax_Preview">\leqslant n+1</span><script type="math/tex">\leqslant n+1</script></span> values of <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span></li>
</ul>
<p>Full Greedy Recursion</p>
<ul>
<li>Once the best split <span class="arithmatex"><span class="MathJax_Preview">(j, s)</span><script type="math/tex">(j, s)</script></span> is found
  1. Partition the data <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;X_{1}=\left\{\left(x_{i}, y_{i}\right) \mid\left(x_{i}, y_{i}\right) \in X\right\} \text { and } x_{i} \in R_{1}(j, s) \\
&amp;X_{2}=\left\{\left(x_{i}, y_{i}\right) \mid\left(x_{i}, y_{i}\right) \in X\right\} \text { and } x_{i} \in R_{2}(j, s)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&X_{1}=\left\{\left(x_{i}, y_{i}\right) \mid\left(x_{i}, y_{i}\right) \in X\right\} \text { and } x_{i} \in R_{1}(j, s) \\
&X_{2}=\left\{\left(x_{i}, y_{i}\right) \mid\left(x_{i}, y_{i}\right) \in X\right\} \text { and } x_{i} \in R_{2}(j, s)
\end{aligned}
</script>
</div>
<p>based on the two resulting region
  2. Repeat the splitting process on both <span class="arithmatex"><span class="MathJax_Preview">X_{1}</span><script type="math/tex">X_{1}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">X_{2}</span><script type="math/tex">X_{2}</script></span></p>
<ul>
<li>The process above is recursively repeated on all the resulting subset of data points <span class="arithmatex"><span class="MathJax_Preview">X_{i}</span><script type="math/tex">X_{i}</script></span> until <span class="arithmatex"><span class="MathJax_Preview">\left|X_{i}\right|</span><script type="math/tex">\left|X_{i}\right|</script></span> is too small</li>
<li>The best splits found in this recursive are recoded in a binary tree</li>
</ul>
<p>Growing a Tree Recursively</p>
<p>How large should the tree be?</p>
<ul>
<li>Very large trees may over fit the data</li>
<li>Small tree may not capture the structure in the data</li>
</ul>
<p>æ ‘å¤ªå¤§ä¼šè¿‡æ‹Ÿåˆæ•°æ®ï¼Œè€Œæ ‘å¤ªå°åˆ™å¯èƒ½æ•æ‰ä¸äº†é‡è¦çš„ç»“æ„. å¦å¤–, ä¸Šé¢çš„çœ‹ä¼¼æ— ç”¨çš„åˆ†å‰²æˆ–è®¸ä¼šå¯¼è‡´ä¸‹é¢å¾—åˆ°å¾ˆå¥½çš„åˆ†å‰². æ›´å¥½çš„ç­–ç•¥æ˜¯ç”Ÿæˆä¸€ä¸ªå¤§æ ‘ <span class="arithmatex"><span class="MathJax_Preview">T_0</span><script type="math/tex">T_0</script></span>ï¼Œåªæœ‰å½“è¾¾åˆ°æœ€å°ç»“ç‚¹è§„æ¨¡ï¼ˆæ¯”å¦‚ 5ï¼‰æ‰åœæ­¢åˆ†å‰²è¿‡ç¨‹ï¼æ¥ç€å¤§æ ‘é‡‡ç”¨ æˆæœ¬å¤æ‚åº¦å‰ªæ (cost-complexity pruning) æ¥è°ƒæ•´</p>
<p>Common solution</p>
<ul>
<li>Grow a large tree <span class="arithmatex"><span class="MathJax_Preview">T_{0}</span><script type="math/tex">T_{0}</script></span></li>
<li>Prune <span class="arithmatex"><span class="MathJax_Preview">T_{0}</span><script type="math/tex">T_{0}</script></span> using cost-complexity pruning</li>
</ul>
<h4 id="cost-complexity-pruning">Cost-complexity pruning<a class="headerlink" href="#cost-complexity-pruning" title="Permanent link">&para;</a></h4>
<ul>
<li>Pruning <span class="arithmatex"><span class="MathJax_Preview">T_{0}</span><script type="math/tex">T_{0}</script></span> corresponds to collapsing any number of its internal nodes</li>
<li>Let <span class="arithmatex"><span class="MathJax_Preview">T_{T}</span><script type="math/tex">T_{T}</script></span> contain the indices of the terminal nodes in tree <span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span></li>
<li>Define</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
C_{\alpha}(T)=\sum_{m \in T_{T}} n_{m} Q_{m}(T)+\alpha|T|
</div>
<script type="math/tex; mode=display">
C_{\alpha}(T)=\sum_{m \in T_{T}} n_{m} Q_{m}(T)+\alpha|T|
</script>
</div>
<p>where</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
n_{m}=\#\left\{x_{i} \in R_{m}\right\} \\
Q_{m}(T)=\frac{1}{n_{m}} \sum_{x_{i} \in R_{m}}\left(y_{i}-\hat{c}_{m}\right)^{2} \text { with } \hat{c}_{m}=\frac{1}{n_{m}} \sum_{x_{i} \in R_{m}} y_{i}
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
n_{m}=\#\left\{x_{i} \in R_{m}\right\} \\
Q_{m}(T)=\frac{1}{n_{m}} \sum_{x_{i} \in R_{m}}\left(y_{i}-\hat{c}_{m}\right)^{2} \text { with } \hat{c}_{m}=\frac{1}{n_{m}} \sum_{x_{i} \in R_{m}} y_{i}
\end{gathered}
</script>
</div>
<ul>
<li>For a given <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>, find the subtree <span class="arithmatex"><span class="MathJax_Preview">T_{\alpha} \subseteq T</span><script type="math/tex">T_{\alpha} \subseteq T</script></span> that minimizes <span class="arithmatex"><span class="MathJax_Preview">C_{\alpha}(T)</span><script type="math/tex">C_{\alpha}(T)</script></span></li>
</ul>
<p>How æ€ä¹ˆè‡ªé€‚åº”åœ°é€‰æ‹© <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>? - <strong>weakest link pruning</strong> æœ€å·®è¿æ¥å‰ªæ</p>
<ul>
<li>Successively collapse the internal node that produces the smallest per-node increase in</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{m} n_{m} Q_{m}(T)
</div>
<script type="math/tex; mode=display">
\sum_{m} n_{m} Q_{m}(T)
</script>
</div>
<p>until left with a one node tree</p>
<ul>
<li>This sequence of collapsed trees contains <span class="arithmatex"><span class="MathJax_Preview">T_{\alpha}</span><script type="math/tex">T_{\alpha}</script></span></li>
</ul>
<h4 id="classification-tree">Classification Tree<a class="headerlink" href="#classification-tree" title="Permanent link">&para;</a></h4>
<p>Definitions needed for node impurity measures</p>
<ul>
<li>In node <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>, representing a region <span class="arithmatex"><span class="MathJax_Preview">R_{m}</span><script type="math/tex">R_{m}</script></span> with <span class="arithmatex"><span class="MathJax_Preview">n_{m}</span><script type="math/tex">n_{m}</script></span> observations</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{p}_{m k}=\frac{1}{n_{m}} \sum_{x_{i} \in R_{m}} I\left(y_{i}=k\right)
</div>
<script type="math/tex; mode=display">
\hat{p}_{m k}=\frac{1}{n_{m}} \sum_{x_{i} \in R_{m}} I\left(y_{i}=k\right)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\hat{p}_{m k}</span><script type="math/tex">\hat{p}_{m k}</script></span> is the proportion of class <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> observations in node <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span></p>
<ul>
<li>Classify the observation in node <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> to class
<span class="arithmatex"><span class="MathJax_Preview">k(m)=\operatorname{argmax}_{k} \hat{p}_{m k}</span><script type="math/tex">k(m)=\operatorname{argmax}_{k} \hat{p}_{m k}</script></span>
the majority vote in node <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span></li>
</ul>
<p>å¦‚ä½•è¡¡é‡èŠ‚ç‚¹çš„ä¸çº¯åº¦? Different measures of node impurity</p>
<ul>
<li>Misclassification error</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{1}{n_{m}} \sum_{i \in R_{m}} I\left(y_{i} \neq k(m)\right)=1-\hat{p}_{m k(m)}
</div>
<script type="math/tex; mode=display">
\frac{1}{n_{m}} \sum_{i \in R_{m}} I\left(y_{i} \neq k(m)\right)=1-\hat{p}_{m k(m)}
</script>
</div>
<ul>
<li>Gini index</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{k=1}^{k} \hat{p}_{m k}\left(1-\hat{p}_{m k}\right)
</div>
<script type="math/tex; mode=display">
\sum_{k=1}^{k} \hat{p}_{m k}\left(1-\hat{p}_{m k}\right)
</script>
</div>
<ul>
<li>Cross-entropy or deviance</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
-\sum_{k=1}^{K} \hat{p}_{m k} \log \hat{p}_{m k}
</div>
<script type="math/tex; mode=display">
-\sum_{k=1}^{K} \hat{p}_{m k} \log \hat{p}_{m k}
</script>
</div>
<p>åœ¨äºŒåˆ†ç±»ä»»åŠ¡ä¸Šæ¯”è¾ƒè¿™å‡ ä¸ªæŒ‡æ ‡. For binary classification, let <span class="arithmatex"><span class="MathJax_Preview">p=\hat{p}_{m 0}</span><script type="math/tex">p=\hat{p}_{m 0}</script></span></p>
<ul>
<li>Misclassification error: <span class="arithmatex"><span class="MathJax_Preview">1-\max (p, 1-p)</span><script type="math/tex">1-\max (p, 1-p)</script></span></li>
<li>Gini index: <span class="arithmatex"><span class="MathJax_Preview">2 p(1-p)</span><script type="math/tex">2 p(1-p)</script></span></li>
<li>Cross-entropy or deviance: <span class="arithmatex"><span class="MathJax_Preview">-p \log p-(1-p) \log (1-p)</span><script type="math/tex">-p \log p-(1-p) \log (1-p)</script></span></li>
</ul>
<p>æ¯”è¾ƒ</p>
<ul>
<li>[æ³¨æ„åˆ°å›¾ä¸­äº¤å‰ç†µè¿›è¡Œäº†ç¼©å°ä½¿å…¶ç»è¿‡åŒä¸€ç‚¹.]</li>
<li>è¿™ä¸‰è€…éƒ½ç±»ä¼¼ï¼Œä½†æ˜¯ äº¤å‰ç†µ (cross-entropy) å’Œ åŸºå°¼æŒ‡æ•° (Gini index) æ˜¯å¯å¾®çš„ï¼Œå› æ­¤æ›´åŠ é€‚åˆæ•°å€¼ä¼˜åŒ–.</li>
<li>å¦å¤–ï¼Œäº¤å‰ç†µå’ŒåŸºå°¼æŒ‡æ•°æ¯”è¯¯åˆ†ç±»ç‡å¯¹ç»“ç‚¹æ¦‚ç‡çš„æ”¹å˜æ›´åŠ æ•æ„Ÿ</li>
</ul>
<p><img alt="" src="../media/ASL-note3/2021-12-26-15-57-40.png" /></p>
<ul>
<li>Cost of binary split of node <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> into nodes <span class="arithmatex"><span class="MathJax_Preview">m_{1}</span><script type="math/tex">m_{1}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">m_{2}</span><script type="math/tex">m_{2}</script></span> is</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{1}{n_{m 1}} Q_{m 1}+\frac{1}{n_{m 2}} Q_{m 2}
</div>
<script type="math/tex; mode=display">
\frac{1}{n_{m 1}} Q_{m 1}+\frac{1}{n_{m 2}} Q_{m 2}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">Q_{m 1}</span><script type="math/tex">Q_{m 1}</script></span> is the impurity measure of node <span class="arithmatex"><span class="MathJax_Preview">m_{1}</span><script type="math/tex">m_{1}</script></span>, similarly for <span class="arithmatex"><span class="MathJax_Preview">Q_{m 2}</span><script type="math/tex">Q_{m 2}</script></span></p>
<ul>
<li>Cross-entropy and Gini are more sensitive to changes in the node probabilities than Misclassification rate</li>
<li>Cross-entropy and Gini measures used to grow the trees (åˆ†å‰²èŠ‚ç‚¹çš„è¿‡ç¨‹ä¸­å¤šç”¨ äº¤å‰ç†µæˆ–Gini)</li>
<li>All measures used to prune tree</li>
</ul>
<h4 id="problems-with-trees">Problems with Trees<a class="headerlink" href="#problems-with-trees" title="Permanent link">&para;</a></h4>
<ul>
<li>Instability<ul>
<li>Tress have high variance due to hierarchical search process</li>
<li>Errors at top nodes propogate to lower ones<ul>
<li>Small change in training data can give very different splits</li>
</ul>
</li>
</ul>
</li>
<li>Lack of smoothness<ul>
<li>Regression trees response surface not smooth</li>
<li>Hit problems when underlying function is smooth</li>
</ul>
</li>
<li>Difficulty in capturing additive structures å¯¹åŠ æ€§ç»“æ„å»ºæ¨¡çš„å›°éš¾<ul>
<li>The binary tree structure precludes the discovery of additive structure like
  <span class="arithmatex"><span class="MathJax_Preview">Y=c_{1} I\left(X_{1}&lt;t_{1}\right)+c_{2} I\left(X_{2}&lt;t_{2}\right)+\epsilon</span><script type="math/tex">Y=c_{1} I\left(X_{1}<t_{1}\right)+c_{2} I\left(X_{2}<t_{2}\right)+\epsilon</script></span></li>
<li>ä¾‹å¦‚, åœ¨å›å½’é—®é¢˜ä¸­, å‡è®¾ <span class="arithmatex"><span class="MathJax_Preview">Y=c_{1} I\left(X_{1}&lt;t_{1}\right)+c_{2} I\left(X_{2}&lt;t_{2}\right)+\varepsilon</span><script type="math/tex">Y=c_{1} I\left(X_{1}<t_{1}\right)+c_{2} I\left(X_{2}<t_{2}\right)+\varepsilon</script></span>, å…¶ä¸­ <span class="arithmatex"><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span> æ˜¯ 0 å‡å€¼å™ªå£°. åˆ™äºŒå‰æ ‘ä¼šåœ¨ <span class="arithmatex"><span class="MathJax_Preview">t_{1}</span><script type="math/tex">t_{1}</script></span> é™„è¿‘å¯¹ <span class="arithmatex"><span class="MathJax_Preview">X_{1}</span><script type="math/tex">X_{1}</script></span> åšç¬¬ ä¸€æ¬¡åˆ†å‰². ä¸ºäº†æ•æ‰åŠ æ€§ç»“æ„, ä¸‹ä¸€å±‚ä¸¤ä¸ªç»“ç‚¹éƒ½éœ€è¦åœ¨ <span class="arithmatex"><span class="MathJax_Preview">t_{2}</span><script type="math/tex">t_{2}</script></span> å¤„å¯¹ <span class="arithmatex"><span class="MathJax_Preview">X_{2}</span><script type="math/tex">X_{2}</script></span> åˆ†å‰². åœ¨å……è¶³æ•°æ®æƒ…å†µä¸‹è¿™ ä¸ªæˆ–è®¸å¯ä»¥å‘ç”Ÿ, ä½†æ˜¯è¿™ä¸ªæ¨¡å‹æ²¡æœ‰ç»™äºˆç‰¹åˆ«é¼“åŠ±æ¥æ‰¾åˆ°è¿™ä¸€ç»“æ„. å¦‚æœæœ‰ 10 ä¸ªè€Œä¸æ˜¯ 2 ä¸ªåŠ æ€§ å½±å“, éœ€è¦èŠ±è´¹å¾ˆå¤šå¶ç„¶çš„åˆ†å‰²æ¥é‡é€ è¿™ä¸ªç»“æ„, å¹¶ä¸”åœ¨ä¼°è®¡çš„æ ‘ä¸­, é€šè¿‡æ•°æ®åˆ†ææ¥è¯†åˆ«å®ƒä¼šå˜ å¾—éå¸¸å›°éš¾. åŸå› å†ä¸€æ¬¡å¯ä»¥å½’ç»“ä¸ºäºŒå‰æ ‘çš„ç»“æ„, æ—¢æœ‰ä¼˜ç‚¹ä¹Ÿæœ‰ä¸è¶³. ä¸ºäº†æ•æ‰åŠ æ€§ç»“æ„, MARS æ–¹æ³• (9.4 èŠ‚) å†ä¸€æ¬¡æ”¾å¼ƒæ ‘çš„ç»“æ„.</li>
</ul>
</li>
</ul>
<h3 id="patient-rule-induction-method-prim">Patient Rule Induction Method (PRIM)<a class="headerlink" href="#patient-rule-induction-method-prim" title="Permanent link">&para;</a></h3>
<p>åŸºäºæ ‘çš„æ–¹æ³•ï¼ˆç”¨äºå›å½’ï¼‰å°†ç‰¹å¾ç©ºé—´åˆ†æˆç›’çŠ¶çš„åŒºåŸŸï¼Œæ¥è¯•å›¾ä½¿æ¯ä¸ªç›’å­ä¸­çš„å“åº”å˜é‡å°½å¯èƒ½ä¸åŒï¼é€šè¿‡äºŒå‰æ ‘å®šä¹‰äº†ä¸æ¯ä¸ªç›’å­ç›¸å…³çš„åˆ†å‰²è§„åˆ™ï¼Œè¿™æœ‰åˆ©äºå®ƒä»¬çš„è§£é‡Šæ€§ï¼</p>
<p>è€å¿ƒè§„åˆ™å½’çº³æ³• (PRIM) ä¹Ÿåœ¨ç‰¹å¾ç©ºé—´ä¸­æ‰¾ç›’å­ï¼Œä½†æ˜¯å®ƒæ˜¯å¯»æ‰¾å…·æœ‰é«˜å¹³å‡å“åº”çš„ç›’å­ï¼å› æ­¤å®ƒæ˜¯åœ¨å¯»æ‰¾ç›®æ ‡å‡½æ•°çš„æœ€å¤§å€¼ï¼Œç§°ä¸ºbump huntingï¼ï¼ˆå¦‚æœéœ€è¦æœ€å°å€¼è€Œéæœ€å¤§å€¼ï¼Œå¯ä»¥ç®€å•åœ°ç”¨è´Ÿçš„å“åº”å˜é‡å€¼æ¥è½¬åŒ–ï¼ï¼‰</p>
<h4 id="prim">PRIM<a class="headerlink" href="#prim" title="Permanent link">&para;</a></h4>
<p>Aim</p>
<ul>
<li>Locate maximum in the response function</li>
</ul>
<p>How: find a rectangular box in the feature space which contains</p>
<ul>
<li>for classification: a clump of points of maximal purity</li>
<li>for regression: a plateau of high scoring points</li>
</ul>
<p>PRIM - a greedy search which is more patient than CART</p>
<h4 id="prim-algorithm">PRIM - Algorithm<a class="headerlink" href="#prim-algorithm" title="Permanent link">&para;</a></h4>
<p>PRIM çš„æ€æƒ³æ˜¯ä»ä¸åŒçš„å˜é‡ç»´åº¦å»ç¼©å‡ box çš„å¤§å°, å°½é‡ä½¿å¾—ç›’å­ä¸­å“åº”å˜é‡å°½å¯èƒ½å¤§ (æ‰€ä»¥ä¹Ÿå« <strong>Bump Hunting</strong>). ç¼©å‡çš„æ–¹å¼å«åš <strong>Peeling</strong> è§ä¸‹; å¦å¤–ä¸ºäº†æ¨¡å‹çš„å¥å£®æ€§, æœ€åä¹Ÿä¼šæ‰©å±•ç›’å­, å«åš <strong>Pasting</strong>. ç›¸å…³å®šä¹‰å¦‚ä¸‹</p>
<ul>
<li>Box <span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span> is defined by the set of inequalities</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=1, \ldots, p
</div>
<script type="math/tex; mode=display">
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=1, \ldots, p
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> is the dimension of the feature vectors</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">B^{\prime}=\operatorname{NewBox}(B, k, 0, a)</span><script type="math/tex">B^{\prime}=\operatorname{NewBox}(B, k, 0, a)</script></span> is defined by the inequalities</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=1, \ldots, k-1 \\
a \leqslant X_{k} \leqslant b_{k} \\
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=k+1, \ldots, p
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=1, \ldots, k-1 \\
a \leqslant X_{k} \leqslant b_{k} \\
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=k+1, \ldots, p
\end{gathered}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">B^{\prime}=\operatorname{NewBox}(B, k, 1, b)</span><script type="math/tex">B^{\prime}=\operatorname{NewBox}(B, k, 1, b)</script></span> is defined by the inequalities</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=1, \ldots, k-1 &amp; a_{k} \leqslant X_{k} \leqslant b \\
a_{j} \leqslant X_{j} \leqslant &amp; b_{j} \text { for } j=k+1, \ldots, p
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=1, \ldots, k-1 & a_{k} \leqslant X_{k} \leqslant b \\
a_{j} \leqslant X_{j} \leqslant & b_{j} \text { for } j=k+1, \ldots, p
\end{aligned}
</script>
</div>
<ul>
<li>Let <span class="arithmatex"><span class="MathJax_Preview">n_{B}=\#</span><script type="math/tex">n_{B}=\#</script></span> of training observations in box <span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span></li>
</ul>
<p>ç¼©å‡æ–¹å¼çš„å…·ä½“å®šä¹‰å¦‚ä¸‹, å³æ¯æ¬¡ä»æ‰€æœ‰å¯èƒ½çš„ç»´åº¦æ”¶ç¼©, è¦æ±‚ç›’å­ä¸­å‡å°‘çš„æ•°æ®é‡åˆšå¥½ä¸º <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>, è¿™é‡Œçš„ <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> ä¸€èˆ¬å¯å– 0.05 æˆ– 0.1 (æ‰©å±•çš„æ–¹å¼ç±»ä¼¼, æ¯æ¬¡æ‰©å±• <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> ä¸ªæ•°æ®ç‚¹, è¿™é‡Œä¸å±•ç¤ºäº†.)</p>
<p>Peeling - Decrease the size of box B for one face</p>
<ul>
<li>Define <span class="arithmatex"><span class="MathJax_Preview">B^{\prime}=\operatorname{Peel}(B, k, 0, \alpha)</span><script type="math/tex">B^{\prime}=\operatorname{Peel}(B, k, 0, \alpha)</script></span> to be the box</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
B^{\prime}=\operatorname{NewBox}(B, k, 0, a)
</div>
<script type="math/tex; mode=display">
B^{\prime}=\operatorname{NewBox}(B, k, 0, a)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> is the smallest scalar for <span class="arithmatex"><span class="MathJax_Preview">\alpha \in(0,1)</span><script type="math/tex">\alpha \in(0,1)</script></span> s.t.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
a&gt;a_{k} \text { and } n_{B^{\prime}} \leqslant(1-\alpha) n_{B}
</div>
<script type="math/tex; mode=display">
a>a_{k} \text { and } n_{B^{\prime}} \leqslant(1-\alpha) n_{B}
</script>
</div>
<ul>
<li>Define <span class="arithmatex"><span class="MathJax_Preview">B^{\prime}=\operatorname{Peel}(B, k, 1, \alpha)</span><script type="math/tex">B^{\prime}=\operatorname{Peel}(B, k, 1, \alpha)</script></span> to be the box</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
B^{\prime}=\operatorname{NewBox}(B, k, 1, b)
</div>
<script type="math/tex; mode=display">
B^{\prime}=\operatorname{NewBox}(B, k, 1, b)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> is the largest scalar for <span class="arithmatex"><span class="MathJax_Preview">\alpha \in(0,1)</span><script type="math/tex">\alpha \in(0,1)</script></span> s.t.
<span class="arithmatex"><span class="MathJax_Preview">b&lt;b_{k}</span><script type="math/tex">b<b_{k}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">n_{B^{\prime}} \leqslant(1-\alpha) n_{B}</span><script type="math/tex">n_{B^{\prime}} \leqslant(1-\alpha) n_{B}</script></span></p>
<p><img alt="" src="../media/ASL-note3/2021-12-26-16-24-49.png" /></p>
<p>æ‰¾ç›’å­çš„æ€»ä½“æµç¨‹å¯è§„èŒƒåœ°å†™ä¸º</p>
<ol>
<li>Set <span class="arithmatex"><span class="MathJax_Preview">i=0</span><script type="math/tex">i=0</script></span> and let <span class="arithmatex"><span class="MathJax_Preview">\alpha \in(0,1)</span><script type="math/tex">\alpha \in(0,1)</script></span></li>
<li>Let <span class="arithmatex"><span class="MathJax_Preview">B_{0}</span><script type="math/tex">B_{0}</script></span> be the minimal box containing all the data</li>
<li>Peeling process: find sequence of decreasing nested boxes while (number of observations in <span class="arithmatex"><span class="MathJax_Preview">B_{i} \geqslant n_{m}</span><script type="math/tex">B_{i} \geqslant n_{m}</script></span> )
   - Compute the trimmed boxes <span class="arithmatex"><span class="MathJax_Preview">C_{k}=P e e l\left(B_{i}, k, 0, \alpha\right)</span><script type="math/tex">C_{k}=P e e l\left(B_{i}, k, 0, \alpha\right)</script></span> and <span class="arithmatex"><span class="MathJax_Preview">C_{k+p}=\operatorname{Peel}\left(B_{i}, k, 1, \alpha\right)</span><script type="math/tex">C_{k+p}=\operatorname{Peel}\left(B_{i}, k, 1, \alpha\right)</script></span> for <span class="arithmatex"><span class="MathJax_Preview">k=1, \ldots, p</span><script type="math/tex">k=1, \ldots, p</script></span>
   - Choose the <span class="arithmatex"><span class="MathJax_Preview">C_{j *}</span><script type="math/tex">C_{j *}</script></span> with highest response mean [ä»æ¯ä¸€ä¸ªç»´åº¦å°è¯•æœç´¢, é€‰æ‹©å¾—åˆ°çš„ç›’å­ä¸­å“åº”é‡æœ€é«˜çš„é‚£ä¸€ä¸ª]
   - Set <span class="arithmatex"><span class="MathJax_Preview">B_{i+1}=C_{j *}</span><script type="math/tex">B_{i+1}=C_{j *}</script></span>
   - Set <span class="arithmatex"><span class="MathJax_Preview">i=i+1</span><script type="math/tex">i=i+1</script></span></li>
<li>Pasting process: find sequence of increasing nested boxed For <span class="arithmatex"><span class="MathJax_Preview">k=1, \ldots, p</span><script type="math/tex">k=1, \ldots, p</script></span>
   - <span class="arithmatex"><span class="MathJax_Preview">C=\operatorname{ExpandBox}\left(B_{i}, k, 0, \alpha\right), D=\operatorname{ExpandBox}\left(B_{i}, k, 1, \alpha\right)</span><script type="math/tex">C=\operatorname{ExpandBox}\left(B_{i}, k, 0, \alpha\right), D=\operatorname{ExpandBox}\left(B_{i}, k, 1, \alpha\right)</script></span>
   - Set <span class="arithmatex"><span class="MathJax_Preview">B_{i+1}=C</span><script type="math/tex">B_{i+1}=C</script></span> and <span class="arithmatex"><span class="MathJax_Preview">i=i+1</span><script type="math/tex">i=i+1</script></span> if <span class="arithmatex"><span class="MathJax_Preview">S_{C}&gt;S_{B_{i}}</span><script type="math/tex">S_{C}>S_{B_{i}}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">S_{C}&gt;S_{D}</span><script type="math/tex">S_{C}>S_{D}</script></span>
   - Set <span class="arithmatex"><span class="MathJax_Preview">B_{i+1}=D</span><script type="math/tex">B_{i+1}=D</script></span> and <span class="arithmatex"><span class="MathJax_Preview">i=i+1</span><script type="math/tex">i=i+1</script></span> if <span class="arithmatex"><span class="MathJax_Preview">S_{C}&gt;S_{B_{i}}</span><script type="math/tex">S_{C}>S_{B_{i}}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">S_{D}&gt;S_{C}</span><script type="math/tex">S_{D}>S_{C}</script></span></li>
</ol>
<p>ä¸Šé¢ä»…ä»…æ˜¯æ‰¾åˆ°äº†ä¸€ä¸ªç›’å­, æ•´ä½“æµç¨‹ä¸º</p>
<ol>
<li>Previous steps produces a sequence of boxes <span class="arithmatex"><span class="MathJax_Preview">B_{1}, \ldots, B_{i}</span><script type="math/tex">B_{1}, \ldots, B_{i}</script></span></li>
<li>Use cross-validation to choose best box - call this box <span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span></li>
<li>Remove the data in box <span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span> from the dataset</li>
<li>Repeat the peeling and pasting steps and the cross-validation step to obtain a second box</li>
<li>Continue these last two steps to get as many boxes as desired</li>
</ol>
<p>PRIM ä¸ CART ç›¸æ¯”çš„ä¼˜ç‚¹æ˜¯å®ƒçš„ <strong>è€å¿ƒ (patience)</strong>. å› ä¸º CART çš„äºŒå€¼åˆ†å‰²å¿«é€Ÿåœ°å°†æ•°æ®åˆ†å‰²å¼€. å‡è®¾ç­‰å¤§å°çš„åˆ†å‰², æœ‰ <span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> ä¸ªè§‚æµ‹æƒ…å†µä¸‹, åœ¨ä½¿ç”¨å®Œæ•°æ®ä¹‹å‰, åªèƒ½è¿›è¡Œ <span class="arithmatex"><span class="MathJax_Preview">\log _{2}(N)-1</span><script type="math/tex">\log _{2}(N)-1</script></span> æ¬¡åˆ†å‰². å¦‚ æœ PRIM åœ¨æ¯ä¸€æ­¥å‰”é™¤æ‰è®­ç»ƒç‚¹çš„æ¯”ä¾‹ä¸º <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>, åˆ™åœ¨ç”¨å®Œæ•°æ®ä¹‹å‰å¤§çº¦éœ€è¦ <span class="arithmatex"><span class="MathJax_Preview">-\log (N) / \log (1-\alpha)</span><script type="math/tex">-\log (N) / \log (1-\alpha)</script></span> æ¬¡ æ™¹é™¤æ­¥éª¤. ä¸¾ä¸ªä¾‹å­, å¦‚æœ <span class="arithmatex"><span class="MathJax_Preview">N=128, \alpha=0.10</span><script type="math/tex">N=128, \alpha=0.10</script></span>, åˆ™ <span class="arithmatex"><span class="MathJax_Preview">\log_{2}(N)-1=6</span><script type="math/tex">\log_{2}(N)-1=6</script></span>, è€Œ <span class="arithmatex"><span class="MathJax_Preview">-\log (N) / \log (1-\alpha) \approx 46</span><script type="math/tex">-\log (N) / \log (1-\alpha) \approx 46</script></span>. è€ƒè™‘æ¯ä¸€æ­¥éœ€è¦æ•´æ•°ä¸ªçš„è§‚æµ‹, PRIM å®é™…ä¸Šå¯ä»¥å‰”é™¤ 29 æ¬¡. åœ¨ä»» ä½•æƒ…å½¢ä¸‹, PRIM çš„èƒ½åŠ›æ›´åŠ è€å¿ƒ, è¿™åº”è¯¥å¸®åŠ©è‡ªä¸Šè€Œä¸‹çš„è´ªå©ªç®—æ³•æ‰¾åˆ°æ›´å¥½çš„è§£.</p>
<h3 id="multivariate-adaptive-regression-splines-mars">Multivariate Adaptive Regression Splines (MARS)<a class="headerlink" href="#multivariate-adaptive-regression-splines-mars" title="Permanent link">&para;</a></h3>
<p>å¤šå˜é‡è‡ªé€‚åº”å›å½’æ ·æ¡ (Multivariate Adaptive Regression Splines, MARS) æ˜¯å›å½’çš„è‡ªé€‚åº”è¿‡ç¨‹(an adaptive procedure for regression)ï¼Œéå¸¸é€‚åˆ<strong>é«˜ç»´é—®é¢˜</strong>ï¼ˆå³ï¼Œå­˜åœ¨å¤§é‡çš„è¾“å…¥ï¼‰ï¼å¯ä»¥ä»ä¸¤ä¸ªè§’åº¦æ¥ç†è§£å®ƒï¼Œé¦–å…ˆï¼Œå®ƒå¯ä»¥çœ‹æˆæ˜¯é€æ­¥çº¿æ€§å›å½’ (stepwise linear regression)çš„æ¨å¹¿ï¼Œå…¶æ¬¡ï¼Œä¹Ÿå¯ä»¥çœ‹æˆæ˜¯ä¸ºäº†æé«˜ CART åœ¨å›å½’ä¸­çš„æ•ˆæœè€Œè¿›è¡Œçš„æ”¹è¿›ï¼æˆ‘ä»¬ä»ç¬¬ä¸€ç§è§‚ç‚¹å¼•å…¥ MARSï¼Œæ¥ç€ä¸ CART è”ç³»èµ·æ¥ï¼</p>
<p>é¦–å…ˆå®šä¹‰ expansions in piecewise linear basis functions <span class="arithmatex"><span class="MathJax_Preview">\left(x-t\right)_{+}</span><script type="math/tex">\left(x-t\right)_{+}</script></span> å’Œ <span class="arithmatex"><span class="MathJax_Preview">\left(t-x\right)_{+}</span><script type="math/tex">\left(t-x\right)_{+}</script></span>, å‡½æ•°å›¾åƒå¦‚ä¸‹:</p>
<p><img alt="" src="../media/ASL-note3/2021-12-26-16-37-00.png" /></p>
<p>MARS - Basis Functions</p>
<ul>
<li>Training data <span class="arithmatex"><span class="MathJax_Preview">\left(x_{1},, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)</span><script type="math/tex">\left(x_{1},, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)</script></span> with <span class="arithmatex"><span class="MathJax_Preview">y_{i} \in \mathbb{R}</span><script type="math/tex">y_{i} \in \mathbb{R}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">x_{i}=\left(x_{i 1}, x_{i 2}, \ldots, x_{i p}\right)^{\top} \in \mathbb{R}^{p}</span><script type="math/tex">x_{i}=\left(x_{i 1}, x_{i 2}, \ldots, x_{i p}\right)^{\top} \in \mathbb{R}^{p}</script></span></li>
<li>For an input vector <span class="arithmatex"><span class="MathJax_Preview">X \in \mathbb{R}^{p}</span><script type="math/tex">X \in \mathbb{R}^{p}</script></span> define</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
h_{0}(X, j, i)=\left(X_{j}-x_{i j}\right)_{+} \text {and } h_{1}(X, j, i)=\left(x_{i j}-X_{j}\right)_{+}
</div>
<script type="math/tex; mode=display">
h_{0}(X, j, i)=\left(X_{j}-x_{i j}\right)_{+} \text {and } h_{1}(X, j, i)=\left(x_{i j}-X_{j}\right)_{+}
</script>
</div>
<ul>
<li>Then define a collection of basis functions</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
C=\left\{h_{0}(X, j, i), h_{1}(X, j, i)_{j=1, \ldots, p, i=1, \ldots, n}\right\}
</div>
<script type="math/tex; mode=display">
C=\left\{h_{0}(X, j, i), h_{1}(X, j, i)_{j=1, \ldots, p, i=1, \ldots, n}\right\}
</script>
</div>
<p>æ ¹æ®ä¸Šé¢çš„å®šä¹‰, å¯ä»¥æœ€å¤šå¯èƒ½æœ‰ <span class="arithmatex"><span class="MathJax_Preview">Np</span><script type="math/tex">Np</script></span> ä¸ªåŸºå‡½æ•°. ç„¶ååŸºäºè¿™ç»„åŸºå‡½æ•°è¿›è¡Œæ‹Ÿåˆ (æ³¨æ„ä¸‹é¢çš„ <span class="arithmatex"><span class="MathJax_Preview">g\left(X, \alpha_{m}\right)</span><script type="math/tex">g\left(X, \alpha_{m}\right)</script></span> æ˜¯ <span class="arithmatex"><span class="MathJax_Preview">C</span><script type="math/tex">C</script></span> ä¸­å¤šä¸ªåŸºå‡½æ•°çš„ä¹˜ç§¯.)</p>
<p>Estimate the regression function using functions from <span class="arithmatex"><span class="MathJax_Preview">C</span><script type="math/tex">C</script></span> and product of functions from <span class="arithmatex"><span class="MathJax_Preview">C</span><script type="math/tex">C</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
f(X)=\beta_{0}+\sum_{m=1}^{M} \beta_{m} g\left(X, \alpha_{m}\right)
</div>
<script type="math/tex; mode=display">
f(X)=\beta_{0}+\sum_{m=1}^{M} \beta_{m} g\left(X, \alpha_{m}\right)
</script>
</div>
<p>where each <span class="arithmatex"><span class="MathJax_Preview">\alpha_{m}=\left(n_{m}, b_{1}, j_{1}, i_{1}, \ldots, b_{n m}, j_{n m}, i_{n m}\right)</span><script type="math/tex">\alpha_{m}=\left(n_{m}, b_{1}, j_{1}, i_{1}, \ldots, b_{n m}, j_{n m}, i_{n m}\right)</script></span> with <span class="arithmatex"><span class="MathJax_Preview">b_{k} \in\{0,1\}</span><script type="math/tex">b_{k} \in\{0,1\}</script></span> s.t.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
g\left(X, \alpha_{m}\right)=\prod_{k=1}^{n_{m}} h_{b_{k}}\left(X, j_{k}, i_{k}\right)
</div>
<script type="math/tex; mode=display">
g\left(X, \alpha_{m}\right)=\prod_{k=1}^{n_{m}} h_{b_{k}}\left(X, j_{k}, i_{k}\right)
</script>
</div>
<p><img alt="" src="../media/ASL-note3/2021-12-26-16-41-58.png" /></p>
<p>å…·ä½“çš„ç­›é€‰ç®—æ³•å¦‚ä¸‹, ä¸Šå›¾ä¸ºç›´è§‚çš„å±•ç¤º.</p>
<p>Initially</p>
<ul>
<li>Set <span class="arithmatex"><span class="MathJax_Preview">g\left(X, \alpha_{0}\right) \equiv 1</span><script type="math/tex">g\left(X, \alpha_{0}\right) \equiv 1</script></span> and <span class="arithmatex"><span class="MathJax_Preview">M=\left\{g\left(X, \alpha_{0}\right)\right\}</span><script type="math/tex">M=\left\{g\left(X, \alpha_{0}\right)\right\}</script></span></li>
</ul>
<p>While <span class="arithmatex"><span class="MathJax_Preview">|M|&lt;N</span><script type="math/tex">|M|<N</script></span> perform the following</p>
<ul>
<li>for each <span class="arithmatex"><span class="MathJax_Preview">(m, j, i) \in\{1, \ldots,|M|\} \times\{1, \ldots, p\} \times\{1, \ldots, n\}</span><script type="math/tex">(m, j, i) \in\{1, \ldots,|M|\} \times\{1, \ldots, p\} \times\{1, \ldots, n\}</script></span>
  1. Augment functions in <span class="arithmatex"><span class="MathJax_Preview">M-g\left(X, \alpha_{0}\right), \ldots, g\left(X, \alpha_{|M|}\right)</span><script type="math/tex">M-g\left(X, \alpha_{0}\right), \ldots, g\left(X, \alpha_{|M|}\right)</script></span> with å°†æ¨¡å‹é›†åˆä¸­ç°æœ‰çš„æ‰€æœ‰å‡½æ•°å’Œ <span class="arithmatex"><span class="MathJax_Preview">C</span><script type="math/tex">C</script></span> ä¸­æ‰€æœ‰åŸºå‡½æ•°ä¹˜ç§¯ä½œä¸ºç»„åˆ (æ³¨æ„æ¯æ¬¡åŠ å…¥çš„éƒ½æ˜¯ä¸€ä¸ªåå°„å¯¹)
  <span class="arithmatex"><span class="MathJax_Preview">g\left(X, \alpha_{m}\right) \cdot h_{0}(X, j, i) \text { and } g\left(X, \alpha_{m}\right) \cdot h_{1}(X, j, i)</span><script type="math/tex">g\left(X, \alpha_{m}\right) \cdot h_{0}(X, j, i) \text { and } g\left(X, \alpha_{m}\right) \cdot h_{1}(X, j, i)</script></span>
  2. Use standard linear regression o estimate the <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}_{1}</span><script type="math/tex">\hat{\beta}_{1}</script></span> &lsquo;s s.t.
  <span class="arithmatex"><span class="MathJax_Preview">\begin{gathered}
  f_{t r y}(X)=\sum_{l=0}^{|M|} \hat{\beta}_{I} g\left(X, \alpha_{l}\right)+\hat{\beta}_{|M|+1} g\left(X, \alpha_{m}\right) \cdot h_{0}(X, j, i)+
  \hat{\beta}_{|M|+2} g\left(X, \alpha_{m}\right) \cdot h_{1}(X, j, i)
  \end{gathered}</span><script type="math/tex">\begin{gathered}
  f_{t r y}(X)=\sum_{l=0}^{|M|} \hat{\beta}_{I} g\left(X, \alpha_{l}\right)+\hat{\beta}_{|M|+1} g\left(X, \alpha_{m}\right) \cdot h_{0}(X, j, i)+
  \hat{\beta}_{|M|+2} g\left(X, \alpha_{m}\right) \cdot h_{1}(X, j, i)
  \end{gathered}</script></span>
  3. Compute and record training error of <span class="arithmatex"><span class="MathJax_Preview">f_{\text {try }}(X)</span><script type="math/tex">f_{\text {try }}(X)</script></span></li>
<li>Let <span class="arithmatex"><span class="MathJax_Preview">\left(m^{*}\right), j^{*}, i^{*}</span><script type="math/tex">\left(m^{*}\right), j^{*}, i^{*}</script></span> be triplet producing lowest training error</li>
<li>Add <span class="arithmatex"><span class="MathJax_Preview">g\left(X, \alpha_{m^{*}}\right) \cdot h_{0}\left(X, j^{*}, i^{*}\right)</span><script type="math/tex">g\left(X, \alpha_{m^{*}}\right) \cdot h_{0}\left(X, j^{*}, i^{*}\right)</script></span> and <span class="arithmatex"><span class="MathJax_Preview">g\left(X, \alpha_{m^{*}}\right) \cdot h_{1}\left(X, j^{*}, i^{*}\right)</span><script type="math/tex">g\left(X, \alpha_{m^{*}}\right) \cdot h_{1}\left(X, j^{*}, i^{*}\right)</script></span> to <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span></li>
</ul>
<h4 id="piecewise-linear-forward-model-building">Piecewise Linear \&amp; Forward Model Building<a class="headerlink" href="#piecewise-linear-forward-model-building" title="Permanent link">&para;</a></h4>
<p>They can operate locally - the product is only non-zero where all individual components are non-zero</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\prod_{k=1}^{n_{m}} h_{b_{k}}\left(X, j_{k}, i_{k}\right)
</div>
<script type="math/tex; mode=display">
\prod_{k=1}^{n_{m}} h_{b_{k}}\left(X, j_{k}, i_{k}\right)
</script>
</div>
<ul>
<li>Locality <span class="arithmatex"><span class="MathJax_Preview">\rightarrow</span><script type="math/tex">\rightarrow</script></span> forward model building strategy can<ul>
<li>build up the regression surface parsimoniously (åå•¬åœ°)</li>
<li>use parameters only when needed, which is very important for high dimensional data</li>
<li>ã€ŒThis is important, since one should â€œspendâ€ parameters carefully in high dimensions, as they can run out quickly.ã€</li>
</ul>
</li>
<li>Computational reasons - innermost loop of model building can be made very efficient</li>
<li>Hierarchical search avoids unnecessary complicated terms æ³¨æ„åˆ°ä¸€ä¸ª 4 é˜¶çš„äº¤å‰é¡¹è¦å‡ºç°åœ¨æ¨¡å‹ä¸­, åˆ™å…¶ä¸­ä¸€å®šæœ‰ä¸€ä¸ª 3 é˜¶äº¤å‰ç›¸åœ¨åŸæœ¬çš„æ¨¡å‹ä¸­ (é¿å…äº†åœ¨å¯è¡Œé€‰æ‹©æ•°ç›®å‘ˆæŒ‡æ•°å¢é•¿çš„ç©ºé—´ä¸­çš„æœç´¢)</li>
</ul>
<h4 id="mars-vs-cart">MARS V.S CART<a class="headerlink" href="#mars-vs-cart" title="Permanent link">&para;</a></h4>
<p>If MARS procedure is amended s.t.</p>
<ol>
<li>Set <span class="arithmatex"><span class="MathJax_Preview">h_{0}(X, j, i)=I\left(X_{j}-x_{i j}&gt;0\right) \leftarrow</span><script type="math/tex">h_{0}(X, j, i)=I\left(X_{j}-x_{i j}>0\right) \leftarrow</script></span> step function</li>
<li>Set <span class="arithmatex"><span class="MathJax_Preview">h_{1}(X, j, i)=I\left(X_{j}-x_{i j} \leqslant 0\right) \leftarrow</span><script type="math/tex">h_{1}(X, j, i)=I\left(X_{j}-x_{i j} \leqslant 0\right) \leftarrow</script></span> step function (ç”¨é˜¶è·ƒå‡½æ•°æ›¿ä»£åˆ†æ®µçº¿æ€§åŸºå‡½æ•°)</li>
<li>When <span class="arithmatex"><span class="MathJax_Preview">g \in M</span><script type="math/tex">g \in M</script></span> is chosen at one iteration s.t.</li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
M=M \cup\left\{g(X) \cdot h_{0}(X, j, i)\right\} \cup\left\{g(X) \cdot h_{1}(X, j, i)\right\}
</div>
<script type="math/tex; mode=display">
M=M \cup\left\{g(X) \cdot h_{0}(X, j, i)\right\} \cup\left\{g(X) \cdot h_{1}(X, j, i)\right\}
</script>
</div>
<p>remove <span class="arithmatex"><span class="MathJax_Preview">g</span><script type="math/tex">g</script></span> from <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> å½“æ¨¡å‹ä¸­çš„é¡¹ä¸å€™é€‰é¡¹ç›¸ä¹˜ï¼Œæ›¿æ¢æˆäº¤å‰é¡¹ï¼Œå› æ­¤ä¸å…è®¸è¯¥é¡¹ä¸å…¶å®ƒå€™é€‰é¡¹è¿›è¡Œäº¤å‰ï¼</p>
<p>Then, MARS forward procedure <span class="arithmatex"><span class="MathJax_Preview">==</span><script type="math/tex">==</script></span> CART tree-growing algorithm</p>
<p>ä¹˜ä¸Šä¸€å¯¹åå°„é˜¶è·ƒå‡½æ•°ç­‰ä»·äºåœ¨é˜¶è·ƒç‚¹åˆ†å‰²å¼€ï¼ç¬¬äºŒæ¡é™åˆ¶è¡¨æ˜ä¸€ä¸ªç»“ç‚¹å¯èƒ½ä¸ä¼šè¢«åˆ†å‰²æˆå¤šæ¬¡ï¼Œå› æ­¤ä½¿å¾— CART æ¨¡å‹èƒ½ç”¨ï¼ˆå¸å¼•äººçš„ï¼‰äºŒå‰æ ‘è¡¨ç¤ºï¼å¦ä¸€æ–¹é¢ï¼Œä¹Ÿæ˜¯è¿™æ¡é™åˆ¶ä½¿å¾— CART å¾ˆéš¾å¯¹åŠ æ€§ç»“æ„è¿›è¡Œå»ºæ¨¡ï¼<strong>MARS æ”¾å¼ƒæ ‘ç»“æ„å¹¶ä¸”å¾—åˆ°æ•æ‰åŠ æ€§å½±å“çš„èƒ½åŠ›</strong>ï¼</p>
<h3 id="hierarchical-mixture-of-experts-hme">Hierarchical Mixture of Experts (HME)<a class="headerlink" href="#hierarchical-mixture-of-experts-hme" title="Permanent link">&para;</a></h3>
<p>ä¸“å®¶çš„åˆ†å±‚æ··åˆ (HME) è¿‡ç¨‹å¯ä»¥çœ‹æˆæ˜¯åŸºäºæ ‘æ–¹æ³•çš„å˜ç§ï¼ä¸»è¦çš„å·®å¼‚æ˜¯æ ‘çš„åˆ†å‰²ä¸æ˜¯ç¡¬å†³å®š (hard decision)ï¼Œè€Œæ˜¯è½¯æ¦‚ç‡çš„å†³å®š (soft probabilistic)ï¼åœ¨æ¯ä¸ªç»“ç‚¹è§‚æµ‹å¾€å·¦æˆ–è€…å¾€å³çš„æ¦‚ç‡å–å†³äºè¾“å…¥å€¼ï¼å› ä¸ºæœ€åçš„å‚æ•°ä¼˜åŒ–é—®é¢˜æ˜¯å…‰æ»‘çš„ï¼Œæ‰€ä»¥æœ‰ä¸€äº›è®¡ç®—çš„ä¼˜åŠ¿ï¼Œä¸åƒåœ¨åŸºäºæ ‘çš„æ–¹å¼ä¸­çš„ç¦»æ•£åˆ†å‰²ç‚¹çš„æœç´¢ï¼è½¯åˆ†å‰²æˆ–è®¸ä¹Ÿå¯ä»¥å¸®åŠ©é¢„æµ‹å‡†ç¡®æ€§ï¼Œå¹¶ä¸”æä¾›å¦å¤–ä¸€ç§æœ‰ç”¨çš„æ•°æ®æè¿°æ–¹å¼ï¼</p>
<p>HMEs å’Œ CART æ ‘çš„å®ç°è¿˜æœ‰å…¶ä»–çš„å·®å¼‚ï¼åœ¨HMEä¸­ï¼Œåœ¨æ¯ä¸ªç»ˆæ­¢ç»“ç‚¹å¤„æ‹Ÿåˆçº¿æ€§ï¼ˆæˆ–è€…é€»è¾‘æ–¯è’‚å›å½’ï¼‰æ¨¡å‹ï¼Œè€Œä¸æ˜¯åƒ CART ä¸­é‚£æ ·æ˜¯å¸¸å€¼ï¼åˆ†å‰²ç‚¹å¯ä»¥æ˜¯å¤šé‡çš„ï¼Œè€Œä¸ä»…ä»…æ˜¯äºŒå€¼çš„ï¼Œå¹¶ä¸”åˆ†å‰²ç‚¹æ˜¯è¾“å…¥çš„çº¿æ€§ç»„åˆçš„æ¦‚ç‡å‡½æ•°ï¼Œè€Œä¸æ˜¯åœ¨æ ‡å‡† CART ä¸­çš„å•ä¸ªè¾“å…¥ï¼ç„¶è€Œï¼Œè¿™äº›é€‰æ‹©çš„ç›¸å¯¹ä¼˜ç‚¹ä¸æ˜¯æ¸…æ™°çš„ï¼Œå¤§éƒ¨åˆ†å°†åœ¨ 9.2 èŠ‚çš„åé¢ä¸­è®¨è®ºï¼</p>
<p>ç®€å•çš„ 2 å±‚ HME æ¨¡å‹å¦‚å›¾ 9.13 æ‰€ç¤ºï¼å¯ä»¥è®¤ä¸ºæ˜¯åœ¨æ¯ä¸ªéç»ˆæ­¢ç»“ç‚¹å¤„è¿›è¡Œè½¯åˆ†å‰²çš„æ ‘ï¼ç„¶è€Œï¼Œè¿™ç§æ–¹æ³•çš„å‘æ˜è€…é‡‡ç”¨ä¸åŒçš„æœ¯è¯­ï¼ç»ˆæ­¢ç»“ç‚¹ç§°ä¸ºä¸“å®¶ (experts)ï¼Œéç»ˆæ­¢ç»“ç‚¹ç§°ä¸ºé—¨æ§ç½‘ç»œ (gating networks)ï¼æƒ³æ³•æ˜¯ï¼Œæ¯ä¸ªä¸“å®¶å¯¹å“åº”å˜é‡æä¾›ä¸€ä¸ªçœ‹æ³•ï¼ˆé¢„æµ‹ï¼‰ï¼Œå¹¶ä¸”é€šè¿‡é—¨æ§ç½‘ç»œå°†è¿™äº›â€œçœ‹æ³•â€ç»“åˆåœ¨ä¸€èµ·ï¼æ­£å¦‚æˆ‘ä»¬æ‰€è§ï¼Œè¿™ä¸ªæ¨¡å‹å½¢å¼ä¸Šæ˜¯æ··åˆæ¨¡å‹ï¼Œå¹¶ä¸”å›¾ä¸­çš„ä¸¤å±‚æ¨¡å‹å¯ä»¥æ¨å¹¿ä¸ºå¤šå±‚ï¼Œå› æ­¤æœ‰äº†ç§°ä¸ºä¸“å®¶çš„åˆ†å±‚æ··åˆ (hierarchical mixtures of experts)</p>
<p><img alt="" src="../media/ASL-note3/2021-12-26-17-31-51.png" /></p>
<p>é¡¶å±‚çš„é—¨æ§ç½‘ç»œæœ‰è¾“å‡º</p>
<div class="arithmatex">
<div class="MathJax_Preview">
g_{j}\left(x, \gamma_{j}\right)=\frac{e^{\gamma_{j}^{T} x}}{\sum_{k=1}^{K} e^{\gamma_{k}^{T} x}} j=1,2, \ldots, K
</div>
<script type="math/tex; mode=display">
g_{j}\left(x, \gamma_{j}\right)=\frac{e^{\gamma_{j}^{T} x}}{\sum_{k=1}^{K} e^{\gamma_{k}^{T} x}} j=1,2, \ldots, K
</script>
</div>
<p>å…¶ä¸­æ¯ä¸ª <span class="arithmatex"><span class="MathJax_Preview">\gamma_{j}</span><script type="math/tex">\gamma_{j}</script></span> æ˜¯æœ«çŸ¥å‚æ•°å‘é‡. è¿™è¡¨ç¤ºè½¯çš„ <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> é‡åˆ†å‰²ï¼ˆå›¾ <span class="arithmatex"><span class="MathJax_Preview">9.13</span><script type="math/tex">9.13</script></span> ä¸­ <span class="arithmatex"><span class="MathJax_Preview">K=2</span><script type="math/tex">K=2</script></span> ï¼‰æ¯ä¸ª <span class="arithmatex"><span class="MathJax_Preview">g_{j}\left(x, \gamma_{j}\right)</span><script type="math/tex">g_{j}\left(x, \gamma_{j}\right)</script></span> æ˜¯å°†ç‰¹å¾ å‘é‡ä¸º <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> çš„è§‚æµ‹èµ‹ç»™ç¬¬ <span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> ä¸ªåˆ†æ”¯. æ³¨æ„åˆ° <span class="arithmatex"><span class="MathJax_Preview">K=2</span><script type="math/tex">K=2</script></span> æ—¶, å¦‚æœæˆ‘ä»¬å°† <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> çš„æŸä¸€ä¸ªå…ƒç´ çš„ç³»æ•°å–ä¸º <span class="arithmatex"><span class="MathJax_Preview">+\infty</span><script type="math/tex">+\infty</script></span> , æ¥ç€æˆ‘ä»¬å¾—åˆ°æ— ç©·å¤§çš„æ–œç‡çš„é€»è¾‘æ–¯è’‚æ›²çº¿. åœ¨è¿™ç§æƒ…å½¢ä¸‹, é—¨çš„æ¦‚ç‡å– 0 æˆ– 1 , å¯¹åº”åœ¨è¯¥è¾“å…¥ ä¸‹çš„ç¡¬åˆ†å‰².</p>
<p>åœ¨ç¬¬äºŒå±‚, é—¨æ§ç½‘ç»œæœ‰ç±»ä¼¼çš„å½¢å¼:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
g_{\ell \mid j}\left(x, \gamma_{j \ell}\right)=\frac{e^{\gamma_{j \ell}^{T} x}}{\sum_{k=1}^{K} e^{\gamma_{j k}^{T}}}, \ell=1,2, \ldots, K
</div>
<script type="math/tex; mode=display">
g_{\ell \mid j}\left(x, \gamma_{j \ell}\right)=\frac{e^{\gamma_{j \ell}^{T} x}}{\sum_{k=1}^{K} e^{\gamma_{j k}^{T}}}, \ell=1,2, \ldots, K
</script>
</div>
<p>è¿™æ˜¯åœ¨ç»™å®šä¸Šä¸€å±‚ç¬¬ <span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> ä¸ªåˆ†æ”¯çš„æƒ…å†µä¸‹, èµ‹äºˆç¬¬ <span class="arithmatex"><span class="MathJax_Preview">\ell</span><script type="math/tex">\ell</script></span> åˆ†æ”¯çš„æ¦‚ç‡.</p>
<p>åœ¨æ¯ä¸ªä¸“å®¶ï¼ˆç»ˆæ­¢ç»“ç‚¹ï¼‰, æˆ‘ä»¬æœ‰å¦‚ä¸‹å½¢å¼çš„å“åº”å˜é‡çš„æ¨¡å‹ (è¿™æ ¹æ®é—®é¢˜è€Œæœ‰ä¸åŒ).</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Y \sim \operatorname{Pr}\left(y \mid x, \theta_{j l}\right)
</div>
<script type="math/tex; mode=display">
Y \sim \operatorname{Pr}\left(y \mid x, \theta_{j l}\right)
</script>
</div>
<ul>
<li>Regression: Gaussian linear regression model <span class="arithmatex"><span class="MathJax_Preview">P\left(y \mid x, \theta_{j l}\right)=N\left(\beta_{j l}^{\top} x, \sigma_{j l}^{2}\right)</span><script type="math/tex">P\left(y \mid x, \theta_{j l}\right)=N\left(\beta_{j l}^{\top} x, \sigma_{j l}^{2}\right)</script></span> where <span class="arithmatex"><span class="MathJax_Preview">\theta_{j l}=\left(\beta_{j l}, \sigma_{j l}^{2}\right)</span><script type="math/tex">\theta_{j l}=\left(\beta_{j l}, \sigma_{j l}^{2}\right)</script></span></li>
<li>Classification: linear logistic regression model <span class="arithmatex"><span class="MathJax_Preview">P\left(Y=1 \mid x, \theta_{j l}\right)=\frac{1}{1+e^{-\theta_{j j}^{\top} x}}</span><script type="math/tex">P\left(Y=1 \mid x, \theta_{j l}\right)=\frac{1}{1+e^{-\theta_{j j}^{\top} x}}</script></span></li>
</ul>
<h4 id="hme">HME<a class="headerlink" href="#hme" title="Permanent link">&para;</a></h4>
<p>The HME represents a mixture probability model. æ€»ä½“æ¨¡å‹æ˜¯ä¸€ä¸ªæ··åˆæ¨¡å‹ï¼Œå…¶ä¸­æ··åˆæ¦‚ç‡ç”±é—¨æ§ç½‘ç»œæ¨¡å‹ç¡®å®š.</p>
<p>The mixture probabilities are determined by the soft splits</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P(y \mid x, \Psi)=\sum_{j=1}^{K} g_{j}\left(x, \gamma_{j}\right) \sum_{l=1}^{K} g_{l \mid j}\left(x, \gamma_{l j}\right) P\left(y \mid x, \theta_{j l}\right)
</div>
<script type="math/tex; mode=display">
P(y \mid x, \Psi)=\sum_{j=1}^{K} g_{j}\left(x, \gamma_{j}\right) \sum_{l=1}^{K} g_{l \mid j}\left(x, \gamma_{l j}\right) P\left(y \mid x, \theta_{j l}\right)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\Psi=\left\{\gamma_{j}, \gamma_{j l}, \theta_{j l}\right\}</span><script type="math/tex">\Psi=\left\{\gamma_{j}, \gamma_{j l}, \theta_{j l}\right\}</script></span></p>
<p>Estimate <span class="arithmatex"><span class="MathJax_Preview">\Psi</span><script type="math/tex">\Psi</script></span> by maximizing the log-likelihood</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\max _{\Psi} \sum_{i=1}^{n} \log P\left(y_{i} \mid X_{i}, \Psi\right)
</div>
<script type="math/tex; mode=display">
\max _{\Psi} \sum_{i=1}^{n} \log P\left(y_{i} \mid X_{i}, \Psi\right)
</script>
</div>
<p>é‡‡ç”¨ EM ç®—æ³•æ±‚è§£.</p>
<p>Introduce the hidden variables <span class="arithmatex"><span class="MathJax_Preview">\Delta_{j}^{i}</span><script type="math/tex">\Delta_{j}^{i}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\Delta_{l \mid j}^{i}</span><script type="math/tex">\Delta_{l \mid j}^{i}</script></span> to indicate the underlying branch decision</p>
<ul>
<li>E-step: Compute posterior probabilities of <span class="arithmatex"><span class="MathJax_Preview">\Delta_{j}^{i}</span><script type="math/tex">\Delta_{j}^{i}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\Delta_{/ j j}^{i}</span><script type="math/tex">\Delta_{/ j j}^{i}</script></span> given <span class="arithmatex"><span class="MathJax_Preview">\Psi^{(t)}</span><script type="math/tex">\Psi^{(t)}</script></span></li>
<li>M-step: Compute <span class="arithmatex"><span class="MathJax_Preview">\Psi^{(t+1)}</span><script type="math/tex">\Psi^{(t+1)}</script></span> by maximization of the expected log-likelihood <span class="arithmatex"><span class="MathJax_Preview">\Psi^{(t+1)}=E_{\Delta \mid X, \Psi^{(t)}} \log L(\Psi \mid \Delta, X)</span><script type="math/tex">\Psi^{(t+1)}=E_{\Delta \mid X, \Psi^{(t)}} \log L(\Psi \mid \Delta, X)</script></span></li>
</ul>
<h4 id="hme-or-cart">HME or CART<a class="headerlink" href="#hme-or-cart" title="Permanent link">&para;</a></h4>
<ul>
<li>Advantages of HME over CART<ul>
<li>Smooth final regression function - <strong>soft splits</strong> allow for smooth transitions from high to low responses</li>
<li>Easier to optimize parameters - the log likelihood is a smooth function and is amendable to numerical optimization</li>
</ul>
</li>
<li>Disadtanges of HME over CART<ul>
<li>Tree topology - no good way for HME</li>
<li>Hard to interpret the model</li>
</ul>
</li>
</ul>
<h2 id="boosting-additive-trees">Boosting &amp; Additive Trees<a class="headerlink" href="#boosting-additive-trees" title="Permanent link">&para;</a></h2>
<p>Boosting æ˜¯æœ€è¿‘ 20 å¹´å†…æå‡ºçš„æœ€æœ‰åŠ›çš„å­¦ä¹ æ–¹æ³•ï¼æœ€åˆæ˜¯ä¸ºäº†åˆ†ç±»é—®é¢˜è€Œè®¾è®¡çš„ï¼Œä½†æ˜¯æˆ‘ä»¬å°†åœ¨è¿™ç« ä¸­çœ‹åˆ°ï¼Œå®ƒä¹Ÿå¯ä»¥å¾ˆå¥½åœ°æ‰©å±•åˆ°å›å½’é—®é¢˜ä¸Šï¼Boostingçš„åŠ¨æœºæ˜¯é›†åˆè®¸å¤šå¼±å­¦ä¹ çš„ç»“æœæ¥å¾—åˆ°æœ‰ç”¨çš„â€œcommitteeâ€ï¼ä»è¿™ç‚¹çœ‹ boosting ä¸ bagging ä»¥åŠå…¶ä»–çš„åŸºäº committee çš„æ–¹å¼ï¼ˆ8.8 èŠ‚ï¼‰ç±»ä¼¼ï¼ç„¶è€Œï¼Œæˆ‘ä»¬åº”è¯¥çœ‹åˆ°è¿™ç§è”ç³»æœ€å¤šæ˜¯è¡¨é¢ä¸Šçš„ï¼Œboosting åœ¨æ ¹æœ¬ä¸Šå­˜åœ¨ä¸åŒï¼</p>
<h3 id="boosting-methods">Boosting Methods<a class="headerlink" href="#boosting-methods" title="Permanent link">&para;</a></h3>
<p>Overview of Boosting</p>
<ul>
<li>Boosting is a procedure to combine the output of many weak classifiers to produce a powerful committee</li>
<li>A weak classifier is one whose error rate is only slightly better than random guessing</li>
<li>Boosting produces a sequence of weak classifier <span class="arithmatex"><span class="MathJax_Preview">G_{m}(x)</span><script type="math/tex">G_{m}(x)</script></span> for <span class="arithmatex"><span class="MathJax_Preview">m=1, \ldots, M</span><script type="math/tex">m=1, \ldots, M</script></span> whose predictions are then combined</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
G(x)=\operatorname{sgn}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)
</div>
<script type="math/tex; mode=display">
G(x)=\operatorname{sgn}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)
</script>
</div>
<p>through a weighted majority to produce the final prediction</p>
<ul>
<li>Each <span class="arithmatex"><span class="MathJax_Preview">\alpha_{m}&gt;0</span><script type="math/tex">\alpha_{m}>0</script></span> is computed by the boosting algorithm and reflects how accurately <span class="arithmatex"><span class="MathJax_Preview">G_{m}</span><script type="math/tex">G_{m}</script></span> classified the data.</li>
</ul>
<p>Most Popular Boosting Algorithm</p>
<p><strong>AdaBoost.M1</strong> algorithm of Freund \&amp; Schapire (1997)</p>
<ul>
<li>Have training data <span class="arithmatex"><span class="MathJax_Preview">\left(x_{i}, y_{i}\right), i=1,2, \ldots, n</span><script type="math/tex">\left(x_{i}, y_{i}\right), i=1,2, \ldots, n</script></span></li>
<li>Introduce a weight <span class="arithmatex"><span class="MathJax_Preview">\omega_{i}=\frac{1}{n}</span><script type="math/tex">\omega_{i}=\frac{1}{n}</script></span> for each training example</li>
<li>for <span class="arithmatex"><span class="MathJax_Preview">m=1, \ldots, M</span><script type="math/tex">m=1, \ldots, M</script></span>
  1. Let <span class="arithmatex"><span class="MathJax_Preview">G_{m}</span><script type="math/tex">G_{m}</script></span> be the weak classifier with minimum error
  $$
  e r r_{m}=\sum_{i=1}^{n} \omega_{i} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)
  $$
  2. Set <span class="arithmatex"><span class="MathJax_Preview">\alpha_{m}=\log \frac{1-e r r_{m}}{e r r_{m}}</span><script type="math/tex">\alpha_{m}=\log \frac{1-e r r_{m}}{e r r_{m}}</script></span>
  3. Set
  $$
  \omega_{i} \leftarrow \omega_{i} e^{\alpha_{m} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}
  $$
  This increases (decreases) <span class="arithmatex"><span class="MathJax_Preview">\omega_{i}</span><script type="math/tex">\omega_{i}</script></span> for <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> misclassified (correctly classified) by <span class="arithmatex"><span class="MathJax_Preview">G_{m}</span><script type="math/tex">G_{m}</script></span>
  4. Normalize the <span class="arithmatex"><span class="MathJax_Preview">\omega_{i}</span><script type="math/tex">\omega_{i}</script></span> &lsquo;s so that they sum to 1</li>
</ul>
<p>What AdaBoost.MI Does</p>
<ul>
<li>As iterations proceed, observations difficult to classify correctly receive more influence</li>
<li>Each successive classifier is forced to concentrate on training observations missed by previous ones in the sequence</li>
</ul>
<p><img alt="" src="../media/ASL-note3/2021-12-28-11-25-48.png" /></p>
<p>è¿™é‡Œç»™å‡ºäº†ä¸€ä¸ªå¾ˆç›´è§‚çš„ä¾‹å­: å¯¹äºä¸€ä¸ªå¯¹è§’çº¿åˆ†å‰²çš„æ•°æ®, é‡‡ç”¨çš„åŸº/å¼±åˆ†ç±»å™¨ä¸ºåªèƒ½åœ¨åæ ‡è½´ä¸Šçš„åˆ’åˆ†. é€šè¿‡è°ƒæ•´æ¯ä¸ª sample çš„æƒé‡, æ¯ä¸€è½®è®­ç»ƒçš„åˆ†ç±»å™¨ä¼šè¶Šæ¥è¶Šå…³æ³¨é‚£äº›å¾ˆéš¾è¢«åˆ’åˆ†çš„ä¾‹å­, ä¾‹å¦‚å·¦å›¾ä¸­å¯¹è§’çº¿ä¸¤ç«¯çš„ sample. è°ƒæ•´æƒé‡ååŸºäºæ–°çš„ weight è®­ç»ƒå¾—åˆ°çš„åˆ†ç±»å™¨å¦‚å·¦å›¾æ‰€ç¤º, å¹¶è¿›ä¸€æ­¥ reweight æ ·æœ¬, æ›´æ–°æ€»ä½“çš„åˆ†ç±»æ¨¡å‹ <span class="arithmatex"><span class="MathJax_Preview">G(x)</span><script type="math/tex">G(x)</script></span>.</p>
<p>Now we need to&hellip;</p>
<ul>
<li>Show AdaBoost fits an additive model in a base learner, optimizing a novel exponential loss function</li>
<li>Show the population minimizer of the exponential loss function is the log-odds of the class probabilities</li>
<li>Present loss function that are more robust than squared error or exponential loss</li>
<li>Argue decision trees are an ideal base learner for data mining applications of boosting</li>
<li>Develop class of gradient boosted methods (GBMs), for boosting trees with any loss function</li>
<li>Emphasize the importance of &ldquo;slow learning&rdquo;</li>
</ul>
<h3 id="boosting-fits-an-additive-model">Boosting Fits an Additive Model<a class="headerlink" href="#boosting-fits-an-additive-model" title="Permanent link">&para;</a></h3>
<p>Additive Model</p>
<p>Boosting fits an additive expansion in a set of elementary basis functions
$$
G(x)=\operatorname{sgn}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)
$$
The basis functions are the weak classifiers <span class="arithmatex"><span class="MathJax_Preview">G_{m}(x) \in\{-1,1\}</span><script type="math/tex">G_{m}(x) \in\{-1,1\}</script></span></p>
<p>More generally, basis functions expansions take the form
$$
f(x)=\sum_{m=1}^{M} \beta_{m} b\left(x ; \gamma_{m}\right)
$$
where <span class="arithmatex"><span class="MathJax_Preview">\beta_{m}</span><script type="math/tex">\beta_{m}</script></span> &lsquo;s are the expansion coefficients and <span class="arithmatex"><span class="MathJax_Preview">b(x ; \gamma) \in \mathbb{R}</span><script type="math/tex">b(x ; \gamma) \in \mathbb{R}</script></span> are simple functions of the input <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> parameterized by <span class="arithmatex"><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span></p>
<p>Examples of Additive Models</p>
<ul>
<li><strong>Single-hidden-layer neural networks</strong> å•å±‚ç¥ç»ç½‘ç»œä¸­ï¼ˆç¬¬ 11 ç« ï¼‰, where
$$
b(x ; \gamma)=\frac{1}{1+\exp \left(-\gamma_{0}-\gamma_{1}^{\top} x\right)}
$$</li>
<li><strong>Multivariate adaptive regression splines</strong> (MARS) å¤šå…ƒè‡ªé€‚åº”å›å½’æ ·æ¡ï¼ˆ9.4 èŠ‚ï¼‰: use trucated-power spline basis functions where <span class="arithmatex"><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> parameterizes the variables and values for the knots</li>
<li><strong>Trees</strong>: <span class="arithmatex"><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> parameterizes the split variables and split points at the internal nodes, and the predictions at the terminal nodes</li>
</ul>
<h4 id="fitting-additive-models">Fitting Additive Models<a class="headerlink" href="#fitting-additive-models" title="Permanent link">&para;</a></h4>
<p>å‘å‰é€æ­¥åŠ æ³•å»ºæ¨¡</p>
<p>Typically fit models by minimizing a loss function averaged over the training data
$$
\min <em 1="1">{\beta</em>\right)\right)
$$
For many loss functions }, \gamma_{1}, \ldots, \beta_{M}, \gamma_{M}} \sum_{i=1}^{n} L\left(y_{i}, \sum_{m=1}^{M} \beta_{m} b\left(x_{i}, \gamma_{m<span class="arithmatex"><span class="MathJax_Preview">L(x, f(X))</span><script type="math/tex">L(x, f(X))</script></span>, basis functions <span class="arithmatex"><span class="MathJax_Preview">b(x ; \gamma)</span><script type="math/tex">b(x ; \gamma)</script></span> is hard to decide</p>
<p>Forward Stagewise Additive Modeling</p>
<p>Greedily add one basis function at a time in the following fashion</p>
<ul>
<li>Set <span class="arithmatex"><span class="MathJax_Preview">f_{0}(x)=0</span><script type="math/tex">f_{0}(x)=0</script></span></li>
<li>for <span class="arithmatex"><span class="MathJax_Preview">m=1, \ldots, M</span><script type="math/tex">m=1, \ldots, M</script></span>
  1. Compute
  $$
  \left(\hat{\beta}<em m="m">{m}, \hat{\gamma}</em>}\right)=\operatorname{argmin<em m="m">{\beta</em>\right)\right)
  $$
  2. Set
  $$
  f_{m}(x)=f_{m-1}(x)+\hat{\beta}}, \gamma_{m}} \sum_{n=1}^{n} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\beta_{m} b\left(x_{i} ; \gamma_{m<em m="m">{m} b\left(x ; \hat{\gamma}</em>\right)
  $$</li>
<li>Note: Previously added terms are not modified</li>
</ul>
<h3 id="exponential-loss-adaboost">Exponential Loss &amp; AdaBoost<a class="headerlink" href="#exponential-loss-adaboost" title="Permanent link">&para;</a></h3>
<p>Forward Stagewise Additive Modeling \&amp; Boosting ç°åœ¨è¯æ˜ AdaBoost.M1 ç®—æ³•ï¼ˆç®—æ³• 10.1ï¼‰ç­‰ä»·äºä½¿ç”¨ä¸‹åˆ—æŸå¤±å‡½æ•°çš„å‘å‰é€æ­¥åŠ æ³•å»ºæ¨¡ (è¿™é‡ŒäºŒåˆ†ç±»æ ‡ç­¾ä¸º <span class="arithmatex"><span class="MathJax_Preview">\{1, -1\}</span><script type="math/tex">\{1, -1\}</script></span>)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L(y, f(x)) = \exp(-yf(x))
</div>
<script type="math/tex; mode=display">
L(y, f(x)) = \exp(-yf(x))
</script>
</div>
<p>At each iteration of forward stagewise additive modeling, must solve this optimization problem</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
\left(\beta_{m}, G_{m}\right) &amp;=\operatorname{argmin}_{\beta, G} \sum_{i=1}^{n} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\beta G\left(x_{i}\right)\right) \\
&amp;=\operatorname{argmin}_{\beta, G} \sum_{i=1}^{n} \exp \left\{-y_{i}\left(f_{m-1}\left(x_{i}\right)+\beta G\left(x_{i}\right)\right)\right\}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\left(\beta_{m}, G_{m}\right) &=\operatorname{argmin}_{\beta, G} \sum_{i=1}^{n} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\beta G\left(x_{i}\right)\right) \\
&=\operatorname{argmin}_{\beta, G} \sum_{i=1}^{n} \exp \left\{-y_{i}\left(f_{m-1}\left(x_{i}\right)+\beta G\left(x_{i}\right)\right)\right\}
\end{aligned}
</script>
</div>
<p>where we assume an <strong>exponential loss</strong> for <span class="arithmatex"><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> and <span class="arithmatex"><span class="MathJax_Preview">G(x) \in\{-1,1\}</span><script type="math/tex">G(x) \in\{-1,1\}</script></span></p>
<p>Re-write: The optimization problem becomes</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;\min _{\beta, G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\} \\
&amp;=\min_{\beta}\left(\min _{G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\}\right)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&\min _{\beta, G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\} \\
&=\min_{\beta}\left(\min _{G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\}\right)
\end{aligned}
</script>
</div>
<p>Note</p>
<div class="arithmatex">
<div class="MathJax_Preview">
y_{i} G\left(x_{i}\right)= \begin{cases}1 &amp; \text { if } y_{i}=G\left(x_{i}\right) \\ -1 &amp; \text { if } y_{1} \neq G\left(x_{i}\right)\end{cases}
</div>
<script type="math/tex; mode=display">
y_{i} G\left(x_{i}\right)= \begin{cases}1 & \text { if } y_{i}=G\left(x_{i}\right) \\ -1 & \text { if } y_{1} \neq G\left(x_{i}\right)\end{cases}
</script>
</div>
<p>This implies <span class="arithmatex"><span class="MathJax_Preview">\exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\}</span><script type="math/tex">\exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\}</script></span> is equal to</p>
<div class="arithmatex">
<div class="MathJax_Preview">
e^{\beta} I\left(y_{i} \neq G\left(x_{i}\right)\right)+e^{-\beta}\left(1-I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)
</div>
<script type="math/tex; mode=display">
e^{\beta} I\left(y_{i} \neq G\left(x_{i}\right)\right)+e^{-\beta}\left(1-I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)
</script>
</div>
<p>å…ˆæ¥çœ‹åˆ†ç±»å™¨çš„ä¼˜åŒ– Optimization for <span class="arithmatex"><span class="MathJax_Preview">G_{m}</span><script type="math/tex">G_{m}</script></span></p>
<p>The optimization problem becomes</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp; \operatorname{argmin}_{G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\} \\
=&amp; \operatorname{argmin}_{G}\left(\left(e^{\beta}-e^{-\beta}\right) \sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G\left(x_{i}\right)\right)+e^{-\beta} \sum_{i=1}^{n} \omega_{i}^{(m)}\right) \\
=&amp; \operatorname{argmin}_{G}\left(\sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
& \operatorname{argmin}_{G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\} \\
=& \operatorname{argmin}_{G}\left(\left(e^{\beta}-e^{-\beta}\right) \sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G\left(x_{i}\right)\right)+e^{-\beta} \sum_{i=1}^{n} \omega_{i}^{(m)}\right) \\
=& \operatorname{argmin}_{G}\left(\sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)
\end{aligned}
</script>
</div>
<p>Therefore</p>
<div class="arithmatex">
<div class="MathJax_Preview">
G_{m}=\operatorname{argmin}_{G}\left(\sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)
</div>
<script type="math/tex; mode=display">
G_{m}=\operatorname{argmin}_{G}\left(\sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)
</script>
</div>
<p><span class="arithmatex"><span class="MathJax_Preview">G_{m}</span><script type="math/tex">G_{m}</script></span> minimizes the weighted error in the AdaBoost algorithm (given <span class="arithmatex"><span class="MathJax_Preview">\omega_{i}^{(m)}</span><script type="math/tex">\omega_{i}^{(m)}</script></span> &lsquo;s have the same definition)</p>
<p>å†æ¥çœ‹æƒé‡ Optimization for <span class="arithmatex"><span class="MathJax_Preview">\beta_{m}</span><script type="math/tex">\beta_{m}</script></span>
Plug <span class="arithmatex"><span class="MathJax_Preview">G_{m}</span><script type="math/tex">G_{m}</script></span> into the original optimization problem</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\min _{\beta}\left(\min_{G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\}\right)
</div>
<script type="math/tex; mode=display">
\min _{\beta}\left(\min_{G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\}\right)
</script>
</div>
<p>and using the previous result, it becomes</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{argmin}_{\beta}\left(\left(e^{\beta}-e^{-\beta}\right) \sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)+e^{-\beta} \sum_{i=1}^{n} \omega_{i}^{(m)}\right)
</div>
<script type="math/tex; mode=display">
\operatorname{argmin}_{\beta}\left(\left(e^{\beta}-e^{-\beta}\right) \sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)+e^{-\beta} \sum_{i=1}^{n} \omega_{i}^{(m)}\right)
</script>
</div>
<p>This quantity is minimized when (æ±‚å¯¼å³å¯)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\beta_{m}=\frac{1}{2} \log \frac{1-e r r_{m}}{e r r_{m}}
</div>
<script type="math/tex; mode=display">
\beta_{m}=\frac{1}{2} \log \frac{1-e r r_{m}}{e r r_{m}}
</script>
</div>
<p>where</p>
<div class="arithmatex">
<div class="MathJax_Preview">
e r r_{m}=\frac{\sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}{\sum_{i=1}^{n} \omega_{i}^{(m)}}
</div>
<script type="math/tex; mode=display">
e r r_{m}=\frac{\sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}{\sum_{i=1}^{n} \omega_{i}^{(m)}}
</script>
</div>
<h4 id="_2">ç­‰ä»·æ€§<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h4>
<p>æ³¨æ„ Update of <span class="arithmatex"><span class="MathJax_Preview">\omega_{i}^{(m)}</span><script type="math/tex">\omega_{i}^{(m)}</script></span></p>
<p>Need the following result</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
-y_{i} G_{m}\left(x_{i}\right) &amp;=-I\left(y_{i}=G\left(x_{i}\right)\right)+I\left(y_{i} \neq G\left(x_{i}\right)\right) \\
&amp;=-\left(1-I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)+I\left(y_{i} \neq G\left(x_{i}\right)\right) \\
&amp;=-1+2 I\left(y_{i} \neq G\left(x_{i}\right)\right)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
-y_{i} G_{m}\left(x_{i}\right) &=-I\left(y_{i}=G\left(x_{i}\right)\right)+I\left(y_{i} \neq G\left(x_{i}\right)\right) \\
&=-\left(1-I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)+I\left(y_{i} \neq G\left(x_{i}\right)\right) \\
&=-1+2 I\left(y_{i} \neq G\left(x_{i}\right)\right)
\end{aligned}
</script>
</div>
<p>The updated weights can then be written as</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
\omega_{i}^{(m+1)}=e^{-y_{i} f_{m}\left(x_{i}\right)} &amp;=e^{-y_{i}\left(f_{m-1}\left(x_{i}\right)+\beta_{m} G_{m}\left(x_{i}\right)\right.} \\
&amp;=\omega_{i}^{(m)} e^{-y_{i} \beta_{m} G_{m}\left(x_{i}\right)} \\
&amp;=\omega_{i}^{(m)} e^{2 \beta_{m} I\left(y_{i} \neq G m\left(x_{i}\right)\right)} e^{-\beta_{m}}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\omega_{i}^{(m+1)}=e^{-y_{i} f_{m}\left(x_{i}\right)} &=e^{-y_{i}\left(f_{m-1}\left(x_{i}\right)+\beta_{m} G_{m}\left(x_{i}\right)\right.} \\
&=\omega_{i}^{(m)} e^{-y_{i} \beta_{m} G_{m}\left(x_{i}\right)} \\
&=\omega_{i}^{(m)} e^{2 \beta_{m} I\left(y_{i} \neq G m\left(x_{i}\right)\right)} e^{-\beta_{m}}
\end{aligned}
</script>
</div>
<p>æ¯”è¾ƒ Adaboost å’Œ minimizing exponential loss in the additive model çš„æ›´æ–°å…¬å¼:</p>
<ol>
<li>æ¨¡å‹æƒé‡</li>
</ol>
<p>Remember from the AdaBoost algorithm,</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\alpha_{m}=\log \frac{1-e r r_{m}}{e r r_{m}}
</div>
<script type="math/tex; mode=display">
\alpha_{m}=\log \frac{1-e r r_{m}}{e r r_{m}}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">e r r_{m}=\sum_{i=1}^{n} \omega_{i} I\left(y_{i} \neq G_m\left(x_{i}\right)\right)</span><script type="math/tex">e r r_{m}=\sum_{i=1}^{n} \omega_{i} I\left(y_{i} \neq G_m\left(x_{i}\right)\right)</script></span>
From minimizing exponential loss in the additive model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\beta_{m}=\frac{1}{2} \log \frac{1-e r r_{m}}{e r r_{m}}
</div>
<script type="math/tex; mode=display">
\beta_{m}=\frac{1}{2} \log \frac{1-e r r_{m}}{e r r_{m}}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">e r r_{m}=\frac{\sum_{i=1}^{n} \omega_{i}^{(m)}\left(\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)\right.}{\sum_{i=1}^{n} \omega_{i}^{(m)}}</span><script type="math/tex">e r r_{m}=\frac{\sum_{i=1}^{n} \omega_{i}^{(m)}\left(\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)\right.}{\sum_{i=1}^{n} \omega_{i}^{(m)}}</script></span></p>
<p>è¿™é‡Œå½¢å¼ä¸Šå¥½åƒå·®äº†ç³»æ•° <span class="arithmatex"><span class="MathJax_Preview">1/2</span><script type="math/tex">1/2</script></span>, å®é™…ä¸Šå¦‚æœæ‰€æœ‰åŸºæ¨¡å‹çš„æƒé‡éƒ½ç¼©å°ä¸€ä¸ªå¸¸æ•°æ˜¯ä¸€æ ·çš„. (äº‹å®ä¸Š, æèˆªä¹¦ä¸­çš„ Adaboost æ¨¡å‹ç³»æ•°æ˜¯å¸¦ <span class="arithmatex"><span class="MathJax_Preview">1/2</span><script type="math/tex">1/2</script></span> çš„.)</p>
<ol start="2">
<li>æ ·æœ¬æƒé‡</li>
</ol>
<p>Remember from the AdaBoost algorithm,</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\omega_{i} \leftarrow \omega_{i} e^{\alpha_{m} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}
</div>
<script type="math/tex; mode=display">
\omega_{i} \leftarrow \omega_{i} e^{\alpha_{m} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\alpha_{m}=\log \frac{1-e r r_{m}}{e r r_{m}}</span><script type="math/tex">\alpha_{m}=\log \frac{1-e r r_{m}}{e r r_{m}}</script></span>
From minimizing exponential loss in the additive model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\omega_{i}^{(m+1)}=\omega_{i}^{(m)} e^{2 \beta I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)} e^{\left(-\beta_{m}\right)}
</div>
<script type="math/tex; mode=display">
\omega_{i}^{(m+1)}=\omega_{i}^{(m)} e^{2 \beta I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)} e^{\left(-\beta_{m}\right)}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\beta_{m}=\frac{1}{2} \log \frac{1-e r r_{m}}{e r r_{m}}</span><script type="math/tex">\beta_{m}=\frac{1}{2} \log \frac{1-e r r_{m}}{e r r_{m}}</script></span>
Note: <span class="arithmatex"><span class="MathJax_Preview">e^{-\beta_{m}}</span><script type="math/tex">e^{-\beta_{m}}</script></span> is the same for all observations</p>
<p>æ€»ç»“ä¸€ä¸‹: We can view the AdaBoost.M1 algorithm as a method that approximates minimizing</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{argmin}_{\beta_{1}, G_{1}, \ldots, \beta_{M}, G_{M}} \sum_{i=1}^{n} \exp \left(-y_{i} \sum_{m=1}^{M} \beta_{m} G_{m}\left(x_{i}\right)\right)
</div>
<script type="math/tex; mode=display">
\operatorname{argmin}_{\beta_{1}, G_{1}, \ldots, \beta_{M}, G_{M}} \sum_{i=1}^{n} \exp \left(-y_{i} \sum_{m=1}^{M} \beta_{m} G_{m}\left(x_{i}\right)\right)
</script>
</div>
<p>via a forward stagewise additive modeling approach</p>
<p>from <a href="https://www.cs.toronto.edu/~mbrubake/teaching/C11/Handouts/AdaBoost.pdf">HERE</a></p>
<p><img alt="" src="../media/ASL-note3/2021-12-28-15-53-14.png" /></p>
<h3 id="loss-functions-robustness">Loss Functions &amp; Robustness<a class="headerlink" href="#loss-functions-robustness" title="Permanent link">&para;</a></h3>
<p>è¿™ä¸€èŠ‚æˆ‘ä»¬è¿›ä¸€æ­¥è®¨è®ºåˆ†ç±»å’Œå›å½’ä¸­ä¸åŒçš„æŸå¤±å‡½æ•°ï¼Œå¹¶ä¸”ç”¨å®ƒä»¬å¯¹æç«¯æ•°æ®çš„é²æ£’æ€§æ¥æè¿°ï¼</p>
<h4 id="loss-functions-for-classification">Loss Functions for Classification<a class="headerlink" href="#loss-functions-for-classification" title="Permanent link">&para;</a></h4>
<ul>
<li>Exponential loss</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
L(y, f(x))=\exp \{-y f(X)\}
</div>
<script type="math/tex; mode=display">
L(y, f(x))=\exp \{-y f(X)\}
</script>
</div>
<ul>
<li>Binomial deviance loss äºŒé¡¹åå·®</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
-L(y, f(x))=\log (1+\exp \{-2 y f(x)\})
</div>
<script type="math/tex; mode=display">
-L(y, f(x))=\log (1+\exp \{-2 y f(x)\})
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">p(x)=P(Y=1 \mid x)=\frac{1}{1+\exp \{-2 f(x)\}}</span><script type="math/tex">p(x)=P(Y=1 \mid x)=\frac{1}{1+\exp \{-2 f(x)\}}</script></span>
- Misclassification loss</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L(y, f(x))=I(y f(x)&lt;0)
</div>
<script type="math/tex; mode=display">
L(y, f(x))=I(y f(x)<0)
</script>
</div>
<p>The Margin</p>
<ul>
<li>These loss functions are functions of the &ldquo;margin&rdquo; é—´éš”: <span class="arithmatex"><span class="MathJax_Preview">y f(X)</span><script type="math/tex">y f(X)</script></span></li>
<li>Classification rule <span class="arithmatex"><span class="MathJax_Preview">G(x)=\operatorname{sign}\{f(X)\}</span><script type="math/tex">G(x)=\operatorname{sign}\{f(X)\}</script></span> <span class="arithmatex"><span class="MathJax_Preview">\rightarrow</span><script type="math/tex">\rightarrow</script></span> training examples with</li>
<li>positive margin <span class="arithmatex"><span class="MathJax_Preview">y_{i} f\left(x_{i}\right)&gt;0</span><script type="math/tex">y_{i} f\left(x_{i}\right)>0</script></span> are correctly classified</li>
<li>negative margin <span class="arithmatex"><span class="MathJax_Preview">y_{i} f\left(x_{i}\right)&lt;0</span><script type="math/tex">y_{i} f\left(x_{i}\right)<0</script></span> are misclassified</li>
<li>Decision boundary defined by <span class="arithmatex"><span class="MathJax_Preview">f(x)=0</span><script type="math/tex">f(x)=0</script></span></li>
<li>Classification algorithms attempt to produce positive margins for each training data point</li>
<li>Loss criterion for classification should penalize negative margins more heavily than positive margins</li>
</ul>
<p>å°½ç®¡æŒ‡æ•°æŸå¤± (10.8) å’ŒäºŒé¡¹åå·® (10.18) å½“åº”ç”¨åˆ°æ€»ä½“çš„è”åˆåˆ†å¸ƒæ—¶ä¼šå¾—åˆ°åŒæ ·çš„è§£, ä½†å¯¹äºæœ‰ é™çš„æ•°æ®é›†å¹¶ä¸ä¸€æ ·. ä¸¤ä¸ªå‡†åˆ™éƒ½æ˜¯ é—´éš” (margin) <span class="arithmatex"><span class="MathJax_Preview">y f(x)</span><script type="math/tex">y f(x)</script></span> çš„å•è°ƒé€’å‡å‡½æ•°. åœ¨åˆ†ç±»é—®é¢˜ä¸­ (å“åº” å˜é‡ä¸º <span class="arithmatex"><span class="MathJax_Preview">-1 / 1), y f(x)</span><script type="math/tex">-1 / 1), y f(x)</script></span> ç±»ä¼¼å›å½’ä¸­çš„æ®‹å·® <span class="arithmatex"><span class="MathJax_Preview">y-f(x)</span><script type="math/tex">y-f(x)</script></span>. åˆ†ç±»å‡†åˆ™ <span class="arithmatex"><span class="MathJax_Preview">G(x)=\operatorname{sign}[f(x)]</span><script type="math/tex">G(x)=\operatorname{sign}[f(x)]</script></span> è¡¨æ˜æœ‰æ­£é—´éš” <span class="arithmatex"><span class="MathJax_Preview">y_{i} f\left(x_{i}\right)&gt;0</span><script type="math/tex">y_{i} f\left(x_{i}\right)>0</script></span> çš„è§‚æµ‹è¢«åˆ†ç±»æ­£ç¡®, è€Œæœ‰è´Ÿé—´éš” <span class="arithmatex"><span class="MathJax_Preview">y_{i} f\left(x_{i}\right)&lt;0</span><script type="math/tex">y_{i} f\left(x_{i}\right)<0</script></span> çš„è§‚æµ‹è¢«é”™è¯¯åˆ†ç±». åˆ¤åˆ«è¾¹ç•Œå®šä¹‰ä¸º <span class="arithmatex"><span class="MathJax_Preview">f(x)=0</span><script type="math/tex">f(x)=0</script></span>. åˆ†ç±»ç®—æ³•çš„ç›®æ ‡æ˜¯å¾—åˆ°å°½å¯èƒ½é¢‘ç¹çš„æ­£é—´éš”. ä»»ä½•ç”¨äºåˆ†ç±»çš„æŸå¤±æ ‡å‡†åº”è¯¥æƒ©ç½šè´Ÿé—´éš” æ¯”æ­£é—´éš”æ›´é‡, å› ä¸ºæ­£é—´éš”çš„è§‚æµ‹å€¼å·²ç»è¢«æ­£ç¡®åˆ†ç±».</p>
<p>å›¾ <span class="arithmatex"><span class="MathJax_Preview">10.4</span><script type="math/tex">10.4</script></span> å±•ç¤ºäº†æŒ‡æ•° (10.8) å’ŒäºŒé¡¹åå·®çš„æ ‡å‡†ä½œä¸ºé—´éš” <span class="arithmatex"><span class="MathJax_Preview">y \cdot f(x)</span><script type="math/tex">y \cdot f(x)</script></span> çš„å‡½æ•°çš„å›¾è±¡. ä¹Ÿæ˜¾ç¤ºäº†è¯¯åˆ†ç±»æŸ å¤± <span class="arithmatex"><span class="MathJax_Preview">L(y, f(x))=I(y \cdot f(x)&lt;0)</span><script type="math/tex">L(y, f(x))=I(y \cdot f(x)<0)</script></span>, å®ƒç»™å‡ºäº†ä»¬é—´éš”çš„å•ä½æƒ©ç½š, è€Œå¯¹æ‰€æœ‰çš„æ­£å€¼æ²¡æœ‰æƒ©ç½š. æŒ‡æ•°æŸ å¤±å’ŒäºŒé¡¹åå·®éƒ½å¯ä»¥çœ‹æˆæ˜¯è¯¯åˆ†ç±»æŸå¤±çš„å•è°ƒè¿ç»­è¿‘ä¼¼. å®ƒä»¬ä¸æ–­åœ°æƒ©ç½šè¶Šæ¥è¶Šå¤§çš„è´Ÿè¾¹é™…å€¼, è¡™ ç½šåŠ›åº¦æ¯”å®ƒä»¬å›æŠ¥è¶Šæ¥è¶Šå¤§çš„æ­£è¾¹é™…å€¼æ›´é‡. å®ƒä»¬çš„åŒºåˆ«åœ¨äºç¨‹åº¦ä¸Š. äºŒé¡¹åå·®çš„å¿¢ç½šå¯¹äºå¤§çš„å¢ åŠ çš„è´Ÿé—´éš”çº¿æ€§å¢é•¿, è€ŒæŒ‡æ•°æ ‡å‡†å¯¹è¿™æ ·è§‚æµ‹çš„å½±å“æŒ‡æ•°å¢é•¿.</p>
<p>åœ¨è®­ç»ƒè¿‡ç¨‹çš„ä»»ä¸€ä¸ªæ—¶åˆ»ï¼ŒæŒ‡æ•°æ ‡å‡†å¯¹å…·æœ‰å¤§çš„è´Ÿè¾¹é™…çš„è§‚æµ‹å€¼ä¸Šæœ‰æ›´å¤§çš„å½±å“ï¼äºŒé¡¹åå·®ç›¸å¯¹åœ°åœ¨è¿™äº›è§‚æµ‹ä¸Šå½±å“è¾ƒå°ï¼Œåœ¨æ‰€æœ‰æ•°æ®ä¸Šçš„å½±å“åˆ†æ•£æ›´å‡åŒ€ï¼ä¹Ÿå› æ­¤åœ¨ç™½å™ªå£°è®¾å®šï¼ˆè´å¶æ–¯è¯¯å·®ç‡ä¸æ¥è¿‘äº0ï¼‰ä¸­æ›´åŠ é²æ£’ï¼Œç‰¹åˆ«åœ¨è®­ç»ƒæ•°æ®ä¸­å­˜åœ¨è¯¯åˆ†ç±»æ ‡ç­¾çš„æƒ…å½¢ä¸­ï¼åœ¨è¿™äº›æƒ…å½¢ä¸‹ï¼ŒAdaBoost çš„è¡¨ç°ä»ç»éªŒä¸Šçœ‹æ˜¾è‘—é€€åŒ–ï¼[ä¹Ÿå³æŒ‡æ•°æŸå¤±ç¨³å®šæ€§è¾ƒå·®]</p>
<p><img alt="" src="../media/ASL-note3/2021-12-28-16-00-26.png" /></p>
<p>æ€»ç»“ä¸€ä¸‹ æŒ‡æ•°æŸå¤±å’ŒäºŒé¡¹åå·®</p>
<ul>
<li>Exponential and deviance loss continuous approx to <strong>misclassification loss</strong></li>
<li>They increasingly penalize negative margin values more heavily than they reward positive ones</li>
<li>Binomial deviance penalty increases linearly with negative margins</li>
<li>Exponential loss penalty increases exponentially with negative margin</li>
<li>Exponential criterion concentrates more of its efforts on large negative margin observations than the binomial criterion</li>
<li>Binomial criterion is far more robust than the exponential criterion in noisy settings</li>
</ul>
<h4 id="robust-loss-functions-for-regression">Robust Loss Functions for Regression<a class="headerlink" href="#robust-loss-functions-for-regression" title="Permanent link">&para;</a></h4>
<ul>
<li>Squared error loss</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
L(y, f(x))=(y-f(x))^{2}
</div>
<script type="math/tex; mode=display">
L(y, f(x))=(y-f(x))^{2}
</script>
</div>
<p>population optimum for this loss function: <span class="arithmatex"><span class="MathJax_Preview">f(X)=E[Y \mid x]</span><script type="math/tex">f(X)=E[Y \mid x]</script></span>
- Absolute loss</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L(y, f(x))=|y-f(x)|
</div>
<script type="math/tex; mode=display">
L(y, f(x))=|y-f(x)|
</script>
</div>
<p>population optimum for this loss function: <span class="arithmatex"><span class="MathJax_Preview">f(x)=\operatorname{median}(Y \mid x)</span><script type="math/tex">f(x)=\operatorname{median}(Y \mid x)</script></span> ä¸­ä½æ•°</p>
<p>ä¸¤è€…çš„å…³ç³»ç±»ä¼¼ä¸Šé¢åˆ†ç±»æŸå¤±çš„æ¯”è¾ƒ</p>
<ul>
<li>On finite samples, squared error loss puts far more emphasis on observations with large <span class="arithmatex"><span class="MathJax_Preview">| y_{i}-f\left(x_{i}\right) |</span><script type="math/tex">| y_{i}-f\left(x_{i}\right) |</script></span> than absolute loss</li>
<li>Thus squared error loss is less robust and performance degrades for long-tail error distributions and mis-labellings.</li>
</ul>
<p>Huber loss</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L(y, f(x))= \begin{cases}(y-f(x))^{2} &amp; \text { for }|y-f(x)| \leqslant \delta \\ 2 \delta|y-f(X)|-\delta^{2} &amp; \text { otherwise }\end{cases}
</div>
<script type="math/tex; mode=display">
L(y, f(x))= \begin{cases}(y-f(x))^{2} & \text { for }|y-f(x)| \leqslant \delta \\ 2 \delta|y-f(X)|-\delta^{2} & \text { otherwise }\end{cases}
</script>
</div>
<ul>
<li>strong resistance to gross outliers while</li>
<li>being nearly as efficient as least squares for Gaussian errors</li>
</ul>
<p>Combine the good properties of squared-error near zero and absolute error loss when <span class="arithmatex"><span class="MathJax_Preview">|y-f|</span><script type="math/tex">|y-f|</script></span> is large</p>
<p><img alt="" src="../media/ASL-note3/2021-12-28-16-13-36.png" /></p>
<h4 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h4>
<ul>
<li>When robustness is an issue, 1. squared error loss for regression; 2. exponential loss for classification are not the best criterion to be optimizing</li>
<li>But both loss functions lead to elegant modular boosting algorithms in the context of <strong>forward stagewise additive modeling</strong></li>
<li>For classification: perform a weighted fit of the base learner to the outputs <span class="arithmatex"><span class="MathJax_Preview">y_{i}</span><script type="math/tex">y_{i}</script></span> with weights <span class="arithmatex"><span class="MathJax_Preview">\omega_{i}=\exp \left\{ - y_{i} f\left(x_{i}\right)\right\}</span><script type="math/tex">\omega_{i}=\exp \left\{ - y_{i} f\left(x_{i}\right)\right\}</script></span></li>
<li>For regression: with squared-error loss one simply fits the base learner to the residuals from the current model <span class="arithmatex"><span class="MathJax_Preview">y_iâˆ’ f_{mâˆ’1}(x_i)</span><script type="math/tex">y_iâˆ’ f_{mâˆ’1}(x_i)</script></span> at each step</li>
<li>More robust criteria in their place do not give rise to such simple feasible boosting algorithms</li>
</ul>
<h3 id="off-the-shelf-procedure-for-data-mining">â€Off-the-Shelfâ€ Procedure for Data Mining<a class="headerlink" href="#off-the-shelf-procedure-for-data-mining" title="Permanent link">&para;</a></h3>
<p><img alt="" src="../media/ASL-note3/2021-12-28-16-23-30.png" /></p>
<p>æ•°æ®æŒ–æ˜çš„ â€œç°è´§â€(off-the-shelf) æ–¹æ³•ï¼ç°è´§æ–¹æ³•æŒ‡çš„æ˜¯å¯ä»¥ç›´æ¥åº”ç”¨åˆ°æ•°æ®ä¸­è€Œä¸éœ€è¦å¤§é‡æ—¶é—´è¿›è¡Œæ•°æ®é¢„å¤„ç†æˆ–è€…å­¦ä¹ è¿‡ç¨‹çš„ç²¾å¿ƒè°ƒå‚ï¼</p>
<p>Trees are great except&hellip;</p>
<ul>
<li>they are inaccurate at making predictions</li>
</ul>
<p>Boosting decision trees improve their accuracy at the cost of</p>
<ul>
<li>speed</li>
<li>interpretability</li>
<li>for AdaBoost, robustness against overlapping class distributions and especially mislabelling of the training data</li>
</ul>
<p>A gradient boosted model is a generalization of tree boosting that attempts to mitigate these problems</p>
<p>It aims to produce an accurate and effective off-the-shelf procedure for data mining</p>
<h3 id="boosting-trees">Boosting Trees<a class="headerlink" href="#boosting-trees" title="Permanent link">&para;</a></h3>
<p>Regression Tree Recap</p>
<ul>
<li>Tree partitions the input space into <span class="arithmatex"><span class="MathJax_Preview">R_{j}, j=1, \ldots, J</span><script type="math/tex">R_{j}, j=1, \ldots, J</script></span></li>
<li>Terminal / Leaf nodes of tree represent the region <span class="arithmatex"><span class="MathJax_Preview">R_{j}</span><script type="math/tex">R_{j}</script></span></li>
<li>The predictive rule is</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
x \in R_{j} \rightarrow f(X)=\gamma_{j}
</div>
<script type="math/tex; mode=display">
x \in R_{j} \rightarrow f(X)=\gamma_{j}
</script>
</div>
<ul>
<li>A tree with parameters <span class="arithmatex"><span class="MathJax_Preview">\theta=\left\{R_{j}, \gamma_{j}\right\}_{j=1}^{J}</span><script type="math/tex">\theta=\left\{R_{j}, \gamma_{j}\right\}_{j=1}^{J}</script></span> is expressed as</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
T(x ; \theta)=\sum_{j=1}^{J} \gamma_{j} I\left(x \in R_{j}\right)
</div>
<script type="math/tex; mode=display">
T(x ; \theta)=\sum_{j=1}^{J} \gamma_{j} I\left(x \in R_{j}\right)
</script>
</div>
<p>with <span class="arithmatex"><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span> usually treated as a meta-parameter</p>
<p>Learning a Regression Tree</p>
<p>Ideally parameters found by minimizing the empirical risk</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta}=\operatorname{argmin}_{\theta} \sum_{j=1}^{J} \sum_{x \in R_{j}} L\left(y_{i}, \gamma_{i}\right)
</div>
<script type="math/tex; mode=display">
\hat{\theta}=\operatorname{argmin}_{\theta} \sum_{j=1}^{J} \sum_{x \in R_{j}} L\left(y_{i}, \gamma_{i}\right)
</script>
</div>
<p>Hard optimization problem, instead settle for approximate suboptimal solutions</p>
<p>Typical approach: Divide optimization into two parts</p>
<ul>
<li>find <span class="arithmatex"><span class="MathJax_Preview">\gamma_{j}</span><script type="math/tex">\gamma_{j}</script></span> given <span class="arithmatex"><span class="MathJax_Preview">R_{j}</span><script type="math/tex">R_{j}</script></span> : typically trivial - mean of the training <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> &lsquo;s falling in <span class="arithmatex"><span class="MathJax_Preview">R_{j}</span><script type="math/tex">R_{j}</script></span></li>
<li>find <span class="arithmatex"><span class="MathJax_Preview">R_{j}</span><script type="math/tex">R_{j}</script></span> : difficult - greedy, top-down recursive partitioning algorithm to approximate</li>
</ul>
<h4 id="the-boosted-tree-model">The Boosted Tree Model<a class="headerlink" href="#the-boosted-tree-model" title="Permanent link">&para;</a></h4>
<p>A boosted tree is a sum of regression / classification trees</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f_{M}(x)=\sum_{m=1}^{M} T\left(x ; \theta_{m}\right)
</div>
<script type="math/tex; mode=display">
f_{M}(x)=\sum_{m=1}^{M} T\left(x ; \theta_{m}\right)
</script>
</div>
<p>learned in a forward stagewise manner</p>
<p>At each step solve</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta}_{m}=\operatorname{argmin}_{\theta_{m}} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \theta_{m}\right)\right)
</div>
<script type="math/tex; mode=display">
\hat{\theta}_{m}=\operatorname{argmin}_{\theta_{m}} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \theta_{m}\right)\right)
</script>
</div>
<p>for the parameters <span class="arithmatex"><span class="MathJax_Preview">\theta_{m}=\left\{R_{j m}, \gamma_{j m}\right\}_{j=1}^{J_{m}}</span><script type="math/tex">\theta_{m}=\left\{R_{j m}, \gamma_{j m}\right\}_{j=1}^{J_{m}}</script></span> of the next tree</p>
<p>How to solve this optimization problem?</p>
<p>ä¹Ÿæ˜¯åœ¨åŠ æ€§æ¨¡å‹çš„æ¡†æ¶ä¹‹ä¸‹è¿›è¡Œé—®é¢˜æ±‚è§£. ä¸Šè¿°é—®é¢˜å¯åˆ†ä¸ºä¸¤éƒ¨åˆ†:</p>
<p>Find <span class="arithmatex"><span class="MathJax_Preview">\gamma_{j m}</span><script type="math/tex">\gamma_{j m}</script></span> given <span class="arithmatex"><span class="MathJax_Preview">R_{j m}</span><script type="math/tex">R_{j m}</script></span> - easy</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\gamma}_{j m}=\operatorname{argmin}_{\gamma_{j m}} \sum_{x_{i} \in R_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma_{j m}\right)
</div>
<script type="math/tex; mode=display">
\hat{\gamma}_{j m}=\operatorname{argmin}_{\gamma_{j m}} \sum_{x_{i} \in R_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma_{j m}\right)
</script>
</div>
<p>Find <span class="arithmatex"><span class="MathJax_Preview">R_{j m}</span><script type="math/tex">R_{j m}</script></span> - not so easy</p>
<p>A few exceptions</p>
<ul>
<li><strong>Squared error loss</strong> - at each stage, fit a regression tree to residual <span class="arithmatex"><span class="MathJax_Preview">y_{i}-f_{m-1}\left(x_{i}\right)</span><script type="math/tex">y_{i}-f_{m-1}\left(x_{i}\right)</script></span></li>
<li><strong>Two class classification and exponential loss</strong> - give rise to an Adaboost method for boosting classification trees</li>
</ul>
<h3 id="gradient-boosting">Gradient Boosting<a class="headerlink" href="#gradient-boosting" title="Permanent link">&para;</a></h3>
<p>see <a href="https://blog.csdn.net/u012328159/article/details/94616127">here</a></p>
<ul>
<li>æå‡æ ‘åˆ©ç”¨åŠ æ³•æ¨¡å‹ä¸å‰å‘åˆ†æ­¥ç®—æ³•å®ç°å­¦ä¹ çš„ä¼˜åŒ–è¿‡ç¨‹ã€‚å½“æŸå¤±å‡½æ•°æ˜¯å¹³æ–¹æŸå¤±å’ŒæŒ‡æ•°æŸå¤±å‡½æ•°æ—¶ï¼Œæ¯ä¸€æ­¥ä¼˜åŒ–æ˜¯å¾ˆç®€å•çš„ã€‚</li>
<li>ä½†æ˜¯ï¼Œå¯¹ä¸€èˆ¬æŸå¤±å‡½æ•°è€Œè¨€ï¼Œæ¯ä¸€æ­¥ä¼˜åŒ–å¾€å¾€æ¯”è¾ƒå›°éš¾ã€‚</li>
<li>è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨æ¢¯åº¦æå‡ç®—æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ã€‚</li>
</ul>
<h4 id="numerical-optimization">Numerical Optimization<a class="headerlink" href="#numerical-optimization" title="Permanent link">&para;</a></h4>
<p>If the loss <span class="arithmatex"><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> is differentiable</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta}_{m}=\operatorname{argmin}_{\theta_{m}} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \theta_{m}\right)\right)
</div>
<script type="math/tex; mode=display">
\hat{\theta}_{m}=\operatorname{argmin}_{\theta_{m}} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \theta_{m}\right)\right)
</script>
</div>
<p>can be approximately solved with numerical optimization</p>
<p>Ex, the loss associated with using any <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> to predict <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L(f)=\sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)
</div>
<script type="math/tex; mode=display">
L(f)=\sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)
</script>
</div>
<p>è¿™é‡Œçš„ <span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> è¦æ±‚æ˜¯æ ‘çš„å’Œ. å¦‚æœå¿½ç•¥è¿™ä¸ªé™å®š, å¯ç›´æ¥çœ‹æˆæ˜¯ä¸€ä¸ªä¼˜åŒ–é—®é¢˜. Goal: find <span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> which minimizes <span class="arithmatex"><span class="MathJax_Preview">L(f)</span><script type="math/tex">L(f)</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}=\operatorname{argmin}_{f} L(f)
</div>
<script type="math/tex; mode=display">
\hat{f}=\operatorname{argmin}_{f} L(f)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">f=\left\{f\left(x_{1}\right), \ldots, f\left(x_{N}\right)\right\}</span><script type="math/tex">f=\left\{f\left(x_{1}\right), \ldots, f\left(x_{N}\right)\right\}</script></span> æ˜¯åœ¨ <span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> ä¸ªæ•°æ®ç‚¹ <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> çš„è¿‘ä¼¼çš„å‡½æ•°å€¼ <span class="arithmatex"><span class="MathJax_Preview">f(x_i)</span><script type="math/tex">f(x_i)</script></span>.</p>
<p>è€ƒè™‘è¦æ±‚æ˜¯æ ‘çš„å’Œçš„é™å®š, è¿™é‡Œæ•°å€¼ä¼˜åŒ–çš„è¿‡ç¨‹, è¦æ±‚ç”¨ä¸€ç»„å‘é‡çš„å’Œæ¥æ±‚è§£.</p>
<p>Numerical optimization approximates</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}=\operatorname{argmin}_{f} L(f)
</div>
<script type="math/tex; mode=display">
\hat{f}=\operatorname{argmin}_{f} L(f)
</script>
</div>
<p>as a sum of vectors</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f_{M}=\sum_{m=0}^{M} h_{m}, h_{m} \in \mathbb{R}^{N}
</div>
<script type="math/tex; mode=display">
f_{M}=\sum_{m=0}^{M} h_{m}, h_{m} \in \mathbb{R}^{N}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">f_{0}=h_{0}</span><script type="math/tex">f_{0}=h_{0}</script></span> is an initial guess and each <span class="arithmatex"><span class="MathJax_Preview">f_{m}</span><script type="math/tex">f_{m}</script></span> is estimated from <span class="arithmatex"><span class="MathJax_Preview">f_{m-1}</span><script type="math/tex">f_{m-1}</script></span></p>
<h4 id="steepest-descent">Steepest Descent<a class="headerlink" href="#steepest-descent" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
h_{m}=-\rho_{m} g_{m}
</div>
<script type="math/tex; mode=display">
h_{m}=-\rho_{m} g_{m}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\rho_{m}</span><script type="math/tex">\rho_{m}</script></span> is a scalar</li>
<li><span class="arithmatex"><span class="MathJax_Preview">g_{m} \in \mathbb{R}^{N}</span><script type="math/tex">g_{m} \in \mathbb{R}^{N}</script></span> is the gradient of <span class="arithmatex"><span class="MathJax_Preview">L(f)</span><script type="math/tex">L(f)</script></span> evaluated at <span class="arithmatex"><span class="MathJax_Preview">f=f_{m-1}</span><script type="math/tex">f=f_{m-1}</script></span></li>
</ul>
<p>Components of <span class="arithmatex"><span class="MathJax_Preview">g_{m}</span><script type="math/tex">g_{m}</script></span> are</p>
<div class="arithmatex">
<div class="MathJax_Preview">
g_{i m}=\left.\frac{\partial L\left(y_{i}, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right|_{f\left(x_{i}\right)=f_{i}, m-1}
</div>
<script type="math/tex; mode=display">
g_{i m}=\left.\frac{\partial L\left(y_{i}, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right|_{f\left(x_{i}\right)=f_{i}, m-1}
</script>
</div>
<p>Step length is the solution to</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\rho_{m}=\operatorname{argmin}_{\rho} L\left(f_{m-1}-\rho g_{m}\right)
</div>
<script type="math/tex; mode=display">
\rho_{m}=\operatorname{argmin}_{\rho} L\left(f_{m-1}-\rho g_{m}\right)
</script>
</div>
<p>Solution is updated: <span class="arithmatex"><span class="MathJax_Preview">f_{m}=f_{m-1}-\rho_{m} g_{m}</span><script type="math/tex">f_{m}=f_{m-1}-\rho_{m} g_{m}</script></span></p>
<h4 id="gradient-tree-boosting">Gradient Tree Boosting<a class="headerlink" href="#gradient-tree-boosting" title="Permanent link">&para;</a></h4>
<p>$$
\hat{\theta}<em _theta__m="\theta_{m">{m}=\operatorname{argmin}</em>\right)\right)
$$
Comparison between Gradient Boosting \&amp; Forward Stagewise Tree boosting}} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \theta_{m</p>
<ul>
<li>Tree predictions <span class="arithmatex"><span class="MathJax_Preview">T\left(x_{i} ; \theta_{m}\right)</span><script type="math/tex">T\left(x_{i} ; \theta_{m}\right)</script></span> are analogous to the negative gradients <span class="arithmatex"><span class="MathJax_Preview">-g_{1 m}, \ldots,-g_{N m}</span><script type="math/tex">-g_{1 m}, \ldots,-g_{N m}</script></span>, but <span class="arithmatex"><span class="MathJax_Preview">t_{m}=\left\{T\left(x_{1} ; \theta_{m}\right), \ldots, T\left(x_{N}, \theta_{m}\right)\right\}</span><script type="math/tex">t_{m}=\left\{T\left(x_{1} ; \theta_{m}\right), \ldots, T\left(x_{N}, \theta_{m}\right)\right\}</script></span> are constrained to be predictions of a <span class="arithmatex"><span class="MathJax_Preview">J_{m}</span><script type="math/tex">J_{m}</script></span> terminal node decision tree, whereas <span class="arithmatex"><span class="MathJax_Preview">-g_{m}</span><script type="math/tex">-g_{m}</script></span> is the unconstrained maximal descent direction</li>
<li>Also, <span class="arithmatex"><span class="MathJax_Preview">\rho_{m}=\operatorname{argmin}_{\rho} L\left(f_{m-1}, \rho g_{m}\right)</span><script type="math/tex">\rho_{m}=\operatorname{argmin}_{\rho} L\left(f_{m-1}, \rho g_{m}\right)</script></span> is analogous to <span class="arithmatex"><span class="MathJax_Preview">\hat{\gamma}_{j m}=\operatorname{argmin}_{\gamma_{j m}} \sum_{x_{i} \in R_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma_{j m}\right)</span><script type="math/tex">\hat{\gamma}_{j m}=\operatorname{argmin}_{\gamma_{j m}} \sum_{x_{i} \in R_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma_{j m}\right)</script></span>, but perform a line search for each terminal node</li>
</ul>
<p>æ¢¯åº¦æå‡å’Œæ¢¯åº¦ä¸‹é™çš„åŒºåˆ«ä¸è”ç³»æ˜¯ä»€ä¹ˆï¼Ÿå…¶å®ä¸¤è€…éƒ½æ˜¯åœ¨è¿­ä»£è¿‡ç¨‹ä¸­ï¼Œåˆ©ç”¨æŸå¤±å‡½æ•°ç›¸å¯¹äºæ¨¡å‹çš„è´Ÿæ¢¯åº¦æ–¹å‘æ¥å¯¹å½“å‰æ¨¡å‹è¿›è¡Œæ›´æ–°ï¼Œåªä¸è¿‡åœ¨æ¢¯åº¦ä¸‹é™ä¸­ï¼Œæ¨¡å‹æ˜¯ä»¥<strong>å‚æ•°åŒ–è¡¨ç¤º</strong>ï¼Œå› æ­¤æ¨¡å‹çš„æ›´æ–°ç­‰ä»·äºå‚æ•°çš„æ›´æ–°ã€‚è€Œåœ¨æ¢¯åº¦æå‡ä¸­ï¼Œæ¨¡å‹å¹¶ä¸éœ€è¦è¿›è¡Œå‚æ•°åŒ–è¡¨ç¤ºï¼Œè€Œæ˜¯ç›´æ¥å®šä¹‰åœ¨å‡½æ•°ç©ºé—´ä¸­ã€‚</p>
<p>If the only goal is to minimize</p>
<p>$$
\hat{f}=\operatorname{argmin}_{f} L(f)
$$
then perform steepest descent</p>
<ul>
<li>However, the ultimate goal is to generalize <span class="arithmatex"><span class="MathJax_Preview">f_{M}(x)</span><script type="math/tex">f_{M}(x)</script></span> to new unseen data</li>
<li>A possible solution is <strong>Gradient Tree Boosting</strong></li>
</ul>
<p>Fit a tree <span class="arithmatex"><span class="MathJax_Preview">T\left(x ; \theta_{m}\right)</span><script type="math/tex">T\left(x ; \theta_{m}\right)</script></span> at <span class="arithmatex"><span class="MathJax_Preview">m_{t h}</span><script type="math/tex">m_{t h}</script></span> iteration whose predictions <span class="arithmatex"><span class="MathJax_Preview">t_{m}</span><script type="math/tex">t_{m}</script></span> are as close as possible to the negative gradient</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\tilde{\theta}_{m}=\operatorname{argmin}_{\theta_{m}} \sum_{i=1}^{N}\left(-g_{i m}-T\left(x_{i} ; \theta_{m}\right)\right)^{2}
</div>
<script type="math/tex; mode=display">
\tilde{\theta}_{m}=\operatorname{argmin}_{\theta_{m}} \sum_{i=1}^{N}\left(-g_{i m}-T\left(x_{i} ; \theta_{m}\right)\right)^{2}
</script>
</div>
<p>For the solution region <span class="arithmatex"><span class="MathJax_Preview">\tilde{R}_{j m}</span><script type="math/tex">\tilde{R}_{j m}</script></span>, set</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\gamma}_{j m}=\operatorname{argmin}_{\gamma_{j m}} \sum_{x_{i} \in \tilde{R}_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma_{j m}\right)
</div>
<script type="math/tex; mode=display">
\hat{\gamma}_{j m}=\operatorname{argmin}_{\gamma_{j m}} \sum_{x_{i} \in \tilde{R}_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma_{j m}\right)
</script>
</div>
<p>The regions <span class="arithmatex"><span class="MathJax_Preview">\tilde{R}_{j m}</span><script type="math/tex">\tilde{R}_{j m}</script></span> would not be identical to <span class="arithmatex"><span class="MathJax_Preview">R_{j m}</span><script type="math/tex">R_{j m}</script></span> that solve the original optimization problem, but they should be similar enough.</p>
<h4 id="gradient-tree-boosting-algorithm">Gradient Tree Boosting Algorithm<a class="headerlink" href="#gradient-tree-boosting-algorithm" title="Permanent link">&para;</a></h4>
<ol>
<li>Initialize <span class="arithmatex"><span class="MathJax_Preview">f_{0}(x)=\operatorname{argmin}_{\gamma} \sum_{i=1}^{N} L\left(y_{i}, \gamma\right)</span><script type="math/tex">f_{0}(x)=\operatorname{argmin}_{\gamma} \sum_{i=1}^{N} L\left(y_{i}, \gamma\right)</script></span></li>
<li>For <span class="arithmatex"><span class="MathJax_Preview">m=1</span><script type="math/tex">m=1</script></span> to <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> :
   - For <span class="arithmatex"><span class="MathJax_Preview">i=1,2, \ldots, N</span><script type="math/tex">i=1,2, \ldots, N</script></span> compute <span class="arithmatex"><span class="MathJax_Preview">r_{i m}=-\left.\left[\frac{\partial\left(L\left(y_{i}, f\left(x_{i}\right)\right)\right)}{\partial f\left(x_{i}\right)}\right]\right|_{f=f_{m-1}}</span><script type="math/tex">r_{i m}=-\left.\left[\frac{\partial\left(L\left(y_{i}, f\left(x_{i}\right)\right)\right)}{\partial f\left(x_{i}\right)}\right]\right|_{f=f_{m-1}}</script></span>
   - Fit a regression tree to the targets <span class="arithmatex"><span class="MathJax_Preview">r_{i m}</span><script type="math/tex">r_{i m}</script></span> giving terminal regions <span class="arithmatex"><span class="MathJax_Preview">R_{j m}, j=1,2, \ldots, J_{m}</span><script type="math/tex">R_{j m}, j=1,2, \ldots, J_{m}</script></span>
   - For <span class="arithmatex"><span class="MathJax_Preview">j=1,2, \ldots, J_{m}</span><script type="math/tex">j=1,2, \ldots, J_{m}</script></span> compute <span class="arithmatex"><span class="MathJax_Preview">\gamma_{j m}=\operatorname{argmin}_{\gamma} \sum_{x_{i} \in R_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma\right)</span><script type="math/tex">\gamma_{j m}=\operatorname{argmin}_{\gamma} \sum_{x_{i} \in R_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma\right)</script></span>
   - Update <span class="arithmatex"><span class="MathJax_Preview">f_{m}(X)=f_{m-1}(x)+\sum_{j=1}^{J_{m}} \gamma_{j m} I\left(x \in R_{j m}\right)</span><script type="math/tex">f_{m}(X)=f_{m-1}(x)+\sum_{j=1}^{J_{m}} \gamma_{j m} I\left(x \in R_{j m}\right)</script></span></li>
<li>Output <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(x)=f_{M}(x)</span><script type="math/tex">\hat{f}(x)=f_{M}(x)</script></span></li>
</ol>
<h3 id="boosting-tree-size">Boosting Tree Size<a class="headerlink" href="#boosting-tree-size" title="Permanent link">&para;</a></h3>
<p>Size of a Boosted Tree</p>
<p>Learning a large pruned tree at each round performs poorly</p>
<p>Better if</p>
<ul>
<li>Restrict all trees to be same size <span class="arithmatex"><span class="MathJax_Preview">J_{m}=J \forall m</span><script type="math/tex">J_{m}=J \forall m</script></span></li>
<li>Perform cross-validation to choose an optimal <span class="arithmatex"><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span></li>
</ul>
<p>å¯ä»¥è€ƒè™‘ä¸‹å¼çš„ ç›®æ ‡å‡½æ•°(target function) çš„æ€§è´¨æ¥å¾—åˆ° <span class="arithmatex"><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span> çš„æœ‰ç”¨å€¼</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\eta=\arg \min _{f} \mathrm{E}_{X Y} L(Y, f(X))
</div>
<script type="math/tex; mode=display">
\eta=\arg \min _{f} \mathrm{E}_{X Y} L(Y, f(X))
</script>
</div>
<p>è¿™é‡ŒæœŸæœ›å€¼æ˜¯å¯¹ <span class="arithmatex"><span class="MathJax_Preview">(X, Y)</span><script type="math/tex">(X, Y)</script></span> çš„æ€»ä½“è”åˆåˆ†å¸ƒè€Œè¨€. ç›®æ ‡å‡½æ•° <span class="arithmatex"><span class="MathJax_Preview">\eta(x)</span><script type="math/tex">\eta(x)</script></span> æ˜¯åœ¨æœ«æ¥æ•°æ®ä¸Šæœ‰æœ€å°é¢„æµ‹é£é™©çš„ å‡½æ•°. è¿™æ˜¯æˆ‘ä»¬è¯•å›¾è¿‘ä¼¼çš„å‡½æ•°.</p>
<p><span class="arithmatex"><span class="MathJax_Preview">\eta(X)</span><script type="math/tex">\eta(X)</script></span> ä¸€ä¸ªç›¸å…³çš„æ€§è´¨æ˜¯åæ ‡å˜é‡ <span class="arithmatex"><span class="MathJax_Preview">X^{T}=\left(X_{1}, X_{2}, \ldots, X_{p}\right)</span><script type="math/tex">X^{T}=\left(X_{1}, X_{2}, \ldots, X_{p}\right)</script></span> é—´äº¤å‰é¡¹çš„é˜¶. è¿™ä¸ªå¯ä»¥é€šè¿‡å®ƒçš„ ANOVA å±•å¼€å¼å¾—åˆ°</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\eta(X)=\sum_{j} \eta_{j}\left(X_{j}\right)+\sum_{j k} \eta_{j k}\left(X_{j}, X_{k}\right)+\sum_{j k l} \eta_{j k l}\left(X_{j}, X_{k}, X_{l}\right)+\cdots
</div>
<script type="math/tex; mode=display">
\eta(X)=\sum_{j} \eta_{j}\left(X_{j}\right)+\sum_{j k} \eta_{j k}\left(X_{j}, X_{k}\right)+\sum_{j k l} \eta_{j k l}\left(X_{j}, X_{k}, X_{l}\right)+\cdots
</script>
</div>
<p>å¼å­ (10.40) ä¸­çš„ç¬¬ä¸€é¡¹æ˜¯åªæœ‰ä¸€ä¸ªé¢„æµ‹å˜é‡ <span class="arithmatex"><span class="MathJax_Preview">X_{j}</span><script type="math/tex">X_{j}</script></span> çš„å‡½æ•°å’Œ. è¿™äº›å‡½æ•° <span class="arithmatex"><span class="MathJax_Preview">\eta_{j}\left(X_{j}\right)</span><script type="math/tex">\eta_{j}\left(X_{j}\right)</script></span> æ˜¯åœ¨æ‰€é‡‡ç”¨çš„è¯¯å·®æŸå¤±å‡†åˆ™ä¸‹è”åˆèµ·æ¥æœ€èƒ½è¿‘ä¼¼ <span class="arithmatex"><span class="MathJax_Preview">\eta(X)</span><script type="math/tex">\eta(X)</script></span> çš„é¡¹. æ¯ä¸€ä¸ª <span class="arithmatex"><span class="MathJax_Preview">\eta_{j}\left(X_{j}\right)</span><script type="math/tex">\eta_{j}\left(X_{j}\right)</script></span> ç§°ä¸º <span class="arithmatex"><span class="MathJax_Preview">X_{j}</span><script type="math/tex">X_{j}</script></span> çš„ä¸»å½±å“. å¼ä¸­ç¬¬äºŒé¡¹æ˜¯é‚£äº›åŠ å…¥åˆ°ä¸»å½±å“ä¸­å°† <span class="arithmatex"><span class="MathJax_Preview">\eta(X)</span><script type="math/tex">\eta(X)</script></span> æ‹Ÿåˆå¾—æœ€å¥½çš„å«ä¸¤ä¸ªå˜é‡çš„å‡½æ•°. è¿™äº›å‡½æ•°è¢«ç§°ä¸ºæ¯ä¸ªå˜é‡å¯¹ <span class="arithmatex"><span class="MathJax_Preview">\left(X_{j}, X_{k}\right)</span><script type="math/tex">\left(X_{j}, X_{k}\right)</script></span> çš„äºŒé˜¶äº¤å‰é¡¹. å¼ä¸­ç¬¬ä¸‰é¡¹è¡¨ç¤ºä¸‰é˜¶äº¤å‰é¡¹ï¼Œä»¥æ­¤ç±»æ¨. å¯¹äºè®¸å¤šå®é™…çš„é—®é¢˜, ä½é˜¶äº¤å‰å½±å“å ä¸»è¦åœ°ä½. å¦‚æœæ¨¡å‹å¾—åˆ°å¼ºçƒˆçš„é«˜é˜¶äº¤å‰å½±å“, æ¯”å¦‚å¤§å‹çš„å†³ç­–æ ‘, åˆ™å¯èƒ½æ­£ç¡®æ€§ä¸å¥½.</p>
<p>Interaction level of tree-based approximation is limited by <span class="arithmatex"><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span>:</p>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">J=2, f_{M}(x)</span><script type="math/tex">J=2, f_{M}(x)</script></span> can only be of the form</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{k} \eta_{k}\left(X_{k}\right)
</div>
<script type="math/tex; mode=display">
\sum_{k} \eta_{k}\left(X_{k}\right)
</script>
</div>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">J=3, f_{M}(x)</span><script type="math/tex">J=3, f_{M}(x)</script></span> can be of the form</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{k} \eta_{k}\left(X_{k}\right)+\sum_{j k} \eta_{j k}\left(X_{j}, X_{k}\right)
</div>
<script type="math/tex; mode=display">
\sum_{k} \eta_{k}\left(X_{k}\right)+\sum_{j k} \eta_{j k}\left(X_{j}, X_{k}\right)
</script>
</div>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">J=4, f_{M}(x)</span><script type="math/tex">J=4, f_{M}(x)</script></span> can be of the form</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{k} \eta_{k}\left(X_{k}\right)+\sum_{j k} \eta_{j k}\left(X_{j}, X_{k}\right)+\sum_{j k l} \eta_{j k l}\left(X_{j}, X_{k}, X_{l}\right)
</div>
<script type="math/tex; mode=display">
\sum_{k} \eta_{k}\left(X_{k}\right)+\sum_{j k} \eta_{j k}\left(X_{j}, X_{k}\right)+\sum_{j k l} \eta_{j k l}\left(X_{j}, X_{k}, X_{l}\right)
</script>
</div>
<p>å¤šå¤§åˆé€‚?</p>
<ul>
<li>For many practical problems, low-order interactions dominate</li>
<li>Therefore, models that produce strong higher-order interaction effects suffer in accuracy</li>
<li><span class="arithmatex"><span class="MathJax_Preview">4 \leqslant J \leqslant 8</span><script type="math/tex">4 \leqslant J \leqslant 8</script></span> works well in the context of boosting</li>
</ul>
<h3 id="regularization">Regularization<a class="headerlink" href="#regularization" title="Permanent link">&para;</a></h3>
<p>é™¤äº†å€™é€‰æ ‘çš„å¤§å° <span class="arithmatex"><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span>, æ¢¯åº¦ boosting çš„å¦ä¸€ä¸ªå…ƒå‚æ•° (meta-parameter) æ˜¯ boosting çš„è¿­ä»£æ¬¡æ•° <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> - æ¯ä¸€æ¬¡è¿­ä»£é€šå¸¸ä¼šé™ä½è®­ç»ƒé£é™© <span class="arithmatex"><span class="MathJax_Preview">L\left(f_{M}\right)</span><script type="math/tex">L\left(f_{M}\right)</script></span>, æ‰€ä»¥å¯¹äºå……åˆ†å¤§çš„ <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span>, è¯¯å·®å¯ä»¥è¾¾åˆ°ä»»æ„å°. ç„¶è€Œ, å¯¹è®­ç»ƒæ•°æ®æ‹Ÿåˆå¾—å¤ªå¥½ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆ, å®ƒä¼šé™ä½é¢„æµ‹æœ«æ¥æ•°æ®çš„æ•ˆæœ. å› æ­¤, å­˜åœ¨ä¸€ä¸ªæœ€å°åŒ–æœ« æ¥é¢„æµ‹é£é™©çš„æœ€ä¼˜å¤§å° <span class="arithmatex"><span class="MathJax_Preview">M^{*}</span><script type="math/tex">M^{*}</script></span>, å®ƒä¾èµ–äºå…·ä½“åº”ç”¨. ä¼°è®¡ <span class="arithmatex"><span class="MathJax_Preview">M^{*}</span><script type="math/tex">M^{*}</script></span> çš„ä¸€ç§æ–¹ä¾¿æ–¹å¼æ˜¯åœ¨éªŒè¯æ ·æœ¬ä¸Šå°†é¢„æµ‹ é£é™©çœ‹æˆæ˜¯å…³äº <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> çš„å‡½æ•°. æœ€å°åŒ–é£é™©çš„ <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> å€¼åˆ™ä½œä¸º <span class="arithmatex"><span class="MathJax_Preview">M^{*}</span><script type="math/tex">M^{*}</script></span> çš„ä¸€ä¸ªä¼°è®¡. è¿™ç±»ä¼¼äºç»å¸¸ç”¨åœ¨ç¥ç» ç½‘ç»œä¸­çš„ æ—©åœ (early stopping) ç­–ç•¥ (11.4 èŠ‚).</p>
<ul>
<li>Control number of boosting rounds</li>
<li>Too large <span class="arithmatex"><span class="MathJax_Preview">M \rightarrow</span><script type="math/tex">M \rightarrow</script></span> danger of over-fitting</li>
<li>There is a <span class="arithmatex"><span class="MathJax_Preview">M^{*}</span><script type="math/tex">M^{*}</script></span> that minimizes future risk</li>
</ul>
<h4 id="shrinkage">Shrinkage<a class="headerlink" href="#shrinkage" title="Permanent link">&para;</a></h4>
<ul>
<li>Scale the contribution of each tree by a factor <span class="arithmatex"><span class="MathJax_Preview">0&lt;\nu&lt;1</span><script type="math/tex">0<\nu<1</script></span></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
f_{m}(x)=f_{m-1}(x)+\nu \cdot \sum_{j=1}^{J} \gamma_{j m} l\left(x \in R_{j m}\right)
</div>
<script type="math/tex; mode=display">
f_{m}(x)=f_{m-1}(x)+\nu \cdot \sum_{j=1}^{J} \gamma_{j m} l\left(x \in R_{j m}\right)
</script>
</div>
<ul>
<li>Smaller <span class="arithmatex"><span class="MathJax_Preview">\nu \rightarrow</span><script type="math/tex">\nu \rightarrow</script></span> larger <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> to obtain low training error</li>
<li>Empirical finding: small <span class="arithmatex"><span class="MathJax_Preview">\nu&lt;0.1</span><script type="math/tex">\nu<0.1</script></span> and sufficiently large <span class="arithmatex"><span class="MathJax_Preview">M \rightarrow</span><script type="math/tex">M \rightarrow</script></span> better result than no shrinkage (especially for regression problems)</li>
</ul>
<p><img alt="" src="../media/ASL-note3/2021-12-30-14-32-13.png" /></p>
<h4 id="subsampling">Subsampling<a class="headerlink" href="#subsampling" title="Permanent link">&para;</a></h4>
<ul>
<li>Stochastic gradient boosting - each iteration sample a fraction <span class="arithmatex"><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span> of the training observations (without replacement)</li>
<li>A typical value <span class="arithmatex"><span class="MathJax_Preview">\eta=0.5</span><script type="math/tex">\eta=0.5</script></span></li>
<li>Empirically subsampling without shrinkage works poorly ä¸ç»“åˆ shrink ä¼¼ä¹æ¯”ä¸è¿›è¡Œé‡‡æ ·è¦å·®?</li>
<li>Subsampling with shrinkage works well</li>
<li>Now we have 4 parameters to estimate <span class="arithmatex"><span class="MathJax_Preview">J, M, \nu, \eta</span><script type="math/tex">J, M, \nu, \eta</script></span>. ä¸€èˆ¬åœ°ï¼Œé€šè¿‡å‰æœŸçš„å°è¯•ç¡®å®šåˆé€‚çš„ <span class="arithmatex"><span class="MathJax_Preview">J, \nu, \eta</span><script type="math/tex">J, \nu, \eta</script></span>, å°† <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> ç•™ä½œä¸»è¦çš„å‚æ•°.</li>
</ul>
<p><img alt="" src="../media/ASL-note3/2021-12-30-14-35-26.png" /></p>
<p><a href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regularization.html">sklearn</a> ä¸­æœ‰ä»½ä»£ç ç”»çš„å›¾ç±»ä¼¼.</p>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2021-2022 Easonshi
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
    
    
      
      
    
    <a href="https://github.com/Lightblues" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
    
    
    
    
      
      
    
    <a href="https://lightblues.github.io/" target="_blank" rel="noopener" title="lightblues.github.io" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M575.8 255.5c0 18-15 32.1-32 32.1h-32l.7 160.2c0 2.7-.2 5.4-.5 8.1v16.2c0 22.1-17.9 40-40 40h-16c-1.1 0-2.2 0-3.3-.1-1.4.1-2.8.1-4.2.1L416 512h-24c-22.1 0-40-17.9-40-40v-88c0-17.7-14.3-32-32-32h-64c-17.7 0-32 14.3-32 32v88c0 22.1-17.9 40-40 40h-55.9c-1.5 0-3-.1-4.5-.2-1.2.1-2.4.2-3.6.2h-16c-22.1 0-40-17.9-40-40V360c0-.9 0-1.9.1-2.8v-69.7h-32c-18 0-32-14-32-32.1 0-9 3-17 10-24L266.4 8c7-7 15-8 22-8s15 2 21 7l255.4 224.5c8 7 12 15 11 24z"/></svg>
    </a>
  
    
    
    
    
    <a href="mailto:oldcitystal@gmail.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.6.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2024 Fonticons, Inc.--><path d="M64 112c-8.8 0-16 7.2-16 16v22.1l172.5 141.6c20.7 17 50.4 17 71.1 0L464 150.1V128c0-8.8-7.2-16-16-16H64zM48 212.2V384c0 8.8 7.2 16 16 16h384c8.8 0 16-7.2 16-16V212.2L322 328.8c-38.4 31.5-93.7 31.5-132 0L48 212.2zM0 128c0-35.3 28.7-64 64-64h384c35.3 0 64 28.7 64 64v256c0 35.3-28.7 64-64 64H64c-35.3 0-64-28.7-64-64V128z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.top"], "search": "../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.fe8b6f2b.min.js"></script>
      
        <script src="../../js/baidu-tongji.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>