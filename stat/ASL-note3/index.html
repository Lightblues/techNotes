
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="O'ver curious">
      
      
        <meta name="author" content="Easonshi">
      
      
        <link rel="canonical" href="https://lightblues.github.io/techNotes/stat/ASL-note3/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.0, mkdocs-material-8.2.9">
    
    
      
        <title>ASL3 - techNotes</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.120efc48.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.9647289d.min.css">
        
      
    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
  </head>
  
  
    
    
      
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent="">
  
    
    
      <script>var palette=__md_get("__palette");if(palette&&"object"==typeof palette.color)for(var key of Object.keys(palette.color))document.body.setAttribute("data-md-color-"+key,palette.color[key])</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#model-inference-averaging" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="techNotes" class="md-header__button md-logo" aria-label="techNotes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            techNotes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              ASL3
            
          </span>
        </div>
      </div>
    </div>
    
      <form class="md-header__option" data-md-component="palette">
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme="default" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to dark mode"  type="radio" name="__palette" id="__palette_1">
          
            <label class="md-header__button md-icon" title="Switch to dark mode" for="__palette_2" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 6H7c-3.31 0-6 2.69-6 6s2.69 6 6 6h10c3.31 0 6-2.69 6-6s-2.69-6-6-6zm0 10H7c-2.21 0-4-1.79-4-4s1.79-4 4-4h10c2.21 0 4 1.79 4 4s-1.79 4-4 4zM7 9c-1.66 0-3 1.34-3 3s1.34 3 3 3 3-1.34 3-3-1.34-3-3-3z"/></svg>
            </label>
          
        
          
          
          <input class="md-option" data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme="slate" data-md-color-primary="" data-md-color-accent=""  aria-label="Switch to light mode"  type="radio" name="__palette" id="__palette_2">
          
            <label class="md-header__button md-icon" title="Switch to light mode" for="__palette_1" hidden>
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M17 7H7a5 5 0 0 0-5 5 5 5 0 0 0 5 5h10a5 5 0 0 0 5-5 5 5 0 0 0-5-5m0 8a3 3 0 0 1-3-3 3 3 0 0 1 3-3 3 3 0 0 1 3 3 3 3 0 0 1-3 3Z"/></svg>
            </label>
          
        
      </form>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        <a href="https://github.com/lightblues/techNotes" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    lightblues/techNotes
  </div>
</a>
      </div>
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
            
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-tabs__inner md-grid">
    <ul class="md-tabs__list">
      
        
  
  


  <li class="md-tabs__item">
    <a href="../.." class="md-tabs__link">
      Home
    </a>
  </li>

      
        
  
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../../code/CS/git-note/" class="md-tabs__link">
        Code
      </a>
    </li>
  

  

      
        
  
  
    
  


  
  
  
    

  
  
  
    <li class="md-tabs__item">
      <a href="../" class="md-tabs__link md-tabs__link--active">
        Stat
      </a>
    </li>
  

  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../Linux/Linux/" class="md-tabs__link">
        Linux
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../os/EFI/" class="md-tabs__link">
        OS
      </a>
    </li>
  

      
        
  
  


  
  
  
    <li class="md-tabs__item">
      <a href="../../NLP/ABSA/" class="md-tabs__link">
        NLP
      </a>
    </li>
  

      
        
  
  


  <li class="md-tabs__item">
    <a href="/" class="md-tabs__link">
      Blog
    </a>
  </li>

      
    </ul>
  </div>
</nav>
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="techNotes" class="md-nav__button md-logo" aria-label="techNotes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    techNotes
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/lightblues/techNotes" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    lightblues/techNotes
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Home
      </a>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2">
          Code
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Code" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Code
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_1" type="checkbox" id="__nav_2_1" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_1">
          CS
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="CS" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          CS
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/CS/git-note/" class="md-nav__link">
        Git
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_2">
          Python
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Python" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          Python
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/Python/Python-note/" class="md-nav__link">
        课程笔记
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_3" type="checkbox" id="__nav_2_3" >
      
      
      
      
        <label class="md-nav__link" for="__nav_2_3">
          JavaScripe
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="JavaScripe" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_3">
          <span class="md-nav__icon md-icon"></span>
          JavaScripe
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/js-mindmap/" class="md-nav__link">
        Mindmap
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/js-note/" class="md-nav__link">
        js note
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/jQuery-note/" class="md-nav__link">
        jQuery
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../code/JavaScript/node-js-note/" class="md-nav__link">
        Node.js
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3">
          Stat
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Stat" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Stat
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
    
  
  
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3_1" type="checkbox" id="__nav_3_1" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_3_1">
          高等统计学习
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="高等统计学习" data-md-level="2">
        <label class="md-nav__title" for="__nav_3_1">
          <span class="md-nav__icon md-icon"></span>
          高等统计学习
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../" class="md-nav__link">
        index
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-mindmap/" class="md-nav__link">
        ASL mindmap
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-note1/" class="md-nav__link">
        ASL1
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-note2/" class="md-nav__link">
        ASL2
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          ASL3
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        ASL3
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#model-inference-averaging" class="md-nav__link">
    Model Inference &amp; Averaging
  </a>
  
    <nav class="md-nav" aria-label="Model Inference &amp; Averaging">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#maximum-likelihood-methods-bootstrap" class="md-nav__link">
    Maximum Likelihood Methods &amp; Bootstrap
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayesian-methods" class="md-nav__link">
    Bayesian Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-em-algorithm" class="md-nav__link">
    The EM Algorithm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mcmc-sampling-from-posterior" class="md-nav__link">
    MCMC - Sampling from Posterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bagging" class="md-nav__link">
    Bagging
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-averaging-stacking" class="md-nav__link">
    Model Averaging &amp; Stacking
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stochastic-search-bumping" class="md-nav__link">
    Stochastic Search: Bumping
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additive-models-trees-related-methods" class="md-nav__link">
    Additive Models, Trees, &amp; Related Methods
  </a>
  
    <nav class="md-nav" aria-label="Additive Models, Trees, &amp; Related Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generalized-additive-models" class="md-nav__link">
    Generalized Additive Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tree-based-methods" class="md-nav__link">
    Tree Based Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#patient-rule-induction-method-prim" class="md-nav__link">
    Patient Rule Induction Method (PRIM)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multivariate-adaptive-regression-splines-mars" class="md-nav__link">
    Multivariate Adaptive Regression Splines (MARS)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hierarchical-mixture-of-experts-hme" class="md-nav__link">
    Hierarchical Mixture of Experts (HME)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#boosting-additive-trees" class="md-nav__link">
    Boosting &amp; Additive Trees
  </a>
  
    <nav class="md-nav" aria-label="Boosting &amp; Additive Trees">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#boosting-methods" class="md-nav__link">
    Boosting Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#boosting-fits-an-additive-model" class="md-nav__link">
    Boosting Fits an Additive Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exponential-loss-adaboost" class="md-nav__link">
    Exponential Loss &amp; AdaBoost
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss-functions-robustness" class="md-nav__link">
    Loss Functions &amp; Robustness
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#off-the-shelf-procedure-for-data-mining" class="md-nav__link">
    ”Off-the-Shelf” Procedure for Data Mining
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#boosting-trees" class="md-nav__link">
    Boosting Trees
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-boosting" class="md-nav__link">
    Gradient Boosting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#boosting-tree-size" class="md-nav__link">
    Boosting Tree Size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regularization" class="md-nav__link">
    Regularization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../ASL-note4/" class="md-nav__link">
        ASL4
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Linux
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Linux" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Linux
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/Linux/" class="md-nav__link">
        General
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/tutor-%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83%E7%9B%B8%E5%85%B3/" class="md-nav__link">
        实验环境相关
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../Linux/Linux-maintain/" class="md-nav__link">
        系统维护
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          OS
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="OS" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          OS
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/EFI/" class="md-nav__link">
        EFI
      </a>
    </li>
  

            
          
            
              
  
  
  
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5_2" type="checkbox" id="__nav_5_2" >
      
      
      
      
        <label class="md-nav__link" for="__nav_5_2">
          OSs
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="OSs" data-md-level="2">
        <label class="md-nav__title" for="__nav_5_2">
          <span class="md-nav__icon md-icon"></span>
          OSs
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/macOS/" class="md-nav__link">
        macOS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Hackintosh/" class="md-nav__link">
        hackintosh
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Arch/" class="md-nav__link">
        Arch
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/CentOS/" class="md-nav__link">
        CentOS
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Ubuntu/" class="md-nav__link">
        Ubuntu
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../os/Windows/" class="md-nav__link">
        Windows
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_6" type="checkbox" id="__nav_6" >
      
      
      
      
        <label class="md-nav__link" for="__nav_6">
          NLP
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="NLP" data-md-level="1">
        <label class="md-nav__title" for="__nav_6">
          <span class="md-nav__icon md-icon"></span>
          NLP
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/ABSA/" class="md-nav__link">
        ABSA
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/Lexicon-%E8%AF%8D%E6%B1%87%E5%A2%9E%E5%BC%BA/" class="md-nav__link">
        Lexicon 词汇增强
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../../NLP/Prompt-%E6%8F%90%E7%A4%BA%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/" class="md-nav__link">
        Prompt 提示语言模型
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="/" class="md-nav__link">
        Blog
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#model-inference-averaging" class="md-nav__link">
    Model Inference &amp; Averaging
  </a>
  
    <nav class="md-nav" aria-label="Model Inference &amp; Averaging">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#maximum-likelihood-methods-bootstrap" class="md-nav__link">
    Maximum Likelihood Methods &amp; Bootstrap
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bayesian-methods" class="md-nav__link">
    Bayesian Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#the-em-algorithm" class="md-nav__link">
    The EM Algorithm
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mcmc-sampling-from-posterior" class="md-nav__link">
    MCMC - Sampling from Posterior
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#bagging" class="md-nav__link">
    Bagging
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#model-averaging-stacking" class="md-nav__link">
    Model Averaging &amp; Stacking
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stochastic-search-bumping" class="md-nav__link">
    Stochastic Search: Bumping
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additive-models-trees-related-methods" class="md-nav__link">
    Additive Models, Trees, &amp; Related Methods
  </a>
  
    <nav class="md-nav" aria-label="Additive Models, Trees, &amp; Related Methods">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#generalized-additive-models" class="md-nav__link">
    Generalized Additive Models
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tree-based-methods" class="md-nav__link">
    Tree Based Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#patient-rule-induction-method-prim" class="md-nav__link">
    Patient Rule Induction Method (PRIM)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multivariate-adaptive-regression-splines-mars" class="md-nav__link">
    Multivariate Adaptive Regression Splines (MARS)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#hierarchical-mixture-of-experts-hme" class="md-nav__link">
    Hierarchical Mixture of Experts (HME)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#boosting-additive-trees" class="md-nav__link">
    Boosting &amp; Additive Trees
  </a>
  
    <nav class="md-nav" aria-label="Boosting &amp; Additive Trees">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#boosting-methods" class="md-nav__link">
    Boosting Methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#boosting-fits-an-additive-model" class="md-nav__link">
    Boosting Fits an Additive Model
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exponential-loss-adaboost" class="md-nav__link">
    Exponential Loss &amp; AdaBoost
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss-functions-robustness" class="md-nav__link">
    Loss Functions &amp; Robustness
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#off-the-shelf-procedure-for-data-mining" class="md-nav__link">
    ”Off-the-Shelf” Procedure for Data Mining
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#boosting-trees" class="md-nav__link">
    Boosting Trees
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gradient-boosting" class="md-nav__link">
    Gradient Boosting
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#boosting-tree-size" class="md-nav__link">
    Boosting Tree Size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regularization" class="md-nav__link">
    Regularization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
  <a href="https://github.com/lightblues/techNotes/edit/main/docs/stat/ASL-note3.md" title="Edit this page" class="md-content__button md-icon">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg>
  </a>



  <h1>ASL3</h1>

<h2 id="model-inference-averaging">Model Inference &amp; Averaging<a class="headerlink" href="#model-inference-averaging" title="Permanent link">&para;</a></h2>
<p>本书的大部分章节中，对于回归而言，模型的拟合（学习）通过最小化平方和实现；或对于分类而言，通过最小化交叉熵实现．事实上，这两种最小化都是用极大似然来拟合的实例．</p>
<p>这章中，我们给出极大似然法的一个一般性的描述，以及用于推断的贝叶斯方法．在第7章中讨论的自助法在本章中也继续讨论，而且描述了它与极大似然和贝叶斯之间的联系．最后，我们提出模型平均和改善的相关技巧，包括 committee 方法、bagging、stacking 和 bumping．</p>
<h3 id="maximum-likelihood-methods-bootstrap">Maximum Likelihood Methods &amp; Bootstrap<a class="headerlink" href="#maximum-likelihood-methods-bootstrap" title="Permanent link">&para;</a></h3>
<h4 id="maximum-likelihood-inference">Maximum Likelihood Inference<a class="headerlink" href="#maximum-likelihood-inference" title="Permanent link">&para;</a></h4>
<p>Specify a probability density / mass function for observations</p>
<div class="arithmatex">
<div class="MathJax_Preview">
z_{i} \sim g_{\theta}(z)
</div>
<script type="math/tex; mode=display">
z_{i} \sim g_{\theta}(z)
</script>
</div>
<p>Likelihood function (function of <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> given <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> )</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L(\theta ; Z)=\prod_{i=1}^{N} g_{\theta}\left(z_{i}\right)
</div>
<script type="math/tex; mode=display">
L(\theta ; Z)=\prod_{i=1}^{N} g_{\theta}\left(z_{i}\right)
</script>
</div>
<p>Easy to deal with the logarithm of <span class="arithmatex"><span class="MathJax_Preview">L(\theta ; Z)</span><script type="math/tex">L(\theta ; Z)</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
l(\theta ; Z)=\sum_{i=1}^{N} /\left(\theta ; z_{i}\right)=\sum_{i=1}^{N} \log g_{\theta}\left(z_{i}\right)
</div>
<script type="math/tex; mode=display">
l(\theta ; Z)=\sum_{i=1}^{N} /\left(\theta ; z_{i}\right)=\sum_{i=1}^{N} \log g_{\theta}\left(z_{i}\right)
</script>
</div>
<p>Maximum Likelihood estimate</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta}=\operatorname{argmax}_{\theta} /(\theta ; Z)
</div>
<script type="math/tex; mode=display">
\hat{\theta}=\operatorname{argmax}_{\theta} /(\theta ; Z)
</script>
</div>
<p>定义得分函数 <strong>Score function</strong></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\dot{l}(\theta ; Z)=\sum_{i=1}^{N} \dot{l}\left(\theta ; z_{i}\right), \text { where } \dot{I}\left(\theta ; z_{i}\right)=\frac{\partial l\left(\theta ; z_{i}\right)}{\partial \theta}
</div>
<script type="math/tex; mode=display">
\dot{l}(\theta ; Z)=\sum_{i=1}^{N} \dot{l}\left(\theta ; z_{i}\right), \text { where } \dot{I}\left(\theta ; z_{i}\right)=\frac{\partial l\left(\theta ; z_{i}\right)}{\partial \theta}
</script>
</div>
<p>Assuming likelihood takes its maximum in the interior of the parameter space, <span class="arithmatex"><span class="MathJax_Preview">l(\hat{\theta} ; Z)=0</span><script type="math/tex">l(\hat{\theta} ; Z)=0</script></span>
信息矩阵 <strong>Information matrix</strong></p>
<div class="arithmatex">
<div class="MathJax_Preview">
I(\theta)=-\sum_{i=1}^{N} \frac{\partial^{2} l\left(\theta ; z_{i}\right)}{\partial \theta \partial \theta^{\top}}
</div>
<script type="math/tex; mode=display">
I(\theta)=-\sum_{i=1}^{N} \frac{\partial^{2} l\left(\theta ; z_{i}\right)}{\partial \theta \partial \theta^{\top}}
</script>
</div>
<ul>
<li>Observed information <span class="arithmatex"><span class="MathJax_Preview">\left.I(\theta)\right|_{\theta=\hat{\theta}}</span><script type="math/tex">\left.I(\theta)\right|_{\theta=\hat{\theta}}</script></span></li>
<li><strong>Fisher information</strong> <span class="arithmatex"><span class="MathJax_Preview">i(\theta)=E_{\theta}[I(\theta)]</span><script type="math/tex">i(\theta)=E_{\theta}[I(\theta)]</script></span> Fisher 信息量（或者期望信息量）</li>
</ul>
<p>可以认为, Fisher 信息量是参数估计的方差.</p>
<p>相关推导参见 <a href="https://esl.hohoweiya.xyz/08-Model-Inference-and-Averaging/8.2-The-Bootstrap-and-Maximum-Likelihood-Methods/index.html">here</a> 的注释.</p>
<p>Sampling distribution of MLE <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}</span><script type="math/tex">\hat{\theta}</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta} \rightarrow \mathcal{N}\left(\theta_{0}, i\left(\theta_{0}\right)^{-1}\right) \text { as } N \rightarrow \infty
</div>
<script type="math/tex; mode=display">
\hat{\theta} \rightarrow \mathcal{N}\left(\theta_{0}, i\left(\theta_{0}\right)^{-1}\right) \text { as } N \rightarrow \infty
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\theta_{0}</span><script type="math/tex">\theta_{0}</script></span> denotes the true value of <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>
Can be approximated by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\mathcal{N}\left(\hat{\theta}, i(\hat{\theta})^{-1}\right) \text { or } \mathcal{N}\left(\hat{\theta}, I(\hat{\theta})^{-1}\right)
</div>
<script type="math/tex; mode=display">
\mathcal{N}\left(\hat{\theta}, i(\hat{\theta})^{-1}\right) \text { or } \mathcal{N}\left(\hat{\theta}, I(\hat{\theta})^{-1}\right)
</script>
</div>
<p>Estimates for standard error of <span class="arithmatex"><span class="MathJax_Preview">\hat{\theta}_{j}</span><script type="math/tex">\hat{\theta}_{j}</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\sqrt{i(\hat{\theta})_{j j}^{-1}} \text { or } \sqrt{l(\hat{\theta})_{j j}^{-1}}
</div>
<script type="math/tex; mode=display">
\sqrt{i(\hat{\theta})_{j j}^{-1}} \text { or } \sqrt{l(\hat{\theta})_{j j}^{-1}}
</script>
</div>
<p>Confidence interval</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta}_{j}-z^{1-\alpha} \cdot \sqrt{i(\hat{\theta})_{j j}^{-1}} \text { or } \hat{\theta}_{j}-z^{1-\alpha} \cdot \sqrt{I(\hat{\theta})_{j j}^{-1}}
</div>
<script type="math/tex; mode=display">
\hat{\theta}_{j}-z^{1-\alpha} \cdot \sqrt{i(\hat{\theta})_{j j}^{-1}} \text { or } \hat{\theta}_{j}-z^{1-\alpha} \cdot \sqrt{I(\hat{\theta})_{j j}^{-1}}
</script>
</div>
<p>Joint confidence region</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\left\{\theta ; 2(l(\hat{\theta})-l(\theta)) \leqslant \chi_{p}^{2,(1-2 \alpha)}\right\}
</div>
<script type="math/tex; mode=display">
\left\{\theta ; 2(l(\hat{\theta})-l(\theta)) \leqslant \chi_{p}^{2,(1-2 \alpha)}\right\}
</script>
</div>
<p>Since <span class="arithmatex"><span class="MathJax_Preview">2\left[l(\hat{\theta})-l\left(\theta_{0}\right)\right] \sim \chi_{p}^{2}</span><script type="math/tex">2\left[l(\hat{\theta})-l\left(\theta_{0}\right)\right] \sim \chi_{p}^{2}</script></span></p>
<h4 id="_1">自助法和最大似然法<a class="headerlink" href="#_1" title="Permanent link">&para;</a></h4>
<p>本质上自助法是非参最大似然或者参数最大似然法的计算机实现．The bootstrap can be thought of as a “nonparametric” MLE.</p>
<p>与最大似然法相比自助法的好处是允许我们在没有公式的情况下计算标准误差和其他一些量的最大似然估计．</p>
<h3 id="bayesian-methods">Bayesian Methods<a class="headerlink" href="#bayesian-methods" title="Permanent link">&para;</a></h3>
<p>Posterior distribution of <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> given prior and data
$$
p(\theta \mid Z)=\frac{p(Z \mid \theta) p(\theta)}{\int p\left(Z \mid \theta^{\prime}\right) p\left(\theta^{\prime}\right) d \theta^{\prime}}
$$
<strong>Predictive distribution</strong> for new data
$$
p\left(z^{\text {new }} \mid Z\right)=\int p\left(z^{\text {new }} \mid \theta\right) p(\theta \mid Z) d \theta
$$
However, by Maximum Likelihood methods, the predictive distribution for new data <span class="arithmatex"><span class="MathJax_Preview">z^{\text {new }}</span><script type="math/tex">z^{\text {new }}</script></span> is <span class="arithmatex"><span class="MathJax_Preview">p\left(z^{\text {new }} \mid \hat{\theta}_{M L E}\right)</span><script type="math/tex">p\left(z^{\text {new }} \mid \hat{\theta}_{M L E}\right)</script></span>, which does not account for the uncertainty in estimating <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span></p>
<h4 id="back-to-example">Back to Example<a class="headerlink" href="#back-to-example" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
Y=\sum_{j=1}^{J} \beta_{j} h_{j}(X)+\epsilon, \text { with } \epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)
</div>
<script type="math/tex; mode=display">
Y=\sum_{j=1}^{J} \beta_{j} h_{j}(X)+\epsilon, \text { with } \epsilon \sim \mathcal{N}\left(0, \sigma^{2}\right)
</script>
</div>
<ul>
<li>Let <span class="arithmatex"><span class="MathJax_Preview">u(x)=\sum_{j=1}^{J} \beta_{j} h_{j}(X), B</span><script type="math/tex">u(x)=\sum_{j=1}^{J} \beta_{j} h_{j}(X), B</script></span> be the design matrix with <span class="arithmatex"><span class="MathJax_Preview">i j_{t h}</span><script type="math/tex">i j_{t h}</script></span> element <span class="arithmatex"><span class="MathJax_Preview">h_{j}\left(x_{i}\right)</span><script type="math/tex">h_{j}\left(x_{i}\right)</script></span></li>
<li>Prior on <span class="arithmatex"><span class="MathJax_Preview">\beta=\left(\beta_{1}, \ldots, \beta_{p}\right)^{\top}: \beta \sim \mathcal{N}(0, \tau \Sigma)</span><script type="math/tex">\beta=\left(\beta_{1}, \ldots, \beta_{p}\right)^{\top}: \beta \sim \mathcal{N}(0, \tau \Sigma)</script></span></li>
<li>Data likelihood: <span class="arithmatex"><span class="MathJax_Preview">p(y \mid X, \beta)=\mathcal{N}\left(y ; B \beta, \sigma^{2} I_{n}\right)</span><script type="math/tex">p(y \mid X, \beta)=\mathcal{N}\left(y ; B \beta, \sigma^{2} I_{n}\right)</script></span></li>
<li>Posterior distribution for <span class="arithmatex"><span class="MathJax_Preview">\beta</span><script type="math/tex">\beta</script></span> :
$$
p(\beta \mid Z)=p(\beta \mid X, y)=\frac{p(y \mid X, \beta) p(\beta)}{p(y \mid X)}=\mathcal{N}\left(\beta ; A^{-1} B^{\top} y, A^{-1} \sigma^{2}\right)
$$
with <span class="arithmatex"><span class="MathJax_Preview">A=B^{\top} B+\frac{\sigma^{2}}{\tau} \Sigma^{-1}</span><script type="math/tex">A=B^{\top} B+\frac{\sigma^{2}}{\tau} \Sigma^{-1}</script></span></li>
</ul>
<p><img alt="" src="../media/ASL-note3/2021-12-22-17-29-21.png" /></p>
<p>可以看到, 对于 <span class="arithmatex"><span class="MathJax_Preview">\tau=1</span><script type="math/tex">\tau=1</script></span> 拟合的曲线更为光滑 (因为我们在光滑性上面强加了更多先验权重). 而 <span class="arithmatex"><span class="MathJax_Preview">\tao=100</span><script type="math/tex">\tao=100</script></span> 时的曲线更接近 MLE 的结果.</p>
<p>实际上, 当 <span class="arithmatex"><span class="MathJax_Preview">\tau \rightarrow \infty</span><script type="math/tex">\tau \rightarrow \infty</script></span> 时称之为 <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> 的 <strong>noninformative prior</strong> 无信息先验. 在高斯模型中，极大似然和参数自助法趋向于与对自由参数使用了无信息先验的贝叶斯保持一致．因为在常值先验情况下，后验分布与似然函数成比例，所以这些方法趋向于一致．这种对应也可以推广到非参的情形．其中非参自助法近似于一个无信息先验的贝叶斯分析；8.4 节将详细介绍．</p>
<h3 id="the-em-algorithm">The EM Algorithm<a class="headerlink" href="#the-em-algorithm" title="Permanent link">&para;</a></h3>
<p>可以参考之前统计计算的 <a href="https://lightblues.github.io/posts/346b3c08/">note</a>, 思路和 <a href="https://jozeelin.github.io/2019/06/14/EM%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%8E%A8%E5%B9%BF/">EM算法及其推广</a> 比较像. ESL 的版本 <a href="https://esl.hohoweiya.xyz/08-Model-Inference-and-Averaging/8.5-The-EM-Algorithm/index.html">here</a>.</p>
<p>EM 算法是简化复杂极大似然问题的一种很受欢迎的工具．我们首先在一个简单的混合模型中讨论它．</p>
<h4 id="gaussian-mixture-models-gmm">Gaussian Mixture Models (GMM)<a class="headerlink" href="#gaussian-mixture-models-gmm" title="Permanent link">&para;</a></h4>
<p>Mathematical definition</p>
<div class="arithmatex">
<div class="MathJax_Preview">
p(x \mid \theta)=\sum_{k=1}^{k} \pi_{k} \cdot N\left(x_{k} ; \mu_{k}, \Sigma_{k}\right)
</div>
<script type="math/tex; mode=display">
p(x \mid \theta)=\sum_{k=1}^{k} \pi_{k} \cdot N\left(x_{k} ; \mu_{k}, \Sigma_{k}\right)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\sum_{k=1}^{K} \pi_{k}=1</span><script type="math/tex">\sum_{k=1}^{K} \pi_{k}=1</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\pi_{k} \geqslant 0</span><script type="math/tex">\pi_{k} \geqslant 0</script></span> for <span class="arithmatex"><span class="MathJax_Preview">k=1, \ldots, K</span><script type="math/tex">k=1, \ldots, K</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\theta=\left(\mu_{1}, \ldots, \mu_{K}, \Sigma_{1}, \ldots, \Sigma_{K}, \pi_{1}, \ldots, \pi_{K}\right)</span><script type="math/tex">\theta=\left(\mu_{1}, \ldots, \mu_{K}, \Sigma_{1}, \ldots, \Sigma_{K}, \pi_{1}, \ldots, \pi_{K}\right)</script></span></p>
<h4 id="parameter-estimation-for-gmm">Parameter Estimation for GMM<a class="headerlink" href="#parameter-estimation-for-gmm" title="Permanent link">&para;</a></h4>
<p>Given <span class="arithmatex"><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> independent training samples <span class="arithmatex"><span class="MathJax_Preview">x_{1}, x_{2}, \ldots, x_{n}</span><script type="math/tex">x_{1}, x_{2}, \ldots, x_{n}</script></span> from a GMM, can still use MLE to estimate <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span>, but&hellip;
Assume <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> is fixed and known, log likelihood of the data is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
I(\theta ; x)=\sum_{i=1}^{n} \log \left(\sum_{k=1}^{k} \pi_{k} N\left(x_{i} ; \mu_{k}, \Sigma_{k}\right)\right)
</div>
<script type="math/tex; mode=display">
I(\theta ; x)=\sum_{i=1}^{n} \log \left(\sum_{k=1}^{k} \pi_{k} N\left(x_{i} ; \mu_{k}, \Sigma_{k}\right)\right)
</script>
</div>
<p>Let&rsquo;s try to maximize it</p>
<ul>
<li>By eyeballing</li>
<li>Analytically</li>
<li>Numerically</li>
</ul>
<h4 id="the-mmalgorithm-minimization">The MMAlgorithm - Minimization<a class="headerlink" href="#the-mmalgorithm-minimization" title="Permanent link">&para;</a></h4>
<p>Majorize-Minimization/Maximization. MM算法是一种迭代优化方法，它利用函数的凸性来找到它们的最大值或最小值。当目标函数难以优化的时候, 转而去找一个容易优化的目标函数替代, 迭代.</p>
<p>例如, 对于最小化问题, 每次迭代找到一个目标函数的上界函数，求上界函数的最小值。</p>
<ul>
<li>To minimize an objective function <span class="arithmatex"><span class="MathJax_Preview">f(\theta)</span><script type="math/tex">f(\theta)</script></span></li>
<li>The MM algorithm is a prescription for constructing optimization algorithms</li>
<li>An MM algorithm creates a surrogate 替代 function that majories the objective function.</li>
<li>When the surrogate function is minimized, the objective function is decreased</li>
<li>For minimization, <span class="arithmatex"><span class="MathJax_Preview">M M \equiv</span><script type="math/tex">M M \equiv</script></span> majorize</li>
</ul>
<h5 id="majorization">Majorization<a class="headerlink" href="#majorization" title="Permanent link">&para;</a></h5>
<p>A function <span class="arithmatex"><span class="MathJax_Preview">g\left(\theta ; \theta^{(t)}\right)</span><script type="math/tex">g\left(\theta ; \theta^{(t)}\right)</script></span> <strong>majorizes</strong> the function <span class="arithmatex"><span class="MathJax_Preview">f(\theta)</span><script type="math/tex">f(\theta)</script></span> at <span class="arithmatex"><span class="MathJax_Preview">\theta^{(t)}</span><script type="math/tex">\theta^{(t)}</script></span> if</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">f\left(\theta^{(t)}\right)=g\left(\theta^{(t)} ; \theta^{(t)}\right)</span><script type="math/tex">f\left(\theta^{(t)}\right)=g\left(\theta^{(t)} ; \theta^{(t)}\right)</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">f(\theta) \leqslant g\left(\theta ; \theta^{(t)}\right)</span><script type="math/tex">f(\theta) \leqslant g\left(\theta ; \theta^{(t)}\right)</script></span> for all <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> 即替代函数是原函数的上届</li>
</ul>
<p>If <span class="arithmatex"><span class="MathJax_Preview">f(\theta)</span><script type="math/tex">f(\theta)</script></span> is difficult to minimize, find <span class="arithmatex"><span class="MathJax_Preview">g\left(\theta ; \theta^{(t)}\right)</span><script type="math/tex">g\left(\theta ; \theta^{(t)}\right)</script></span>, which is easy to minimize. Let
$$
\theta^{(t+1)}=\operatorname{argmin}_{\theta} g\left(\theta ; \theta^{(t)}\right)
$$</p>
<p>MM minimization algorithm satisfied the <strong>descent property</strong> as
$$
\begin{aligned}
f\left(\theta^{(t+1)}\right) &amp; \leqslant g\left(\theta^{(t+1)} ; \theta^{(t)}\right) \
&amp; \leqslant g\left(\theta^{(t)} ; \theta^{(t)}\right) \
&amp;=f\left(\theta^{(t)}\right)
\end{aligned}
$$
In summary
$$
f\left(\theta^{(t+1)}\right) \leqslant f\left(\theta^{(t)}\right)
$$
The descent property makes the MM algorithm very stable algorithms converges to local minima or saddle point</p>
<h5 id="big-question">Big Question<a class="headerlink" href="#big-question" title="Permanent link">&para;</a></h5>
<p>How do we majorize / minorize a function?
Some generic tricks and tools</p>
<ul>
<li>Jensen&rsquo;s inequality</li>
<li>Chord above the graph property of a convex function</li>
<li>Supporting hyperplane property of a convex function</li>
<li>Quadratic upper bound principle</li>
<li>Arithmetic-geometric mean inequality</li>
<li>Cauchy-Schwartz inequality</li>
</ul>
<p>We&rsquo;ll only focus on Jensen&rsquo;s inequality here</p>
<h5 id="jensens-inequality">Jensen&rsquo;s Inequality<a class="headerlink" href="#jensens-inequality" title="Permanent link">&para;</a></h5>
<p>You&rsquo;ve probably minorized via Jensen&rsquo;s Inequality without noticing it</p>
<p>Remember Jensen&rsquo;s Inequality [画图很容易理解]</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">h(\cdot)</span><script type="math/tex">h(\cdot)</script></span> be a concave function</li>
<li><span class="arithmatex"><span class="MathJax_Preview">\pi_{1}, \ldots, \pi_{K}</span><script type="math/tex">\pi_{1}, \ldots, \pi_{K}</script></span> be <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> non-negative numbers, with <span class="arithmatex"><span class="MathJax_Preview">\sum_{k=1}^{K} \pi_{k}=1</span><script type="math/tex">\sum_{k=1}^{K} \pi_{k}=1</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> arbituary numbers <span class="arithmatex"><span class="MathJax_Preview">a_{1}, \ldots, a_{K}</span><script type="math/tex">a_{1}, \ldots, a_{K}</script></span></li>
</ul>
<p>Then</p>
<div class="arithmatex">
<div class="MathJax_Preview">
h\left(\sum_{k=1}^{K} \pi_{k} a_{k}\right) \geqslant \sum_{k=1}^{K} \pi_{k} h\left(a_{k}\right)
</div>
<script type="math/tex; mode=display">
h\left(\sum_{k=1}^{K} \pi_{k} a_{k}\right) \geqslant \sum_{k=1}^{K} \pi_{k} h\left(a_{k}\right)
</script>
</div>
<h4 id="expectation-maximization">Expectation - Maximization<a class="headerlink" href="#expectation-maximization" title="Permanent link">&para;</a></h4>
<p>EM 算法参见 <a href="https://jozeelin.github.io/2019/06/14/EM%E7%AE%97%E6%B3%95%E5%8F%8A%E5%85%B6%E6%8E%A8%E5%B9%BF/">here</a>. 注意到, 链接中理解 Z 是隐变量(一部分的因变量, 但无法观测.) 「用Y表示观测随机变量的数据，Z表示隐随机变量的数据。Y和Z连在一起称为完全数据，观测数据Y又称为不完全数据。」</p>
<p>除了上面链接中的理解, 这里直接基于 Jenson 不等式</p>
<ul>
<li>The EM algorithm is a MM algorithm EM 算法是 MM 算法的一种</li>
<li>EM uses Jensen&rsquo;s inequality to minorize the log likelihood</li>
</ul>
<p>Step 1</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
l(\theta ; X) &amp;=\log \left(\sum_{j=1}^{n_{z}} p\left(X, Z=z_{j} \mid \theta\right)\right) \\
&amp;=\log \left(\sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \frac{p\left(X, Z=z_{j} \mid \theta\right)}{f^{(t)}\left(Z=z_{j}\right)}\right) \\
&amp; \geqslant \sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \log \left(\frac{P\left(X, Z=z_{j} \mid \theta\right)}{f^{(t)}\left(Z=z_{j}\right)}\right)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
l(\theta ; X) &=\log \left(\sum_{j=1}^{n_{z}} p\left(X, Z=z_{j} \mid \theta\right)\right) \\
&=\log \left(\sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \frac{p\left(X, Z=z_{j} \mid \theta\right)}{f^{(t)}\left(Z=z_{j}\right)}\right) \\
& \geqslant \sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \log \left(\frac{P\left(X, Z=z_{j} \mid \theta\right)}{f^{(t)}\left(Z=z_{j}\right)}\right)
\end{aligned}
</script>
</div>
<p>Summary of step 1 , we&rsquo;ve got</p>
<div class="arithmatex">
<div class="MathJax_Preview">
l(\theta ; X) \geqslant \sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \log \left(\frac{p\left(X, Z=z_{j} \mid \theta\right)}{f^{(t)}\left(Z=z_{j}\right)}\right)
</div>
<script type="math/tex; mode=display">
l(\theta ; X) \geqslant \sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \log \left(\frac{p\left(X, Z=z_{j} \mid \theta\right)}{f^{(t)}\left(Z=z_{j}\right)}\right)
</script>
</div>
<p>Step 2: find <span class="arithmatex"><span class="MathJax_Preview">f^{(t)}(Z)</span><script type="math/tex">f^{(t)}(Z)</script></span></p>
<p>The lower bound must touch the <span class="arithmatex"><span class="MathJax_Preview">\log</span><script type="math/tex">\log</script></span> likelihood at <span class="arithmatex"><span class="MathJax_Preview">\theta^{(t)}</span><script type="math/tex">\theta^{(t)}</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
l\left(\theta^{(t)} ; X\right)=\sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \log \left(\frac{p\left(X, Z=z_{j} \mid \theta^{(t)}\right)}{f^{(t)}\left(Z=z_{j}\right)}\right)
</div>
<script type="math/tex; mode=display">
l\left(\theta^{(t)} ; X\right)=\sum_{j=1}^{n_{z}} f^{(t)}\left(Z=z_{j}\right) \log \left(\frac{p\left(X, Z=z_{j} \mid \theta^{(t)}\right)}{f^{(t)}\left(Z=z_{j}\right)}\right)
</script>
</div>
<p>From this we can calculate <span class="arithmatex"><span class="MathJax_Preview">f^{(t)}(Z)</span><script type="math/tex">f^{(t)}(Z)</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
f^{(t)}(Z)=p\left(Z \mid X, \theta^{(t)}\right)
</div>
<script type="math/tex; mode=display">
f^{(t)}(Z)=p\left(Z \mid X, \theta^{(t)}\right)
</script>
</div>
<p>The log likelihood is minorized by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
g\left(\theta ; \theta^{(t)}\right)=\sum_{j=1}^{n_{z}} p\left(Z=z_{j} \mid X, \theta^{(t)}\right) \log \left(\frac{p\left(X, Z=z_{j} \mid \theta\right)}{p\left(Z=z_{j} \mid X, \theta^{(t)}\right)}\right)
</div>
<script type="math/tex; mode=display">
g\left(\theta ; \theta^{(t)}\right)=\sum_{j=1}^{n_{z}} p\left(Z=z_{j} \mid X, \theta^{(t)}\right) \log \left(\frac{p\left(X, Z=z_{j} \mid \theta\right)}{p\left(Z=z_{j} \mid X, \theta^{(t)}\right)}\right)
</script>
</div>
<p>Step 3: find \&amp; max <span class="arithmatex"><span class="MathJax_Preview">g\left(\theta ; \theta^{(t)}\right)</span><script type="math/tex">g\left(\theta ; \theta^{(t)}\right)</script></span>
What are the <span class="arithmatex"><span class="MathJax_Preview">Z^{\prime} s</span><script type="math/tex">Z^{\prime} s</script></span> and where did they come from?</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> is a <strong>random variable</strong> whose pdf conditioned on <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span> is completely determined by <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span></li>
<li>Choice of <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> should make the maximization step easy</li>
</ul>
<h4 id="two-component-gmm">Two Component GMM<a class="headerlink" href="#two-component-gmm" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;Y_{1} \sim N\left(\mu_{1}, \sigma_{1}^{2}\right) \\
&amp;Y_{2} \sim N\left(\mu_{2}, \sigma_{2}^{2}\right) \\
&amp;Y=(1-\Delta) \cdot Y_{1}+\Delta \cdot Y_{2}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&Y_{1} \sim N\left(\mu_{1}, \sigma_{1}^{2}\right) \\
&Y_{2} \sim N\left(\mu_{2}, \sigma_{2}^{2}\right) \\
&Y=(1-\Delta) \cdot Y_{1}+\Delta \cdot Y_{2}
\end{aligned}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\Delta=\{0,1\}</span><script type="math/tex">\Delta=\{0,1\}</script></span> with <span class="arithmatex"><span class="MathJax_Preview">\operatorname{Pr}(\Delta=1)=\pi</span><script type="math/tex">\operatorname{Pr}(\Delta=1)=\pi</script></span>
Equivalent two-step representation</p>
<ul>
<li>Generate a <span class="arithmatex"><span class="MathJax_Preview">\Delta</span><script type="math/tex">\Delta</script></span> with probability <span class="arithmatex"><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span></li>
<li>Depending on the outcome of <span class="arithmatex"><span class="MathJax_Preview">\Delta</span><script type="math/tex">\Delta</script></span>, deliver either <span class="arithmatex"><span class="MathJax_Preview">Y_{1}</span><script type="math/tex">Y_{1}</script></span> or <span class="arithmatex"><span class="MathJax_Preview">Y_{2}</span><script type="math/tex">Y_{2}</script></span></li>
</ul>
<p>Let <span class="arithmatex"><span class="MathJax_Preview">\phi_{\theta}(x)</span><script type="math/tex">\phi_{\theta}(x)</script></span> denote the normal density with parameter <span class="arithmatex"><span class="MathJax_Preview">\theta=\left(\mu, \sigma^{2}\right)</span><script type="math/tex">\theta=\left(\mu, \sigma^{2}\right)</script></span>, then the density of <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P_{Y}(y)=(1-\pi) \phi_{\theta_{1}}(y)+\pi \phi_{\theta_{2}}(y)
</div>
<script type="math/tex; mode=display">
P_{Y}(y)=(1-\pi) \phi_{\theta_{1}}(y)+\pi \phi_{\theta_{2}}(y)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\theta=\left(\pi, \theta_{1}, \theta_{2}\right)=\left(\pi, \mu_{1}, \sigma_{1}^{2}, \mu_{2}, \sigma_{2}^{2}\right)</span><script type="math/tex">\theta=\left(\pi, \theta_{1}, \theta_{2}\right)=\left(\pi, \mu_{1}, \sigma_{1}^{2}, \mu_{2}, \sigma_{2}^{2}\right)</script></span></p>
<p>The log likelihood of the data</p>
<div class="arithmatex">
<div class="MathJax_Preview">
I(\theta ; Y)=\sum_{i=1}^{N} \log \left[(1-\pi) \phi_{\theta_{1}}\left(y_{i}\right)+\pi \phi_{\theta_{2}}\left(y_{i}\right)\right]
</div>
<script type="math/tex; mode=display">
I(\theta ; Y)=\sum_{i=1}^{N} \log \left[(1-\pi) \phi_{\theta_{1}}\left(y_{i}\right)+\pi \phi_{\theta_{2}}\left(y_{i}\right)\right]
</script>
</div>
<p><strong>Introduce latent variables</strong> <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i}</span><script type="math/tex">\Delta_{i}</script></span> taking values 0 or 1</p>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i}=1, Y_{i}</span><script type="math/tex">\Delta_{i}=1, Y_{i}</script></span> comes from model 2</li>
<li>Otherwise it comes from model 1</li>
</ul>
<p>Suppose we know the values of <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i}</span><script type="math/tex">\Delta_{i}</script></span>, the log likelihood can be written</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
I(\theta ; Y, \Delta)=&amp; \sum_{i=1}^{N}\left[\left(1-\Delta_{i}\right) \log \phi_{\theta_{1}}\left(y_{i}\right)+\Delta_{i} \log \phi_{\theta_{2}}\left(y_{i}\right)\right]+\\
&amp; \sum_{i=1}^{N}\left[\left(1-\Delta_{i}\right) \log (1-\pi)+\Delta_{i} \log \pi\right]
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
I(\theta ; Y, \Delta)=& \sum_{i=1}^{N}\left[\left(1-\Delta_{i}\right) \log \phi_{\theta_{1}}\left(y_{i}\right)+\Delta_{i} \log \phi_{\theta_{2}}\left(y_{i}\right)\right]+\\
& \sum_{i=1}^{N}\left[\left(1-\Delta_{i}\right) \log (1-\pi)+\Delta_{i} \log \pi\right]
\end{aligned}
</script>
</div>
<ul>
<li>The MLE of <span class="arithmatex"><span class="MathJax_Preview">\mu_{1}</span><script type="math/tex">\mu_{1}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\sigma_{1}^{2}</span><script type="math/tex">\sigma_{1}^{2}</script></span> would be the sample mean and variance for those data with <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i}=0</span><script type="math/tex">\Delta_{i}=0</script></span></li>
<li>The MLE of <span class="arithmatex"><span class="MathJax_Preview">\mu_{2}</span><script type="math/tex">\mu_{2}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\sigma_{2}^{2}</span><script type="math/tex">\sigma_{2}^{2}</script></span> would be the sample mean and variance for those data with <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i}=1</span><script type="math/tex">\Delta_{i}=1</script></span></li>
<li>The MLE of <span class="arithmatex"><span class="MathJax_Preview">\pi</span><script type="math/tex">\pi</script></span> would be the proportion of <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i}=1</span><script type="math/tex">\Delta_{i}=1</script></span></li>
</ul>
<p>However, <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i}</span><script type="math/tex">\Delta_{i}</script></span> are unknown, and we substibute with its expected value</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\gamma_{i}(\theta)=E\left(\Delta_{i} \mid \theta, Y\right)=\operatorname{Pr}\left(\Delta_{i}=1 \mid \theta, Y\right)
</div>
<script type="math/tex; mode=display">
\gamma_{i}(\theta)=E\left(\Delta_{i} \mid \theta, Y\right)=\operatorname{Pr}\left(\Delta_{i}=1 \mid \theta, Y\right)
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\gamma_{i}</span><script type="math/tex">\gamma_{i}</script></span> is called <strong>responsibility</strong> of model 2 for observation <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span></li>
<li>posteior of the probability that observation <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> belongs to model 2 given the data and current estimate</li>
</ul>
<p>For each iteration....</p>
<p>Expectation step</p>
<ul>
<li>soft assignment of each observation to each model</li>
<li>the current parameter estimates are used to assign responsibilities according to the relative density of the training points under each model</li>
</ul>
<p>Maximization step</p>
<ul>
<li>these responsibilities are used in weighted maximum-likelihood fits to update the parameter estimates</li>
</ul>
<h5 id="em-algorithm-for-two-component-gmm">EM Algorithm for Two-component GMM<a class="headerlink" href="#em-algorithm-for-two-component-gmm" title="Permanent link">&para;</a></h5>
<ol>
<li>Take initial guesses for the parameters <span class="arithmatex"><span class="MathJax_Preview">\left(\mu_{1}, \sigma_{1}^{2}, \mu_{2}, \sigma_{2}^{2}, \pi\right)</span><script type="math/tex">\left(\mu_{1}, \sigma_{1}^{2}, \mu_{2}, \sigma_{2}^{2}, \pi\right)</script></span></li>
<li>Expectation Step: compute the responsibilities</li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\gamma}_{i}=\frac{\hat{\pi} \phi_{\hat{\theta}_{2}}\left(y_{i}\right)}{(1-\hat{\pi}) \phi_{\hat{\theta}_{1}}\left(y_{i}\right)+\hat{\pi} \phi_{\hat{\theta}_{2}}\left(y_{i}\right)}
</div>
<script type="math/tex; mode=display">
\hat{\gamma}_{i}=\frac{\hat{\pi} \phi_{\hat{\theta}_{2}}\left(y_{i}\right)}{(1-\hat{\pi}) \phi_{\hat{\theta}_{1}}\left(y_{i}\right)+\hat{\pi} \phi_{\hat{\theta}_{2}}\left(y_{i}\right)}
</script>
</div>
<ol start="3">
<li>Maximization Step: compute the weighted means and variances and the mixing probability</li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
\hat{\mu}_{1}=\frac{\sum_{i}\left(1-\hat{\gamma}_{i}\right) y_{i}}{\sum_{i}\left(1-\hat{\gamma}_{i}\right)}, \quad \hat{\sigma}_{1}^{2}=\frac{\sum_{i}\left(1-\hat{\gamma}_{i}\right)\left(y_{i}-\hat{\mu}_{1}\right)^{2}}{\sum_{i}\left(1-\hat{\gamma}_{i}\right)} \\
\hat{\mu}_{2}=\frac{\sum_{i} \hat{\gamma}_{i} y_{i}}{\sum_{i} \hat{\gamma}_{i}}, \quad \hat{\sigma}_{1}^{2}=\frac{\sum_{i} \hat{\gamma}_{i}\left(y_{i}-\hat{\mu}_{1}\right)^{2}}{\sum_{i} \hat{\gamma}_{i}} \\
\hat{\pi}=\sum_{i} \frac{\hat{\gamma}_{i}}{N}
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
\hat{\mu}_{1}=\frac{\sum_{i}\left(1-\hat{\gamma}_{i}\right) y_{i}}{\sum_{i}\left(1-\hat{\gamma}_{i}\right)}, \quad \hat{\sigma}_{1}^{2}=\frac{\sum_{i}\left(1-\hat{\gamma}_{i}\right)\left(y_{i}-\hat{\mu}_{1}\right)^{2}}{\sum_{i}\left(1-\hat{\gamma}_{i}\right)} \\
\hat{\mu}_{2}=\frac{\sum_{i} \hat{\gamma}_{i} y_{i}}{\sum_{i} \hat{\gamma}_{i}}, \quad \hat{\sigma}_{1}^{2}=\frac{\sum_{i} \hat{\gamma}_{i}\left(y_{i}-\hat{\mu}_{1}\right)^{2}}{\sum_{i} \hat{\gamma}_{i}} \\
\hat{\pi}=\sum_{i} \frac{\hat{\gamma}_{i}}{N}
\end{gathered}
</script>
</div>
<ol start="4">
<li>Iterate steps 2 and 3 until convergence</li>
</ol>
<h3 id="mcmc-sampling-from-posterior">MCMC - Sampling from Posterior<a class="headerlink" href="#mcmc-sampling-from-posterior" title="Permanent link">&para;</a></h3>
<p>在定义了一个贝叶斯模型后，有人便想要从得到的后验分布中采样来对参数进行推断．除了简单的模型，这通常是一个很困难的计算问题．参见 <a href="https://esl.hohoweiya.xyz/08-Model-Inference-and-Averaging/8.6-MCMC-for-Sampling-from-the-Posterior/index.html">here</a> 的注释.</p>
<p>这节我们讨论用于后验采样的 马尔科夫蒙特卡洛法 (Markov chain Monte Carlo)．我们将要看到吉布斯采样（一个 MCMC 过程）与 EM 算法非常相关：主要的区别在于前者从条件分布中采样而非对它们最大化．</p>
<h4 id="markov-chain-monte-carlo-method">Markov Chain Monte Carlo Method<a class="headerlink" href="#markov-chain-monte-carlo-method" title="Permanent link">&para;</a></h4>
<p>Aim</p>
<ul>
<li>Generate independent samples <span class="arithmatex"><span class="MathJax_Preview">\left\{x^{(r)}\right\}_{r=1}^{R}</span><script type="math/tex">\left\{x^{(r)}\right\}_{r=1}^{R}</script></span> from a pdf <span class="arithmatex"><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span></li>
<li>Can then use <span class="arithmatex"><span class="MathJax_Preview">x^{(r)}</span><script type="math/tex">x^{(r)}</script></span> to estimate expectations of functions under this distribution</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
E[\phi(x)]=\int_{x} \phi(x) p(x) d x \approx \frac{1}{R} \sum_{r=1} \phi\left(x^{(r)}\right)
</div>
<script type="math/tex; mode=display">
E[\phi(x)]=\int_{x} \phi(x) p(x) d x \approx \frac{1}{R} \sum_{r=1} \phi\left(x^{(r)}\right)
</script>
</div>
<p>Not an easy task</p>
<ul>
<li>Sampling from <span class="arithmatex"><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> is hard in general</li>
<li>Especially when <span class="arithmatex"><span class="MathJax_Preview">x \in \mathbb{R}^{p}</span><script type="math/tex">x \in \mathbb{R}^{p}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> is large</li>
</ul>
<p>Common approach</p>
<ul>
<li>Monte Carlo Markov Chain methods such as Metropolis-Hastings and Gibbs sampling</li>
</ul>
<p>MCMC Assumptions 基本思路就是, 找到一个和 p 比较像的分布来近似.</p>
<ul>
<li>Want to draw samples from <span class="arithmatex"><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span></li>
<li>Can evaluate <span class="arithmatex"><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> within a normalization factor</li>
<li>That is, can evaluate a function <span class="arithmatex"><span class="MathJax_Preview">p^{*}(x)</span><script type="math/tex">p^{*}(x)</script></span> such that</li>
</ul>
<p>$$
p(x)=\frac{p^{*}(x)}{Z}
$$
where <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span> is a constant</p>
<h4 id="the-metropolis-hastings-method">The Metropolis-Hastings Method<a class="headerlink" href="#the-metropolis-hastings-method" title="Permanent link">&para;</a></h4>
<p>Initially</p>
<ul>
<li>Have an initial state <span class="arithmatex"><span class="MathJax_Preview">x^{(1)}</span><script type="math/tex">x^{(1)}</script></span></li>
<li>Define a proposal density <span class="arithmatex"><span class="MathJax_Preview">Q\left(x^{\prime} ; x^{(t)}\right)</span><script type="math/tex">Q\left(x^{\prime} ; x^{(t)}\right)</script></span> depending on the current state <span class="arithmatex"><span class="MathJax_Preview">x^{(t)}</span><script type="math/tex">x^{(t)}</script></span></li>
<li>Must be able to draw samples from <span class="arithmatex"><span class="MathJax_Preview">Q\left(x^{\prime} ; x^{(t)}\right)</span><script type="math/tex">Q\left(x^{\prime} ; x^{(t)}\right)</script></span></li>
</ul>
<p>At each iteration</p>
<ul>
<li>A <strong>tentative</strong> new state <span class="arithmatex"><span class="MathJax_Preview">x^{\prime}</span><script type="math/tex">x^{\prime}</script></span> is generated from the proposal density <span class="arithmatex"><span class="MathJax_Preview">Q\left(x^{\prime} ; x^{(t)}\right)</span><script type="math/tex">Q\left(x^{\prime} ; x^{(t)}\right)</script></span></li>
<li>Compute</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
a=\min \left(1, \frac{p^{*}\left(x^{\prime}\right) Q\left(x^{(t)} ; x^{\prime}\right)}{p^{*}\left(x^{(t)}\right) Q\left(x^{\prime} ; x^{(t)}\right)}\right)
</div>
<script type="math/tex; mode=display">
a=\min \left(1, \frac{p^{*}\left(x^{\prime}\right) Q\left(x^{(t)} ; x^{\prime}\right)}{p^{*}\left(x^{(t)}\right) Q\left(x^{\prime} ; x^{(t)}\right)}\right)
</script>
</div>
<ul>
<li>Accept new state <span class="arithmatex"><span class="MathJax_Preview">x^{\prime}</span><script type="math/tex">x^{\prime}</script></span> with probability a</li>
<li>Set</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
x^{(t+1)}= \begin{cases}x^{\prime} &amp; \text { if state is accepted } \\ x^{(t)} &amp; \text { if state is not accepted }\end{cases}
</div>
<script type="math/tex; mode=display">
x^{(t+1)}= \begin{cases}x^{\prime} & \text { if state is accepted } \\ x^{(t)} & \text { if state is not accepted }\end{cases}
</script>
</div>
<p><img alt="" src="../media/ASL-note3/2021-12-24-11-19-16.png" /></p>
<p>Convergence</p>
<ul>
<li>For any <span class="arithmatex"><span class="MathJax_Preview">Q</span><script type="math/tex">Q</script></span> s.t. <span class="arithmatex"><span class="MathJax_Preview">Q\left(x^{\prime} ; x\right)&gt;0 \forall x, x^{\prime}</span><script type="math/tex">Q\left(x^{\prime} ; x\right)>0 \forall x, x^{\prime}</script></span> as <span class="arithmatex"><span class="MathJax_Preview">t \rightarrow \infty</span><script type="math/tex">t \rightarrow \infty</script></span></li>
<li>The probability distribution of <span class="arithmatex"><span class="MathJax_Preview">x^{(t)}</span><script type="math/tex">x^{(t)}</script></span> tends to <span class="arithmatex"><span class="MathJax_Preview">p(x)=p^{*}(x) / Z</span><script type="math/tex">p(x)=p^{*}(x) / Z</script></span></li>
</ul>
<h4 id="gibbs-sampling">Gibbs Sampling<a class="headerlink" href="#gibbs-sampling" title="Permanent link">&para;</a></h4>
<p>Given a state <span class="arithmatex"><span class="MathJax_Preview">x^{(t)} \in \mathbb{R}^{p}</span><script type="math/tex">x^{(t)} \in \mathbb{R}^{p}</script></span>, generate a <strong>new state</strong> with</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
x_{1}^{(t+1)} \sim p\left(x_{1} \mid x_{2}^{(t)}, x_{3}^{(t)}, \ldots, x_{p}^{(t)}\right) \\
x_{2}^{(t+1)} \sim p\left(x_{2} \mid x_{1}^{(t+1)}, x_{3}^{(t)}, \ldots, x_{p}^{(t)}\right) \\
x_{3}^{(t+1)} \sim p\left(x_{3} \mid x_{1}^{(t+1)}, x_{2}^{(t+1)}, \ldots, x_{p}^{(t)}\right)
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
x_{1}^{(t+1)} \sim p\left(x_{1} \mid x_{2}^{(t)}, x_{3}^{(t)}, \ldots, x_{p}^{(t)}\right) \\
x_{2}^{(t+1)} \sim p\left(x_{2} \mid x_{1}^{(t+1)}, x_{3}^{(t)}, \ldots, x_{p}^{(t)}\right) \\
x_{3}^{(t+1)} \sim p\left(x_{3} \mid x_{1}^{(t+1)}, x_{2}^{(t+1)}, \ldots, x_{p}^{(t)}\right)
\end{gathered}
</script>
</div>
<p>where it is assumed that we can generate samples from <span class="arithmatex"><span class="MathJax_Preview">p\left(x_{i} \mid\left\{x_{j}\right\}_{j \neq i}\right)</span><script type="math/tex">p\left(x_{i} \mid\left\{x_{j}\right\}_{j \neq i}\right)</script></span>
Convergence</p>
<ul>
<li>Gibbs sampling is a Metropolis method</li>
<li>The probability distribution of <span class="arithmatex"><span class="MathJax_Preview">x^{(t)}</span><script type="math/tex">x^{(t)}</script></span> tends to <span class="arithmatex"><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> as <span class="arithmatex"><span class="MathJax_Preview">t \rightarrow \infty</span><script type="math/tex">t \rightarrow \infty</script></span>, as long as <span class="arithmatex"><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> does not have pathological properties</li>
</ul>
<p><img alt="" src="../media/ASL-note3/2021-12-24-10-57-52.png" /></p>
<h4 id="evolution-of-markov-chain">Evolution of Markov Chain<a class="headerlink" href="#evolution-of-markov-chain" title="Permanent link">&para;</a></h4>
<p>Markov chain defined by an initial <span class="arithmatex"><span class="MathJax_Preview">p^{(0)}(x)</span><script type="math/tex">p^{(0)}(x)</script></span> and a transition probability <span class="arithmatex"><span class="MathJax_Preview">T\left(x^{\prime} ; x\right)</span><script type="math/tex">T\left(x^{\prime} ; x\right)</script></span></p>
<p>Let <span class="arithmatex"><span class="MathJax_Preview">p^{(t)}(x)</span><script type="math/tex">p^{(t)}(x)</script></span> be the pdf of the state after <span class="arithmatex"><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span> applications of the Markov chain</p>
<p>The pdf of the state at the <span class="arithmatex"><span class="MathJax_Preview">(t+1)_{t h}</span><script type="math/tex">(t+1)_{t h}</script></span> iteration of the Markov Chain is given by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
p^{(t+1)}\left(x^{\prime}\right)=\int_{x} T\left(x^{\prime} ; x\right) p^{(t)}(x) d x
</div>
<script type="math/tex; mode=display">
p^{(t+1)}\left(x^{\prime}\right)=\int_{x} T\left(x^{\prime} ; x\right) p^{(t)}(x) d x
</script>
</div>
<p>Want to find a chain s.t. as <span class="arithmatex"><span class="MathJax_Preview">t \rightarrow \infty</span><script type="math/tex">t \rightarrow \infty</script></span> then <span class="arithmatex"><span class="MathJax_Preview">p^{(t)}(x) \rightarrow p(x)</span><script type="math/tex">p^{(t)}(x) \rightarrow p(x)</script></span></p>
<p>When designing a MCMC method, construct a chain with the following properties</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">p(x)</span><script type="math/tex">p(x)</script></span> is an invariant distribution of the chain</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
p\left(x^{\prime}\right)=\int_{x} T\left(x^{\prime} ; x\right) p(x) d x
</div>
<script type="math/tex; mode=display">
p\left(x^{\prime}\right)=\int_{x} T\left(x^{\prime} ; x\right) p(x) d x
</script>
</div>
<ul>
<li>The chain is ergodic</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
p^{(t)}(x) \rightarrow p(x) \text { as } t \rightarrow \infty \text { for any } p^{(0)}(x)
</div>
<script type="math/tex; mode=display">
p^{(t)}(x) \rightarrow p(x) \text { as } t \rightarrow \infty \text { for any } p^{(0)}(x)
</script>
</div>
<h4 id="gibbs-sampling-for-gmm">Gibbs Sampling for GMM<a class="headerlink" href="#gibbs-sampling-for-gmm" title="Permanent link">&para;</a></h4>
<p>Close connection between Gibbs sampling and the EM algorithm in exponential family models</p>
<p>Let</p>
<ul>
<li>the parameter <span class="arithmatex"><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> of the distribution</li>
<li>latent / missing data <span class="arithmatex"><span class="MathJax_Preview">Z^{m}</span><script type="math/tex">Z^{m}</script></span></li>
</ul>
<p>be parameter for Gibbs sampler</p>
<p>To estimate the parameter of a GMM at each iteration</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;\Delta_{i}^{(t+1)} \sim p\left(\Delta_{i} \mid \theta^{(t)}, Z\right) \text { for } i=1, \ldots, n \\
&amp;\theta^{(t+1)} \sim p\left(\theta \mid \Delta^{(t+1)}, Z\right)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&\Delta_{i}^{(t+1)} \sim p\left(\Delta_{i} \mid \theta^{(t)}, Z\right) \text { for } i=1, \ldots, n \\
&\theta^{(t+1)} \sim p\left(\theta \mid \Delta^{(t+1)}, Z\right)
\end{aligned}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\Delta_{i} \in\{1, \ldots, K\}</span><script type="math/tex">\Delta_{i} \in\{1, \ldots, K\}</script></span> represents which component training example <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> is assigned to</p>
<h3 id="bagging">Bagging<a class="headerlink" href="#bagging" title="Permanent link">&para;</a></h3>
<p>本节中提到了随机森林和 boosting 方式．ISLR 在讲基于树的方法时是将这两种方式与 bagging 一起介绍的. 简单来说，bagging 和随机森林都是针对 bootstrap 样本，且前者可以看成后者的特殊形式；而 boosting 是针对残差样本．</p>
<p>Starting point</p>
<ul>
<li>Training set <span class="arithmatex"><span class="MathJax_Preview">Z=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</span><script type="math/tex">Z=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</script></span></li>
<li><span class="arithmatex"><span class="MathJax_Preview">\hat{f}(x)</span><script type="math/tex">\hat{f}(x)</script></span> be the prediction at input <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> learned from <span class="arithmatex"><span class="MathJax_Preview">Z</span><script type="math/tex">Z</script></span></li>
</ul>
<p>Goal</p>
<ul>
<li>Obtain a prediction at input <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> with lower variance than <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(x)</span><script type="math/tex">\hat{f}(x)</script></span></li>
</ul>
<p>How - Bootstrap aggregation a.k.a. <strong>Bagging</strong></p>
<ul>
<li>Obtain bootstrap samples <span class="arithmatex"><span class="MathJax_Preview">Z^{*1}, \ldots, Z^{* B}</span><script type="math/tex">Z^{*1}, \ldots, Z^{* B}</script></span></li>
<li>For each <span class="arithmatex"><span class="MathJax_Preview">Z^{*b}</span><script type="math/tex">Z^{*b}</script></span>, fit the model and get prediction <span class="arithmatex"><span class="MathJax_Preview">\hat{f}^{* b}(x)</span><script type="math/tex">\hat{f}^{* b}(x)</script></span></li>
<li>The bagged estimate is then</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}_{\text {bag }}(x)=\frac{1}{B} \sum_{b=1}^{B} \hat{f}^{* b}(x)
</div>
<script type="math/tex; mode=display">
\hat{f}_{\text {bag }}(x)=\frac{1}{B} \sum_{b=1}^{B} \hat{f}^{* b}(x)
</script>
</div>
<p><img alt="" src="../media/ASL-note3/2021-12-24-11-44-55.png" /></p>
<p>上面的橙色是对于分类结果进行 bagging, 绿色是对于分类概率进行 bagging.</p>
<h4 id="bagging-for-classification-and-0-1-loss">Bagging for Classification and 0-1 Loss<a class="headerlink" href="#bagging-for-classification-and-0-1-loss" title="Permanent link">&para;</a></h4>
<p>Squared error loss</p>
<ul>
<li>Bagging can dramatically reduce the variance of unstable procedures, leading to improved prediction</li>
</ul>
<p>Classification with 0-1 loss [因为偏差和方差的不可加性]</p>
<ul>
<li>Bagging a good classifier can make it better</li>
<li>Bagging a bad classifier can make things worse</li>
<li>Can understand the bagging effect in terms of a consensus of independent weak learners or the wisdom of crowds</li>
</ul>
<h3 id="model-averaging-stacking">Model Averaging &amp; Stacking<a class="headerlink" href="#model-averaging-stacking" title="Permanent link">&para;</a></h3>
<h4 id="bayesian-model-averaging">Bayesian Model Averaging<a class="headerlink" href="#bayesian-model-averaging" title="Permanent link">&para;</a></h4>
<p>Starting point</p>
<ul>
<li>Training data <span class="arithmatex"><span class="MathJax_Preview">X=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</span><script type="math/tex">X=\left\{\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)\right\}</script></span></li>
<li>A set of candidate models <span class="arithmatex"><span class="MathJax_Preview">M_{1}, \ldots, M_{M}</span><script type="math/tex">M_{1}, \ldots, M_{M}</script></span></li>
</ul>
<p>Goal</p>
<ul>
<li>Estimate quantity <span class="arithmatex"><span class="MathJax_Preview">\zeta</span><script type="math/tex">\zeta</script></span> - usually a prediction of <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> at <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span></li>
</ul>
<p>A Bayesian solution</p>
<ul>
<li>The posterior distribution of <span class="arithmatex"><span class="MathJax_Preview">\zeta</span><script type="math/tex">\zeta</script></span></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
p(\zeta \mid Z)=\sum_{m=1}^{M} p\left(\zeta \mid M_{m}, Z\right) P\left(M_{m} \mid Z\right)
</div>
<script type="math/tex; mode=display">
p(\zeta \mid Z)=\sum_{m=1}^{M} p\left(\zeta \mid M_{m}, Z\right) P\left(M_{m} \mid Z\right)
</script>
</div>
<ul>
<li>Posterior mean</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
E[\zeta \mid Z]=\sum_{m=1}^{M} E\left[\zeta \mid M_{m}, Z\right] P\left(M_{m} \mid Z\right)
</div>
<script type="math/tex; mode=display">
E[\zeta \mid Z]=\sum_{m=1}^{M} E\left[\zeta \mid M_{m}, Z\right] P\left(M_{m} \mid Z\right)
</script>
</div>
<p>关键是估计单个模型的概率</p>
<p>Committee method 委员会方法对每个模型赋予相同的权重</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(M_{m} \mid Z\right) \approx \frac{1}{M}
</div>
<script type="math/tex; mode=display">
P\left(M_{m} \mid Z\right) \approx \frac{1}{M}
</script>
</div>
<p>BIC approach (7.7节)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P\left(M_{m} \mid Z\right) \approx-2 \log \operatorname{lik}+d_{m} \log (n)
</div>
<script type="math/tex; mode=display">
P\left(M_{m} \mid Z\right) \approx-2 \log \operatorname{lik}+d_{m} \log (n)
</script>
</div>
<p>Hardcore Bayesian 全贝叶斯方法. [然而，相比更简单的 BIC 近似，我们没有看到任何实际的证据来表明值得这样做]</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
P\left(M_{m} \mid Z\right) &amp; \propto P\left(M_{m}\right) p\left(Z \mid M_{m}\right) \\
&amp; \propto P\left(M_{m}\right) \int p\left(Z \mid \theta, M_{m}\right) p\left(\theta \mid M_{m}\right) d \theta_{m}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
P\left(M_{m} \mid Z\right) & \propto P\left(M_{m}\right) p\left(Z \mid M_{m}\right) \\
& \propto P\left(M_{m}\right) \int p\left(Z \mid \theta, M_{m}\right) p\left(\theta \mid M_{m}\right) d \theta_{m}
\end{aligned}
</script>
</div>
<h4 id="frequentist-model-averaging">Frequentist Model Averaging<a class="headerlink" href="#frequentist-model-averaging" title="Permanent link">&para;</a></h4>
<p>Starting point</p>
<ul>
<li>Predictions <span class="arithmatex"><span class="MathJax_Preview">\hat{f}_{1}(x), \hat{f}_{2}(x), \ldots, \hat{f}_{M}(x)</span><script type="math/tex">\hat{f}_{1}(x), \hat{f}_{2}(x), \ldots, \hat{f}_{M}(x)</script></span></li>
</ul>
<p>Goal</p>
<ul>
<li>For squared error loss, find weights <span class="arithmatex"><span class="MathJax_Preview">\omega=\left(\omega_{1}, \ldots, \omega_{M}\right)</span><script type="math/tex">\omega=\left(\omega_{1}, \ldots, \omega_{M}\right)</script></span> s.t.</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\omega}=\operatorname{argmin}_{\omega} E_{P_{Y \mid X=X}}\left(Y-\sum_{m=1}^{M} \omega_{m} \hat{f}_{m}(x)\right)^{2}
</div>
<script type="math/tex; mode=display">
\hat{\omega}=\operatorname{argmin}_{\omega} E_{P_{Y \mid X=X}}\left(Y-\sum_{m=1}^{M} \omega_{m} \hat{f}_{m}(x)\right)^{2}
</script>
</div>
<p>Solution: <strong>population linear regression</strong> of <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> on <span class="arithmatex"><span class="MathJax_Preview">\hat{F}(x)</span><script type="math/tex">\hat{F}(x)</script></span> 总体线性回归 (population linear regression)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\omega}=E_{P}\left[\hat{F}(x) \hat{F}(x)^{\top}\right]^{-1} E_{p}[\hat{F}(x) Y]
</div>
<script type="math/tex; mode=display">
\hat{\omega}=E_{P}\left[\hat{F}(x) \hat{F}(x)^{\top}\right]^{-1} E_{p}[\hat{F}(x) Y]
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\hat{F}(x) \equiv\left[\hat{f}_{1}(x), \ldots, \hat{f}_{M}(x)\right]^{\top}</span><script type="math/tex">\hat{F}(x) \equiv\left[\hat{f}_{1}(x), \ldots, \hat{f}_{M}(x)\right]^{\top}</script></span></p>
<p>For this <span class="arithmatex"><span class="MathJax_Preview">\hat{\omega}</span><script type="math/tex">\hat{\omega}</script></span>, the full regression model has smaller error than any single model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
E_{P}\left(Y-\sum_{m=1}^{M} \omega_{m} \hat{f}_{m}(x)\right)^{2} \leqslant E_{P}\left(Y-\hat{f}_{m}(x)\right)^{2} \forall m
</div>
<script type="math/tex; mode=display">
E_{P}\left(Y-\sum_{m=1}^{M} \omega_{m} \hat{f}_{m}(x)\right)^{2} \leqslant E_{P}\left(Y-\hat{f}_{m}(x)\right)^{2} \forall m
</script>
</div>
<p>Combining models never makes things worse (at population level)</p>
<p>But cannot estimate the population <span class="arithmatex"><span class="MathJax_Preview">\hat{\omega}</span><script type="math/tex">\hat{\omega}</script></span>. Any thoughts?</p>
<p>显然无法直接计算 population 级别的线性回归, 而直接用训练集会有问题, 例如线性回归的几个模型中, 所选变量最多的模型表现最好, 则 bagging 学习到的权重, 复杂模型的权重会过分高(=1). 因此, 下面的 堆栈泛化 (Stacked generalization) 或 stacking 方法, 采用类似留一的思路, 留出一个 sample 进行训练.</p>
<h4 id="stacked-generalization-stacking">Stacked Generalization (stacking)<a class="headerlink" href="#stacked-generalization-stacking" title="Permanent link">&para;</a></h4>
<p>Denote <span class="arithmatex"><span class="MathJax_Preview">\hat{f}_{m}^{-i}(x)</span><script type="math/tex">\hat{f}_{m}^{-i}(x)</script></span> as the prediction at <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> using</p>
<ul>
<li>the <span class="arithmatex"><span class="MathJax_Preview">m_{\text {th }}</span><script type="math/tex">m_{\text {th }}</script></span> model</li>
<li>learnt from the dataset with <span class="arithmatex"><span class="MathJax_Preview">i_{t h}</span><script type="math/tex">i_{t h}</script></span> training example removed</li>
</ul>
<p>Then the stacking weights are given by</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\omega}^{s t}=\operatorname{argmin}_{\omega} \sum_{i=1}^{n}\left(y_{i}-\sum_{m=1}^{M} \omega_{m} \hat{f}_{m}^{-i}\left(x_{i}\right)\right)^{2}
</div>
<script type="math/tex; mode=display">
\hat{\omega}^{s t}=\operatorname{argmin}_{\omega} \sum_{i=1}^{n}\left(y_{i}-\sum_{m=1}^{M} \omega_{m} \hat{f}_{m}^{-i}\left(x_{i}\right)\right)^{2}
</script>
</div>
<p>The final prediction at point <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{m} \hat{\omega}_{m}^{s t} \hat{f}_{m}(x)
</div>
<script type="math/tex; mode=display">
\sum_{m} \hat{\omega}_{m}^{s t} \hat{f}_{m}(x)
</script>
</div>
<ul>
<li>Better results by forcing <span class="arithmatex"><span class="MathJax_Preview">\hat{\omega}_{m}^{s t}</span><script type="math/tex">\hat{\omega}_{m}^{s t}</script></span> to be <span class="arithmatex"><span class="MathJax_Preview">\geqslant 0</span><script type="math/tex">\geqslant 0</script></span> and sum to 1</li>
<li>Stacking and model selection via leave-one-out cross-validation are closely related</li>
<li>Can apply stacking to other non-linear methods to combine predictions from different models</li>
</ul>
<h3 id="stochastic-search-bumping">Stochastic Search: Bumping<a class="headerlink" href="#stochastic-search-bumping" title="Permanent link">&para;</a></h3>
<p>这章描述的最后一个方法不涉及平均或者结合模型，但是是寻找一个更好单模型的方法．Bumping 采用 bootstrap 采样在模型空间中随机移动．对于拟合方法经常会找到许多局部最小值的问题，bumping 可以帮助这些方法避免陷入坏的解．</p>
<p>General Approach</p>
<ul>
<li>Draw bootstrap samples <span class="arithmatex"><span class="MathJax_Preview">Z^{*1}, \ldots, Z^{* B}</span><script type="math/tex">Z^{*1}, \ldots, Z^{* B}</script></span></li>
<li>For <span class="arithmatex"><span class="MathJax_Preview">b=1, \ldots,, B</span><script type="math/tex">b=1, \ldots,, B</script></span>
Fit the model to <span class="arithmatex"><span class="MathJax_Preview">Z^{*b}</span><script type="math/tex">Z^{*b}</script></span> giving <span class="arithmatex"><span class="MathJax_Preview">\hat{f}^{* b}(x)</span><script type="math/tex">\hat{f}^{* b}(x)</script></span></li>
<li>Choose the model obtained from bootstrap sample <span class="arithmatex"><span class="MathJax_Preview">\hat{b}</span><script type="math/tex">\hat{b}</script></span> which minimizes the training error</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{b}=\operatorname{argmin}_{b} \frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{f}^{* b}\left(x_{i}\right)\right)^{2}
</div>
<script type="math/tex; mode=display">
\hat{b}=\operatorname{argmin}_{b} \frac{1}{n} \sum_{i=1}^{n}\left(y_{i}-\hat{f}^{* b}\left(x_{i}\right)\right)^{2}
</script>
</div>
<p>The model predictions are then <span class="arithmatex"><span class="MathJax_Preview">\hat{f}^{* \hat{b}}(x)</span><script type="math/tex">\hat{f}^{* \hat{b}}(x)</script></span></p>
<p><img alt="" src="../media/ASL-note3/2021-12-24-13-37-59.png" /></p>
<p>例如, 对于异或(exclusive or)(XOR) 问题而言, 由于数据平衡的特性, 贪心的 CART 算法在两个特征上都无法找到合适的分割 (分割的结果是可能随机的), 因此如左图无法得到一个好的结果. 而在 bumping 算法中, 通过每次随机采样一系列的点, <strong>打破了数据的平衡</strong>, (然后根据测试结果找最好的模型), 从右图可以看到, 一个 bumping 20次的模型即可得到一个很好的结果.</p>
<ul>
<li>Bumping pertubs the training data</li>
<li>Therefore explore different areas of the model space</li>
<li>Must ensure the complexity of each model ﬁt is comparable</li>
</ul>
<h2 id="additive-models-trees-related-methods">Additive Models, Trees, &amp; Related Methods<a class="headerlink" href="#additive-models-trees-related-methods" title="Permanent link">&para;</a></h2>
<p>这章中我们开始对监督学习中一些特定的方法进行讨论．这里每个技巧都假设了 未知回归函数（不同的）结构形式，而且通过这样处理巧妙地解决了维数灾难． 当然，它们要为错误地确定模型类型付出可能的代价，所以在每种情形下都需要 做出一个权衡．第 3-6 章留下的问题都将继续讨论．我们描述 5 个相关的技巧： 广义可加模型 (generalized addiƟve models)，树 (trees)，多元自适应回归样条 (MARS)，耐心规则归纳法 (PRIM)，以及 混合层次专家 (HME)．</p>
<ul>
<li>Generalized Additive Models</li>
<li>Tree Based Methods</li>
<li>Patient Rule Induction Method (PRIM)</li>
<li>Multivariate Adaptive Regression Splines - MARS</li>
<li>Hierarchical Mixture of Experts</li>
</ul>
<h3 id="generalized-additive-models">Generalized Additive Models<a class="headerlink" href="#generalized-additive-models" title="Permanent link">&para;</a></h3>
<p>回归模型在许多数据分析中起着重要的作用，提供了预测和分类的规则、以及理 解不同输入变量重要性的数据分析工具．</p>
<p>尽管非常简单，但是传统的线性模型经常在这些情形下失效：实际生活中，变量的影响往往不是线性的．在前面的章节中我们讨论使用预定义的基函数来实现非 线性的技巧．这部分描述更多 自动灵活 (automatic flexible) 的统计方法来识别和 表征非线性回归的影响．这些方法被称为“广义可加模型”．</p>
<p>A <strong>generalized additive model</strong> has the form</p>
<div class="arithmatex">
<div class="MathJax_Preview">
E\left(Y \mid X_{1}, \ldots, X_{p}\right)=\alpha+f_{1}\left(X_{1}\right)+f_{2}\left(X_{2}\right)+\ldots+f_{p}\left(X_{p}\right)
</div>
<script type="math/tex; mode=display">
E\left(Y \mid X_{1}, \ldots, X_{p}\right)=\alpha+f_{1}\left(X_{1}\right)+f_{2}\left(X_{2}\right)+\ldots+f_{p}\left(X_{p}\right)
</script>
</div>
<p>where each <span class="arithmatex"><span class="MathJax_Preview">f_{j}{ }^{\prime} s</span><script type="math/tex">f_{j}{ }^{\prime} s</script></span> are smooth, potentially non-parametric functions</p>
<p>Here we consider each <span class="arithmatex"><span class="MathJax_Preview">f_{j}</span><script type="math/tex">f_{j}</script></span> is fit using a <strong>scatter plot smoother</strong></p>
<ul>
<li>cubic smoothing spline</li>
<li>kernel smoother</li>
</ul>
<p>进一步定义对于二分类任务 Definition for Binary Classification</p>
<p>A generalized additive model has the form</p>
<div class="arithmatex">
<div class="MathJax_Preview">
g(P(Y=1 \mid X))=\alpha+f_{1}\left(X_{1}\right)+f_{2}\left(X_{2}\right)+\ldots+f_{p}\left(X_{p}\right)
</div>
<script type="math/tex; mode=display">
g(P(Y=1 \mid X))=\alpha+f_{1}\left(X_{1}\right)+f_{2}\left(X_{2}\right)+\ldots+f_{p}\left(X_{p}\right)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">g(\cdot)</span><script type="math/tex">g(\cdot)</script></span> is a <strong>link function</strong>.</p>
<p>Common link functions are</p>
<ul>
<li>Identity: <span class="arithmatex"><span class="MathJax_Preview">g(z)=z</span><script type="math/tex">g(z)=z</script></span><ul>
<li>used for linear and additive models for Gaussian response data</li>
<li>高斯响应数据</li>
</ul>
</li>
<li>Logit: <span class="arithmatex"><span class="MathJax_Preview">g(z)=\log \frac{z}{1-z}</span><script type="math/tex">g(z)=\log \frac{z}{1-z}</script></span><ul>
<li>我们通过一个线性回归模型和 logit 链接函数将预测变量与二进制响应变量关联起来</li>
<li>used for model binomial probabilities</li>
</ul>
</li>
<li>Log: <span class="arithmatex"><span class="MathJax_Preview">g(z)=\log (z)</span><script type="math/tex">g(z)=\log (z)</script></span><ul>
<li>used for log-linear or log-additive models for possion count data</li>
<li>泊松计数数据</li>
</ul>
</li>
</ul>
<p>上面的三种情形都是来自指数族采样模型，另外也包括 Gamma 分布和负二项分布．这个分布族产生了著名的广义线性模型类，它们都是以同样的方式扩展为<strong>广义可加模型</strong>．</p>
<p>Advantages of GAM</p>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">f_{j}^{\prime}</span><script type="math/tex">f_{j}^{\prime}</script></span> s are estimated in a flexible way, can reveal <strong>non-linear relationship</strong> between input <span class="arithmatex"><span class="MathJax_Preview">X_{j}</span><script type="math/tex">X_{j}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span></li>
<li>Efficient algorithms to fit them if <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> is not too large</li>
</ul>
<h4 id="fitting-gam">Fitting GAM<a class="headerlink" href="#fitting-gam" title="Permanent link">&para;</a></h4>
<p>Additive model [类似 5.4 节中讨论的惩罚平方和的 (5.9) 准则]</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Y=\alpha+f_{1}\left(X_{1}\right)+f_{2}\left(X_{2}\right)+\ldots+f_{p}\left(X_{p}\right)+\epsilon
</div>
<script type="math/tex; mode=display">
Y=\alpha+f_{1}\left(X_{1}\right)+f_{2}\left(X_{2}\right)+\ldots+f_{p}\left(X_{p}\right)+\epsilon
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">E(\epsilon)=0</span><script type="math/tex">E(\epsilon)=0</script></span></p>
<p>How to estimate the parameters?</p>
<ul>
<li>Training data <span class="arithmatex"><span class="MathJax_Preview">\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{n}</span><script type="math/tex">\left\{\left(x_{i}, y_{i}\right)\right\}_{i=1}^{n}</script></span></li>
<li>Minimize a penalized sum-of-squares</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
\operatorname{PRSS}\left(\alpha, f_{1}, f_{2}, \ldots, f_{n}\right)=
\sum_{i=1}^{n}\left(y_{i}-\alpha-\sum_{j=1}^{p} f_{j}\left(x_{i j}\right)\right)^{2}+\sum_{j=1}^{p} \lambda_{j} \int_{t} f_{j}^{\prime \prime}(t)^{2} d t
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
\operatorname{PRSS}\left(\alpha, f_{1}, f_{2}, \ldots, f_{n}\right)=
\sum_{i=1}^{n}\left(y_{i}-\alpha-\sum_{j=1}^{p} f_{j}\left(x_{i j}\right)\right)^{2}+\sum_{j=1}^{p} \lambda_{j} \int_{t} f_{j}^{\prime \prime}(t)^{2} d t
\end{gathered}
</script>
</div>
<p>where each <span class="arithmatex"><span class="MathJax_Preview">\lambda_{j} \geqslant 0</span><script type="math/tex">\lambda_{j} \geqslant 0</script></span></p>
<p>One solution</p>
<ul>
<li>Let each <span class="arithmatex"><span class="MathJax_Preview">f_{j}\left(X_{j}\right)</span><script type="math/tex">f_{j}\left(X_{j}\right)</script></span> be a <strong>cubic smoothing spline</strong> with knots at <span class="arithmatex"><span class="MathJax_Preview">x_{i j}</span><script type="math/tex">x_{i j}</script></span> and response <span class="arithmatex"><span class="MathJax_Preview">y_{i}</span><script type="math/tex">y_{i}</script></span> for <span class="arithmatex"><span class="MathJax_Preview">i=1, \ldots, n</span><script type="math/tex">i=1, \ldots, n</script></span> [可以证明上式的最小值是「可加三次样条模型」, 即分别对于每个组分 <span class="arithmatex"><span class="MathJax_Preview">X_j</span><script type="math/tex">X_j</script></span> 进行拟合 <span class="arithmatex"><span class="MathJax_Preview">f_j</span><script type="math/tex">f_j</script></span> 三次样条]</li>
<li>This solution minimizes <span class="arithmatex"><span class="MathJax_Preview">P R S S\left(\alpha, f_{1}, f_{2}, \ldots, f_{n}\right)</span><script type="math/tex">P R S S\left(\alpha, f_{1}, f_{2}, \ldots, f_{n}\right)</script></span></li>
<li>However, it is not the only minimizer as <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> is not identifiable</li>
</ul>
<p>但在没有约束的情况下, 解不唯一, 因此规定每个组分 <span class="arithmatex"><span class="MathJax_Preview">f_j</span><script type="math/tex">f_j</script></span> 拟合的均值为 0. 此时, 当输入矩阵列满秩, 上式有唯一解.</p>
<p>To combat this, assume</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{i=1}^{n} f_{j}\left(x_{i j}\right)=0 \text { for } j=1, \ldots, p
</div>
<script type="math/tex; mode=display">
\sum_{i=1}^{n} f_{j}\left(x_{i j}\right)=0 \text { for } j=1, \ldots, p
</script>
</div>
<p>This assumption yields <span class="arithmatex"><span class="MathJax_Preview">\hat{\alpha}=\operatorname{ave}\left(y_{i}\right)</span><script type="math/tex">\hat{\alpha}=\operatorname{ave}\left(y_{i}\right)</script></span></p>
<p>If the data matrix</p>
<div class="arithmatex">
<div class="MathJax_Preview">
X=\left[\begin{array}{cccc}
x_{11} &amp; x_{12} &amp; \ldots &amp; x_{1 n} \\
x_{21} &amp; x_{22} &amp; \ldots &amp; x_{2 n} \\
\ldots &amp; \ldots &amp; \ldots &amp; \ldots \\
x_{p 1} &amp; x_{p 2} &amp; \ldots &amp; x_{p n}
\end{array}\right]
</div>
<script type="math/tex; mode=display">
X=\left[\begin{array}{cccc}
x_{11} & x_{12} & \ldots & x_{1 n} \\
x_{21} & x_{22} & \ldots & x_{2 n} \\
\ldots & \ldots & \ldots & \ldots \\
x_{p 1} & x_{p 2} & \ldots & x_{p n}
\end{array}\right]
</script>
</div>
<p>has <strong>full column rank</strong>, <span class="arithmatex"><span class="MathJax_Preview">\operatorname{PRSS}\left(\alpha, f_{1}, f_{2}, \ldots, f_{n}\right)</span><script type="math/tex">\operatorname{PRSS}\left(\alpha, f_{1}, f_{2}, \ldots, f_{n}\right)</script></span> is convex and the minimizer is unique.</p>
<p><span class="arithmatex"><span class="MathJax_Preview">\exists</span><script type="math/tex">\exists</script></span> a simple iterative procedure for finding this solution</p>
<h5 id="backfitting-algorithm">Backfitting Algorithm<a class="headerlink" href="#backfitting-algorithm" title="Permanent link">&para;</a></h5>
<p>根据上面的定理可以得到如下拟合算法</p>
<p>Step 1: Initialize</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\alpha}=\frac{1}{n} \sum_{i} y_{i}, \quad f_{j} \equiv 0 \forall j
</div>
<script type="math/tex; mode=display">
\hat{\alpha}=\frac{1}{n} \sum_{i} y_{i}, \quad f_{j} \equiv 0 \forall j
</script>
</div>
<p>Step 2: Cycle until convergence <span class="arithmatex"><span class="MathJax_Preview">j=1,2, \ldots, p, 1,2, \ldots, p, 1,2, \ldots</span><script type="math/tex">j=1,2, \ldots, p, 1,2, \ldots, p, 1,2, \ldots</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
\hat{f}_{j} \leftarrow S_{j}\left[\left\{y_{i}-\hat{\alpha}-\sum_{k \neq j} \hat{f}_{k}\left(x_{i k}\right)\right\}_{i=1}^{n}\right] \\
\hat{f}_{j} \leftarrow \hat{f}_{j}-\frac{1}{n} \sum_{i=1}^{n} \hat{f}_{j}\left(x_{i j}\right)
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
\hat{f}_{j} \leftarrow S_{j}\left[\left\{y_{i}-\hat{\alpha}-\sum_{k \neq j} \hat{f}_{k}\left(x_{i k}\right)\right\}_{i=1}^{n}\right] \\
\hat{f}_{j} \leftarrow \hat{f}_{j}-\frac{1}{n} \sum_{i=1}^{n} \hat{f}_{j}\left(x_{i j}\right)
\end{gathered}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">S_{j}\left[\left\{y_{i}-\hat{\alpha}-\sum_{k \neq j} \hat{f}_{k}\left(x_{i k}\right)\right\}_{i=1}^{n}\right]</span><script type="math/tex">S_{j}\left[\left\{y_{i}-\hat{\alpha}-\sum_{k \neq j} \hat{f}_{k}\left(x_{i k}\right)\right\}_{i=1}^{n}\right]</script></span> denotes the <strong>cubic smoothing spine</strong> with knots at <span class="arithmatex"><span class="MathJax_Preview">x_{i j}</span><script type="math/tex">x_{i j}</script></span> and responses <span class="arithmatex"><span class="MathJax_Preview">y_{i}-\hat{\alpha}-\sum_{k \neq j} \hat{f}_{k}\left(x_{i k}\right)</span><script type="math/tex">y_{i}-\hat{\alpha}-\sum_{k \neq j} \hat{f}_{k}\left(x_{i k}\right)</script></span> for <span class="arithmatex"><span class="MathJax_Preview">i=1, \ldots, n</span><script type="math/tex">i=1, \ldots, n</script></span></p>
<p>Could use other smoothing operators for <span class="arithmatex"><span class="MathJax_Preview">S_{j}</span><script type="math/tex">S_{j}</script></span></p>
<p>[实际上, 第 2(2) 步不是必需的，因为光滑样条对 0 均值响应变量拟合均值为 0. 但机器舍入误差会导致下降，所以建议进行调整.]</p>
<h4 id="example-additive-logistic-regression">Example: Additive Logistic Regression<a class="headerlink" href="#example-additive-logistic-regression" title="Permanent link">&para;</a></h4>
<p>Generalized Additive Logistic Model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\log \frac{P(Y=1 \mid X)}{P(Y=0 \mid X)}=\alpha+f_{1}\left(X_{1}\right)+\ldots+f_{p}\left(X_{p}\right)
</div>
<script type="math/tex; mode=display">
\log \frac{P(Y=1 \mid X)}{P(Y=0 \mid X)}=\alpha+f_{1}\left(X_{1}\right)+\ldots+f_{p}\left(X_{p}\right)
</script>
</div>
<p>Functions <span class="arithmatex"><span class="MathJax_Preview">f_{1}, \ldots, f_{p}</span><script type="math/tex">f_{1}, \ldots, f_{p}</script></span> estimated by a backfitting algorithm within a Newton-Rapson procedure</p>
<p>Goal: maximize the log-likelihood</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L=\sum_{i} L_{i}=\sum_{i}\left[y_{i} \log P\left(Y=1 \mid x_{i}\right)+\left(1-y_{i}\right) \log \left(Y=0 \mid x_{i}\right)\right]
</div>
<script type="math/tex; mode=display">
L=\sum_{i} L_{i}=\sum_{i}\left[y_{i} \log P\left(Y=1 \mid x_{i}\right)+\left(1-y_{i}\right) \log \left(Y=0 \mid x_{i}\right)\right]
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">P\left(Y=1 \mid x_{i}\right)=\frac{e^{\eta_{i}}}{1+e^{\eta_{i}}}</span><script type="math/tex">P\left(Y=1 \mid x_{i}\right)=\frac{e^{\eta_{i}}}{1+e^{\eta_{i}}}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\eta_{i}=\alpha+f_{1}\left(x_{i 1}\right)+\ldots+f_{p}\left(x_{i p}\right)</span><script type="math/tex">\eta_{i}=\alpha+f_{1}\left(x_{i 1}\right)+\ldots+f_{p}\left(x_{i p}\right)</script></span></p>
<p>How: iteratively perform unitl convergence</p>
<ul>
<li>Let each <span class="arithmatex"><span class="MathJax_Preview">\hat{\eta}_{i}=\hat{\alpha}+\sum \hat{f}_{j}\left(x_{i j}\right)</span><script type="math/tex">\hat{\eta}_{i}=\hat{\alpha}+\sum \hat{f}_{j}\left(x_{i j}\right)</script></span> be the estimate of <span class="arithmatex"><span class="MathJax_Preview">\eta_{i}</span><script type="math/tex">\eta_{i}</script></span> given the current estimates of the parameters <span class="arithmatex"><span class="MathJax_Preview">\alpha, f_{1}, \ldots, f_{p}</span><script type="math/tex">\alpha, f_{1}, \ldots, f_{p}</script></span></li>
<li>Use a <strong>Newton-Raphson update</strong> step to produce a new estimate, <span class="arithmatex"><span class="MathJax_Preview">\hat{\eta}_{i}^{n e w}</span><script type="math/tex">\hat{\eta}_{i}^{n e w}</script></span>, of <span class="arithmatex"><span class="MathJax_Preview">\eta_{i}</span><script type="math/tex">\eta_{i}</script></span> s.t. <span class="arithmatex"><span class="MathJax_Preview">L_{i}\left(\hat{\eta}_{i}^{n e w}\right) \geqslant L_{i}\left(\hat{\eta}_{i}\right)</span><script type="math/tex">L_{i}\left(\hat{\eta}_{i}^{n e w}\right) \geqslant L_{i}\left(\hat{\eta}_{i}\right)</script></span> for each <span class="arithmatex"><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span> [因为这里是二值响应变量没有具体的数值响应, 所以用 Newton 方法来生成一个近似的值?]</li>
<li>Use Backfitting to fit an additive model to the targets <span class="arithmatex"><span class="MathJax_Preview">\hat{\eta}_{i}^{n e w} \forall i</span><script type="math/tex">\hat{\eta}_{i}^{n e w} \forall i</script></span></li>
<li>This produces new estimates of <span class="arithmatex"><span class="MathJax_Preview">\hat{\alpha}, \hat{f}_{j} \forall j</span><script type="math/tex">\hat{\alpha}, \hat{f}_{j} \forall j</script></span></li>
</ul>
<h4 id="gam-summary">GAM - Summary<a class="headerlink" href="#gam-summary" title="Permanent link">&para;</a></h4>
<p>Pros</p>
<ul>
<li>Extension of linear models - more flexible but still interpretable</li>
<li>Parameter estimate via Backfitting method is simple</li>
<li><strong>Backfitting</strong> allows the appropriate fitting method for each input variable 允许对于每个输入变量去选择合适的拟合方法</li>
</ul>
<p>Cons</p>
<ul>
<li>No feature selection is performed</li>
<li>Backfitting is not feasible for large <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span></li>
</ul>
<p>For large <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> <strong>forward stagewise fitting</strong> (such as boosting 第10章) can be a solution&hellip;</p>
<h3 id="tree-based-methods">Tree Based Methods<a class="headerlink" href="#tree-based-methods" title="Permanent link">&para;</a></h3>
<p><img alt="" src="../media/ASL-note3/2021-12-26-10-12-06.png" /></p>
<h4 id="regression-tree">Regression Tree<a class="headerlink" href="#regression-tree" title="Permanent link">&para;</a></h4>
<p>Aim: approximate a regression function <span class="arithmatex"><span class="MathJax_Preview">f: \mathbb{R}^{p} \rightarrow \mathbb{R}</span><script type="math/tex">f: \mathbb{R}^{p} \rightarrow \mathbb{R}</script></span> with</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}(x)=\sum_{i=1}^{M} f_{m}(x) l\left(x \in R_{m}\right)
</div>
<script type="math/tex; mode=display">
\hat{f}(x)=\sum_{i=1}^{M} f_{m}(x) l\left(x \in R_{m}\right)
</script>
</div>
<p>with regions <span class="arithmatex"><span class="MathJax_Preview">R_{1}, \ldots, R_{M}</span><script type="math/tex">R_{1}, \ldots, R_{M}</script></span> partition <span class="arithmatex"><span class="MathJax_Preview">\mathbb{R}^{p}</span><script type="math/tex">\mathbb{R}^{p}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">f_{m}: \mathbb{R}^{p} \rightarrow \mathbb{R}</span><script type="math/tex">f_{m}: \mathbb{R}^{p} \rightarrow \mathbb{R}</script></span></p>
<p>Challenge: (assuming a specific form for <span class="arithmatex"><span class="MathJax_Preview">f_{m}{ }^{\prime} \mathrm{s}</span><script type="math/tex">f_{m}{ }^{\prime} \mathrm{s}</script></span> )</p>
<ul>
<li>Find <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> and the regions <span class="arithmatex"><span class="MathJax_Preview">R_{1}, \ldots, R_{M}</span><script type="math/tex">R_{1}, \ldots, R_{M}</script></span> s.t. <span class="arithmatex"><span class="MathJax_Preview">\hat{f} \approx f</span><script type="math/tex">\hat{f} \approx f</script></span></li>
<li>For training data <span class="arithmatex"><span class="MathJax_Preview">\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)</span><script type="math/tex">\left(x_{1}, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)</script></span> with each <span class="arithmatex"><span class="MathJax_Preview">x_{i} \in \mathbb{R}^{p}, y_{i} \in \mathbb{R}</span><script type="math/tex">x_{i} \in \mathbb{R}^{p}, y_{i} \in \mathbb{R}</script></span></li>
</ul>
<h5 id="piecewise-constant">Piecewise Constant<a class="headerlink" href="#piecewise-constant" title="Permanent link">&para;</a></h5>
<p>Let <span class="arithmatex"><span class="MathJax_Preview">f_{m}(x)=c_{m}</span><script type="math/tex">f_{m}(x)=c_{m}</script></span> such that the regression function becomes</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f(x)=\sum_{i=1}^{M} c_{m} l\left(x \in R_{m}\right)
</div>
<script type="math/tex; mode=display">
f(x)=\sum_{i=1}^{M} c_{m} l\left(x \in R_{m}\right)
</script>
</div>
<p>If we know the regions <span class="arithmatex"><span class="MathJax_Preview">R_{1}, \ldots, R_{M}</span><script type="math/tex">R_{1}, \ldots, R_{M}</script></span> then to minimize</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{argmin}_{c_{1}, \ldots, c_{n}} \sum_{i=1}^{n}\left(y_{i}-\sum_{m=1}^{M} c_{m} l\left(x_{i} \in \mathbb{R}_{m}\right)\right)^{2}
</div>
<script type="math/tex; mode=display">
\operatorname{argmin}_{c_{1}, \ldots, c_{n}} \sum_{i=1}^{n}\left(y_{i}-\sum_{m=1}^{M} c_{m} l\left(x_{i} \in \mathbb{R}_{m}\right)\right)^{2}
</script>
</div>
<p>one would set</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{c}_{m}=\frac{\sum_{i} \sum_{m} y_{i} l\left(x_{i} \in R_{m}\right)}{\sum_{i} \sum_{m} I\left(x_{i} \in R_{m}\right)}
</div>
<script type="math/tex; mode=display">
\hat{c}_{m}=\frac{\sum_{i} \sum_{m} y_{i} l\left(x_{i} \in R_{m}\right)}{\sum_{i} \sum_{m} I\left(x_{i} \in R_{m}\right)}
</script>
</div>
<p>也即, 预测为每个 region 上的响应变量均值. 但找出使得平方和最小的最优二分在计算上一般是不可行的．因此我们采用一种贪婪算法.</p>
<h5 id="finding-the-optimal-partition">Finding the Optimal Partition<a class="headerlink" href="#finding-the-optimal-partition" title="Permanent link">&para;</a></h5>
<p>First step of the greedy approach</p>
<ul>
<li>Let: <span class="arithmatex"><span class="MathJax_Preview">R_{1}(j, s)=\left\{X \mid X_{j} \leqslant s\right\}</span><script type="math/tex">R_{1}(j, s)=\left\{X \mid X_{j} \leqslant s\right\}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">R_{2}(j, s)=\left\{X \mid X_{j}&gt;s\right\}</span><script type="math/tex">R_{2}(j, s)=\left\{X \mid X_{j}>s\right\}</script></span></li>
<li>Choose <span class="arithmatex"><span class="MathJax_Preview">(j, s)</span><script type="math/tex">(j, s)</script></span> to minimize</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min_{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}
</div>
<script type="math/tex; mode=display">
\min _{c_{1}} \sum_{x_{i} \in R_{1}(j, s)}\left(y_{i}-c_{1}\right)^{2}+\min_{c_{2}} \sum_{x_{i} \in R_{2}(j, s)}\left(y_{i}-c_{2}\right)^{2}
</script>
</div>
<ul>
<li>For a fixed <span class="arithmatex"><span class="MathJax_Preview">(j, s)</span><script type="math/tex">(j, s)</script></span> the minimum occurs when</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{c}_{k}=A v e\left(y_{i} \mid\left(x_{i}, y_{i}\right) \in X\right) \text { and } x_{i} \in R_{k}(j, s) \text { for } k=1,2
</div>
<script type="math/tex; mode=display">
\hat{c}_{k}=A v e\left(y_{i} \mid\left(x_{i}, y_{i}\right) \in X\right) \text { and } x_{i} \in R_{k}(j, s) \text { for } k=1,2
</script>
</div>
<ul>
<li>Determination of best pair <span class="arithmatex"><span class="MathJax_Preview">(j, s)</span><script type="math/tex">(j, s)</script></span> feasible as for each <span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> only have to check <span class="arithmatex"><span class="MathJax_Preview">\leqslant n+1</span><script type="math/tex">\leqslant n+1</script></span> values of <span class="arithmatex"><span class="MathJax_Preview">s</span><script type="math/tex">s</script></span></li>
</ul>
<p>Full Greedy Recursion</p>
<ul>
<li>Once the best split <span class="arithmatex"><span class="MathJax_Preview">(j, s)</span><script type="math/tex">(j, s)</script></span> is found
  1. Partition the data <span class="arithmatex"><span class="MathJax_Preview">X</span><script type="math/tex">X</script></span></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;X_{1}=\left\{\left(x_{i}, y_{i}\right) \mid\left(x_{i}, y_{i}\right) \in X\right\} \text { and } x_{i} \in R_{1}(j, s) \\
&amp;X_{2}=\left\{\left(x_{i}, y_{i}\right) \mid\left(x_{i}, y_{i}\right) \in X\right\} \text { and } x_{i} \in R_{2}(j, s)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&X_{1}=\left\{\left(x_{i}, y_{i}\right) \mid\left(x_{i}, y_{i}\right) \in X\right\} \text { and } x_{i} \in R_{1}(j, s) \\
&X_{2}=\left\{\left(x_{i}, y_{i}\right) \mid\left(x_{i}, y_{i}\right) \in X\right\} \text { and } x_{i} \in R_{2}(j, s)
\end{aligned}
</script>
</div>
<p>based on the two resulting region
  2. Repeat the splitting process on both <span class="arithmatex"><span class="MathJax_Preview">X_{1}</span><script type="math/tex">X_{1}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">X_{2}</span><script type="math/tex">X_{2}</script></span></p>
<ul>
<li>The process above is recursively repeated on all the resulting subset of data points <span class="arithmatex"><span class="MathJax_Preview">X_{i}</span><script type="math/tex">X_{i}</script></span> until <span class="arithmatex"><span class="MathJax_Preview">\left|X_{i}\right|</span><script type="math/tex">\left|X_{i}\right|</script></span> is too small</li>
<li>The best splits found in this recursive are recoded in a binary tree</li>
</ul>
<p>Growing a Tree Recursively</p>
<p>How large should the tree be?</p>
<ul>
<li>Very large trees may over fit the data</li>
<li>Small tree may not capture the structure in the data</li>
</ul>
<p>树太大会过拟合数据，而树太小则可能捕捉不了重要的结构. 另外, 上面的看似无用的分割或许会导致下面得到很好的分割. 更好的策略是生成一个大树 <span class="arithmatex"><span class="MathJax_Preview">T_0</span><script type="math/tex">T_0</script></span>，只有当达到最小结点规模（比如 5）才停止分割过程．接着大树采用 成本复杂度剪枝 (cost-complexity pruning) 来调整</p>
<p>Common solution</p>
<ul>
<li>Grow a large tree <span class="arithmatex"><span class="MathJax_Preview">T_{0}</span><script type="math/tex">T_{0}</script></span></li>
<li>Prune <span class="arithmatex"><span class="MathJax_Preview">T_{0}</span><script type="math/tex">T_{0}</script></span> using cost-complexity pruning</li>
</ul>
<h4 id="cost-complexity-pruning">Cost-complexity pruning<a class="headerlink" href="#cost-complexity-pruning" title="Permanent link">&para;</a></h4>
<ul>
<li>Pruning <span class="arithmatex"><span class="MathJax_Preview">T_{0}</span><script type="math/tex">T_{0}</script></span> corresponds to collapsing any number of its internal nodes</li>
<li>Let <span class="arithmatex"><span class="MathJax_Preview">T_{T}</span><script type="math/tex">T_{T}</script></span> contain the indices of the terminal nodes in tree <span class="arithmatex"><span class="MathJax_Preview">T</span><script type="math/tex">T</script></span></li>
<li>Define</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
C_{\alpha}(T)=\sum_{m \in T_{T}} n_{m} Q_{m}(T)+\alpha|T|
</div>
<script type="math/tex; mode=display">
C_{\alpha}(T)=\sum_{m \in T_{T}} n_{m} Q_{m}(T)+\alpha|T|
</script>
</div>
<p>where</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
n_{m}=\#\left\{x_{i} \in R_{m}\right\} \\
Q_{m}(T)=\frac{1}{n_{m}} \sum_{x_{i} \in R_{m}}\left(y_{i}-\hat{c}_{m}\right)^{2} \text { with } \hat{c}_{m}=\frac{1}{n_{m}} \sum_{x_{i} \in R_{m}} y_{i}
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
n_{m}=\#\left\{x_{i} \in R_{m}\right\} \\
Q_{m}(T)=\frac{1}{n_{m}} \sum_{x_{i} \in R_{m}}\left(y_{i}-\hat{c}_{m}\right)^{2} \text { with } \hat{c}_{m}=\frac{1}{n_{m}} \sum_{x_{i} \in R_{m}} y_{i}
\end{gathered}
</script>
</div>
<ul>
<li>For a given <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>, find the subtree <span class="arithmatex"><span class="MathJax_Preview">T_{\alpha} \subseteq T</span><script type="math/tex">T_{\alpha} \subseteq T</script></span> that minimizes <span class="arithmatex"><span class="MathJax_Preview">C_{\alpha}(T)</span><script type="math/tex">C_{\alpha}(T)</script></span></li>
</ul>
<p>How 怎么自适应地选择 <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>? - <strong>weakest link pruning</strong> 最差连接剪枝</p>
<ul>
<li>Successively collapse the internal node that produces the smallest per-node increase in</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{m} n_{m} Q_{m}(T)
</div>
<script type="math/tex; mode=display">
\sum_{m} n_{m} Q_{m}(T)
</script>
</div>
<p>until left with a one node tree</p>
<ul>
<li>This sequence of collapsed trees contains <span class="arithmatex"><span class="MathJax_Preview">T_{\alpha}</span><script type="math/tex">T_{\alpha}</script></span></li>
</ul>
<h4 id="classification-tree">Classification Tree<a class="headerlink" href="#classification-tree" title="Permanent link">&para;</a></h4>
<p>Definitions needed for node impurity measures</p>
<ul>
<li>In node <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span>, representing a region <span class="arithmatex"><span class="MathJax_Preview">R_{m}</span><script type="math/tex">R_{m}</script></span> with <span class="arithmatex"><span class="MathJax_Preview">n_{m}</span><script type="math/tex">n_{m}</script></span> observations</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{p}_{m k}=\frac{1}{n_{m}} \sum_{x_{i} \in R_{m}} I\left(y_{i}=k\right)
</div>
<script type="math/tex; mode=display">
\hat{p}_{m k}=\frac{1}{n_{m}} \sum_{x_{i} \in R_{m}} I\left(y_{i}=k\right)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\hat{p}_{m k}</span><script type="math/tex">\hat{p}_{m k}</script></span> is the proportion of class <span class="arithmatex"><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> observations in node <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span></p>
<ul>
<li>Classify the observation in node <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> to class
<span class="arithmatex"><span class="MathJax_Preview">k(m)=\operatorname{argmax}_{k} \hat{p}_{m k}</span><script type="math/tex">k(m)=\operatorname{argmax}_{k} \hat{p}_{m k}</script></span>
the majority vote in node <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span></li>
</ul>
<p>如何衡量节点的不纯度? Different measures of node impurity</p>
<ul>
<li>Misclassification error</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{1}{n_{m}} \sum_{i \in R_{m}} I\left(y_{i} \neq k(m)\right)=1-\hat{p}_{m k(m)}
</div>
<script type="math/tex; mode=display">
\frac{1}{n_{m}} \sum_{i \in R_{m}} I\left(y_{i} \neq k(m)\right)=1-\hat{p}_{m k(m)}
</script>
</div>
<ul>
<li>Gini index</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{k=1}^{k} \hat{p}_{m k}\left(1-\hat{p}_{m k}\right)
</div>
<script type="math/tex; mode=display">
\sum_{k=1}^{k} \hat{p}_{m k}\left(1-\hat{p}_{m k}\right)
</script>
</div>
<ul>
<li>Cross-entropy or deviance</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
-\sum_{k=1}^{K} \hat{p}_{m k} \log \hat{p}_{m k}
</div>
<script type="math/tex; mode=display">
-\sum_{k=1}^{K} \hat{p}_{m k} \log \hat{p}_{m k}
</script>
</div>
<p>在二分类任务上比较这几个指标. For binary classification, let <span class="arithmatex"><span class="MathJax_Preview">p=\hat{p}_{m 0}</span><script type="math/tex">p=\hat{p}_{m 0}</script></span></p>
<ul>
<li>Misclassification error: <span class="arithmatex"><span class="MathJax_Preview">1-\max (p, 1-p)</span><script type="math/tex">1-\max (p, 1-p)</script></span></li>
<li>Gini index: <span class="arithmatex"><span class="MathJax_Preview">2 p(1-p)</span><script type="math/tex">2 p(1-p)</script></span></li>
<li>Cross-entropy or deviance: <span class="arithmatex"><span class="MathJax_Preview">-p \log p-(1-p) \log (1-p)</span><script type="math/tex">-p \log p-(1-p) \log (1-p)</script></span></li>
</ul>
<p>比较</p>
<ul>
<li>[注意到图中交叉熵进行了缩小使其经过同一点.]</li>
<li>这三者都类似，但是 交叉熵 (cross-entropy) 和 基尼指数 (Gini index) 是可微的，因此更加适合数值优化.</li>
<li>另外，交叉熵和基尼指数比误分类率对结点概率的改变更加敏感</li>
</ul>
<p><img alt="" src="../media/ASL-note3/2021-12-26-15-57-40.png" /></p>
<ul>
<li>Cost of binary split of node <span class="arithmatex"><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> into nodes <span class="arithmatex"><span class="MathJax_Preview">m_{1}</span><script type="math/tex">m_{1}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">m_{2}</span><script type="math/tex">m_{2}</script></span> is</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\frac{1}{n_{m 1}} Q_{m 1}+\frac{1}{n_{m 2}} Q_{m 2}
</div>
<script type="math/tex; mode=display">
\frac{1}{n_{m 1}} Q_{m 1}+\frac{1}{n_{m 2}} Q_{m 2}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">Q_{m 1}</span><script type="math/tex">Q_{m 1}</script></span> is the impurity measure of node <span class="arithmatex"><span class="MathJax_Preview">m_{1}</span><script type="math/tex">m_{1}</script></span>, similarly for <span class="arithmatex"><span class="MathJax_Preview">Q_{m 2}</span><script type="math/tex">Q_{m 2}</script></span></p>
<ul>
<li>Cross-entropy and Gini are more sensitive to changes in the node probabilities than Misclassification rate</li>
<li>Cross-entropy and Gini measures used to grow the trees (分割节点的过程中多用 交叉熵或Gini)</li>
<li>All measures used to prune tree</li>
</ul>
<h4 id="problems-with-trees">Problems with Trees<a class="headerlink" href="#problems-with-trees" title="Permanent link">&para;</a></h4>
<ul>
<li>Instability<ul>
<li>Tress have high variance due to hierarchical search process</li>
<li>Errors at top nodes propogate to lower ones<ul>
<li>Small change in training data can give very different splits</li>
</ul>
</li>
</ul>
</li>
<li>Lack of smoothness<ul>
<li>Regression trees response surface not smooth</li>
<li>Hit problems when underlying function is smooth</li>
</ul>
</li>
<li>Difficulty in capturing additive structures 对加性结构建模的困难<ul>
<li>The binary tree structure precludes the discovery of additive structure like
  <span class="arithmatex"><span class="MathJax_Preview">Y=c_{1} I\left(X_{1}&lt;t_{1}\right)+c_{2} I\left(X_{2}&lt;t_{2}\right)+\epsilon</span><script type="math/tex">Y=c_{1} I\left(X_{1}<t_{1}\right)+c_{2} I\left(X_{2}<t_{2}\right)+\epsilon</script></span></li>
<li>例如, 在回归问题中, 假设 <span class="arithmatex"><span class="MathJax_Preview">Y=c_{1} I\left(X_{1}&lt;t_{1}\right)+c_{2} I\left(X_{2}&lt;t_{2}\right)+\varepsilon</span><script type="math/tex">Y=c_{1} I\left(X_{1}<t_{1}\right)+c_{2} I\left(X_{2}<t_{2}\right)+\varepsilon</script></span>, 其中 <span class="arithmatex"><span class="MathJax_Preview">\varepsilon</span><script type="math/tex">\varepsilon</script></span> 是 0 均值噪声. 则二叉树会在 <span class="arithmatex"><span class="MathJax_Preview">t_{1}</span><script type="math/tex">t_{1}</script></span> 附近对 <span class="arithmatex"><span class="MathJax_Preview">X_{1}</span><script type="math/tex">X_{1}</script></span> 做第 一次分割. 为了捕捉加性结构, 下一层两个结点都需要在 <span class="arithmatex"><span class="MathJax_Preview">t_{2}</span><script type="math/tex">t_{2}</script></span> 处对 <span class="arithmatex"><span class="MathJax_Preview">X_{2}</span><script type="math/tex">X_{2}</script></span> 分割. 在充足数据情况下这 个或许可以发生, 但是这个模型没有给予特别鼓励来找到这一结构. 如果有 10 个而不是 2 个加性 影响, 需要花费很多偶然的分割来重造这个结构, 并且在估计的树中, 通过数据分析来识别它会变 得非常困难. 原因再一次可以归结为二叉树的结构, 既有优点也有不足. 为了捕捉加性结构, MARS 方法 (9.4 节) 再一次放弃树的结构.</li>
</ul>
</li>
</ul>
<h3 id="patient-rule-induction-method-prim">Patient Rule Induction Method (PRIM)<a class="headerlink" href="#patient-rule-induction-method-prim" title="Permanent link">&para;</a></h3>
<p>基于树的方法（用于回归）将特征空间分成盒状的区域，来试图使每个盒子中的响应变量尽可能不同．通过二叉树定义了与每个盒子相关的分割规则，这有利于它们的解释性．</p>
<p>耐心规则归纳法 (PRIM) 也在特征空间中找盒子，但是它是寻找具有高平均响应的盒子．因此它是在寻找目标函数的最大值，称为bump hunting．（如果需要最小值而非最大值，可以简单地用负的响应变量值来转化．）</p>
<h4 id="prim">PRIM<a class="headerlink" href="#prim" title="Permanent link">&para;</a></h4>
<p>Aim</p>
<ul>
<li>Locate maximum in the response function</li>
</ul>
<p>How: find a rectangular box in the feature space which contains</p>
<ul>
<li>for classification: a clump of points of maximal purity</li>
<li>for regression: a plateau of high scoring points</li>
</ul>
<p>PRIM - a greedy search which is more patient than CART</p>
<h4 id="prim-algorithm">PRIM - Algorithm<a class="headerlink" href="#prim-algorithm" title="Permanent link">&para;</a></h4>
<p>PRIM 的思想是从不同的变量维度去缩减 box 的大小, 尽量使得盒子中响应变量尽可能大 (所以也叫 <strong>Bump Hunting</strong>). 缩减的方式叫做 <strong>Peeling</strong> 见下; 另外为了模型的健壮性, 最后也会扩展盒子, 叫做 <strong>Pasting</strong>. 相关定义如下</p>
<ul>
<li>Box <span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span> is defined by the set of inequalities</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=1, \ldots, p
</div>
<script type="math/tex; mode=display">
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=1, \ldots, p
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">p</span><script type="math/tex">p</script></span> is the dimension of the feature vectors</p>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">B^{\prime}=\operatorname{NewBox}(B, k, 0, a)</span><script type="math/tex">B^{\prime}=\operatorname{NewBox}(B, k, 0, a)</script></span> is defined by the inequalities</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{gathered}
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=1, \ldots, k-1 \\
a \leqslant X_{k} \leqslant b_{k} \\
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=k+1, \ldots, p
\end{gathered}
</div>
<script type="math/tex; mode=display">
\begin{gathered}
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=1, \ldots, k-1 \\
a \leqslant X_{k} \leqslant b_{k} \\
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=k+1, \ldots, p
\end{gathered}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">B^{\prime}=\operatorname{NewBox}(B, k, 1, b)</span><script type="math/tex">B^{\prime}=\operatorname{NewBox}(B, k, 1, b)</script></span> is defined by the inequalities</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=1, \ldots, k-1 &amp; a_{k} \leqslant X_{k} \leqslant b \\
a_{j} \leqslant X_{j} \leqslant &amp; b_{j} \text { for } j=k+1, \ldots, p
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
a_{j} \leqslant X_{j} \leqslant b_{j} \text { for } j=1, \ldots, k-1 & a_{k} \leqslant X_{k} \leqslant b \\
a_{j} \leqslant X_{j} \leqslant & b_{j} \text { for } j=k+1, \ldots, p
\end{aligned}
</script>
</div>
<ul>
<li>Let <span class="arithmatex"><span class="MathJax_Preview">n_{B}=\#</span><script type="math/tex">n_{B}=\#</script></span> of training observations in box <span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span></li>
</ul>
<p>缩减方式的具体定义如下, 即每次从所有可能的维度收缩, 要求盒子中减少的数据量刚好为 <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>, 这里的 <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> 一般可取 0.05 或 0.1 (扩展的方式类似, 每次扩展 <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span> 个数据点, 这里不展示了.)</p>
<p>Peeling - Decrease the size of box B for one face</p>
<ul>
<li>Define <span class="arithmatex"><span class="MathJax_Preview">B^{\prime}=\operatorname{Peel}(B, k, 0, \alpha)</span><script type="math/tex">B^{\prime}=\operatorname{Peel}(B, k, 0, \alpha)</script></span> to be the box</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
B^{\prime}=\operatorname{NewBox}(B, k, 0, a)
</div>
<script type="math/tex; mode=display">
B^{\prime}=\operatorname{NewBox}(B, k, 0, a)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">a</span><script type="math/tex">a</script></span> is the smallest scalar for <span class="arithmatex"><span class="MathJax_Preview">\alpha \in(0,1)</span><script type="math/tex">\alpha \in(0,1)</script></span> s.t.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
a&gt;a_{k} \text { and } n_{B^{\prime}} \leqslant(1-\alpha) n_{B}
</div>
<script type="math/tex; mode=display">
a>a_{k} \text { and } n_{B^{\prime}} \leqslant(1-\alpha) n_{B}
</script>
</div>
<ul>
<li>Define <span class="arithmatex"><span class="MathJax_Preview">B^{\prime}=\operatorname{Peel}(B, k, 1, \alpha)</span><script type="math/tex">B^{\prime}=\operatorname{Peel}(B, k, 1, \alpha)</script></span> to be the box</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
B^{\prime}=\operatorname{NewBox}(B, k, 1, b)
</div>
<script type="math/tex; mode=display">
B^{\prime}=\operatorname{NewBox}(B, k, 1, b)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">b</span><script type="math/tex">b</script></span> is the largest scalar for <span class="arithmatex"><span class="MathJax_Preview">\alpha \in(0,1)</span><script type="math/tex">\alpha \in(0,1)</script></span> s.t.
<span class="arithmatex"><span class="MathJax_Preview">b&lt;b_{k}</span><script type="math/tex">b<b_{k}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">n_{B^{\prime}} \leqslant(1-\alpha) n_{B}</span><script type="math/tex">n_{B^{\prime}} \leqslant(1-\alpha) n_{B}</script></span></p>
<p><img alt="" src="../media/ASL-note3/2021-12-26-16-24-49.png" /></p>
<p>找盒子的总体流程可规范地写为</p>
<ol>
<li>Set <span class="arithmatex"><span class="MathJax_Preview">i=0</span><script type="math/tex">i=0</script></span> and let <span class="arithmatex"><span class="MathJax_Preview">\alpha \in(0,1)</span><script type="math/tex">\alpha \in(0,1)</script></span></li>
<li>Let <span class="arithmatex"><span class="MathJax_Preview">B_{0}</span><script type="math/tex">B_{0}</script></span> be the minimal box containing all the data</li>
<li>Peeling process: find sequence of decreasing nested boxes while (number of observations in <span class="arithmatex"><span class="MathJax_Preview">B_{i} \geqslant n_{m}</span><script type="math/tex">B_{i} \geqslant n_{m}</script></span> )
   - Compute the trimmed boxes <span class="arithmatex"><span class="MathJax_Preview">C_{k}=P e e l\left(B_{i}, k, 0, \alpha\right)</span><script type="math/tex">C_{k}=P e e l\left(B_{i}, k, 0, \alpha\right)</script></span> and <span class="arithmatex"><span class="MathJax_Preview">C_{k+p}=\operatorname{Peel}\left(B_{i}, k, 1, \alpha\right)</span><script type="math/tex">C_{k+p}=\operatorname{Peel}\left(B_{i}, k, 1, \alpha\right)</script></span> for <span class="arithmatex"><span class="MathJax_Preview">k=1, \ldots, p</span><script type="math/tex">k=1, \ldots, p</script></span>
   - Choose the <span class="arithmatex"><span class="MathJax_Preview">C_{j *}</span><script type="math/tex">C_{j *}</script></span> with highest response mean [从每一个维度尝试搜索, 选择得到的盒子中响应量最高的那一个]
   - Set <span class="arithmatex"><span class="MathJax_Preview">B_{i+1}=C_{j *}</span><script type="math/tex">B_{i+1}=C_{j *}</script></span>
   - Set <span class="arithmatex"><span class="MathJax_Preview">i=i+1</span><script type="math/tex">i=i+1</script></span></li>
<li>Pasting process: find sequence of increasing nested boxed For <span class="arithmatex"><span class="MathJax_Preview">k=1, \ldots, p</span><script type="math/tex">k=1, \ldots, p</script></span>
   - <span class="arithmatex"><span class="MathJax_Preview">C=\operatorname{ExpandBox}\left(B_{i}, k, 0, \alpha\right), D=\operatorname{ExpandBox}\left(B_{i}, k, 1, \alpha\right)</span><script type="math/tex">C=\operatorname{ExpandBox}\left(B_{i}, k, 0, \alpha\right), D=\operatorname{ExpandBox}\left(B_{i}, k, 1, \alpha\right)</script></span>
   - Set <span class="arithmatex"><span class="MathJax_Preview">B_{i+1}=C</span><script type="math/tex">B_{i+1}=C</script></span> and <span class="arithmatex"><span class="MathJax_Preview">i=i+1</span><script type="math/tex">i=i+1</script></span> if <span class="arithmatex"><span class="MathJax_Preview">S_{C}&gt;S_{B_{i}}</span><script type="math/tex">S_{C}>S_{B_{i}}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">S_{C}&gt;S_{D}</span><script type="math/tex">S_{C}>S_{D}</script></span>
   - Set <span class="arithmatex"><span class="MathJax_Preview">B_{i+1}=D</span><script type="math/tex">B_{i+1}=D</script></span> and <span class="arithmatex"><span class="MathJax_Preview">i=i+1</span><script type="math/tex">i=i+1</script></span> if <span class="arithmatex"><span class="MathJax_Preview">S_{C}&gt;S_{B_{i}}</span><script type="math/tex">S_{C}>S_{B_{i}}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">S_{D}&gt;S_{C}</span><script type="math/tex">S_{D}>S_{C}</script></span></li>
</ol>
<p>上面仅仅是找到了一个盒子, 整体流程为</p>
<ol>
<li>Previous steps produces a sequence of boxes <span class="arithmatex"><span class="MathJax_Preview">B_{1}, \ldots, B_{i}</span><script type="math/tex">B_{1}, \ldots, B_{i}</script></span></li>
<li>Use cross-validation to choose best box - call this box <span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span></li>
<li>Remove the data in box <span class="arithmatex"><span class="MathJax_Preview">B</span><script type="math/tex">B</script></span> from the dataset</li>
<li>Repeat the peeling and pasting steps and the cross-validation step to obtain a second box</li>
<li>Continue these last two steps to get as many boxes as desired</li>
</ol>
<p>PRIM 与 CART 相比的优点是它的 <strong>耐心 (patience)</strong>. 因为 CART 的二值分割快速地将数据分割开. 假设等大小的分割, 有 <span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> 个观测情况下, 在使用完数据之前, 只能进行 <span class="arithmatex"><span class="MathJax_Preview">\log _{2}(N)-1</span><script type="math/tex">\log _{2}(N)-1</script></span> 次分割. 如 果 PRIM 在每一步剔除掉训练点的比例为 <span class="arithmatex"><span class="MathJax_Preview">\alpha</span><script type="math/tex">\alpha</script></span>, 则在用完数据之前大约需要 <span class="arithmatex"><span class="MathJax_Preview">-\log (N) / \log (1-\alpha)</span><script type="math/tex">-\log (N) / \log (1-\alpha)</script></span> 次 晹除步骤. 举个例子, 如果 <span class="arithmatex"><span class="MathJax_Preview">N=128, \alpha=0.10</span><script type="math/tex">N=128, \alpha=0.10</script></span>, 则 <span class="arithmatex"><span class="MathJax_Preview">\log_{2}(N)-1=6</span><script type="math/tex">\log_{2}(N)-1=6</script></span>, 而 <span class="arithmatex"><span class="MathJax_Preview">-\log (N) / \log (1-\alpha) \approx 46</span><script type="math/tex">-\log (N) / \log (1-\alpha) \approx 46</script></span>. 考虑每一步需要整数个的观测, PRIM 实际上可以剔除 29 次. 在任 何情形下, PRIM 的能力更加耐心, 这应该帮助自上而下的贪婪算法找到更好的解.</p>
<h3 id="multivariate-adaptive-regression-splines-mars">Multivariate Adaptive Regression Splines (MARS)<a class="headerlink" href="#multivariate-adaptive-regression-splines-mars" title="Permanent link">&para;</a></h3>
<p>多变量自适应回归样条 (Multivariate Adaptive Regression Splines, MARS) 是回归的自适应过程(an adaptive procedure for regression)，非常适合<strong>高维问题</strong>（即，存在大量的输入）．可以从两个角度来理解它，首先，它可以看成是逐步线性回归 (stepwise linear regression)的推广，其次，也可以看成是为了提高 CART 在回归中的效果而进行的改进．我们从第一种观点引入 MARS，接着与 CART 联系起来．</p>
<p>首先定义 expansions in piecewise linear basis functions <span class="arithmatex"><span class="MathJax_Preview">\left(x-t\right)_{+}</span><script type="math/tex">\left(x-t\right)_{+}</script></span> 和 <span class="arithmatex"><span class="MathJax_Preview">\left(t-x\right)_{+}</span><script type="math/tex">\left(t-x\right)_{+}</script></span>, 函数图像如下:</p>
<p><img alt="" src="../media/ASL-note3/2021-12-26-16-37-00.png" /></p>
<p>MARS - Basis Functions</p>
<ul>
<li>Training data <span class="arithmatex"><span class="MathJax_Preview">\left(x_{1},, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)</span><script type="math/tex">\left(x_{1},, y_{1}\right), \ldots,\left(x_{n}, y_{n}\right)</script></span> with <span class="arithmatex"><span class="MathJax_Preview">y_{i} \in \mathbb{R}</span><script type="math/tex">y_{i} \in \mathbb{R}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">x_{i}=\left(x_{i 1}, x_{i 2}, \ldots, x_{i p}\right)^{\top} \in \mathbb{R}^{p}</span><script type="math/tex">x_{i}=\left(x_{i 1}, x_{i 2}, \ldots, x_{i p}\right)^{\top} \in \mathbb{R}^{p}</script></span></li>
<li>For an input vector <span class="arithmatex"><span class="MathJax_Preview">X \in \mathbb{R}^{p}</span><script type="math/tex">X \in \mathbb{R}^{p}</script></span> define</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
h_{0}(X, j, i)=\left(X_{j}-x_{i j}\right)_{+} \text {and } h_{1}(X, j, i)=\left(x_{i j}-X_{j}\right)_{+}
</div>
<script type="math/tex; mode=display">
h_{0}(X, j, i)=\left(X_{j}-x_{i j}\right)_{+} \text {and } h_{1}(X, j, i)=\left(x_{i j}-X_{j}\right)_{+}
</script>
</div>
<ul>
<li>Then define a collection of basis functions</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
C=\left\{h_{0}(X, j, i), h_{1}(X, j, i)_{j=1, \ldots, p, i=1, \ldots, n}\right\}
</div>
<script type="math/tex; mode=display">
C=\left\{h_{0}(X, j, i), h_{1}(X, j, i)_{j=1, \ldots, p, i=1, \ldots, n}\right\}
</script>
</div>
<p>根据上面的定义, 可以最多可能有 <span class="arithmatex"><span class="MathJax_Preview">Np</span><script type="math/tex">Np</script></span> 个基函数. 然后基于这组基函数进行拟合 (注意下面的 <span class="arithmatex"><span class="MathJax_Preview">g\left(X, \alpha_{m}\right)</span><script type="math/tex">g\left(X, \alpha_{m}\right)</script></span> 是 <span class="arithmatex"><span class="MathJax_Preview">C</span><script type="math/tex">C</script></span> 中多个基函数的乘积.)</p>
<p>Estimate the regression function using functions from <span class="arithmatex"><span class="MathJax_Preview">C</span><script type="math/tex">C</script></span> and product of functions from <span class="arithmatex"><span class="MathJax_Preview">C</span><script type="math/tex">C</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
f(X)=\beta_{0}+\sum_{m=1}^{M} \beta_{m} g\left(X, \alpha_{m}\right)
</div>
<script type="math/tex; mode=display">
f(X)=\beta_{0}+\sum_{m=1}^{M} \beta_{m} g\left(X, \alpha_{m}\right)
</script>
</div>
<p>where each <span class="arithmatex"><span class="MathJax_Preview">\alpha_{m}=\left(n_{m}, b_{1}, j_{1}, i_{1}, \ldots, b_{n m}, j_{n m}, i_{n m}\right)</span><script type="math/tex">\alpha_{m}=\left(n_{m}, b_{1}, j_{1}, i_{1}, \ldots, b_{n m}, j_{n m}, i_{n m}\right)</script></span> with <span class="arithmatex"><span class="MathJax_Preview">b_{k} \in\{0,1\}</span><script type="math/tex">b_{k} \in\{0,1\}</script></span> s.t.</p>
<div class="arithmatex">
<div class="MathJax_Preview">
g\left(X, \alpha_{m}\right)=\prod_{k=1}^{n_{m}} h_{b_{k}}\left(X, j_{k}, i_{k}\right)
</div>
<script type="math/tex; mode=display">
g\left(X, \alpha_{m}\right)=\prod_{k=1}^{n_{m}} h_{b_{k}}\left(X, j_{k}, i_{k}\right)
</script>
</div>
<p><img alt="" src="../media/ASL-note3/2021-12-26-16-41-58.png" /></p>
<p>具体的筛选算法如下, 上图为直观的展示.</p>
<p>Initially</p>
<ul>
<li>Set <span class="arithmatex"><span class="MathJax_Preview">g\left(X, \alpha_{0}\right) \equiv 1</span><script type="math/tex">g\left(X, \alpha_{0}\right) \equiv 1</script></span> and <span class="arithmatex"><span class="MathJax_Preview">M=\left\{g\left(X, \alpha_{0}\right)\right\}</span><script type="math/tex">M=\left\{g\left(X, \alpha_{0}\right)\right\}</script></span></li>
</ul>
<p>While <span class="arithmatex"><span class="MathJax_Preview">|M|&lt;N</span><script type="math/tex">|M|<N</script></span> perform the following</p>
<ul>
<li>for each <span class="arithmatex"><span class="MathJax_Preview">(m, j, i) \in\{1, \ldots,|M|\} \times\{1, \ldots, p\} \times\{1, \ldots, n\}</span><script type="math/tex">(m, j, i) \in\{1, \ldots,|M|\} \times\{1, \ldots, p\} \times\{1, \ldots, n\}</script></span>
  1. Augment functions in <span class="arithmatex"><span class="MathJax_Preview">M-g\left(X, \alpha_{0}\right), \ldots, g\left(X, \alpha_{|M|}\right)</span><script type="math/tex">M-g\left(X, \alpha_{0}\right), \ldots, g\left(X, \alpha_{|M|}\right)</script></span> with 将模型集合中现有的所有函数和 <span class="arithmatex"><span class="MathJax_Preview">C</span><script type="math/tex">C</script></span> 中所有基函数乘积作为组合 (注意每次加入的都是一个反射对)
  <span class="arithmatex"><span class="MathJax_Preview">g\left(X, \alpha_{m}\right) \cdot h_{0}(X, j, i) \text { and } g\left(X, \alpha_{m}\right) \cdot h_{1}(X, j, i)</span><script type="math/tex">g\left(X, \alpha_{m}\right) \cdot h_{0}(X, j, i) \text { and } g\left(X, \alpha_{m}\right) \cdot h_{1}(X, j, i)</script></span>
  2. Use standard linear regression o estimate the <span class="arithmatex"><span class="MathJax_Preview">\hat{\beta}_{1}</span><script type="math/tex">\hat{\beta}_{1}</script></span> &lsquo;s s.t.
  <span class="arithmatex"><span class="MathJax_Preview">\begin{gathered}
  f_{t r y}(X)=\sum_{l=0}^{|M|} \hat{\beta}_{I} g\left(X, \alpha_{l}\right)+\hat{\beta}_{|M|+1} g\left(X, \alpha_{m}\right) \cdot h_{0}(X, j, i)+
  \hat{\beta}_{|M|+2} g\left(X, \alpha_{m}\right) \cdot h_{1}(X, j, i)
  \end{gathered}</span><script type="math/tex">\begin{gathered}
  f_{t r y}(X)=\sum_{l=0}^{|M|} \hat{\beta}_{I} g\left(X, \alpha_{l}\right)+\hat{\beta}_{|M|+1} g\left(X, \alpha_{m}\right) \cdot h_{0}(X, j, i)+
  \hat{\beta}_{|M|+2} g\left(X, \alpha_{m}\right) \cdot h_{1}(X, j, i)
  \end{gathered}</script></span>
  3. Compute and record training error of <span class="arithmatex"><span class="MathJax_Preview">f_{\text {try }}(X)</span><script type="math/tex">f_{\text {try }}(X)</script></span></li>
<li>Let <span class="arithmatex"><span class="MathJax_Preview">\left(m^{*}\right), j^{*}, i^{*}</span><script type="math/tex">\left(m^{*}\right), j^{*}, i^{*}</script></span> be triplet producing lowest training error</li>
<li>Add <span class="arithmatex"><span class="MathJax_Preview">g\left(X, \alpha_{m^{*}}\right) \cdot h_{0}\left(X, j^{*}, i^{*}\right)</span><script type="math/tex">g\left(X, \alpha_{m^{*}}\right) \cdot h_{0}\left(X, j^{*}, i^{*}\right)</script></span> and <span class="arithmatex"><span class="MathJax_Preview">g\left(X, \alpha_{m^{*}}\right) \cdot h_{1}\left(X, j^{*}, i^{*}\right)</span><script type="math/tex">g\left(X, \alpha_{m^{*}}\right) \cdot h_{1}\left(X, j^{*}, i^{*}\right)</script></span> to <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span></li>
</ul>
<h4 id="piecewise-linear-forward-model-building">Piecewise Linear \&amp; Forward Model Building<a class="headerlink" href="#piecewise-linear-forward-model-building" title="Permanent link">&para;</a></h4>
<p>They can operate locally - the product is only non-zero where all individual components are non-zero</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\prod_{k=1}^{n_{m}} h_{b_{k}}\left(X, j_{k}, i_{k}\right)
</div>
<script type="math/tex; mode=display">
\prod_{k=1}^{n_{m}} h_{b_{k}}\left(X, j_{k}, i_{k}\right)
</script>
</div>
<ul>
<li>Locality <span class="arithmatex"><span class="MathJax_Preview">\rightarrow</span><script type="math/tex">\rightarrow</script></span> forward model building strategy can<ul>
<li>build up the regression surface parsimoniously (吝啬地)</li>
<li>use parameters only when needed, which is very important for high dimensional data</li>
<li>「This is important, since one should “spend” parameters carefully in high dimensions, as they can run out quickly.」</li>
</ul>
</li>
<li>Computational reasons - innermost loop of model building can be made very efficient</li>
<li>Hierarchical search avoids unnecessary complicated terms 注意到一个 4 阶的交叉项要出现在模型中, 则其中一定有一个 3 阶交叉相在原本的模型中 (避免了在可行选择数目呈指数增长的空间中的搜索)</li>
</ul>
<h4 id="mars-vs-cart">MARS V.S CART<a class="headerlink" href="#mars-vs-cart" title="Permanent link">&para;</a></h4>
<p>If MARS procedure is amended s.t.</p>
<ol>
<li>Set <span class="arithmatex"><span class="MathJax_Preview">h_{0}(X, j, i)=I\left(X_{j}-x_{i j}&gt;0\right) \leftarrow</span><script type="math/tex">h_{0}(X, j, i)=I\left(X_{j}-x_{i j}>0\right) \leftarrow</script></span> step function</li>
<li>Set <span class="arithmatex"><span class="MathJax_Preview">h_{1}(X, j, i)=I\left(X_{j}-x_{i j} \leqslant 0\right) \leftarrow</span><script type="math/tex">h_{1}(X, j, i)=I\left(X_{j}-x_{i j} \leqslant 0\right) \leftarrow</script></span> step function (用阶跃函数替代分段线性基函数)</li>
<li>When <span class="arithmatex"><span class="MathJax_Preview">g \in M</span><script type="math/tex">g \in M</script></span> is chosen at one iteration s.t.</li>
</ol>
<div class="arithmatex">
<div class="MathJax_Preview">
M=M \cup\left\{g(X) \cdot h_{0}(X, j, i)\right\} \cup\left\{g(X) \cdot h_{1}(X, j, i)\right\}
</div>
<script type="math/tex; mode=display">
M=M \cup\left\{g(X) \cdot h_{0}(X, j, i)\right\} \cup\left\{g(X) \cdot h_{1}(X, j, i)\right\}
</script>
</div>
<p>remove <span class="arithmatex"><span class="MathJax_Preview">g</span><script type="math/tex">g</script></span> from <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> 当模型中的项与候选项相乘，替换成交叉项，因此不允许该项与其它候选项进行交叉．</p>
<p>Then, MARS forward procedure <span class="arithmatex"><span class="MathJax_Preview">==</span><script type="math/tex">==</script></span> CART tree-growing algorithm</p>
<p>乘上一对反射阶跃函数等价于在阶跃点分割开．第二条限制表明一个结点可能不会被分割成多次，因此使得 CART 模型能用（吸引人的）二叉树表示．另一方面，也是这条限制使得 CART 很难对加性结构进行建模．<strong>MARS 放弃树结构并且得到捕捉加性影响的能力</strong>．</p>
<h3 id="hierarchical-mixture-of-experts-hme">Hierarchical Mixture of Experts (HME)<a class="headerlink" href="#hierarchical-mixture-of-experts-hme" title="Permanent link">&para;</a></h3>
<p>专家的分层混合 (HME) 过程可以看成是基于树方法的变种．主要的差异是树的分割不是硬决定 (hard decision)，而是软概率的决定 (soft probabilistic)．在每个结点观测往左或者往右的概率取决于输入值．因为最后的参数优化问题是光滑的，所以有一些计算的优势，不像在基于树的方式中的离散分割点的搜索．软分割或许也可以帮助预测准确性，并且提供另外一种有用的数据描述方式．</p>
<p>HMEs 和 CART 树的实现还有其他的差异．在HME中，在每个终止结点处拟合线性（或者逻辑斯蒂回归）模型，而不是像 CART 中那样是常值．分割点可以是多重的，而不仅仅是二值的，并且分割点是输入的线性组合的概率函数，而不是在标准 CART 中的单个输入．然而，这些选择的相对优点不是清晰的，大部分将在 9.2 节的后面中讨论．</p>
<p>简单的 2 层 HME 模型如图 9.13 所示．可以认为是在每个非终止结点处进行软分割的树．然而，这种方法的发明者采用不同的术语．终止结点称为专家 (experts)，非终止结点称为门控网络 (gating networks)．想法是，每个专家对响应变量提供一个看法（预测），并且通过门控网络将这些“看法”结合在一起．正如我们所见，这个模型形式上是混合模型，并且图中的两层模型可以推广为多层，因此有了称为专家的分层混合 (hierarchical mixtures of experts)</p>
<p><img alt="" src="../media/ASL-note3/2021-12-26-17-31-51.png" /></p>
<p>顶层的门控网络有输出</p>
<div class="arithmatex">
<div class="MathJax_Preview">
g_{j}\left(x, \gamma_{j}\right)=\frac{e^{\gamma_{j}^{T} x}}{\sum_{k=1}^{K} e^{\gamma_{k}^{T} x}} j=1,2, \ldots, K
</div>
<script type="math/tex; mode=display">
g_{j}\left(x, \gamma_{j}\right)=\frac{e^{\gamma_{j}^{T} x}}{\sum_{k=1}^{K} e^{\gamma_{k}^{T} x}} j=1,2, \ldots, K
</script>
</div>
<p>其中每个 <span class="arithmatex"><span class="MathJax_Preview">\gamma_{j}</span><script type="math/tex">\gamma_{j}</script></span> 是末知参数向量. 这表示软的 <span class="arithmatex"><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> 重分割（图 <span class="arithmatex"><span class="MathJax_Preview">9.13</span><script type="math/tex">9.13</script></span> 中 <span class="arithmatex"><span class="MathJax_Preview">K=2</span><script type="math/tex">K=2</script></span> ）每个 <span class="arithmatex"><span class="MathJax_Preview">g_{j}\left(x, \gamma_{j}\right)</span><script type="math/tex">g_{j}\left(x, \gamma_{j}\right)</script></span> 是将特征 向量为 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 的观测赋给第 <span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> 个分支. 注意到 <span class="arithmatex"><span class="MathJax_Preview">K=2</span><script type="math/tex">K=2</script></span> 时, 如果我们将 <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> 的某一个元素的系数取为 <span class="arithmatex"><span class="MathJax_Preview">+\infty</span><script type="math/tex">+\infty</script></span> , 接着我们得到无穷大的斜率的逻辑斯蒂曲线. 在这种情形下, 门的概率取 0 或 1 , 对应在该输入 下的硬分割.</p>
<p>在第二层, 门控网络有类似的形式:</p>
<div class="arithmatex">
<div class="MathJax_Preview">
g_{\ell \mid j}\left(x, \gamma_{j \ell}\right)=\frac{e^{\gamma_{j \ell}^{T} x}}{\sum_{k=1}^{K} e^{\gamma_{j k}^{T}}}, \ell=1,2, \ldots, K
</div>
<script type="math/tex; mode=display">
g_{\ell \mid j}\left(x, \gamma_{j \ell}\right)=\frac{e^{\gamma_{j \ell}^{T} x}}{\sum_{k=1}^{K} e^{\gamma_{j k}^{T}}}, \ell=1,2, \ldots, K
</script>
</div>
<p>这是在给定上一层第 <span class="arithmatex"><span class="MathJax_Preview">j</span><script type="math/tex">j</script></span> 个分支的情况下, 赋予第 <span class="arithmatex"><span class="MathJax_Preview">\ell</span><script type="math/tex">\ell</script></span> 分支的概率.</p>
<p>在每个专家（终止结点）, 我们有如下形式的响应变量的模型 (这根据问题而有不同).</p>
<div class="arithmatex">
<div class="MathJax_Preview">
Y \sim \operatorname{Pr}\left(y \mid x, \theta_{j l}\right)
</div>
<script type="math/tex; mode=display">
Y \sim \operatorname{Pr}\left(y \mid x, \theta_{j l}\right)
</script>
</div>
<ul>
<li>Regression: Gaussian linear regression model <span class="arithmatex"><span class="MathJax_Preview">P\left(y \mid x, \theta_{j l}\right)=N\left(\beta_{j l}^{\top} x, \sigma_{j l}^{2}\right)</span><script type="math/tex">P\left(y \mid x, \theta_{j l}\right)=N\left(\beta_{j l}^{\top} x, \sigma_{j l}^{2}\right)</script></span> where <span class="arithmatex"><span class="MathJax_Preview">\theta_{j l}=\left(\beta_{j l}, \sigma_{j l}^{2}\right)</span><script type="math/tex">\theta_{j l}=\left(\beta_{j l}, \sigma_{j l}^{2}\right)</script></span></li>
<li>Classification: linear logistic regression model <span class="arithmatex"><span class="MathJax_Preview">P\left(Y=1 \mid x, \theta_{j l}\right)=\frac{1}{1+e^{-\theta_{j j}^{\top} x}}</span><script type="math/tex">P\left(Y=1 \mid x, \theta_{j l}\right)=\frac{1}{1+e^{-\theta_{j j}^{\top} x}}</script></span></li>
</ul>
<h4 id="hme">HME<a class="headerlink" href="#hme" title="Permanent link">&para;</a></h4>
<p>The HME represents a mixture probability model. 总体模型是一个混合模型，其中混合概率由门控网络模型确定.</p>
<p>The mixture probabilities are determined by the soft splits</p>
<div class="arithmatex">
<div class="MathJax_Preview">
P(y \mid x, \Psi)=\sum_{j=1}^{K} g_{j}\left(x, \gamma_{j}\right) \sum_{l=1}^{K} g_{l \mid j}\left(x, \gamma_{l j}\right) P\left(y \mid x, \theta_{j l}\right)
</div>
<script type="math/tex; mode=display">
P(y \mid x, \Psi)=\sum_{j=1}^{K} g_{j}\left(x, \gamma_{j}\right) \sum_{l=1}^{K} g_{l \mid j}\left(x, \gamma_{l j}\right) P\left(y \mid x, \theta_{j l}\right)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\Psi=\left\{\gamma_{j}, \gamma_{j l}, \theta_{j l}\right\}</span><script type="math/tex">\Psi=\left\{\gamma_{j}, \gamma_{j l}, \theta_{j l}\right\}</script></span></p>
<p>Estimate <span class="arithmatex"><span class="MathJax_Preview">\Psi</span><script type="math/tex">\Psi</script></span> by maximizing the log-likelihood</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\max _{\Psi} \sum_{i=1}^{n} \log P\left(y_{i} \mid X_{i}, \Psi\right)
</div>
<script type="math/tex; mode=display">
\max _{\Psi} \sum_{i=1}^{n} \log P\left(y_{i} \mid X_{i}, \Psi\right)
</script>
</div>
<p>采用 EM 算法求解.</p>
<p>Introduce the hidden variables <span class="arithmatex"><span class="MathJax_Preview">\Delta_{j}^{i}</span><script type="math/tex">\Delta_{j}^{i}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\Delta_{l \mid j}^{i}</span><script type="math/tex">\Delta_{l \mid j}^{i}</script></span> to indicate the underlying branch decision</p>
<ul>
<li>E-step: Compute posterior probabilities of <span class="arithmatex"><span class="MathJax_Preview">\Delta_{j}^{i}</span><script type="math/tex">\Delta_{j}^{i}</script></span> and <span class="arithmatex"><span class="MathJax_Preview">\Delta_{/ j j}^{i}</span><script type="math/tex">\Delta_{/ j j}^{i}</script></span> given <span class="arithmatex"><span class="MathJax_Preview">\Psi^{(t)}</span><script type="math/tex">\Psi^{(t)}</script></span></li>
<li>M-step: Compute <span class="arithmatex"><span class="MathJax_Preview">\Psi^{(t+1)}</span><script type="math/tex">\Psi^{(t+1)}</script></span> by maximization of the expected log-likelihood <span class="arithmatex"><span class="MathJax_Preview">\Psi^{(t+1)}=E_{\Delta \mid X, \Psi^{(t)}} \log L(\Psi \mid \Delta, X)</span><script type="math/tex">\Psi^{(t+1)}=E_{\Delta \mid X, \Psi^{(t)}} \log L(\Psi \mid \Delta, X)</script></span></li>
</ul>
<h4 id="hme-or-cart">HME or CART<a class="headerlink" href="#hme-or-cart" title="Permanent link">&para;</a></h4>
<ul>
<li>Advantages of HME over CART<ul>
<li>Smooth final regression function - <strong>soft splits</strong> allow for smooth transitions from high to low responses</li>
<li>Easier to optimize parameters - the log likelihood is a smooth function and is amendable to numerical optimization</li>
</ul>
</li>
<li>Disadtanges of HME over CART<ul>
<li>Tree topology - no good way for HME</li>
<li>Hard to interpret the model</li>
</ul>
</li>
</ul>
<h2 id="boosting-additive-trees">Boosting &amp; Additive Trees<a class="headerlink" href="#boosting-additive-trees" title="Permanent link">&para;</a></h2>
<p>Boosting 是最近 20 年内提出的最有力的学习方法．最初是为了分类问题而设计的，但是我们将在这章中看到，它也可以很好地扩展到回归问题上．Boosting的动机是集合许多弱学习的结果来得到有用的“committee”．从这点看 boosting 与 bagging 以及其他的基于 committee 的方式（8.8 节）类似．然而，我们应该看到这种联系最多是表面上的，boosting 在根本上存在不同．</p>
<h3 id="boosting-methods">Boosting Methods<a class="headerlink" href="#boosting-methods" title="Permanent link">&para;</a></h3>
<p>Overview of Boosting</p>
<ul>
<li>Boosting is a procedure to combine the output of many weak classifiers to produce a powerful committee</li>
<li>A weak classifier is one whose error rate is only slightly better than random guessing</li>
<li>Boosting produces a sequence of weak classifier <span class="arithmatex"><span class="MathJax_Preview">G_{m}(x)</span><script type="math/tex">G_{m}(x)</script></span> for <span class="arithmatex"><span class="MathJax_Preview">m=1, \ldots, M</span><script type="math/tex">m=1, \ldots, M</script></span> whose predictions are then combined</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
G(x)=\operatorname{sgn}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)
</div>
<script type="math/tex; mode=display">
G(x)=\operatorname{sgn}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)
</script>
</div>
<p>through a weighted majority to produce the final prediction</p>
<ul>
<li>Each <span class="arithmatex"><span class="MathJax_Preview">\alpha_{m}&gt;0</span><script type="math/tex">\alpha_{m}>0</script></span> is computed by the boosting algorithm and reflects how accurately <span class="arithmatex"><span class="MathJax_Preview">G_{m}</span><script type="math/tex">G_{m}</script></span> classified the data.</li>
</ul>
<p>Most Popular Boosting Algorithm</p>
<p><strong>AdaBoost.M1</strong> algorithm of Freund \&amp; Schapire (1997)</p>
<ul>
<li>Have training data <span class="arithmatex"><span class="MathJax_Preview">\left(x_{i}, y_{i}\right), i=1,2, \ldots, n</span><script type="math/tex">\left(x_{i}, y_{i}\right), i=1,2, \ldots, n</script></span></li>
<li>Introduce a weight <span class="arithmatex"><span class="MathJax_Preview">\omega_{i}=\frac{1}{n}</span><script type="math/tex">\omega_{i}=\frac{1}{n}</script></span> for each training example</li>
<li>for <span class="arithmatex"><span class="MathJax_Preview">m=1, \ldots, M</span><script type="math/tex">m=1, \ldots, M</script></span>
  1. Let <span class="arithmatex"><span class="MathJax_Preview">G_{m}</span><script type="math/tex">G_{m}</script></span> be the weak classifier with minimum error
  $$
  e r r_{m}=\sum_{i=1}^{n} \omega_{i} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)
  $$
  2. Set <span class="arithmatex"><span class="MathJax_Preview">\alpha_{m}=\log \frac{1-e r r_{m}}{e r r_{m}}</span><script type="math/tex">\alpha_{m}=\log \frac{1-e r r_{m}}{e r r_{m}}</script></span>
  3. Set
  $$
  \omega_{i} \leftarrow \omega_{i} e^{\alpha_{m} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}
  $$
  This increases (decreases) <span class="arithmatex"><span class="MathJax_Preview">\omega_{i}</span><script type="math/tex">\omega_{i}</script></span> for <span class="arithmatex"><span class="MathJax_Preview">x_{i}</span><script type="math/tex">x_{i}</script></span> misclassified (correctly classified) by <span class="arithmatex"><span class="MathJax_Preview">G_{m}</span><script type="math/tex">G_{m}</script></span>
  4. Normalize the <span class="arithmatex"><span class="MathJax_Preview">\omega_{i}</span><script type="math/tex">\omega_{i}</script></span> &lsquo;s so that they sum to 1</li>
</ul>
<p>What AdaBoost.MI Does</p>
<ul>
<li>As iterations proceed, observations difficult to classify correctly receive more influence</li>
<li>Each successive classifier is forced to concentrate on training observations missed by previous ones in the sequence</li>
</ul>
<p><img alt="" src="../media/ASL-note3/2021-12-28-11-25-48.png" /></p>
<p>这里给出了一个很直观的例子: 对于一个对角线分割的数据, 采用的基/弱分类器为只能在坐标轴上的划分. 通过调整每个 sample 的权重, 每一轮训练的分类器会越来越关注那些很难被划分的例子, 例如左图中对角线两端的 sample. 调整权重后基于新的 weight 训练得到的分类器如左图所示, 并进一步 reweight 样本, 更新总体的分类模型 <span class="arithmatex"><span class="MathJax_Preview">G(x)</span><script type="math/tex">G(x)</script></span>.</p>
<p>Now we need to&hellip;</p>
<ul>
<li>Show AdaBoost fits an additive model in a base learner, optimizing a novel exponential loss function</li>
<li>Show the population minimizer of the exponential loss function is the log-odds of the class probabilities</li>
<li>Present loss function that are more robust than squared error or exponential loss</li>
<li>Argue decision trees are an ideal base learner for data mining applications of boosting</li>
<li>Develop class of gradient boosted methods (GBMs), for boosting trees with any loss function</li>
<li>Emphasize the importance of &ldquo;slow learning&rdquo;</li>
</ul>
<h3 id="boosting-fits-an-additive-model">Boosting Fits an Additive Model<a class="headerlink" href="#boosting-fits-an-additive-model" title="Permanent link">&para;</a></h3>
<p>Additive Model</p>
<p>Boosting fits an additive expansion in a set of elementary basis functions
$$
G(x)=\operatorname{sgn}\left(\sum_{m=1}^{M} \alpha_{m} G_{m}(x)\right)
$$
The basis functions are the weak classifiers <span class="arithmatex"><span class="MathJax_Preview">G_{m}(x) \in\{-1,1\}</span><script type="math/tex">G_{m}(x) \in\{-1,1\}</script></span></p>
<p>More generally, basis functions expansions take the form
$$
f(x)=\sum_{m=1}^{M} \beta_{m} b\left(x ; \gamma_{m}\right)
$$
where <span class="arithmatex"><span class="MathJax_Preview">\beta_{m}</span><script type="math/tex">\beta_{m}</script></span> &lsquo;s are the expansion coefficients and <span class="arithmatex"><span class="MathJax_Preview">b(x ; \gamma) \in \mathbb{R}</span><script type="math/tex">b(x ; \gamma) \in \mathbb{R}</script></span> are simple functions of the input <span class="arithmatex"><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> parameterized by <span class="arithmatex"><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span></p>
<p>Examples of Additive Models</p>
<ul>
<li><strong>Single-hidden-layer neural networks</strong> 单层神经网络中（第 11 章）, where
$$
b(x ; \gamma)=\frac{1}{1+\exp \left(-\gamma_{0}-\gamma_{1}^{\top} x\right)}
$$</li>
<li><strong>Multivariate adaptive regression splines</strong> (MARS) 多元自适应回归样条（9.4 节）: use trucated-power spline basis functions where <span class="arithmatex"><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> parameterizes the variables and values for the knots</li>
<li><strong>Trees</strong>: <span class="arithmatex"><span class="MathJax_Preview">\gamma</span><script type="math/tex">\gamma</script></span> parameterizes the split variables and split points at the internal nodes, and the predictions at the terminal nodes</li>
</ul>
<h4 id="fitting-additive-models">Fitting Additive Models<a class="headerlink" href="#fitting-additive-models" title="Permanent link">&para;</a></h4>
<p>向前逐步加法建模</p>
<p>Typically fit models by minimizing a loss function averaged over the training data
$$
\min <em 1="1">{\beta</em>, \gamma_{1}, \ldots, \beta_{M}, \gamma_{M}} \sum_{i=1}^{n} L\left(y_{i}, \sum_{m=1}^{M} \beta_{m} b\left(x_{i}, \gamma_{m}\right)\right)
$$
For many loss functions <span class="arithmatex"><span class="MathJax_Preview">L(x, f(X))</span><script type="math/tex">L(x, f(X))</script></span>, basis functions <span class="arithmatex"><span class="MathJax_Preview">b(x ; \gamma)</span><script type="math/tex">b(x ; \gamma)</script></span> is hard to decide</p>
<p>Forward Stagewise Additive Modeling</p>
<p>Greedily add one basis function at a time in the following fashion</p>
<ul>
<li>Set <span class="arithmatex"><span class="MathJax_Preview">f_{0}(x)=0</span><script type="math/tex">f_{0}(x)=0</script></span></li>
<li>for <span class="arithmatex"><span class="MathJax_Preview">m=1, \ldots, M</span><script type="math/tex">m=1, \ldots, M</script></span>
  1. Compute
  $$
  \left(\hat{\beta}<em m="m">{m}, \hat{\gamma}</em>\right)=\operatorname{argmin}<em m="m">{\beta</em>, \gamma_{m}} \sum_{n=1}^{n} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\beta_{m} b\left(x_{i} ; \gamma_{m}\right)\right)
  $$
  2. Set
  $$
  f_{m}(x)=f_{m-1}(x)+\hat{\beta}<em m="m">{m} b\left(x ; \hat{\gamma}</em>\right)
  $$</li>
<li>Note: Previously added terms are not modified</li>
</ul>
<h3 id="exponential-loss-adaboost">Exponential Loss &amp; AdaBoost<a class="headerlink" href="#exponential-loss-adaboost" title="Permanent link">&para;</a></h3>
<p>Forward Stagewise Additive Modeling \&amp; Boosting 现在证明 AdaBoost.M1 算法（算法 10.1）等价于使用下列损失函数的向前逐步加法建模 (这里二分类标签为 <span class="arithmatex"><span class="MathJax_Preview">\{1, -1\}</span><script type="math/tex">\{1, -1\}</script></span>)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L(y, f(x)) = \exp(-yf(x))
</div>
<script type="math/tex; mode=display">
L(y, f(x)) = \exp(-yf(x))
</script>
</div>
<p>At each iteration of forward stagewise additive modeling, must solve this optimization problem</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
\left(\beta_{m}, G_{m}\right) &amp;=\operatorname{argmin}_{\beta, G} \sum_{i=1}^{n} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\beta G\left(x_{i}\right)\right) \\
&amp;=\operatorname{argmin}_{\beta, G} \sum_{i=1}^{n} \exp \left\{-y_{i}\left(f_{m-1}\left(x_{i}\right)+\beta G\left(x_{i}\right)\right)\right\}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\left(\beta_{m}, G_{m}\right) &=\operatorname{argmin}_{\beta, G} \sum_{i=1}^{n} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\beta G\left(x_{i}\right)\right) \\
&=\operatorname{argmin}_{\beta, G} \sum_{i=1}^{n} \exp \left\{-y_{i}\left(f_{m-1}\left(x_{i}\right)+\beta G\left(x_{i}\right)\right)\right\}
\end{aligned}
</script>
</div>
<p>where we assume an <strong>exponential loss</strong> for <span class="arithmatex"><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> and <span class="arithmatex"><span class="MathJax_Preview">G(x) \in\{-1,1\}</span><script type="math/tex">G(x) \in\{-1,1\}</script></span></p>
<p>Re-write: The optimization problem becomes</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp;\min _{\beta, G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\} \\
&amp;=\min_{\beta}\left(\min _{G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\}\right)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
&\min _{\beta, G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\} \\
&=\min_{\beta}\left(\min _{G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\}\right)
\end{aligned}
</script>
</div>
<p>Note</p>
<div class="arithmatex">
<div class="MathJax_Preview">
y_{i} G\left(x_{i}\right)= \begin{cases}1 &amp; \text { if } y_{i}=G\left(x_{i}\right) \\ -1 &amp; \text { if } y_{1} \neq G\left(x_{i}\right)\end{cases}
</div>
<script type="math/tex; mode=display">
y_{i} G\left(x_{i}\right)= \begin{cases}1 & \text { if } y_{i}=G\left(x_{i}\right) \\ -1 & \text { if } y_{1} \neq G\left(x_{i}\right)\end{cases}
</script>
</div>
<p>This implies <span class="arithmatex"><span class="MathJax_Preview">\exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\}</span><script type="math/tex">\exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\}</script></span> is equal to</p>
<div class="arithmatex">
<div class="MathJax_Preview">
e^{\beta} I\left(y_{i} \neq G\left(x_{i}\right)\right)+e^{-\beta}\left(1-I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)
</div>
<script type="math/tex; mode=display">
e^{\beta} I\left(y_{i} \neq G\left(x_{i}\right)\right)+e^{-\beta}\left(1-I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)
</script>
</div>
<p>先来看分类器的优化 Optimization for <span class="arithmatex"><span class="MathJax_Preview">G_{m}</span><script type="math/tex">G_{m}</script></span></p>
<p>The optimization problem becomes</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
&amp; \operatorname{argmin}_{G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\} \\
=&amp; \operatorname{argmin}_{G}\left(\left(e^{\beta}-e^{-\beta}\right) \sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G\left(x_{i}\right)\right)+e^{-\beta} \sum_{i=1}^{n} \omega_{i}^{(m)}\right) \\
=&amp; \operatorname{argmin}_{G}\left(\sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
& \operatorname{argmin}_{G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\} \\
=& \operatorname{argmin}_{G}\left(\left(e^{\beta}-e^{-\beta}\right) \sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G\left(x_{i}\right)\right)+e^{-\beta} \sum_{i=1}^{n} \omega_{i}^{(m)}\right) \\
=& \operatorname{argmin}_{G}\left(\sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)
\end{aligned}
</script>
</div>
<p>Therefore</p>
<div class="arithmatex">
<div class="MathJax_Preview">
G_{m}=\operatorname{argmin}_{G}\left(\sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)
</div>
<script type="math/tex; mode=display">
G_{m}=\operatorname{argmin}_{G}\left(\sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)
</script>
</div>
<p><span class="arithmatex"><span class="MathJax_Preview">G_{m}</span><script type="math/tex">G_{m}</script></span> minimizes the weighted error in the AdaBoost algorithm (given <span class="arithmatex"><span class="MathJax_Preview">\omega_{i}^{(m)}</span><script type="math/tex">\omega_{i}^{(m)}</script></span> &lsquo;s have the same definition)</p>
<p>再来看权重 Optimization for <span class="arithmatex"><span class="MathJax_Preview">\beta_{m}</span><script type="math/tex">\beta_{m}</script></span>
Plug <span class="arithmatex"><span class="MathJax_Preview">G_{m}</span><script type="math/tex">G_{m}</script></span> into the original optimization problem</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\min _{\beta}\left(\min_{G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\}\right)
</div>
<script type="math/tex; mode=display">
\min _{\beta}\left(\min_{G} \sum_{i=1}^{n} \omega_{i}^{(m)} \exp \left\{-y_{i} \beta G\left(x_{i}\right)\right\}\right)
</script>
</div>
<p>and using the previous result, it becomes</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{argmin}_{\beta}\left(\left(e^{\beta}-e^{-\beta}\right) \sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)+e^{-\beta} \sum_{i=1}^{n} \omega_{i}^{(m)}\right)
</div>
<script type="math/tex; mode=display">
\operatorname{argmin}_{\beta}\left(\left(e^{\beta}-e^{-\beta}\right) \sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)+e^{-\beta} \sum_{i=1}^{n} \omega_{i}^{(m)}\right)
</script>
</div>
<p>This quantity is minimized when (求导即可)</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\beta_{m}=\frac{1}{2} \log \frac{1-e r r_{m}}{e r r_{m}}
</div>
<script type="math/tex; mode=display">
\beta_{m}=\frac{1}{2} \log \frac{1-e r r_{m}}{e r r_{m}}
</script>
</div>
<p>where</p>
<div class="arithmatex">
<div class="MathJax_Preview">
e r r_{m}=\frac{\sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}{\sum_{i=1}^{n} \omega_{i}^{(m)}}
</div>
<script type="math/tex; mode=display">
e r r_{m}=\frac{\sum_{i=1}^{n} \omega_{i}^{(m)} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}{\sum_{i=1}^{n} \omega_{i}^{(m)}}
</script>
</div>
<h4 id="_2">等价性<a class="headerlink" href="#_2" title="Permanent link">&para;</a></h4>
<p>注意 Update of <span class="arithmatex"><span class="MathJax_Preview">\omega_{i}^{(m)}</span><script type="math/tex">\omega_{i}^{(m)}</script></span></p>
<p>Need the following result</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
-y_{i} G_{m}\left(x_{i}\right) &amp;=-I\left(y_{i}=G\left(x_{i}\right)\right)+I\left(y_{i} \neq G\left(x_{i}\right)\right) \\
&amp;=-\left(1-I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)+I\left(y_{i} \neq G\left(x_{i}\right)\right) \\
&amp;=-1+2 I\left(y_{i} \neq G\left(x_{i}\right)\right)
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
-y_{i} G_{m}\left(x_{i}\right) &=-I\left(y_{i}=G\left(x_{i}\right)\right)+I\left(y_{i} \neq G\left(x_{i}\right)\right) \\
&=-\left(1-I\left(y_{i} \neq G\left(x_{i}\right)\right)\right)+I\left(y_{i} \neq G\left(x_{i}\right)\right) \\
&=-1+2 I\left(y_{i} \neq G\left(x_{i}\right)\right)
\end{aligned}
</script>
</div>
<p>The updated weights can then be written as</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\begin{aligned}
\omega_{i}^{(m+1)}=e^{-y_{i} f_{m}\left(x_{i}\right)} &amp;=e^{-y_{i}\left(f_{m-1}\left(x_{i}\right)+\beta_{m} G_{m}\left(x_{i}\right)\right.} \\
&amp;=\omega_{i}^{(m)} e^{-y_{i} \beta_{m} G_{m}\left(x_{i}\right)} \\
&amp;=\omega_{i}^{(m)} e^{2 \beta_{m} I\left(y_{i} \neq G m\left(x_{i}\right)\right)} e^{-\beta_{m}}
\end{aligned}
</div>
<script type="math/tex; mode=display">
\begin{aligned}
\omega_{i}^{(m+1)}=e^{-y_{i} f_{m}\left(x_{i}\right)} &=e^{-y_{i}\left(f_{m-1}\left(x_{i}\right)+\beta_{m} G_{m}\left(x_{i}\right)\right.} \\
&=\omega_{i}^{(m)} e^{-y_{i} \beta_{m} G_{m}\left(x_{i}\right)} \\
&=\omega_{i}^{(m)} e^{2 \beta_{m} I\left(y_{i} \neq G m\left(x_{i}\right)\right)} e^{-\beta_{m}}
\end{aligned}
</script>
</div>
<p>比较 Adaboost 和 minimizing exponential loss in the additive model 的更新公式:</p>
<ol>
<li>模型权重</li>
</ol>
<p>Remember from the AdaBoost algorithm,</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\alpha_{m}=\log \frac{1-e r r_{m}}{e r r_{m}}
</div>
<script type="math/tex; mode=display">
\alpha_{m}=\log \frac{1-e r r_{m}}{e r r_{m}}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">e r r_{m}=\sum_{i=1}^{n} \omega_{i} I\left(y_{i} \neq G_m\left(x_{i}\right)\right)</span><script type="math/tex">e r r_{m}=\sum_{i=1}^{n} \omega_{i} I\left(y_{i} \neq G_m\left(x_{i}\right)\right)</script></span>
From minimizing exponential loss in the additive model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\beta_{m}=\frac{1}{2} \log \frac{1-e r r_{m}}{e r r_{m}}
</div>
<script type="math/tex; mode=display">
\beta_{m}=\frac{1}{2} \log \frac{1-e r r_{m}}{e r r_{m}}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">e r r_{m}=\frac{\sum_{i=1}^{n} \omega_{i}^{(m)}\left(\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)\right.}{\sum_{i=1}^{n} \omega_{i}^{(m)}}</span><script type="math/tex">e r r_{m}=\frac{\sum_{i=1}^{n} \omega_{i}^{(m)}\left(\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)\right.}{\sum_{i=1}^{n} \omega_{i}^{(m)}}</script></span></p>
<p>这里形式上好像差了系数 <span class="arithmatex"><span class="MathJax_Preview">1/2</span><script type="math/tex">1/2</script></span>, 实际上如果所有基模型的权重都缩小一个常数是一样的. (事实上, 李航书中的 Adaboost 模型系数是带 <span class="arithmatex"><span class="MathJax_Preview">1/2</span><script type="math/tex">1/2</script></span> 的.)</p>
<ol start="2">
<li>样本权重</li>
</ol>
<p>Remember from the AdaBoost algorithm,</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\omega_{i} \leftarrow \omega_{i} e^{\alpha_{m} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}
</div>
<script type="math/tex; mode=display">
\omega_{i} \leftarrow \omega_{i} e^{\alpha_{m} I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\alpha_{m}=\log \frac{1-e r r_{m}}{e r r_{m}}</span><script type="math/tex">\alpha_{m}=\log \frac{1-e r r_{m}}{e r r_{m}}</script></span>
From minimizing exponential loss in the additive model</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\omega_{i}^{(m+1)}=\omega_{i}^{(m)} e^{2 \beta I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)} e^{\left(-\beta_{m}\right)}
</div>
<script type="math/tex; mode=display">
\omega_{i}^{(m+1)}=\omega_{i}^{(m)} e^{2 \beta I\left(y_{i} \neq G_{m}\left(x_{i}\right)\right)} e^{\left(-\beta_{m}\right)}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">\beta_{m}=\frac{1}{2} \log \frac{1-e r r_{m}}{e r r_{m}}</span><script type="math/tex">\beta_{m}=\frac{1}{2} \log \frac{1-e r r_{m}}{e r r_{m}}</script></span>
Note: <span class="arithmatex"><span class="MathJax_Preview">e^{-\beta_{m}}</span><script type="math/tex">e^{-\beta_{m}}</script></span> is the same for all observations</p>
<p>总结一下: We can view the AdaBoost.M1 algorithm as a method that approximates minimizing</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\operatorname{argmin}_{\beta_{1}, G_{1}, \ldots, \beta_{M}, G_{M}} \sum_{i=1}^{n} \exp \left(-y_{i} \sum_{m=1}^{M} \beta_{m} G_{m}\left(x_{i}\right)\right)
</div>
<script type="math/tex; mode=display">
\operatorname{argmin}_{\beta_{1}, G_{1}, \ldots, \beta_{M}, G_{M}} \sum_{i=1}^{n} \exp \left(-y_{i} \sum_{m=1}^{M} \beta_{m} G_{m}\left(x_{i}\right)\right)
</script>
</div>
<p>via a forward stagewise additive modeling approach</p>
<p>from <a href="https://www.cs.toronto.edu/~mbrubake/teaching/C11/Handouts/AdaBoost.pdf">HERE</a></p>
<p><img alt="" src="../media/ASL-note3/2021-12-28-15-53-14.png" /></p>
<h3 id="loss-functions-robustness">Loss Functions &amp; Robustness<a class="headerlink" href="#loss-functions-robustness" title="Permanent link">&para;</a></h3>
<p>这一节我们进一步讨论分类和回归中不同的损失函数，并且用它们对极端数据的鲁棒性来描述．</p>
<h4 id="loss-functions-for-classification">Loss Functions for Classification<a class="headerlink" href="#loss-functions-for-classification" title="Permanent link">&para;</a></h4>
<ul>
<li>Exponential loss</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
L(y, f(x))=\exp \{-y f(X)\}
</div>
<script type="math/tex; mode=display">
L(y, f(x))=\exp \{-y f(X)\}
</script>
</div>
<ul>
<li>Binomial deviance loss 二项偏差</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
-L(y, f(x))=\log (1+\exp \{-2 y f(x)\})
</div>
<script type="math/tex; mode=display">
-L(y, f(x))=\log (1+\exp \{-2 y f(x)\})
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">p(x)=P(Y=1 \mid x)=\frac{1}{1+\exp \{-2 f(x)\}}</span><script type="math/tex">p(x)=P(Y=1 \mid x)=\frac{1}{1+\exp \{-2 f(x)\}}</script></span>
- Misclassification loss</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L(y, f(x))=I(y f(x)&lt;0)
</div>
<script type="math/tex; mode=display">
L(y, f(x))=I(y f(x)<0)
</script>
</div>
<p>The Margin</p>
<ul>
<li>These loss functions are functions of the &ldquo;margin&rdquo; 间隔: <span class="arithmatex"><span class="MathJax_Preview">y f(X)</span><script type="math/tex">y f(X)</script></span></li>
<li>Classification rule <span class="arithmatex"><span class="MathJax_Preview">G(x)=\operatorname{sign}\{f(X)\}</span><script type="math/tex">G(x)=\operatorname{sign}\{f(X)\}</script></span> <span class="arithmatex"><span class="MathJax_Preview">\rightarrow</span><script type="math/tex">\rightarrow</script></span> training examples with</li>
<li>positive margin <span class="arithmatex"><span class="MathJax_Preview">y_{i} f\left(x_{i}\right)&gt;0</span><script type="math/tex">y_{i} f\left(x_{i}\right)>0</script></span> are correctly classified</li>
<li>negative margin <span class="arithmatex"><span class="MathJax_Preview">y_{i} f\left(x_{i}\right)&lt;0</span><script type="math/tex">y_{i} f\left(x_{i}\right)<0</script></span> are misclassified</li>
<li>Decision boundary defined by <span class="arithmatex"><span class="MathJax_Preview">f(x)=0</span><script type="math/tex">f(x)=0</script></span></li>
<li>Classification algorithms attempt to produce positive margins for each training data point</li>
<li>Loss criterion for classification should penalize negative margins more heavily than positive margins</li>
</ul>
<p>尽管指数损失 (10.8) 和二项偏差 (10.18) 当应用到总体的联合分布时会得到同样的解, 但对于有 限的数据集并不一样. 两个准则都是 间隔 (margin) <span class="arithmatex"><span class="MathJax_Preview">y f(x)</span><script type="math/tex">y f(x)</script></span> 的单调递减函数. 在分类问题中 (响应 变量为 <span class="arithmatex"><span class="MathJax_Preview">-1 / 1), y f(x)</span><script type="math/tex">-1 / 1), y f(x)</script></span> 类似回归中的残差 <span class="arithmatex"><span class="MathJax_Preview">y-f(x)</span><script type="math/tex">y-f(x)</script></span>. 分类准则 <span class="arithmatex"><span class="MathJax_Preview">G(x)=\operatorname{sign}[f(x)]</span><script type="math/tex">G(x)=\operatorname{sign}[f(x)]</script></span> 表明有正间隔 <span class="arithmatex"><span class="MathJax_Preview">y_{i} f\left(x_{i}\right)&gt;0</span><script type="math/tex">y_{i} f\left(x_{i}\right)>0</script></span> 的观测被分类正确, 而有负间隔 <span class="arithmatex"><span class="MathJax_Preview">y_{i} f\left(x_{i}\right)&lt;0</span><script type="math/tex">y_{i} f\left(x_{i}\right)<0</script></span> 的观测被错误分类. 判别边界定义为 <span class="arithmatex"><span class="MathJax_Preview">f(x)=0</span><script type="math/tex">f(x)=0</script></span>. 分类算法的目标是得到尽可能频繁的正间隔. 任何用于分类的损失标准应该惩罚负间隔 比正间隔更重, 因为正间隔的观测值已经被正确分类.</p>
<p>图 <span class="arithmatex"><span class="MathJax_Preview">10.4</span><script type="math/tex">10.4</script></span> 展示了指数 (10.8) 和二项偏差的标准作为间隔 <span class="arithmatex"><span class="MathJax_Preview">y \cdot f(x)</span><script type="math/tex">y \cdot f(x)</script></span> 的函数的图象. 也显示了误分类损 失 <span class="arithmatex"><span class="MathJax_Preview">L(y, f(x))=I(y \cdot f(x)&lt;0)</span><script type="math/tex">L(y, f(x))=I(y \cdot f(x)<0)</script></span>, 它给出了们间隔的单位惩罚, 而对所有的正值没有惩罚. 指数损 失和二项偏差都可以看成是误分类损失的单调连续近似. 它们不断地惩罚越来越大的负边际值, 衙 罚力度比它们回报越来越大的正边际值更重. 它们的区别在于程度上. 二项偏差的忢罚对于大的增 加的负间隔线性增长, 而指数标准对这样观测的影响指数增长.</p>
<p>在训练过程的任一个时刻，指数标准对具有大的负边际的观测值上有更大的影响．二项偏差相对地在这些观测上影响较小，在所有数据上的影响分散更均匀．也因此在白噪声设定（贝叶斯误差率不接近于0）中更加鲁棒，特别在训练数据中存在误分类标签的情形中．在这些情形下，AdaBoost 的表现从经验上看显著退化．[也即指数损失稳定性较差]</p>
<p><img alt="" src="../media/ASL-note3/2021-12-28-16-00-26.png" /></p>
<p>总结一下 指数损失和二项偏差</p>
<ul>
<li>Exponential and deviance loss continuous approx to <strong>misclassification loss</strong></li>
<li>They increasingly penalize negative margin values more heavily than they reward positive ones</li>
<li>Binomial deviance penalty increases linearly with negative margins</li>
<li>Exponential loss penalty increases exponentially with negative margin</li>
<li>Exponential criterion concentrates more of its efforts on large negative margin observations than the binomial criterion</li>
<li>Binomial criterion is far more robust than the exponential criterion in noisy settings</li>
</ul>
<h4 id="robust-loss-functions-for-regression">Robust Loss Functions for Regression<a class="headerlink" href="#robust-loss-functions-for-regression" title="Permanent link">&para;</a></h4>
<ul>
<li>Squared error loss</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
L(y, f(x))=(y-f(x))^{2}
</div>
<script type="math/tex; mode=display">
L(y, f(x))=(y-f(x))^{2}
</script>
</div>
<p>population optimum for this loss function: <span class="arithmatex"><span class="MathJax_Preview">f(X)=E[Y \mid x]</span><script type="math/tex">f(X)=E[Y \mid x]</script></span>
- Absolute loss</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L(y, f(x))=|y-f(x)|
</div>
<script type="math/tex; mode=display">
L(y, f(x))=|y-f(x)|
</script>
</div>
<p>population optimum for this loss function: <span class="arithmatex"><span class="MathJax_Preview">f(x)=\operatorname{median}(Y \mid x)</span><script type="math/tex">f(x)=\operatorname{median}(Y \mid x)</script></span> 中位数</p>
<p>两者的关系类似上面分类损失的比较</p>
<ul>
<li>On finite samples, squared error loss puts far more emphasis on observations with large <span class="arithmatex"><span class="MathJax_Preview">| y_{i}-f\left(x_{i}\right) |</span><script type="math/tex">| y_{i}-f\left(x_{i}\right) |</script></span> than absolute loss</li>
<li>Thus squared error loss is less robust and performance degrades for long-tail error distributions and mis-labellings.</li>
</ul>
<p>Huber loss</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L(y, f(x))= \begin{cases}(y-f(x))^{2} &amp; \text { for }|y-f(x)| \leqslant \delta \\ 2 \delta|y-f(X)|-\delta^{2} &amp; \text { otherwise }\end{cases}
</div>
<script type="math/tex; mode=display">
L(y, f(x))= \begin{cases}(y-f(x))^{2} & \text { for }|y-f(x)| \leqslant \delta \\ 2 \delta|y-f(X)|-\delta^{2} & \text { otherwise }\end{cases}
</script>
</div>
<ul>
<li>strong resistance to gross outliers while</li>
<li>being nearly as efficient as least squares for Gaussian errors</li>
</ul>
<p>Combine the good properties of squared-error near zero and absolute error loss when <span class="arithmatex"><span class="MathJax_Preview">|y-f|</span><script type="math/tex">|y-f|</script></span> is large</p>
<p><img alt="" src="../media/ASL-note3/2021-12-28-16-13-36.png" /></p>
<h4 id="summary">Summary<a class="headerlink" href="#summary" title="Permanent link">&para;</a></h4>
<ul>
<li>When robustness is an issue, 1. squared error loss for regression; 2. exponential loss for classification are not the best criterion to be optimizing</li>
<li>But both loss functions lead to elegant modular boosting algorithms in the context of <strong>forward stagewise additive modeling</strong></li>
<li>For classification: perform a weighted fit of the base learner to the outputs <span class="arithmatex"><span class="MathJax_Preview">y_{i}</span><script type="math/tex">y_{i}</script></span> with weights <span class="arithmatex"><span class="MathJax_Preview">\omega_{i}=\exp \left\{ - y_{i} f\left(x_{i}\right)\right\}</span><script type="math/tex">\omega_{i}=\exp \left\{ - y_{i} f\left(x_{i}\right)\right\}</script></span></li>
<li>For regression: with squared-error loss one simply fits the base learner to the residuals from the current model <span class="arithmatex"><span class="MathJax_Preview">y_i− f_{m−1}(x_i)</span><script type="math/tex">y_i− f_{m−1}(x_i)</script></span> at each step</li>
<li>More robust criteria in their place do not give rise to such simple feasible boosting algorithms</li>
</ul>
<h3 id="off-the-shelf-procedure-for-data-mining">”Off-the-Shelf” Procedure for Data Mining<a class="headerlink" href="#off-the-shelf-procedure-for-data-mining" title="Permanent link">&para;</a></h3>
<p><img alt="" src="../media/ASL-note3/2021-12-28-16-23-30.png" /></p>
<p>数据挖掘的 “现货”(off-the-shelf) 方法．现货方法指的是可以直接应用到数据中而不需要大量时间进行数据预处理或者学习过程的精心调参．</p>
<p>Trees are great except&hellip;</p>
<ul>
<li>they are inaccurate at making predictions</li>
</ul>
<p>Boosting decision trees improve their accuracy at the cost of</p>
<ul>
<li>speed</li>
<li>interpretability</li>
<li>for AdaBoost, robustness against overlapping class distributions and especially mislabelling of the training data</li>
</ul>
<p>A gradient boosted model is a generalization of tree boosting that attempts to mitigate these problems</p>
<p>It aims to produce an accurate and effective off-the-shelf procedure for data mining</p>
<h3 id="boosting-trees">Boosting Trees<a class="headerlink" href="#boosting-trees" title="Permanent link">&para;</a></h3>
<p>Regression Tree Recap</p>
<ul>
<li>Tree partitions the input space into <span class="arithmatex"><span class="MathJax_Preview">R_{j}, j=1, \ldots, J</span><script type="math/tex">R_{j}, j=1, \ldots, J</script></span></li>
<li>Terminal / Leaf nodes of tree represent the region <span class="arithmatex"><span class="MathJax_Preview">R_{j}</span><script type="math/tex">R_{j}</script></span></li>
<li>The predictive rule is</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
x \in R_{j} \rightarrow f(X)=\gamma_{j}
</div>
<script type="math/tex; mode=display">
x \in R_{j} \rightarrow f(X)=\gamma_{j}
</script>
</div>
<ul>
<li>A tree with parameters <span class="arithmatex"><span class="MathJax_Preview">\theta=\left\{R_{j}, \gamma_{j}\right\}_{j=1}^{J}</span><script type="math/tex">\theta=\left\{R_{j}, \gamma_{j}\right\}_{j=1}^{J}</script></span> is expressed as</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
T(x ; \theta)=\sum_{j=1}^{J} \gamma_{j} I\left(x \in R_{j}\right)
</div>
<script type="math/tex; mode=display">
T(x ; \theta)=\sum_{j=1}^{J} \gamma_{j} I\left(x \in R_{j}\right)
</script>
</div>
<p>with <span class="arithmatex"><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span> usually treated as a meta-parameter</p>
<p>Learning a Regression Tree</p>
<p>Ideally parameters found by minimizing the empirical risk</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta}=\operatorname{argmin}_{\theta} \sum_{j=1}^{J} \sum_{x \in R_{j}} L\left(y_{i}, \gamma_{i}\right)
</div>
<script type="math/tex; mode=display">
\hat{\theta}=\operatorname{argmin}_{\theta} \sum_{j=1}^{J} \sum_{x \in R_{j}} L\left(y_{i}, \gamma_{i}\right)
</script>
</div>
<p>Hard optimization problem, instead settle for approximate suboptimal solutions</p>
<p>Typical approach: Divide optimization into two parts</p>
<ul>
<li>find <span class="arithmatex"><span class="MathJax_Preview">\gamma_{j}</span><script type="math/tex">\gamma_{j}</script></span> given <span class="arithmatex"><span class="MathJax_Preview">R_{j}</span><script type="math/tex">R_{j}</script></span> : typically trivial - mean of the training <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> &lsquo;s falling in <span class="arithmatex"><span class="MathJax_Preview">R_{j}</span><script type="math/tex">R_{j}</script></span></li>
<li>find <span class="arithmatex"><span class="MathJax_Preview">R_{j}</span><script type="math/tex">R_{j}</script></span> : difficult - greedy, top-down recursive partitioning algorithm to approximate</li>
</ul>
<h4 id="the-boosted-tree-model">The Boosted Tree Model<a class="headerlink" href="#the-boosted-tree-model" title="Permanent link">&para;</a></h4>
<p>A boosted tree is a sum of regression / classification trees</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f_{M}(x)=\sum_{m=1}^{M} T\left(x ; \theta_{m}\right)
</div>
<script type="math/tex; mode=display">
f_{M}(x)=\sum_{m=1}^{M} T\left(x ; \theta_{m}\right)
</script>
</div>
<p>learned in a forward stagewise manner</p>
<p>At each step solve</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta}_{m}=\operatorname{argmin}_{\theta_{m}} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \theta_{m}\right)\right)
</div>
<script type="math/tex; mode=display">
\hat{\theta}_{m}=\operatorname{argmin}_{\theta_{m}} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \theta_{m}\right)\right)
</script>
</div>
<p>for the parameters <span class="arithmatex"><span class="MathJax_Preview">\theta_{m}=\left\{R_{j m}, \gamma_{j m}\right\}_{j=1}^{J_{m}}</span><script type="math/tex">\theta_{m}=\left\{R_{j m}, \gamma_{j m}\right\}_{j=1}^{J_{m}}</script></span> of the next tree</p>
<p>How to solve this optimization problem?</p>
<p>也是在加性模型的框架之下进行问题求解. 上述问题可分为两部分:</p>
<p>Find <span class="arithmatex"><span class="MathJax_Preview">\gamma_{j m}</span><script type="math/tex">\gamma_{j m}</script></span> given <span class="arithmatex"><span class="MathJax_Preview">R_{j m}</span><script type="math/tex">R_{j m}</script></span> - easy</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\gamma}_{j m}=\operatorname{argmin}_{\gamma_{j m}} \sum_{x_{i} \in R_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma_{j m}\right)
</div>
<script type="math/tex; mode=display">
\hat{\gamma}_{j m}=\operatorname{argmin}_{\gamma_{j m}} \sum_{x_{i} \in R_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma_{j m}\right)
</script>
</div>
<p>Find <span class="arithmatex"><span class="MathJax_Preview">R_{j m}</span><script type="math/tex">R_{j m}</script></span> - not so easy</p>
<p>A few exceptions</p>
<ul>
<li><strong>Squared error loss</strong> - at each stage, fit a regression tree to residual <span class="arithmatex"><span class="MathJax_Preview">y_{i}-f_{m-1}\left(x_{i}\right)</span><script type="math/tex">y_{i}-f_{m-1}\left(x_{i}\right)</script></span></li>
<li><strong>Two class classification and exponential loss</strong> - give rise to an Adaboost method for boosting classification trees</li>
</ul>
<h3 id="gradient-boosting">Gradient Boosting<a class="headerlink" href="#gradient-boosting" title="Permanent link">&para;</a></h3>
<p>see <a href="https://blog.csdn.net/u012328159/article/details/94616127">here</a></p>
<ul>
<li>提升树利用加法模型与前向分步算法实现学习的优化过程。当损失函数是平方损失和指数损失函数时，每一步优化是很简单的。</li>
<li>但是，对一般损失函数而言，每一步优化往往比较困难。</li>
<li>这里我们使用梯度提升算法来解决这个问题。</li>
</ul>
<h4 id="numerical-optimization">Numerical Optimization<a class="headerlink" href="#numerical-optimization" title="Permanent link">&para;</a></h4>
<p>If the loss <span class="arithmatex"><span class="MathJax_Preview">L</span><script type="math/tex">L</script></span> is differentiable</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\theta}_{m}=\operatorname{argmin}_{\theta_{m}} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \theta_{m}\right)\right)
</div>
<script type="math/tex; mode=display">
\hat{\theta}_{m}=\operatorname{argmin}_{\theta_{m}} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \theta_{m}\right)\right)
</script>
</div>
<p>can be approximately solved with numerical optimization</p>
<p>Ex, the loss associated with using any <span class="arithmatex"><span class="MathJax_Preview">f(x)</span><script type="math/tex">f(x)</script></span> to predict <span class="arithmatex"><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> is</p>
<div class="arithmatex">
<div class="MathJax_Preview">
L(f)=\sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)
</div>
<script type="math/tex; mode=display">
L(f)=\sum_{i=1}^{N} L\left(y_{i}, f\left(x_{i}\right)\right)
</script>
</div>
<p>这里的 <span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> 要求是树的和. 如果忽略这个限定, 可直接看成是一个优化问题. Goal: find <span class="arithmatex"><span class="MathJax_Preview">f</span><script type="math/tex">f</script></span> which minimizes <span class="arithmatex"><span class="MathJax_Preview">L(f)</span><script type="math/tex">L(f)</script></span></p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}=\operatorname{argmin}_{f} L(f)
</div>
<script type="math/tex; mode=display">
\hat{f}=\operatorname{argmin}_{f} L(f)
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">f=\left\{f\left(x_{1}\right), \ldots, f\left(x_{N}\right)\right\}</span><script type="math/tex">f=\left\{f\left(x_{1}\right), \ldots, f\left(x_{N}\right)\right\}</script></span> 是在 <span class="arithmatex"><span class="MathJax_Preview">N</span><script type="math/tex">N</script></span> 个数据点 <span class="arithmatex"><span class="MathJax_Preview">x_i</span><script type="math/tex">x_i</script></span> 的近似的函数值 <span class="arithmatex"><span class="MathJax_Preview">f(x_i)</span><script type="math/tex">f(x_i)</script></span>.</p>
<p>考虑要求是树的和的限定, 这里数值优化的过程, 要求用一组向量的和来求解.</p>
<p>Numerical optimization approximates</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{f}=\operatorname{argmin}_{f} L(f)
</div>
<script type="math/tex; mode=display">
\hat{f}=\operatorname{argmin}_{f} L(f)
</script>
</div>
<p>as a sum of vectors</p>
<div class="arithmatex">
<div class="MathJax_Preview">
f_{M}=\sum_{m=0}^{M} h_{m}, h_{m} \in \mathbb{R}^{N}
</div>
<script type="math/tex; mode=display">
f_{M}=\sum_{m=0}^{M} h_{m}, h_{m} \in \mathbb{R}^{N}
</script>
</div>
<p>where <span class="arithmatex"><span class="MathJax_Preview">f_{0}=h_{0}</span><script type="math/tex">f_{0}=h_{0}</script></span> is an initial guess and each <span class="arithmatex"><span class="MathJax_Preview">f_{m}</span><script type="math/tex">f_{m}</script></span> is estimated from <span class="arithmatex"><span class="MathJax_Preview">f_{m-1}</span><script type="math/tex">f_{m-1}</script></span></p>
<h4 id="steepest-descent">Steepest Descent<a class="headerlink" href="#steepest-descent" title="Permanent link">&para;</a></h4>
<div class="arithmatex">
<div class="MathJax_Preview">
h_{m}=-\rho_{m} g_{m}
</div>
<script type="math/tex; mode=display">
h_{m}=-\rho_{m} g_{m}
</script>
</div>
<ul>
<li><span class="arithmatex"><span class="MathJax_Preview">\rho_{m}</span><script type="math/tex">\rho_{m}</script></span> is a scalar</li>
<li><span class="arithmatex"><span class="MathJax_Preview">g_{m} \in \mathbb{R}^{N}</span><script type="math/tex">g_{m} \in \mathbb{R}^{N}</script></span> is the gradient of <span class="arithmatex"><span class="MathJax_Preview">L(f)</span><script type="math/tex">L(f)</script></span> evaluated at <span class="arithmatex"><span class="MathJax_Preview">f=f_{m-1}</span><script type="math/tex">f=f_{m-1}</script></span></li>
</ul>
<p>Components of <span class="arithmatex"><span class="MathJax_Preview">g_{m}</span><script type="math/tex">g_{m}</script></span> are</p>
<div class="arithmatex">
<div class="MathJax_Preview">
g_{i m}=\left.\frac{\partial L\left(y_{i}, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right|_{f\left(x_{i}\right)=f_{i}, m-1}
</div>
<script type="math/tex; mode=display">
g_{i m}=\left.\frac{\partial L\left(y_{i}, f\left(x_{i}\right)\right)}{\partial f\left(x_{i}\right)}\right|_{f\left(x_{i}\right)=f_{i}, m-1}
</script>
</div>
<p>Step length is the solution to</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\rho_{m}=\operatorname{argmin}_{\rho} L\left(f_{m-1}-\rho g_{m}\right)
</div>
<script type="math/tex; mode=display">
\rho_{m}=\operatorname{argmin}_{\rho} L\left(f_{m-1}-\rho g_{m}\right)
</script>
</div>
<p>Solution is updated: <span class="arithmatex"><span class="MathJax_Preview">f_{m}=f_{m-1}-\rho_{m} g_{m}</span><script type="math/tex">f_{m}=f_{m-1}-\rho_{m} g_{m}</script></span></p>
<h4 id="gradient-tree-boosting">Gradient Tree Boosting<a class="headerlink" href="#gradient-tree-boosting" title="Permanent link">&para;</a></h4>
<p>$$
\hat{\theta}<em _theta__m="\theta_{m">{m}=\operatorname{argmin}</em>} \sum_{i=1}^{N} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+T\left(x_{i} ; \theta_{m}\right)\right)
$$
Comparison between Gradient Boosting \&amp; Forward Stagewise Tree boosting</p>
<ul>
<li>Tree predictions <span class="arithmatex"><span class="MathJax_Preview">T\left(x_{i} ; \theta_{m}\right)</span><script type="math/tex">T\left(x_{i} ; \theta_{m}\right)</script></span> are analogous to the negative gradients <span class="arithmatex"><span class="MathJax_Preview">-g_{1 m}, \ldots,-g_{N m}</span><script type="math/tex">-g_{1 m}, \ldots,-g_{N m}</script></span>, but <span class="arithmatex"><span class="MathJax_Preview">t_{m}=\left\{T\left(x_{1} ; \theta_{m}\right), \ldots, T\left(x_{N}, \theta_{m}\right)\right\}</span><script type="math/tex">t_{m}=\left\{T\left(x_{1} ; \theta_{m}\right), \ldots, T\left(x_{N}, \theta_{m}\right)\right\}</script></span> are constrained to be predictions of a <span class="arithmatex"><span class="MathJax_Preview">J_{m}</span><script type="math/tex">J_{m}</script></span> terminal node decision tree, whereas <span class="arithmatex"><span class="MathJax_Preview">-g_{m}</span><script type="math/tex">-g_{m}</script></span> is the unconstrained maximal descent direction</li>
<li>Also, <span class="arithmatex"><span class="MathJax_Preview">\rho_{m}=\operatorname{argmin}_{\rho} L\left(f_{m-1}, \rho g_{m}\right)</span><script type="math/tex">\rho_{m}=\operatorname{argmin}_{\rho} L\left(f_{m-1}, \rho g_{m}\right)</script></span> is analogous to <span class="arithmatex"><span class="MathJax_Preview">\hat{\gamma}_{j m}=\operatorname{argmin}_{\gamma_{j m}} \sum_{x_{i} \in R_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma_{j m}\right)</span><script type="math/tex">\hat{\gamma}_{j m}=\operatorname{argmin}_{\gamma_{j m}} \sum_{x_{i} \in R_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma_{j m}\right)</script></span>, but perform a line search for each terminal node</li>
</ul>
<p>梯度提升和梯度下降的区别与联系是什么？其实两者都是在迭代过程中，利用损失函数相对于模型的负梯度方向来对当前模型进行更新，只不过在梯度下降中，模型是以<strong>参数化表示</strong>，因此模型的更新等价于参数的更新。而在梯度提升中，模型并不需要进行参数化表示，而是直接定义在函数空间中。</p>
<p>If the only goal is to minimize</p>
<p>$$
\hat{f}=\operatorname{argmin}_{f} L(f)
$$
then perform steepest descent</p>
<ul>
<li>However, the ultimate goal is to generalize <span class="arithmatex"><span class="MathJax_Preview">f_{M}(x)</span><script type="math/tex">f_{M}(x)</script></span> to new unseen data</li>
<li>A possible solution is <strong>Gradient Tree Boosting</strong></li>
</ul>
<p>Fit a tree <span class="arithmatex"><span class="MathJax_Preview">T\left(x ; \theta_{m}\right)</span><script type="math/tex">T\left(x ; \theta_{m}\right)</script></span> at <span class="arithmatex"><span class="MathJax_Preview">m_{t h}</span><script type="math/tex">m_{t h}</script></span> iteration whose predictions <span class="arithmatex"><span class="MathJax_Preview">t_{m}</span><script type="math/tex">t_{m}</script></span> are as close as possible to the negative gradient</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\tilde{\theta}_{m}=\operatorname{argmin}_{\theta_{m}} \sum_{i=1}^{N}\left(-g_{i m}-T\left(x_{i} ; \theta_{m}\right)\right)^{2}
</div>
<script type="math/tex; mode=display">
\tilde{\theta}_{m}=\operatorname{argmin}_{\theta_{m}} \sum_{i=1}^{N}\left(-g_{i m}-T\left(x_{i} ; \theta_{m}\right)\right)^{2}
</script>
</div>
<p>For the solution region <span class="arithmatex"><span class="MathJax_Preview">\tilde{R}_{j m}</span><script type="math/tex">\tilde{R}_{j m}</script></span>, set</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\hat{\gamma}_{j m}=\operatorname{argmin}_{\gamma_{j m}} \sum_{x_{i} \in \tilde{R}_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma_{j m}\right)
</div>
<script type="math/tex; mode=display">
\hat{\gamma}_{j m}=\operatorname{argmin}_{\gamma_{j m}} \sum_{x_{i} \in \tilde{R}_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma_{j m}\right)
</script>
</div>
<p>The regions <span class="arithmatex"><span class="MathJax_Preview">\tilde{R}_{j m}</span><script type="math/tex">\tilde{R}_{j m}</script></span> would not be identical to <span class="arithmatex"><span class="MathJax_Preview">R_{j m}</span><script type="math/tex">R_{j m}</script></span> that solve the original optimization problem, but they should be similar enough.</p>
<h4 id="gradient-tree-boosting-algorithm">Gradient Tree Boosting Algorithm<a class="headerlink" href="#gradient-tree-boosting-algorithm" title="Permanent link">&para;</a></h4>
<ol>
<li>Initialize <span class="arithmatex"><span class="MathJax_Preview">f_{0}(x)=\operatorname{argmin}_{\gamma} \sum_{i=1}^{N} L\left(y_{i}, \gamma\right)</span><script type="math/tex">f_{0}(x)=\operatorname{argmin}_{\gamma} \sum_{i=1}^{N} L\left(y_{i}, \gamma\right)</script></span></li>
<li>For <span class="arithmatex"><span class="MathJax_Preview">m=1</span><script type="math/tex">m=1</script></span> to <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> :
   - For <span class="arithmatex"><span class="MathJax_Preview">i=1,2, \ldots, N</span><script type="math/tex">i=1,2, \ldots, N</script></span> compute <span class="arithmatex"><span class="MathJax_Preview">r_{i m}=-\left.\left[\frac{\partial\left(L\left(y_{i}, f\left(x_{i}\right)\right)\right)}{\partial f\left(x_{i}\right)}\right]\right|_{f=f_{m-1}}</span><script type="math/tex">r_{i m}=-\left.\left[\frac{\partial\left(L\left(y_{i}, f\left(x_{i}\right)\right)\right)}{\partial f\left(x_{i}\right)}\right]\right|_{f=f_{m-1}}</script></span>
   - Fit a regression tree to the targets <span class="arithmatex"><span class="MathJax_Preview">r_{i m}</span><script type="math/tex">r_{i m}</script></span> giving terminal regions <span class="arithmatex"><span class="MathJax_Preview">R_{j m}, j=1,2, \ldots, J_{m}</span><script type="math/tex">R_{j m}, j=1,2, \ldots, J_{m}</script></span>
   - For <span class="arithmatex"><span class="MathJax_Preview">j=1,2, \ldots, J_{m}</span><script type="math/tex">j=1,2, \ldots, J_{m}</script></span> compute <span class="arithmatex"><span class="MathJax_Preview">\gamma_{j m}=\operatorname{argmin}_{\gamma} \sum_{x_{i} \in R_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma\right)</span><script type="math/tex">\gamma_{j m}=\operatorname{argmin}_{\gamma} \sum_{x_{i} \in R_{j m}} L\left(y_{i}, f_{m-1}\left(x_{i}\right)+\gamma\right)</script></span>
   - Update <span class="arithmatex"><span class="MathJax_Preview">f_{m}(X)=f_{m-1}(x)+\sum_{j=1}^{J_{m}} \gamma_{j m} I\left(x \in R_{j m}\right)</span><script type="math/tex">f_{m}(X)=f_{m-1}(x)+\sum_{j=1}^{J_{m}} \gamma_{j m} I\left(x \in R_{j m}\right)</script></span></li>
<li>Output <span class="arithmatex"><span class="MathJax_Preview">\hat{f}(x)=f_{M}(x)</span><script type="math/tex">\hat{f}(x)=f_{M}(x)</script></span></li>
</ol>
<h3 id="boosting-tree-size">Boosting Tree Size<a class="headerlink" href="#boosting-tree-size" title="Permanent link">&para;</a></h3>
<p>Size of a Boosted Tree</p>
<p>Learning a large pruned tree at each round performs poorly</p>
<p>Better if</p>
<ul>
<li>Restrict all trees to be same size <span class="arithmatex"><span class="MathJax_Preview">J_{m}=J \forall m</span><script type="math/tex">J_{m}=J \forall m</script></span></li>
<li>Perform cross-validation to choose an optimal <span class="arithmatex"><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span></li>
</ul>
<p>可以考虑下式的 目标函数(target function) 的性质来得到 <span class="arithmatex"><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span> 的有用值</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\eta=\arg \min _{f} \mathrm{E}_{X Y} L(Y, f(X))
</div>
<script type="math/tex; mode=display">
\eta=\arg \min _{f} \mathrm{E}_{X Y} L(Y, f(X))
</script>
</div>
<p>这里期望值是对 <span class="arithmatex"><span class="MathJax_Preview">(X, Y)</span><script type="math/tex">(X, Y)</script></span> 的总体联合分布而言. 目标函数 <span class="arithmatex"><span class="MathJax_Preview">\eta(x)</span><script type="math/tex">\eta(x)</script></span> 是在末来数据上有最小预测风险的 函数. 这是我们试图近似的函数.</p>
<p><span class="arithmatex"><span class="MathJax_Preview">\eta(X)</span><script type="math/tex">\eta(X)</script></span> 一个相关的性质是坐标变量 <span class="arithmatex"><span class="MathJax_Preview">X^{T}=\left(X_{1}, X_{2}, \ldots, X_{p}\right)</span><script type="math/tex">X^{T}=\left(X_{1}, X_{2}, \ldots, X_{p}\right)</script></span> 间交叉项的阶. 这个可以通过它的 ANOVA 展开式得到</p>
<div class="arithmatex">
<div class="MathJax_Preview">
\eta(X)=\sum_{j} \eta_{j}\left(X_{j}\right)+\sum_{j k} \eta_{j k}\left(X_{j}, X_{k}\right)+\sum_{j k l} \eta_{j k l}\left(X_{j}, X_{k}, X_{l}\right)+\cdots
</div>
<script type="math/tex; mode=display">
\eta(X)=\sum_{j} \eta_{j}\left(X_{j}\right)+\sum_{j k} \eta_{j k}\left(X_{j}, X_{k}\right)+\sum_{j k l} \eta_{j k l}\left(X_{j}, X_{k}, X_{l}\right)+\cdots
</script>
</div>
<p>式子 (10.40) 中的第一项是只有一个预测变量 <span class="arithmatex"><span class="MathJax_Preview">X_{j}</span><script type="math/tex">X_{j}</script></span> 的函数和. 这些函数 <span class="arithmatex"><span class="MathJax_Preview">\eta_{j}\left(X_{j}\right)</span><script type="math/tex">\eta_{j}\left(X_{j}\right)</script></span> 是在所采用的误差损失准则下联合起来最能近似 <span class="arithmatex"><span class="MathJax_Preview">\eta(X)</span><script type="math/tex">\eta(X)</script></span> 的项. 每一个 <span class="arithmatex"><span class="MathJax_Preview">\eta_{j}\left(X_{j}\right)</span><script type="math/tex">\eta_{j}\left(X_{j}\right)</script></span> 称为 <span class="arithmatex"><span class="MathJax_Preview">X_{j}</span><script type="math/tex">X_{j}</script></span> 的主影响. 式中第二项是那些加入到主影响中将 <span class="arithmatex"><span class="MathJax_Preview">\eta(X)</span><script type="math/tex">\eta(X)</script></span> 拟合得最好的含两个变量的函数. 这些函数被称为每个变量对 <span class="arithmatex"><span class="MathJax_Preview">\left(X_{j}, X_{k}\right)</span><script type="math/tex">\left(X_{j}, X_{k}\right)</script></span> 的二阶交叉项. 式中第三项表示三阶交叉项，以此类推. 对于许多实际的问题, 低阶交叉影响占主要地位. 如果模型得到强烈的高阶交叉影响, 比如大型的决策树, 则可能正确性不好.</p>
<p>Interaction level of tree-based approximation is limited by <span class="arithmatex"><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span>:</p>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">J=2, f_{M}(x)</span><script type="math/tex">J=2, f_{M}(x)</script></span> can only be of the form</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{k} \eta_{k}\left(X_{k}\right)
</div>
<script type="math/tex; mode=display">
\sum_{k} \eta_{k}\left(X_{k}\right)
</script>
</div>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">J=3, f_{M}(x)</span><script type="math/tex">J=3, f_{M}(x)</script></span> can be of the form</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{k} \eta_{k}\left(X_{k}\right)+\sum_{j k} \eta_{j k}\left(X_{j}, X_{k}\right)
</div>
<script type="math/tex; mode=display">
\sum_{k} \eta_{k}\left(X_{k}\right)+\sum_{j k} \eta_{j k}\left(X_{j}, X_{k}\right)
</script>
</div>
<ul>
<li>If <span class="arithmatex"><span class="MathJax_Preview">J=4, f_{M}(x)</span><script type="math/tex">J=4, f_{M}(x)</script></span> can be of the form</li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
\sum_{k} \eta_{k}\left(X_{k}\right)+\sum_{j k} \eta_{j k}\left(X_{j}, X_{k}\right)+\sum_{j k l} \eta_{j k l}\left(X_{j}, X_{k}, X_{l}\right)
</div>
<script type="math/tex; mode=display">
\sum_{k} \eta_{k}\left(X_{k}\right)+\sum_{j k} \eta_{j k}\left(X_{j}, X_{k}\right)+\sum_{j k l} \eta_{j k l}\left(X_{j}, X_{k}, X_{l}\right)
</script>
</div>
<p>多大合适?</p>
<ul>
<li>For many practical problems, low-order interactions dominate</li>
<li>Therefore, models that produce strong higher-order interaction effects suffer in accuracy</li>
<li><span class="arithmatex"><span class="MathJax_Preview">4 \leqslant J \leqslant 8</span><script type="math/tex">4 \leqslant J \leqslant 8</script></span> works well in the context of boosting</li>
</ul>
<h3 id="regularization">Regularization<a class="headerlink" href="#regularization" title="Permanent link">&para;</a></h3>
<p>除了候选树的大小 <span class="arithmatex"><span class="MathJax_Preview">J</span><script type="math/tex">J</script></span>, 梯度 boosting 的另一个元参数 (meta-parameter) 是 boosting 的迭代次数 <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> - 每一次迭代通常会降低训练风险 <span class="arithmatex"><span class="MathJax_Preview">L\left(f_{M}\right)</span><script type="math/tex">L\left(f_{M}\right)</script></span>, 所以对于充分大的 <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span>, 误差可以达到任意小. 然而, 对训练数据拟合得太好会导致过拟合, 它会降低预测末来数据的效果. 因此, 存在一个最小化末 来预测风险的最优大小 <span class="arithmatex"><span class="MathJax_Preview">M^{*}</span><script type="math/tex">M^{*}</script></span>, 它依赖于具体应用. 估计 <span class="arithmatex"><span class="MathJax_Preview">M^{*}</span><script type="math/tex">M^{*}</script></span> 的一种方便方式是在验证样本上将预测 风险看成是关于 <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> 的函数. 最小化风险的 <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> 值则作为 <span class="arithmatex"><span class="MathJax_Preview">M^{*}</span><script type="math/tex">M^{*}</script></span> 的一个估计. 这类似于经常用在神经 网络中的 早停 (early stopping) 策略 (11.4 节).</p>
<ul>
<li>Control number of boosting rounds</li>
<li>Too large <span class="arithmatex"><span class="MathJax_Preview">M \rightarrow</span><script type="math/tex">M \rightarrow</script></span> danger of over-fitting</li>
<li>There is a <span class="arithmatex"><span class="MathJax_Preview">M^{*}</span><script type="math/tex">M^{*}</script></span> that minimizes future risk</li>
</ul>
<h4 id="shrinkage">Shrinkage<a class="headerlink" href="#shrinkage" title="Permanent link">&para;</a></h4>
<ul>
<li>Scale the contribution of each tree by a factor <span class="arithmatex"><span class="MathJax_Preview">0&lt;\nu&lt;1</span><script type="math/tex">0<\nu<1</script></span></li>
</ul>
<div class="arithmatex">
<div class="MathJax_Preview">
f_{m}(x)=f_{m-1}(x)+\nu \cdot \sum_{j=1}^{J} \gamma_{j m} l\left(x \in R_{j m}\right)
</div>
<script type="math/tex; mode=display">
f_{m}(x)=f_{m-1}(x)+\nu \cdot \sum_{j=1}^{J} \gamma_{j m} l\left(x \in R_{j m}\right)
</script>
</div>
<ul>
<li>Smaller <span class="arithmatex"><span class="MathJax_Preview">\nu \rightarrow</span><script type="math/tex">\nu \rightarrow</script></span> larger <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> to obtain low training error</li>
<li>Empirical finding: small <span class="arithmatex"><span class="MathJax_Preview">\nu&lt;0.1</span><script type="math/tex">\nu<0.1</script></span> and sufficiently large <span class="arithmatex"><span class="MathJax_Preview">M \rightarrow</span><script type="math/tex">M \rightarrow</script></span> better result than no shrinkage (especially for regression problems)</li>
</ul>
<p><img alt="" src="../media/ASL-note3/2021-12-30-14-32-13.png" /></p>
<h4 id="subsampling">Subsampling<a class="headerlink" href="#subsampling" title="Permanent link">&para;</a></h4>
<ul>
<li>Stochastic gradient boosting - each iteration sample a fraction <span class="arithmatex"><span class="MathJax_Preview">\eta</span><script type="math/tex">\eta</script></span> of the training observations (without replacement)</li>
<li>A typical value <span class="arithmatex"><span class="MathJax_Preview">\eta=0.5</span><script type="math/tex">\eta=0.5</script></span></li>
<li>Empirically subsampling without shrinkage works poorly 不结合 shrink 似乎比不进行采样要差?</li>
<li>Subsampling with shrinkage works well</li>
<li>Now we have 4 parameters to estimate <span class="arithmatex"><span class="MathJax_Preview">J, M, \nu, \eta</span><script type="math/tex">J, M, \nu, \eta</script></span>. 一般地，通过前期的尝试确定合适的 <span class="arithmatex"><span class="MathJax_Preview">J, \nu, \eta</span><script type="math/tex">J, \nu, \eta</script></span>, 将 <span class="arithmatex"><span class="MathJax_Preview">M</span><script type="math/tex">M</script></span> 留作主要的参数.</li>
</ul>
<p><img alt="" src="../media/ASL-note3/2021-12-30-14-35-26.png" /></p>
<p><a href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_gradient_boosting_regularization.html">sklearn</a> 中有份代码画的图类似.</p>

              
            </article>
          </div>
        </div>
        
          <a href="#" class="md-top md-icon" data-md-component="top" data-md-state="hidden">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg>
            Back to top
          </a>
        
      </main>
      
        <footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        
        <a href="../ASL-note2/" class="md-footer__link md-footer__link--prev" aria-label="Previous: ASL2" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              ASL2
            </div>
          </div>
        </a>
      
      
        
        <a href="../ASL-note4/" class="md-footer__link md-footer__link--next" aria-label="Next: ASL4" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              ASL4
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
    <div class="md-copyright__highlight">
      Copyright &copy; 2021-2022 Easonshi
    </div>
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
        <div class="md-social">
  
    
    
      
      
    
    <a href="https://github.com/Lightblues" target="_blank" rel="noopener" title="github.com" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
    </a>
  
    
    
      
      
    
    <a href="https://lightblues.github.io/" target="_blank" rel="noopener" title="lightblues.github.io" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 576 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M575.8 255.5c0 18-15 32.1-32 32.1h-32l.7 160.1c0 2.8-.2 5.4-.5 8.1V472c0 22.1-17.9 40-40 40h-16c-1.1 0-2.2-.9-3.3-.1-1.4-.8-2.8.1-4.2.1H392c-22.1 0-40-17.9-40-40v-88c0-17.7-14.3-32-32-32h-64c-17.7 0-32 14.3-32 32v88c0 22.1-17.9 40-40 40h-55.9c-1.5 0-3-.1-4.5-.2-1.2.1-2.4.2-3.6.2h-16c-22.09 0-40-17.9-40-40V360c0-.9.03-1.9.09-2.8v-69.6H32.05C14.02 287.6 0 273.5 0 255.5c0-9 3.004-17 10.01-24L266.4 8.016c7-7.014 15-8.016 22-8.016s15 2.004 21.1 7.014L564.8 231.5c8 7 12.1 15 11 24z"/></svg>
    </a>
  
    
    
    <a href="mailto:oldcitystal@gmail.com" target="_blank" rel="noopener" title="" class="md-social__link">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><!--! Font Awesome Free 6.1.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2022 Fonticons, Inc.--><path d="M0 128c0-35.35 28.65-64 64-64h384c35.3 0 64 28.65 64 64v256c0 35.3-28.7 64-64 64H64c-35.35 0-64-28.7-64-64V128zm48 0v22.1l172.5 141.6c20.6 17 50.4 17 71 0L464 150.1v-23c0-7.9-7.2-16-16-16H64c-8.84 0-16 8.1-16 16v.9zm0 84.2V384c0 8.8 7.16 16 16 16h384c8.8 0 16-7.2 16-16V212.2L322 328.8c-38.4 31.5-93.6 31.5-132.9 0L48 212.2z"/></svg>
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["navigation.tabs", "navigation.sections", "navigation.top"], "search": "../../assets/javascripts/workers/search.2a1c317c.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.6e54b5cd.min.js"></script>
      
        <script src="../../js/baidu-tongji.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML"></script>
      
    
  </body>
</html>